{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1713c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b942b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep L ea r ni n g\n",
      " I a n Go o d f e l l o w\n",
      " Y o s h u a B e n g i o\n",
      " A a r o n C o u r v i l l eC on t e n t s\n",
      " Website viii\n",
      " Acknowledgments ix\n",
      " Notation xiii\n",
      "  1Introduction 1\n",
      "         1.1WhoShouldReadThisBook?. . . . . .. . . . . . . . .. . . . .8\n",
      "        1.2HistoricalTrendsinDeepLearning. . . . . . . . .. . . . . . . .12\n",
      "       IAppliedMathandMachineLearningBasics 27\n",
      "   2LinearAlgebra 29\n",
      "        2.1Scalars,Vectors,MatricesandTensors. . . . . . . . .. . . . . .29\n",
      "        2.2MultiplyingMatricesandVectors. . . . . . . . .. . . . . . . ..32\n",
      "        2.3IdentityandInverseMatrices. . . . . . . . .. . . . . . . .. . .34\n",
      "        2.4LinearDependenceandSpan. . . . . . . . .. . . . . . . .. . .35\n",
      "      2.5Norms. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .37\n",
      "        2.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . .38\n",
      "     2.7Eigendecomposition. . . . . . . . . .. . . . . . . .. . . . . . . .40\n",
      "       2.8SingularValueDecomposition. . . . . . . .. . . . . . . .. . . .42\n",
      "       2.9TheMoore-PenrosePseudoinverse. . . . . . . . .. . . . . . . ..43\n",
      "       2.10TheTraceOperator. . . . . . . . .. . . . . . . .. . . . . . . .44\n",
      "       2.11TheDeterminant. .. . . . . . . .. . . . . . . .. . . . . . . . .45\n",
      "       2.12Example:PrincipalComponentsAnalysis. . . . . . . . .. . . .45\n",
      "     3ProbabilityandInformationTheory 51\n",
      "       3.1WhyProbability?. . . . .. . . . . . . . .. . . . . . . .. . . . .52\n",
      "iC O NT E NT S\n",
      "       3.2RandomVariables. . . . .. . . . . . . .. . . . . . . . .. . . .54\n",
      "      3.3ProbabilityDistributions. . . . . . . . .. . . . . . . .. . . . . .54\n",
      "      3.4MarginalProbability. . . . . . . . .. . . . . . . . .. . . . . . .56\n",
      "       3.5ConditionalProbability. .. . . . . . . .. . . . . . . .. . . . .57\n",
      "         3.6TheChainRuleofConditionalProbabilities. . . . . . . . .. . .57\n",
      "       3.7IndependenceandConditionalIndependence. . . . . . . . .. . .58\n",
      "       3.8Expectation,VarianceandCovariance. . . . . . . . . .. . . . .58\n",
      "      3.9CommonProbabilityDistributions. . . . . . . . . . . . . . .. .60\n",
      "         3.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . .. .65\n",
      "       3.11Bayes’Rule. . . . . . . . . .. . . . . . . .. . . . . . . .. . . .68\n",
      "        3.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . .69\n",
      "      3.13InformationTheory. . . . . . . . . .. . . . . . . .. . . . . . . .71\n",
      "       3.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .73\n",
      "   4NumericalComputation 78\n",
      "       4.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . . .78\n",
      "      4.2PoorConditioning. . . . . . . . .. . . . . . . .. . . . . . . . .80\n",
      "      4.3Gradient-BasedOptimization. . . . . . .. . . . . . . .. . . . .80\n",
      "      4.4ConstrainedOptimization. . . . . . . . . . . . .. . . . . . . ..91\n",
      "        4.5Example:LinearLeastSquares. . . . . . .. . . . . . . . .. . .94\n",
      "    5MachineLearningBasics 96\n",
      "      5.1LearningAlgorithms. . . . . . . . . . .. . . . . . . .. . . . . .97\n",
      "        5.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . .. . . . .108\n",
      "        5.3HyperparametersandValidationSets.. . . . . . . .. . . . . . .118\n",
      "        5.4Estimators,BiasandVariance. . . . . .. . . . . . . .. . . . . .120\n",
      "       5.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . .. . .129\n",
      "      5.6BayesianStatistics. . . . . . . . . . .. . . . . . . .. . . . . . .133\n",
      "       5.7SupervisedLearningAlgorithms. . .. . . . . . . .. . . . . . . .137\n",
      "      5.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . .. .142\n",
      "       5.9StochasticGradientDescent. . . .. . . . . . . . .. . . . . . . .149\n",
      "        5.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . ..151\n",
      "        5.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . .. .152\n",
      "     IIDeepNetworks:ModernPractices 162\n",
      "    6DeepFeedforwardNetworks 164\n",
      "        6.1Example:LearningXOR.. . . . . . . . .. . . . . . . .. . . . .167\n",
      "       6.2Gradient-BasedLearning.. . . . . . . .. . . . . . . .. . . . . .172\n",
      "iiC O NT E NT S\n",
      "       6.3HiddenUnits. . . . . .. . . . . . . .. . . . . . . . .. . . . . .187\n",
      "       6.4ArchitectureDesign. . . . . . . . .. . . . . . . .. . . . . . . ..193\n",
      "    6.5Back-PropagationandOtherDiﬀerentiation\n",
      "      Algorithms. . . .. . . . . . . . .. . . . . . . .. . . . . . . . ..200\n",
      "       6.6HistoricalNotes. . . . . . .. . . . . . . .. . . . . . . . .. . . .220\n",
      "     7RegularizationforDeepLearning 224\n",
      "       7.1ParameterNormPenalties. . . . .. . . . . . . . .. . . . . . . .226\n",
      "        7.2NormPenaltiesasConstrainedOptimization. . . . . . . .. . . .233\n",
      "       7.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .235\n",
      "      7.4DatasetAugmentation. . . . . . . . . .. . . . . . . . .. . . . .236\n",
      "       7.5NoiseRobustness. . . . . . . . .. . . . . . . .. . . . . . . .. .238\n",
      "     7.6Semi-SupervisedLearning. . . . . . . . . . . . . . . .. . . . . .240\n",
      "      7.7MultitaskLearning. . . . . . . . . . . .. . . . . . . .. . . . . .241\n",
      "       7.8EarlyStopping. . . . . . . . .. . . . . . . .. . . . . . . .. . .241\n",
      "      7.9ParameterTyingandParameterSharing . . . . . . . . . . . . . .249\n",
      "      7.10SparseRepresentations. . . . . . . . .. . . . . . . .. . . . . . .251\n",
      "         7.11BaggingandOtherEnsembleMethods.. . . . . . . . .. . . . .253\n",
      "      7.12Dropout. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .255\n",
      "      7.13AdversarialTraining. . . . . . . .. . . . . . . . .. . . . . . . .265\n",
      "      7.14TangentDistance,TangentPropandManifold\n",
      "     TangentClassiﬁer. . . . . . . . . . . .. . . . . . . . .. . . . . .267\n",
      "      8OptimizationforTrainingDeepModels 271\n",
      "        8.1HowLearningDiﬀersfromPureOptimization. . . . . . . . . . .272\n",
      "        8.2ChallengesinNeuralNetworkOptimization. . . . .. . . . . . .279\n",
      "      8.3BasicAlgorithms. . . . . . . . . .. . . . . . . .. . . . . . . . .290\n",
      "       8.4ParameterInitializationStrategies.. . . . . . . . .. . . . . . .296\n",
      "        8.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .302\n",
      "       8.6ApproximateSecond-OrderMethods.. . . . . . . .. . . . . . .307\n",
      "       8.7OptimizationStrategiesandMeta-Algorithms. . . . .. . . . . .313\n",
      "   9ConvolutionalNetworks 326\n",
      "      9.1TheConvolutionOperation. . . . . . . . . . . . . . . .. . . . .327\n",
      "       9.2Motivation. .. . . . . . . .. . . . . . . . .. . . . . . . .. . . .329\n",
      "      9.3Pooling. . . . . . . . . . . . .. . . . . . . .. . . . . . . . .. . .335\n",
      "           9.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .339\n",
      "        9.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . .342\n",
      "       9.6StructuredOutputs.. . . . . . . .. . . . . . . . .. . . . . . . .352\n",
      "       9.7DataTypes. . . . . .. . . . . . . .. . . . . . . .. . . . . . . .354\n",
      "iiiC O NT E NT S\n",
      "       9.8EﬃcientConvolutionAlgorithms. . . . . . . .. . . . . . . .. .356\n",
      "        9.9RandomorUnsupervisedFeatures. . . . . . . .. . . . . . . ..356\n",
      "     9.10TheNeuroscientiﬁcBasisforConvolutional\n",
      "     Networks. . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .358\n",
      "          9.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .365\n",
      "      10 SequenceModeling:RecurrentandRecursiveNets 367\n",
      "      10.1UnfoldingComputationalGraphs. . . . . . . . . . . . .. . . . .369\n",
      "        10.2RecurrentNeuralNetworks. . .. . . . . . . . .. . . . . . . ..372\n",
      "     10.3BidirectionalRNNs . . . . . . . . . . . . . .. . . . . . . . .. . .388\n",
      "  10.4Encoder-DecoderSequence-to-Sequence\n",
      "      Architectures. . .. . . . . . . .. . . . . . . . .. . . . . . . ..390\n",
      "       10.5DeepRecurrentNetworks. . . . . . . .. . . . . . . . .. . . . .392\n",
      "       10.6RecursiveNeuralNetworks. . . . .. . . . . . . . .. . . . . . . .394\n",
      "        10.7TheChallengeofLong-TermDependencies. . . . . . . . . .. . .396\n",
      "       10.8EchoStateNetworks. . . . . . . . . .. . . . . . . . .. . . . . .399\n",
      "       10.9LeakyUnitsandOtherStrategiesforMultiple\n",
      "      TimeScales. . . . .. . . . . . . .. . . . . . . . .. . . . . . . .402\n",
      "          10.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .404\n",
      "      10.11 OptimizationforLong-TermDependencies. . . . . . . .. . . . .408\n",
      "     10.12 ExplicitMemory. . . . . . . . . .. . . . . . . . .. . . . . . . .412\n",
      "  11 PracticalMethodology 416\n",
      "      11.1PerformanceMetrics. . . . . . . . . .. . . . . . . .. . . . . . .417\n",
      "       11.2DefaultBaselineModels. . . . . . . .. . . . . . . .. . . . . . .420\n",
      "         11.3DeterminingWhethertoGatherMoreData. . . . . . . . . .. .421\n",
      "      11.4SelectingHyperparameters. . . . . . . . .. . . . . . . .. . . . .422\n",
      "       11.5DebuggingStrategies. . . . .. . . . . . . .. . . . . . . . .. . .431\n",
      "       11.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . .435\n",
      " 12 Applications 438\n",
      "        12.1Large-ScaleDeepLearning.. . . . . . . .. . . . . . . .. . . . .438\n",
      "       12.2ComputerVision. . . . . . . . .. . . . . . . .. . . . . . . .. .447\n",
      "      12.3SpeechRecognition . . . . . .. . . . . . . .. . . . . . . . .. . .453\n",
      "        12.4NaturalLanguageProcessing. . .. . . . . . . .. . . . . . . ..456\n",
      "       12.5OtherApplications. . . . . . . . .. . . . . . . .. . . . . . . ..473\n",
      "ivC O NT E NT S\n",
      "    IIIDeepLearningResearch 482\n",
      "   13 LinearFactorModels 485\n",
      "        13.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . .486\n",
      "       13.2IndependentComponentAnalysis(ICA). . . . . . . . . . . .. .487\n",
      "        13.3SlowFeatureAnalysis. . . . . .. . . . . . . . .. . . . . . . ..489\n",
      "       13.4SparseCoding. . . . . .. . . . . . . .. . . . . . . . .. . . . . .492\n",
      "       13.5ManifoldInterpretationofPCA. . . . . . . . . . . . . . . . .. .496\n",
      " 14 Autoencoders 499\n",
      "      14.1UndercompleteAutoencoders. . . . . . . . . .. . . . . . . .. .500\n",
      "      14.2RegularizedAutoencoders. . . . . .. . . . . . . .. . . . . . . .501\n",
      "         14.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .505\n",
      "       14.4StochasticEncodersandDecoders. . . . . . . . . . .. . . . . . .506\n",
      "       14.5DenoisingAutoencoders. .. . . . . . . .. . . . . . . . .. . . .507\n",
      "       14.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . .513\n",
      "      14.7ContractiveAutoencoders. . . . . . . . . . . . .. . . . . . . ..518\n",
      "       14.8PredictiveSparseDecomposition. . . . . . . .. . . . . . . . ..521\n",
      "       14.9ApplicationsofAutoencoders. . . . .. . . . . . . . .. . . . . .522\n",
      "  15 RepresentationLearning 524\n",
      "       15.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .526\n",
      "         15.2TransferLearningandDomainAdaptation. . . .. . . . . . . ..534\n",
      "        15.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .539\n",
      "      15.4DistributedRepresentation. . . . . . . . . . . .. . . . . . . . ..544\n",
      "        15.5ExponentialGainsfromDepth. . . . . . . . . .. . . . . . . ..550\n",
      "         15.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .552\n",
      "      16 StructuredProbabilisticModelsforDeepLearning 555\n",
      "         16.1TheChallengeofUnstructuredModeling.. . . . . . . .. . . . .556\n",
      "          16.2UsingGraphstoDescribeModelStructure. .. . . . . . . .. . .560\n",
      "        16.3SamplingfromGraphicalModels. . .. . . . . . . .. . . . . . .577\n",
      "       16.4AdvantagesofStructuredModeling .. . . . . . . . .. . . . . . .579\n",
      "       16.5LearningaboutDependencies. . . .. . . . . . . .. . . . . . . .579\n",
      "       16.6InferenceandApproximateInference. . . . . . . . .. . . . . . .580\n",
      "      16.7TheDeepLearningApproachtoStructured\n",
      "      ProbabilisticModels. .. . . . . . . .. . . . . . . .. . . . . . .581\n",
      "   17 MonteCarloMethods 587\n",
      "        17.1SamplingandMonteCarloMethods. . . . . . . .. . . . . . . .587\n",
      "vC O NT E NT S\n",
      "      17.2ImportanceSampling. . . . . . . . . . . .. . . . . . . .. . . . .589\n",
      "         17.3MarkovChainMonteCarloMethods. . . . .. . . . . . . .. . .592\n",
      "      17.4GibbsSampling . . . . . . .. . . . . . . . .. . . . . . . .. . . .596\n",
      "      17.5TheChallengeofMixingbetweenSeparated\n",
      "     Modes. . . . . . . . . .. . . . . . . .. . . . . . . . .. . . . . .597\n",
      "    18 ConfrontingthePartitionFunction 603\n",
      "        18.1TheLog-LikelihoodGradient.. . . . . . . .. . . . . . . . .. .604\n",
      "        18.2StochasticMaximumLikelihoodandContrastiveDivergence. . .605\n",
      "     18.3Pseudolikelihood. . . . . . . . . . .. . . . . . . . .. . . . . . .613\n",
      "        18.4ScoreMatchingandRatioMatching. . . . . . . .. . . . . . . .615\n",
      "       18.5DenoisingScoreMatching. . . . . . . . .. . . . . . . .. . . . .617\n",
      "      18.6Noise-ContrastiveEstimation. . . . .. . . . . . . .. . . . . . .618\n",
      "       18.7EstimatingthePartitionFunction. . . . . . . . . . .. . . . . . .621\n",
      "  19 ApproximateInference 629\n",
      "        19.1InferenceasOptimization.. . . . . . . . .. . . . . . . .. . . .631\n",
      "       19.2ExpectationMaximization. .. . . . . . . .. . . . . . . . .. . .632\n",
      "         19.3MAPInferenceandSparseCoding.. . . . . . . . .. . . . . . .633\n",
      "       19.4VariationalInferenceandLearning. . . . . . . . . . . . . . .. .636\n",
      "       19.5LearnedApproximateInference. . .. . . . . . . . .. . . . . . .648\n",
      "   20 DeepGenerativeModels 651\n",
      "      20.1BoltzmannMachines. . . . . . . . . . .. . . . . . . . .. . . . .651\n",
      "       20.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . .. . .653\n",
      "        20.3DeepBeliefNetworks.. . . . . . . .. . . . . . . .. . . . . . . .657\n",
      "       20.4DeepBoltzmannMachines. . . . . . . . . .. . . . . . . .. . . .660\n",
      "        20.5BoltzmannMachinesforReal-ValuedData. . . . . . . . .. . . .673\n",
      "     20.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . ..679\n",
      "         20.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .681\n",
      "       20.8OtherBoltzmannMachines. . . . .. . . . . . . .. . . . . . . .683\n",
      "       20.9Back-PropagationthroughRandomOperations. . . . . .. . . .684\n",
      "      20.10 DirectedGenerativeNets. . . . . . . . . . . .. . . . . . . . .. .688\n",
      "       20.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . .. .707\n",
      "      20.12 GenerativeStochasticNetworks. . .. . . . . . . .. . . . . . . .710\n",
      "     20.13 OtherGenerationSchemes. . . . . . . . . . . . .. . . . . . . . .712\n",
      "    20.14 EvaluatingGenerativeModels . . . . . . . . . . . . .. . . . . . .713\n",
      "     20.15 Conclusion. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . .716\n",
      " Bibliography 717\n",
      "v iC O NT E NT S\n",
      " Index 773\n",
      "v iiW e bsi t e\n",
      "www.deeplearningbook.org\n",
      "           Thisbookisaccompaniedbytheabovewebsite.Thewebsiteprovidesa\n",
      "         varietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\n",
      "            mistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\n",
      "viiiAcknowledgments\n",
      "            Thisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\n",
      "             Wewouldliketothankthosewhocommentedonourproposalforthebook\n",
      "          andhelpedplanitscontentsandorganization:GuillaumeAlain,KyunghyunCho,\n",
      "         ÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\n",
      "Rohée.\n",
      "              Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthe\n",
      "           bookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,IshaqAden-Ali,\n",
      "         GuillaumeAlain,IonAndroutsopoulos,LauraBall,FredBertsch,OlexaBilaniuk,\n",
      "          UfukCanBiçici,MatkoBošnjak,JohnBoersma,FrançoisBrault,GregBrockman,\n",
      "         AlexandredeBrébisson,PierreLucCarrier,SarathChandar,PawelChilinski,\n",
      "     MarkDaoust, OlegDashevskii, LaurentDinh,StephanDreseitl, Gudmundur\n",
      "          Einarsson,HannesvonEssen,JimFan,MiaoFan,MeireFortunato,Frédéric\n",
      "          Francis,NandodeFreitas,ÇağlarGülçehre,JurgenVanGael,YaroslavGanin,\n",
      "          JavierAlonsoGarcía,AydinGerek,StefanHeil,JonathanHunt,GopiJeyaram,\n",
      "         ChingizKabytayev,LukaszKaiser,VarunKanade,AsifullahKhan,AkielKhan,\n",
      "           JohnKing,DiederikP.Kingma,DominikLaupheimer,YannLeCun,MinhLê,Max\n",
      "        Marion,RudolfMathey,MatíasMattamala,AbhinavMaurya,VincentMichalski,\n",
      "           KevinMurphy,OlegMürk,HungNgo,RomanNovak,AugustusQ.Odena,Simon\n",
      "          Pavlik,KarlPichotta,EddiePierce,KariPulli,RousselRahman,TapaniRaiko,\n",
      "          AnuragRanjan,JohannesRoith,MihaelaRosca,HalisSak,CésarSalgado,Grigory\n",
      "          Sapunov,YoshinoriSasaki,MikeSchuster,JulianSerban,NirShabat,KenShirriﬀ,\n",
      "          AndreSimpelo,ScottStanley,DavidSussillo,IlyaSutskever,CarlesGeladaSáez,\n",
      "        GrahamTaylor,ValentinTolmer,MassimilianoTomassoli,AnTran,Shubhendu\n",
      "        Trivedi,AlexeyUmnov,VincentVanhoucke,RobertViragh,MarcoVisentini-\n",
      "        Scarzanella,MartinVita,DavidWarde-Farley,DustinWebb,Shan-ConradWolf,\n",
      "            KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.\n",
      "             Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\n",
      " individualchapters:\n",
      "ixC O NT E NT S\n",
      "   •Notation:ZhangYuanhang.\n",
      "•         Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\n",
      "       CharlieGorichanaz,BennedHedegaard,BrendanLoudermilk,PetrosMani-\n",
      "        atis,EricMorris,CosminPârvulescu,MurielRambeloarison,AlfredoSolano\n",
      "  andTimothyWhelan.\n",
      "•          Chapter, :AmjadAlmahairi,NikolaBanić,KevinBennett, 2LinearAlgebra\n",
      "       PhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\n",
      "        SergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,Gitanjali\n",
      "        GulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\n",
      "•          Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\n",
      "        Arulkumaran,Ana-MariaCretu,VincentDumoulin,RuiFa,StephanGouws,\n",
      "         ArtemOboturov,PatrickPan,AnttiRasmus,AlexeySurkovandVolker\n",
      "Tresp.\n",
      "•          Chapter, :TranLamAn,IanFischer,William 4NumericalComputation\n",
      "     Gandler,MahendraKariyaandHuYuhuang.\n",
      "•         Chapter, :DzmitryBahdanau,MarkCramer, 5MachineLearningBasics\n",
      "         EricDolores,JustinDomingue,RonFedkiw,NikhilGarg,Guillaumede\n",
      "         Laboulaye,JonMcKay,MakotoOtsuka,BobPepin,PhilipPopien,Klaus\n",
      "         Radke,EmmanuelRayner,EricSabo,ImranSaleh,PeterShepard,Kee-Bong\n",
      "       Song,ZhengSun,AlexandreTorresandAndyWu.\n",
      "•         Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\n",
      "          ElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKrueger\n",
      "   andAdityaKumarPraharaj.\n",
      "•         Chapter, :BrianBartoldson,Morten 7RegularizationforDeepLearning\n",
      "         Kolbæk,KshitijLauria,InkyuLee,SunilMohan, HaiPhongPhanand\n",
      " JoshuaSalisbury.\n",
      "•         Chapter,8OptimizationforTrainingDeepModels:MarcelAckermann,\n",
      "         TusharAgarwal,PeterArmitage,RowelAtienza,AndrewBrock,MaxHayden\n",
      "    Chiz, GregoryGalperin, AaronGolden, RussellHowes, HillMa, Tegan\n",
      "    Maharaj, JamesMartens,KashifRasul,Thomas Stanley, Klaus Strobl,\n",
      "    NicholasTurnerandDavidZhang.\n",
      "•         Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Jane\n",
      "         Bromley,KonstantinDivilov,EricJensen,MehdiMirza,AlexPaino,Guil-\n",
      "        laumeRochette,MarjorieSayer,RyanStoutandWentaoWu.\n",
      "xC O NT E NT S\n",
      "•         Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen\n",
      "   Eraslan, Nasos Evangelou,StevenHickson, ChristophKamann, Martin\n",
      "         Krasser,RazvanPascanu,DiogoPernes,RyanPilgrim,LorenzovonRitter,\n",
      "          RuiRodrigues,DmitriySerdyuk,DongyuShi,KaiyuYangandRuiqingYin.\n",
      "         •Chapter, :DanielBecksteinandKenjiKaneda. 11PracticalMethodology\n",
      "•         Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\n",
      "Roscher.\n",
      "       •Chapter,13LinearFactorModels:JayanthKoushik.\n",
      "     •Chapter, :HassanMasum. 14Autoencoders\n",
      "•          Chapter, :MateoTorres-Ruiz,KunalGhoshand 15RepresentationLearning\n",
      " RodneyMelchers.\n",
      "•          Chapter, :DengQingyu 16StructuredProbabilisticModelsforDeepLearning\n",
      "          ,HarryBraviner,TimothyCogan,DiegoMarez,AntonVarfolomandVictor\n",
      "Xie.\n",
      "•           Chapter,18ConfrontingthePartitionFunction:SamBowmanandJinKim.\n",
      "      •Chapter, :YujiaBao. 19ApproximateInference\n",
      "•         Chapter, :NicolasChapados,DanielGalvez, 20DeepGenerativeModels\n",
      "        WenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.\n",
      "         •Bibliography:LukasMichelbacher,LeslieN.SmithandMaxXie.\n",
      "             Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor\n",
      "           datafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions\n",
      "  throughoutthetext.\n",
      "            WewouldliketothankLuWangforwritingpdf2htmlEX,whichweused\n",
      "              tomakethewebversionofthebook,andforoﬀeringsupporttoimprovethe\n",
      "           qualityoftheresultingHTML.WealsothankSimonLefrançoisforincorporating\n",
      "             MITPress’seditstoourmanuscriptbackintothewebedition,andforhelping\n",
      "     incorporatereaderfeedbackfromtheweb.\n",
      "       We would liketothank Ian’swifeDaniela FloriGoodfellowforpatiently\n",
      "              supportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\n",
      "            WewouldliketothanktheGoogleBrainteamforprovidinganintellectual\n",
      "            environmentwhereIancoulddevoteatremendousamountoftimetowritingthis\n",
      "x iC O NT E NT S\n",
      "           bookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\n",
      "           tothankIan’sformermanager,GregCorrado,andhiscurrentmanager,Samy\n",
      "             Bengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀrey\n",
      "      Hintonforencouragement whenwritingwasdiﬃcult.\n",
      "x iiN ot at i o n\n",
      "          Thissectionprovidesaconcisereferencedescribingthenotationusedthroughout\n",
      "           thisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\n",
      "         concepts,wedescribemostoftheseideasinchapters2–4.\n",
      "  NumbersandArrays\n",
      "     aAscalar(integerorreal)\n",
      "  aAvector\n",
      "  AAmatrix\n",
      "  AAtensor\n",
      "I n        Identitymatrixwithrowsandcolumns n n\n",
      "I      Identitymatrixwithdimensionalityimpliedby\n",
      "context\n",
      "e( ) i   Standardbasisvector[0    ,...,0,1,0    ,...,  0]witha\n",
      "   1atpositioni\n",
      "diag()a      Asquare,diagonalmatrixwithdiagonalentries\n",
      "  givenbya\n",
      "    aAscalarrandomvariable\n",
      "    aAvector-valuedrandomvariable\n",
      "    AAmatrix-valuedrandomvariable\n",
      "xiiiC O NT E NT S\n",
      "  SetsandGraphs\n",
      "  A Aset\n",
      "     R Thesetofrealnumbers\n",
      "       {}01, Thesetcontaining0and1\n",
      "               {} 01,,...,nThesetofallintegersbetweenand0n\n",
      "        []a,bTherealintervalincludingandab\n",
      "         (]a,bTherealintervalexcludingbutincluding a b\n",
      "A B\\       Setsubtraction,i.e., thesetcontainingtheele-\n",
      "       mentsofthatarenotin A B\n",
      "  G Agraph\n",
      " Pa G(x i    )Theparentsofx i inG\n",
      "Indexing\n",
      "a iElementi ofvectora     ,withindexingstartingat1\n",
      "a − i        Allelementsofvectorexceptforelement a i\n",
      "A i , j      Elementofmatrix i,jA\n",
      "A i , :    RowofmatrixiA\n",
      "A : , i     ColumnofmatrixiA\n",
      "A i , j , k         Element ofa3-Dtensor ( )i,j,k A\n",
      "A : : , , i     2-Dsliceofa3-Dtensor\n",
      "a i       Elementoftherandomvector i a\n",
      "  LinearAlgebraOperations\n",
      "A   TransposeofmatrixA\n",
      "A+   Moore-PenrosepseudoinverseofA\n",
      "         ABElement-wise(Hadamard)productofandAB\n",
      "   det()ADeterminantofA\n",
      "x ivC O NT E NT S\n",
      "Calculusdy\n",
      "dx      Derivativeofwithrespectto y x\n",
      "∂y\n",
      "∂x       Partialderivativeofwithrespectto y x\n",
      "∇ x       y Gradientofwithrespectto y x\n",
      "∇ X        y Matrixderivativesofwithrespectto y X\n",
      "∇ Xy    Tensorcontainingderivativesofy  withrespectto\n",
      "X\n",
      "∂f\n",
      "∂x    JacobianmatrixJ∈ Rm n ×  off: Rn → Rm\n",
      "∇2\n",
      "x            f f f () (xorH)()xTheHessianmatrixofatinputpointx\n",
      "        fd()xx Deﬁniteintegralovertheentiredomainofx\n",
      "\n",
      "S          fd()xx x Deﬁniteintegralwithrespecttoovertheset S\n",
      "   ProbabilityandInformationTheory\n",
      "        ab Therandomvariablesaandbareindependent ⊥\n",
      "        abc Theyareconditionallyindependentgivenc ⊥|\n",
      " P()a       Aprobabilitydistributionoveradiscretevariable\n",
      "p()a       Aprobabilitydistributionoveracontinuousvari-\n",
      "         able,oroveravariablewhosetypehasnotbeen\n",
      "speciﬁed\n",
      "        a Randomvariableahasdistribution ∼P P\n",
      "E x ∼ P            [()] () () () fxor EfxExpectationoffxwithrespecttoPx\n",
      "       Var(())fx Varianceofunderx fx()P()\n",
      "          Cov(()())fx,gxCovarianceofandunderx fx()gx()P()\n",
      "       H()x Shannonentropyoftherandomvariablex\n",
      "D K L       ( )PQ Kullback-LeiblerdivergenceofPandQ\n",
      "  N(; )xµ, Σ   Gaussiandistributionoverx withmeanµand\n",
      " covariance Σ\n",
      "x vC O NT E NT S\n",
      "Functions\n",
      "            f f : A B→Thefunctionwithdomainandrange A B\n",
      "         fg fg ◦Compositionofthefunctionsand\n",
      " f(;)xθ  Afunctionofx  parametrizedbyθ. (Sometimes\n",
      " wewritef(x    )andomittheargumentθ tolighten\n",
      "notation)\n",
      "     logx x Naturallogarithmof\n",
      "  σx()Logisticsigmoid,1\n",
      "  1+exp()−x\n",
      "    ζx x () log(1+exp( Softplus, ))\n",
      "||||x pLp  normofx\n",
      " ||||xL2  normofx\n",
      "x+      Positivepartof,i.e.,x max(0),x\n",
      "1 c o n d i ti o n        is1iftheconditionistrue,0otherwise\n",
      "    Sometimesweuseafunctionf         whoseargumentisascalarbutapplyittoa\n",
      "   vector,matrix,ortensor:f(x),f(X ),orf( X     ).Thisdenotestheapplicationoff\n",
      "      tothearrayelement-wise.Forexample,if C=σ( X  ),then C i , j , k=σ( X i , j , k  )forall\n",
      "      validvaluesof,and.ijk\n",
      "  DatasetsandDistributions\n",
      "p d a ta    Thedatageneratingdistribution\n",
      "ˆp d a ta       Theempiricaldistributiondeﬁnedbythetraining\n",
      "set\n",
      "     XAsetoftrainingexamples\n",
      "x( ) i      The-thexample(input)fromadataset i\n",
      "y( ) i ory( ) i   Thetargetassociatedwithx( ) i  forsupervisedlearn-\n",
      "ing\n",
      "X The  mn×    matrixwithinputexamplex( ) i inrow\n",
      "X i , :\n",
      "x v i C h a p t e r 1\n",
      "I n t r o duction\n",
      "           Inventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\n",
      "            backtoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,\n",
      "          Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\n",
      "             Galatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin\n",
      "    2004Sparkes1996Tandy1997 ; ,;,).\n",
      "        Whenprogrammablecomputerswereﬁrstconceived,peoplewonderedwhether\n",
      "           suchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas\n",
      "   built(Lovelace1842,).Today,  artiﬁcialintelligence     (AI)isathrivingﬁeldwith\n",
      "          manypracticalapplicationsandactiveresearchtopics.Welooktointelligent\n",
      "          softwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\n",
      "      inmedicineandsupportbasicscientiﬁcresearch.\n",
      "            Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved\n",
      "          problemsthatareintellectuallydiﬃcultforhumanbeingsbutrelativelystraight-\n",
      "            forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-\n",
      "          ematicalrules. Thetruechallengetoartiﬁcialintelligenceprovedtobesolving\n",
      "              thetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\n",
      "         formally—problemsthatwesolveintuitively,thatfeelautomatic,likerecognizing\n",
      "     spokenwordsorfacesinimages.\n",
      "             Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\n",
      "             toallowcomputerstolearnfromexperienceandunderstandtheworldintermsof\n",
      "            ahierarchyofconcepts,witheachconceptdeﬁnedthroughitsrelationtosimpler\n",
      "          concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\n",
      "            forhumanoperatorstoformallyspecifyalltheknowledgethatthecomputerneeds.\n",
      "           Thehierarchyofconceptsenablesthecomputertolearncomplicatedconceptsby\n",
      "              buildingthemoutofsimplerones.Ifwedrawagraphshowinghowtheseconcepts\n",
      "1  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "                arebuiltontopofeachother,thegraphisdeep,withmanylayers.Forthisreason,\n",
      "       wecallthisapproachtoAI . deeplearning\n",
      "             ManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\n",
      "          environmentsanddidnotrequirecomputerstohavemuchknowledgeabout\n",
      "          theworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld\n",
      "             championGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\n",
      "          world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove\n",
      "          inonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis a\n",
      "          tremendousaccomplishment, butthechallengeisnotduetothediﬃcultyof\n",
      "            describingthesetofchesspiecesandallowablemovestothecomputer.Chess\n",
      "             canbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\n",
      "      providedaheadoftimebytheprogrammer.\n",
      "           Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmental\n",
      "            undertakingsforahumanbeingareamongtheeasiestforacomputer.Computers\n",
      "              havelongbeenabletodefeateventhebesthumanchessplayerbutonlyrecently\n",
      "            havebegunmatchingsomeoftheabilitiesofaveragehumanbeingstorecognize\n",
      "   objects orspeech.A person’s everyday life requires an immenseamount of\n",
      "           knowledgeabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,\n",
      "            andthereforediﬃculttoarticulateinaformalway.Computersneedtocapture\n",
      "              thissameknowledgeinordertobehaveinanintelligentway.Oneofthekey\n",
      "            challengesinartiﬁcialintelligenceishowtogetthisinformalknowledgeintoa\n",
      "computer.\n",
      "        Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledge\n",
      "           abouttheworldinformallanguages.Acomputercanreasonautomaticallyabout\n",
      "            statementsintheseformallanguagesusinglogicalinferencerules.Thisisknownas\n",
      "the  knowledgebase         approachtoartiﬁcialintelligence.Noneoftheseprojectshas\n",
      "               ledtoamajorsuccess.OneofthemostfamoussuchprojectsisCyc(Lenatand\n",
      "              Guha1989,).Cycisaninferenceengineandadatabaseofstatementsinalanguage\n",
      "              calledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan\n",
      "          unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\n",
      "            toaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\n",
      "            aboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\n",
      "            enginedetectedaninconsistencyinthestory: itknewthatpeopledonothave\n",
      "            electricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\n",
      "        entity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether\n",
      "        Fredwasstillapersonwhilehewasshaving.\n",
      "         Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\n",
      "            thatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\n",
      "2  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "        patternsfromrawdata.Thiscapabilityisknownas  machinelearning .The\n",
      "         introductionofmachinelearningenabledcomputerstotackleproblemsinvolving\n",
      "            knowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\n",
      "   machinelearningalgorithmcalled  logisticregression   candeterminewhetherto\n",
      "          recommend cesareandelivery(Mor-Yosef1990etal.,).Asimplemachinelearning\n",
      "          algorithmcallednaiveBayescanseparatelegitimatee-mailfromspame-mail.\n",
      "         Theperformanceofthesesimplemachinelearningalgorithmsdependsheavily\n",
      " ontherepresentation         ofthedatatheyaregiven.Forexample,whenlogistic\n",
      "            regressionisusedtorecommend cesareandelivery,theAIsystemdoesnotexamine\n",
      "            thepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\n",
      "             information,suchasthepresenceorabsenceofauterinescar.Eachpieceof\n",
      "           informationincludedintherepresentationofthepatientisknownasafeature.\n",
      "            Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\n",
      "          variousoutcomes. However,itcannotinﬂuencehowfeaturesaredeﬁnedinany\n",
      "             way.IflogisticregressionweregivenanMRIscanofthepatient,ratherthan\n",
      "            thedoctor’sformalizedreport,itwouldnotbeabletomakeusefulpredictions.\n",
      "           IndividualpixelsinanMRIscanhavenegligiblecorrelationwithanycomplications\n",
      "    thatmightoccurduringdelivery.\n",
      "         Thisdependenceonrepresentationsisageneralphenomenonthatappears\n",
      "          throughoutcomputerscienceandevendailylife.Incomputerscience,operations\n",
      "             suchassearchingacollectionofdatacanproceedexponentiallyfasterifthecollec-\n",
      "          tionisstructuredandindexedintelligently.Peoplecaneasilyperformarithmetic\n",
      "           onArabicnumeralsbutﬁndarithmeticonRomannumeralsmuchmoretime\n",
      "            consuming.Itisnotsurprisingthatthechoiceofrepresentationhasanenormous\n",
      "           eﬀectontheperformanceofmachinelearningalgorithms.Forasimplevisual\n",
      "   example,seeﬁgure.1.1\n",
      "            Manyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof\n",
      "             featurestoextractforthattask,thenprovidingthesefeaturestoasimplemachine\n",
      "          learningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom\n",
      "               soundisanestimateofthesizeofthespeaker’svocaltract.Thisfeaturegivesa\n",
      "            strongclueastowhetherthespeakerisaman,woman,orchild.\n",
      "     For manytasks, however, it isdiﬃcult toknowwhat features should be\n",
      "             extracted.Forexample,supposethatwewouldliketowriteaprogramtodetect\n",
      "               carsinphotographs.Weknowthatcarshavewheels,sowemightliketousethe\n",
      "             presenceofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactly\n",
      "               whatawheellookslikeintermsofpixelvalues.Awheelhasasimplegeometri c\n",
      "              shape,butitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesun\n",
      "                 glaringoﬀthemetalpartsofthewheel,thefenderofthecaroranobjectinthe\n",
      "3  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "\n",
      "         Figure1.1:Example ofdiﬀerentrepresentations:supposewewanttoseparate two\n",
      "                 categoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\n",
      "              werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\n",
      "               ontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\n",
      "           solvewithaverticalline.(FigureproducedincollaborationwithDavidWarde-Farley.)\n",
      "        foregroundobscuringpartofthewheel,andsoon.\n",
      "             Onesolutiontothisproblemistousemachinelearningtodiscovernotonly\n",
      "          themappingfromrepresentationtooutputbutalsotherepresentationitself.\n",
      "    Thisapproachisknownas  representationlearning  . Learnedrepresentations\n",
      "           oftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\n",
      "            representations.TheyalsoenableAIsystemstorapidlyadapttonewtasks,with\n",
      "         minimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\n",
      "                 goodsetoffeaturesforasimpletaskinminutes,orforacomplextaskinhoursto\n",
      "            months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\n",
      "             humantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.\n",
      "         Thequintessentialexampleofarepresentationlearningalgorithmistheau-\n",
      "toencoder       .Anautoencoderisthecombinationofanencoder  function,which\n",
      "         convertstheinputdataintoadiﬀerentrepresentation,andadecoderfunction,\n",
      "          whichconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\n",
      "             aretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun\n",
      "              throughtheencoderandthenthedecoder,buttheyarealsotrainedtomakethe\n",
      "         newrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencoders\n",
      "      aimtoachievediﬀerentkindsofproperties.\n",
      "           Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\n",
      "  toseparatethe  factorsofvariation      thatexplaintheobserveddata.Inthis\n",
      "4  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "             context,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;\n",
      "            thefactorsareusuallynotcombinedbymultiplication.Suchfactorsareoftennot\n",
      "           quantitiesthataredirectlyobserved.Instead,theymayexistaseitherunobserved\n",
      "           objectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.\n",
      "             Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\n",
      "             explanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\n",
      "              conceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\n",
      "           Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s\n",
      "             age,theirsex,theiraccentandthewordstheyarespeaking.Whenanalyzingan\n",
      "               imageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\n",
      "       andtheangleandbrightnessofthesun.\n",
      "          Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications\n",
      "               isthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare\n",
      "                abletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\n",
      "              toblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.\n",
      "            Mostapplicationsrequireusto thefactorsofvariationanddiscardthe disentangle\n",
      "      onesthatwedonotcareabout.\n",
      "            Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures\n",
      "             fromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,\n",
      "         canbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof\n",
      "               thedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe\n",
      "            originalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.\n",
      " Deeplearning         solvesthiscentralprobleminrepresentationlearningbyintro-\n",
      "          ducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\n",
      "            Deeplearningenablesthecomputertobuildcomplexconceptsoutofsimplercon-\n",
      "             cepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\n",
      "             animageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\n",
      "        whichareinturndeﬁnedintermsofedges.\n",
      "           Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeep\n",
      " network,or  multilayerperceptron       (MLP).Amultilayerperceptronisjusta\n",
      "           mathematicalfunctionmappingsomesetofinputvaluestooutputvalues.The\n",
      "            functionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\n",
      "          applicationofadiﬀerentmathematicalfunctionasprovidinganewrepresentation\n",
      "  oftheinput.\n",
      "            Theideaoflearningtherightrepresentationforthedataprovidesoneper-\n",
      "           spectiveondeeplearning.Anotherperspectiveondeeplearningisthatdepth\n",
      "            enablesthecomputertolearnamultistepcomputerprogram.Eachlayerofthe\n",
      "            representationcanbethoughtofasthestateofthecomputer’smemory after\n",
      "5  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "Visible layer\n",
      "(input pixels)1st hidden layer\n",
      "(edges)2nd hi dden layer\n",
      "(corners and\n",
      "contours)3rd hidden layer\n",
      "(object parts)  CAR PERSONANIMALOutput\n",
      "(object identity)\n",
      "               Figure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand\n",
      "              themeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\n",
      "                ofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\n",
      "          complicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.\n",
      "            Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa\n",
      "              seriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The\n",
      "    inputispresentedatthe visiblelayer        ,sonamedbecauseitcontainsthevariablesthat\n",
      "        weareabletoobserve.Thenaseriesof  hiddenlayers   extractsincreasinglyabstract\n",
      "              featuresfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven\n",
      "             inthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining\n",
      "             therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\n",
      "              offeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily\n",
      "            identifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden\n",
      "              layer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\n",
      "            extendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\n",
      "              layer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer\n",
      "             candetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand\n",
      "               corners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\n",
      "             beusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\n",
      "    fromZeilerandFergus2014().\n",
      "6  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           executinganothersetofinstructionsinparallel.Networkswithgreaterdepthcan\n",
      "         executemoreinstructionsinsequence.Sequentialinstructionsoﬀergreatpower\n",
      "            becauselaterinstructionscanreferbacktotheresultsofearlierinstructions.Ac-\n",
      "              cordingtothisviewofdeeplearning,notalltheinformationinalayer’sactivations\n",
      "          necessarilyencodesfactorsofvariationthatexplaintheinput.Therepresentation\n",
      "             alsostoresstateinformationthathelpstoexecuteaprogramthatcanmakesense\n",
      "            oftheinput. Thisstateinformationcouldbeanalogoustoacounterorpointer\n",
      "              inatraditionalcomputerprogram.Ithasnothingtodowiththecontentofthe\n",
      "          inputspeciﬁcally,butithelpsthemodeltoorganizeitsprocessing.\n",
      "               Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis\n",
      "            basedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\n",
      "              thearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough\n",
      "             aﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven\n",
      "           itsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths\n",
      "            dependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\n",
      "            bedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe\n",
      "              allowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3\n",
      "           choiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.\n",
      "x 1 x 1σ\n",
      "w 1 w 1×\n",
      "x 2 x 2 w 2 w 2×+El e me n t\n",
      "S e t\n",
      "+\n",
      "×\n",
      "σ\n",
      "xx wwEl e me n t\n",
      "S e t\n",
      "L ogi s t i c\n",
      "R e gr e s s i onL ogi s t i c\n",
      "R e gr e s s i on\n",
      "            Figure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\n",
      "               eachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\n",
      "            outputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.\n",
      "             Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,\n",
      "σ ( wTx ),where σ          isthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\n",
      "             logisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\n",
      "               three.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\n",
      "7  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa\n",
      "              modelasbeingnotthedepthofthecomputationalgraphbutthedepthofthe\n",
      "             graphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n",
      "           oftheﬂowchartofthecomputationsneededtocomputetherepresentationof\n",
      "            eachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\n",
      "            Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned\n",
      "           giveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\n",
      "               observinganimageofafacewithoneeyeinshadowmayinitiallyseeonlyone\n",
      "               eye.Afterdetectingthatafaceispresent,thesystemcantheninferthatasecond\n",
      "              eyeisprobablypresentaswell.Inthiscase,thegraphofconceptsincludesonly\n",
      "             twolayers—alayerforeyesandalayerforfaces—butthegraphofcomputations\n",
      " includes 2 n           layersifwereﬁneourestimateofeachconceptgiventheother ntimes.\n",
      "             Becauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe\n",
      "          computationalgraph,orthedepthoftheprobabilisticmodelinggraph—ismost\n",
      "          relevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements\n",
      "             fromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\n",
      "               depthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof\n",
      "            acomputerprogram. Noristhereaconsensusabouthowmuchdepthamodel\n",
      "             requirestoqualifyas“deep.”However,deeplearningcanbesafelyregardedasthe\n",
      "            studyofmodelsthatinvolveagreateramountofcompositionofeitherlearned\n",
      "        functionsorlearnedconceptsthantraditionalmachinelearningdoes.\n",
      "             Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\n",
      "            Speciﬁcally,itisatypeofmachinelearning,atechniquethatenablescomputer\n",
      "           systemstoimprovewithexperienceanddata.Wecontendthatmachinelearning\n",
      "             istheonlyviableapproachtobuildingAIsystemsthatcanoperateincomplicated\n",
      "          real-worldenvironments.Deeplearningisaparticularkindofmachinelearning\n",
      "            thatachievesgreatpowerandﬂexibilitybyrepresentingtheworldasanested\n",
      "            hierarchyofconcepts,witheachconceptdeﬁnedinrelationtosimplerconcepts,and\n",
      "           moreabstractrepresentationscomputedintermsoflessabstractones.Figure1.4\n",
      "          illustratestherelationshipbetweenthesediﬀerentAIdisciplines.Figuregives1.5\n",
      "      ahigh-levelschematicofhoweachworks.\n",
      "     1.1WhoShouldReadThisBook?\n",
      "                Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwotarget\n",
      "          audiencesinmind. Oneofthesetargetaudiencesisuniversitystudents(under-\n",
      "          graduateorgraduate)learningaboutmachinelearning,includingthosewhoare\n",
      "           beginningacareerindeeplearningandartiﬁcialintelligenceresearch.Theother\n",
      "8  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "AIMachine learningRepresentation learningDeep learning\n",
      "Example:\n",
      "Knowledge\n",
      "basesExample:\n",
      "Logistic\n",
      "regressionExample:\n",
      "Shallow\n",
      "autoencoders Example:\n",
      "MLPs\n",
      "              Figure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\n",
      "                 whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\n",
      "              toAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\n",
      "             targetaudienceissoftwareengineerswhodonothaveamachinelearningorstatis-\n",
      "             ticsbackground butwanttorapidlyacquireoneandbeginusingdeeplearningin\n",
      "            theirproductorplatform.Deeplearninghasalreadyprovedusefulinmanysoft-\n",
      "         waredisciplines,includingcomputervision,speechandaudioprocessing,natural\n",
      "        languageprocessing,robotics,bioinformaticsandchemistry,videogames,search\n",
      "    engines,onlineadvertisingandﬁnance.\n",
      "            Thisbookhasbeenorganizedintothreepartstobestaccommo dateavariety\n",
      "          ofreaders.Partintroducesbasicmathematicaltoolsandmachinelearning I\n",
      "          concepts.Partdescribesthemostestablisheddeeplearningalgorithms,which II\n",
      "          areessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthat III\n",
      "           arewidelybelievedtobeimportantforfutureresearchindeeplearning.\n",
      "9  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "InputHand-\n",
      "designed \n",
      "programOutput\n",
      "InputHand-\n",
      "designed \n",
      "featuresMappi ng from \n",
      "featuresOutput\n",
      "InputFeaturesMappi ng from \n",
      "featuresOutput\n",
      "InputSimple \n",
      "featuresMappi ng from \n",
      "featuresOutput\n",
      "Additional \n",
      "layers of more \n",
      "abstrac t \n",
      "features\n",
      "Rule-based\n",
      "systemsClassic\n",
      "machine\n",
      "learning Representation\n",
      "learningDeep\n",
      "learning\n",
      "             Figure1.5: Flow chartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach\n",
      "            otherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\n",
      "  learnfromdata.\n",
      "             Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\n",
      "         orbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\n",
      "             machinelearningconceptscanskippart,forexample,whilethosewhojustwant I\n",
      "              toimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\n",
      "1 0  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "1. Introduction\n",
      "Part I: Applied Math and Machine Learning Basics\n",
      "2. Linear Algebra3. Probability and \n",
      "Information Th eory\n",
      "4. Numerical \n",
      "Computation5. Machine Learning \n",
      "Basics\n",
      "Part II: Deep Networks: Modern Practices\n",
      "6. Deep Feedforward \n",
      "Networks\n",
      "   7. Regularization 8. Optimization 9.  CNNs 10.  RNNs\n",
      "11. Practical \n",
      "Methodology12. Applications\n",
      "Part III: Deep Learning Research\n",
      "13. Linear Factor \n",
      "Models14. Autoencoders15. Representation \n",
      "Learning\n",
      "16. Structured \n",
      "Probabilistic Models17. Monte  Carlo \n",
      "Methods\n",
      "18. Partition \n",
      "Function19. Inference\n",
      "20. Deep Generative \n",
      "Models\n",
      "              Figure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\n",
      "           indicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\n",
      "1 1  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           chapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6\n",
      "  ofthebook.\n",
      "            Wedoassumethatallreaderscomefromacomputersciencebackground. We\n",
      "        assumefamiliaritywithprogramming,abasicunderstandingofcomputational\n",
      "          performanceissues,complexitytheory,introductorylevelcalculusandsomeofthe\n",
      "   terminologyofgraphtheory.\n",
      "     1.2HistoricalTrendsinDeepLearning\n",
      "            Itiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\n",
      "            providingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\n",
      "•              Deeplearninghashadalongandrichhistory,buthasgonebymanynames,\n",
      "         reﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin\n",
      "popularity.\n",
      "•           Deeplearninghasbecomemoreusefulastheamountofavailabletraining\n",
      "  datahasincreased.\n",
      "•           Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n",
      "        (bothhardwareandsoftware)fordeeplearninghasimproved.\n",
      "•        Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\n",
      "  accuracyovertime.\n",
      "         1.2.1TheManyNamesandChangingFortunesofNeuralNet-\n",
      "works\n",
      "               Weexpectthatmanyreadersofthisbookhaveheardofdeeplearningasanexciting\n",
      "               newtechnology,andaresurprisedtoseeamentionof“history”inabookaboutan\n",
      "             emergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deeplearningonly\n",
      "            appearstobenew,becauseitwasrelativelyunpopularforseveralyearspreceding\n",
      "            itscurrentpopularity,andbecauseithasgonethroughmanydiﬀerentnames,only\n",
      "           recentlybeingcalled“deeplearning.”Theﬁeldhasbeenrebrandedmanytimes,\n",
      "        reﬂectingtheinﬂuenceofdiﬀerentresearchersanddiﬀerentperspectives.\n",
      "            Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\n",
      "          Somebasiccontext,however,isusefulforunderstandingdeeplearning.Broadly\n",
      "           speaking,therehavebeenthreewavesofdevelopment:deeplearningknownas\n",
      "c y b e r net i c s      inthe1940s–1960s,deeplearningknownas c o nnec t i o n i s m inthe\n",
      "1 2  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "      1940 1950 1960 1970 1980 1990 2000\n",
      "Year0.0000000.0000500.0001000.0001500.0002000.000250    FrequencyofWordorPhrase\n",
      "cybernetics\n",
      "   (connectionism+neuralnetworks)\n",
      "              Figure1.7:Twoofthethreehistoricalwavesofartiﬁcialneuralnetsresearch,asmeasured\n",
      "           bythefrequencyofthephrases“cybernetics”and“connectionism”or“neuralnetworks,”\n",
      "              accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).Theﬁrstwave\n",
      "            startedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheoriesofbiological\n",
      "            learning(McCullochandPitts1943Hebb1949 ,;,)andimplementationsoftheﬁrstmodels,\n",
      "             suchastheperceptron( ,),enablingthetrainingofasingleneuron.The Rosenblatt1958\n",
      "            secondwavestartedwiththeconnectionistapproachofthe1980–1995 period,withback-\n",
      "              propagation( ,)totrainaneuralnetworkwithoneortwohidden Rumelhartetal.1986a\n",
      "            layers. Thecurrentandthirdwave,deeplearning,startedaround2006( , Hintonetal.\n",
      "                2006Bengio 2007Ranzato2007a ; etal.,; etal.,)andisjustnowappearinginbookform\n",
      "              asof2016. The othertwowavessimilarlyappearedinbookformmuchlaterthanthe\n",
      "   correspondingscientiﬁcactivityoccurred.\n",
      "          1980s–1990s,andthecurrentresurgenceunderthenamedeeplearningbeginning\n",
      "        in2006.Thisisquantitativelyillustratedinﬁgure.1.7\n",
      "           Someoftheearliestlearningalgorithmswerecognizetodaywereintendedto\n",
      "           becomputationalmodelsofbiologicallearning,thatis,modelsofhowlearning\n",
      "              happensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeep\n",
      "    learninghasgonebyis   artiﬁcialneuralnetworks   (ANNs).Thecorresponding\n",
      "           perspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired\n",
      "             bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\n",
      "           Whilethekindsofneuralnetworksusedformachinelearninghavesometimes\n",
      "           beenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\n",
      "           generallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\n",
      "             perspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\n",
      "             thebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\n",
      "          conceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe\n",
      "         computationalprinciplesbehindthebrainandduplicateitsfunctionality.Another\n",
      "1 3  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "             perspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\n",
      "          principlesthatunderliehumanintelligence,somachinelearningmodelsthatshed\n",
      "             lightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve\n",
      " engineeringapplications.\n",
      "         Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective\n",
      "             onthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral\n",
      "            principleoflearningmultiplelevelsofcomposition,whichcanbeappliedinmachine\n",
      "       learningframeworksthatarenotnecessarilyneurallyinspired.\n",
      "          Theearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\n",
      "         motivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto\n",
      "   takeasetofn inputvaluesx 1     ,...,x n     andassociatethemwithanoutputy.\n",
      "       Thesemodelswouldlearnasetofweightsw 1     ,...,w n   andcomputetheiroutput\n",
      "f( x w, )=x 1w 1+ · · ·+x nw n        .Thisﬁrstwaveofneuralnetworksresearchwas\n",
      "       knownascybernetics,asillustratedinﬁgure.1.7\n",
      "          TheMcCulloch-Pittsneuron( ,)wasanearlymodel McCullochandPitts1943\n",
      "           ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof\n",
      "   inputsbytestingwhetherf( x w,         )ispositiveornegative.Ofcourse,forthemodel\n",
      "             tocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe\n",
      "              setcorrectly.Theseweightscouldbesetbythehumanoperator.Inthe1950s,the\n",
      "           perceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearnthe\n",
      "           weightsthatdeﬁnedthecategoriesgivenexamplesofinputsfromeachcategory.\n",
      "The   adaptivelinearelement       (ADALINE),whichdatesfromaboutthesame\n",
      "     time,simplyreturnedthevalueoff( x       )itselftopredictarealnumber(Widrow\n",
      "            andHoﬀ1960,)andcouldalsolearntopredictthesenumbersfromdata.\n",
      "          Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-\n",
      "            chinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\n",
      "       wasaspecialcaseofanalgorithmcalled   stochasticgradientdescent .Slightly\n",
      "          modiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant\n",
      "      trainingalgorithmsfordeeplearningmodelstoday.\n",
      "   Modelsbasedonthef( x w,        )usedbytheperceptronandADALINEarecalled\n",
      " linearmodels          .Thesemodelsremainsomeofthemostwidelyusedmachine\n",
      "             learningmodels,thoughinmanycasestheyaretrainedindiﬀerentwaysthanthe\n",
      "   originalmodelsweretrained.\n",
      "          Linearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\n",
      "  XORfunction,wheref([0,1] , w   )=1andf([1,0] , w   )=1butf([1,1] , w  )=0\n",
      "andf([0,0] , w           )=0.Criticswhoobservedtheseﬂawsinlinearmodelscaused\n",
      "          abacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n",
      "            1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.\n",
      "1 4  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           Today,neuroscienceisregardedasanimportantsourceofinspirationfordeep\n",
      "            learningresearchers,butitisnolongerthepredominantguidefortheﬁeld.\n",
      "          Themainreasonforthediminishedrole ofneuroscienceindeeplearning\n",
      "             researchtodayisthatwesimplydonothaveenoughinformationaboutthebrain\n",
      "               touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\n",
      "                bythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\n",
      "         least)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\n",
      "               abletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n",
      "        well-studiedpartsofthebrain( ,). OlshausenandField2005\n",
      "             Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\n",
      "            cansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto\n",
      "             “see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\n",
      "              tosendvisualsignalstothatarea(VonMelchner2000etal.,).Thissuggeststhat\n",
      "              muchofthemammal ianbrainmightuseasinglealgorithmtosolvemostofthe\n",
      "          diﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning\n",
      "         researchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying\n",
      "         naturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\n",
      "            theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\n",
      "           researchgroupstostudymanyorevenalltheseapplicationareassimultaneously.\n",
      "           Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasic\n",
      "           ideaofhavingmanycomputationalunitsthatbecomeintelligentonlyviatheir\n",
      "           interactionswitheachotherisinspiredbythebrain.Theneocognitron(Fukushima,\n",
      "          1980)introducedapowerfulmodelarchitectureforprocessingimagesthatwas\n",
      "            inspiredbythestructureofthemammal ianvisualsystemandlaterbecamethe\n",
      "             basisforthemodernconvolutionalnetwork( ,),aswewillsee LeCunetal.1998b\n",
      "             insection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\n",
      "the   rectiﬁedlinearunit      .Theoriginalcognitron(Fukushima1975,)introduced\n",
      "            amorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\n",
      "         function.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom\n",
      "            manyviewpoints,with ()and ()citing NairandHinton2010 Glorotetal.2011a\n",
      "           neuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrettetal.2009\n",
      "          orientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it\n",
      "              neednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\n",
      "          diﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism\n",
      "            hasnotyetledtoanimprovementinmachinelearningperformance.Also,while\n",
      "        neurosciencehassuccessfullyinspiredseveralneuralnetworkarchitectures,we\n",
      "            donotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch\n",
      "          guidanceforthelearningalgorithmsweusetotrainthesearchitectures.\n",
      "1 5  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\n",
      "                Whileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\n",
      "           inﬂuencethanresearchersworkinginothermachinelearningﬁelds,suchaskernel\n",
      "            machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\n",
      "           tosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,\n",
      "        especiallyappliedmathfundamentalslikelinearalgebra,probability,information\n",
      "         theory,andnumericaloptimization.Whilesomedeeplearningresearcherscite\n",
      "           neuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n",
      "  neuroscienceatall.\n",
      "         Itis worth notingthat theeﬀorttounderstandhowthe brainworkson\n",
      "    an algorithmic level is alive andwell.This endeavor isprimarily knownas\n",
      "           “computationalneuroscience”andisaseparateﬁeldofstudyfromdeeplearning.\n",
      "             Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The\n",
      "            ﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\n",
      "            thatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof\n",
      "        computationalneuroscienceisprimarilyconcernedwithbuildingmoreaccurate\n",
      "      modelsofhowthebrainactuallyworks.\n",
      "            Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\n",
      "    partviaamovementcalledconnectionism ,or   paralleldistributedprocess-\n",
      "ing          ( ,; ,). Connectionismarosein Rumelhartetal.1986cMcClellandetal.1995\n",
      "          thecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\n",
      "          tounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During\n",
      "          theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\n",
      "           Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof\n",
      "          howthebraincouldactuallyimplementthemusingneurons.Theconnectionists\n",
      "            begantostudymodelsofcognitionthatcouldactuallybegroundedinneural\n",
      "         implementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\n",
      "           totheworkofpsychologistDonaldHebbinthe1940s(,). Hebb1949\n",
      "            Thecentralideainconnectionismisthatalargenumberofsimplecomputational\n",
      "          unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsightapplies\n",
      "             equallytoneuronsinbiologicalnervoussystemsasitdoestohiddenunitsin\n",
      " computationalmodels.\n",
      "          Severalkeyconceptsaroseduringtheconnectionismmovementofthe1980s\n",
      "      thatremaincentraltotoday’sdeeplearning.\n",
      "      Oneoftheseconceptsisthatof  distributedrepresentation   (Hintonetal.,\n",
      "              1986).Thisistheideathateachinputtoasystemshouldberepresentedby\n",
      "            manyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany\n",
      "            possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\n",
      "1 6  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "               cars,trucks,andbirds,andtheseobjectscaneachbered,green,orblue.Oneway\n",
      "             ofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\n",
      "             thatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\n",
      "             bird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron\n",
      "            mustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\n",
      "            improveonthissituationistouseadistributedrepresentation,withthreeneurons\n",
      "           describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\n",
      "              onlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n",
      "             learnaboutrednessfromimagesofcars,trucksandbirds,notjustfromimages\n",
      "          ofonespeciﬁccategoryofobjects. Theconceptofdistributedrepresentationis\n",
      "            centraltothisbookandisdescribedingreaterdetailinchapter.15\n",
      "         Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\n",
      "           cessfuluseofback-propagationtotraindeepneuralnetworkswithinternalrepre-\n",
      "        sentationsandthepopularizationoftheback-propagationalgorithm(Rumelhart\n",
      "            etal.,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\n",
      "            but,asofthiswriting,isthedominantapproachtotrainingdeepmodels.\n",
      "         Duringthe1990s,researchersmadeimportantadvancesinmodelingsequences\n",
      "            withneuralnetworks. ()and ()identiﬁedsomeof Hochreiter1991 Bengioetal.1994\n",
      "         thefundamentalmathematicaldiﬃcultiesinmodelinglongsequences,describedin\n",
      "         section.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\n",
      "            memory (LSTM)networktoresolvesomeofthesediﬃculties.Today,theLSTMis\n",
      "          widelyusedformanysequencemodelingtasks,includingmanynaturallanguage\n",
      "   processingtasksatGoogle.\n",
      "           Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\n",
      "            turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\n",
      "           callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll\n",
      "      theseunreasonableexpectations,investorsweredisappointed.Simultaneously,\n",
      "           otherﬁeldsofmachinelearningmadeadvances.Kernelmachines( , Boseretal.\n",
      "            1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; etal.,)andgraphicalmodels(\n",
      "            dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\n",
      "             ledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\n",
      "         Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\n",
      "             onsometasks( ,; ,).TheCanadianInstitute LeCunetal.1998bBengioetal.2001\n",
      "          forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\n",
      "         viaitsNeuralComputationandAdaptivePerception(NCAP)researchinitiative.\n",
      "           ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHintonat\n",
      "           UniversityofToronto,YoshuaBengioatUniversityofMontreal,andYannLeCun\n",
      "         atNewYorkUniversity.ThemultidisciplinaryCIFARNCAPresearchinitiative\n",
      "1 7  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "         alsoincludedneuroscientistsandexpertsinhumanandcomputervision.\n",
      "            Atthispoint,deepnetworksweregenerallybelievedtobeverydiﬃcultto\n",
      "             train.Wenowknowthatalgorithmsthathaveexistedsincethe1980sworkquite\n",
      "              well,butthiswasnotapparentcirca2006.Theissueisperhapssimplythatthese\n",
      "         algorithmsweretoocomputationallycostlytoallowmuchexperimentationwith\n",
      "     thehardwareavailableatthetime.\n",
      "           Thethirdwaveofneuralnetworksresearchbeganwithabreakthroughin\n",
      "             2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\n",
      "      networkcould be eﬃcientlytrainedusinga strategycalled greedylayer-wise\n",
      "             pretraining( ,),whichwedescribeinmoredetailinsection. Hintonetal.2006 15.1\n",
      "          TheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy\n",
      "              couldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengioetal.2007\n",
      "          Ranzato 2007aetal.,)andsystematicallyhelpedtoimprovegeneralizationon\n",
      "            testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\n",
      "            term“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper\n",
      "            neuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\n",
      "          theoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n",
      "             2011Pascanu 2014aMontufar 2014 ; etal.,; etal.,).Atthistime,deepneural\n",
      "         networksoutperformedcompetingAIsystemsbasedonothermachinelearning\n",
      "          technologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity\n",
      "              ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\n",
      "           learningresearchhaschangeddramaticallywithinthetimeofthiswave.The\n",
      "            thirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\n",
      "             abilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\n",
      "            moreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\n",
      "     modelstoleveragelargelabeleddatasets.\n",
      "   1.2.2IncreasingDatasetSizes\n",
      "             Onemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasacrucial\n",
      "          technologyeventhoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere\n",
      "           conductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\n",
      "               applicationssincethe1990sbutwasoftenregardedasbeingmoreofanartthana\n",
      "             technologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\n",
      "             thatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\n",
      "            Fortunately,theamountofskillrequiredreducesastheamountoftrainingdata\n",
      "         increases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks\n",
      "            todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\n",
      "            problemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\n",
      "1 8  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           undergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\n",
      "           importantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\n",
      "             theresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\n",
      "            datasetshasexpandedremarkablyovertime.Thistrendisdrivenbytheincreasing\n",
      "             digitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,\n",
      "             moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\n",
      "           networkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n",
      "            intoadatasetappropriateformachinelearningapplications.Theageof“BigData”\n",
      "    1900 1950 198520002015100101102103104105106107108109   Datasetsize(num berexamples)\n",
      "IrisMNIST PublicSVHN\n",
      "ImageNet\n",
      "CIFAR-10ImageNet10k\n",
      "ILSVRC 2014Sports-1M\n",
      "   RotatedTvs.C     Tvs.Gvs.FCriminals CanadianHansard\n",
      "WMT\n",
      "            Figure1.8:Increasingdatasetsizeovertime.Intheearly1900s,statisticiansstudied\n",
      "          datasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,; Garson1900\n",
      "             Gosset1908Anderson1935Fisher1936 ,; ,;,).Inthe1950sthroughthe1980s,thepioneers\n",
      "           ofbiologicall yinspiredmachinelearningoftenworkedwithsmallsyntheticdatasets,such\n",
      "             aslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\n",
      "            demonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow\n",
      "             andHoﬀ1960Rumelhart 1986b ,; etal.,).Inthe1980sand1990s,machinelearning\n",
      "            becamemorestatisticalandbegantoleveragelargerdatasetscontainingtensofthousands\n",
      "              ofexamples,suchastheMNISTdataset(showninﬁgure)ofscansofhandwritten 1.9\n",
      "             numbers( ,).Intheﬁrstdecadeofthe2000s,moresophisticated LeCunetal.1998b\n",
      "             datasetsofthissamesize,suchastheCIFAR-10dataset(KrizhevskyandHinton2009,),\n",
      "               continuedtobeproduced.Towardtheendofthatdecadeandthroughouttheﬁrsthalfof\n",
      "            the2010s,signiﬁcantlylargerdatasets,containinghundredsofthousandstotensofmillions\n",
      "           ofexamples,completelychangedwhatwaspossiblewithdeeplearning.Thesedatasets\n",
      "            includedthepublicStreetViewHouseNumbersdataset( ,),various Netzeretal.2011\n",
      "             versionsoftheImageNetdataset( ,,; Dengetal.20092010aRussakovsky 2014aetal.,),\n",
      "                andtheSports-1Mdataset( ,).Atthetopofthegraph,weseethat Karpathyetal.2014\n",
      "           datasetsoftranslatedsentences,suchasIBM’sdatasetconstructedfromtheCanadian\n",
      "             Hansard(Brown 1990etal.,)andtheWMT2014EnglishtoFrenchdataset(Schwenk,\n",
      "        2014),aretypicallyfaraheadofotherdatasetsizes.\n",
      "1 9  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "            Figure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational\n",
      "           InstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\n",
      "              The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith\n",
      "           machinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\n",
      "             andassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple\n",
      "              classiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning\n",
      "            research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\n",
      "             GeoﬀreyHintonhasdescribeditas“thedrosophilaofmachinelearning,”meaningthatit\n",
      "          enablesmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\n",
      "       conditions,muchasbiologistsoftenstudyfruitﬂies.\n",
      "           hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\n",
      "          estimation—general izingwelltonewdataafterobservingonlyasmallamount\n",
      "            ofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\n",
      "          isthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\n",
      "           performancewitharound5,000labeledexamplespercategoryandwillmatchor\n",
      "2 0  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "           exceedhumanperformancewhentrainedwithadatasetcontainingatleast10\n",
      "          millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\n",
      "            animportantresearcharea,focusinginparticularonhowwecantakeadvantage\n",
      "         oflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\n",
      "learning.\n",
      "   1.2.3IncreasingModelSizes\n",
      "           Anotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\n",
      "           comparativelylittlesuccesssincethe1980sisthatwehavethecomputational\n",
      "             resourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-\n",
      "            ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\n",
      "           Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\n",
      "           Biologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\n",
      "            ourmachinelearningmodelshavehadanumberofconnectionsperneuronwithin\n",
      "         anorderofmagnitudeofevenmammal ianbrainsfordecades.\n",
      "            Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\n",
      "             smalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11\n",
      "            units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\n",
      "            growthisdrivenbyfastercomputerswithlargermemory andbytheavailability\n",
      "            oflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\n",
      "            complextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\n",
      "           enablefasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumber\n",
      "            ofneuronsasthehumanbrainuntilatleastthe2050s. Biologicalneuronsmay\n",
      "         representmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological\n",
      "         neuralnetworksmaybeevenlargerthanthisplotportrays.\n",
      "           Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\n",
      "           neuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-\n",
      "           lems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational\n",
      "            systemspointofview,aresmallerthanthenervoussystemofevenrelatively\n",
      "    primitivevertebrateanimalslikefrogs.\n",
      "             Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n",
      "           theadventofgeneralpurposeGPUs(describedinsection ),fasternetwork 12.1.2\n",
      "          connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\n",
      "             themostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\n",
      "      expectedtocontinuewellintothefuture.\n",
      "2 1  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "   1950 1985 2000 2015101102103104  Connectionsperneuron\n",
      "12\n",
      "34\n",
      "567\n",
      "89\n",
      "10\n",
      " FruitﬂyMouseCatHuman\n",
      "             Figure1.10:Numberofconnectionsperneuronovertime.Initially,thenumberofconnec-\n",
      "           tionsbetweenneuronsinartiﬁcialneuralnetworkswaslimitedbyhardwarecapabilities.\n",
      "            Today,thenumberofconnectionsbetweenneuronsismostlyadesignconsideration.Some\n",
      "              artiﬁcialneuralnetworkshavenearlyasmanyconnectionsperneuronasacat,andit\n",
      "              isquitecommonforotherneuralnetworkstohaveasmanyconnectionsperneuronas\n",
      "             smallermammalslikemice.Eventhehumanbraindoesnothaveanexorbitantamount\n",
      "          ofconnectionsperneuron.Biological neuralnetworksizesfrom (). Wikipedia2015\n",
      "       1.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n",
      "   2.Neocognitron(Fukushima1980,)\n",
      "       3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al .2006\n",
      "       4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n",
      "       5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al .2009\n",
      "       6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al .2010\n",
      "      7.Distributedautoencoder( ,) Le e t al .2012\n",
      "       8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al .2012\n",
      "         9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al .2013\n",
      "     10.GoogLeNet( ,) Szegedy e t al .2014a\n",
      "      1.2.4IncreasingAccuracy,ComplexityandReal-WorldImpact\n",
      "            Sincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide\n",
      "         accuraterecognitionandprediction.Moreover,deeplearninghasconsistentlybeen\n",
      "         appliedwithsuccesstobroaderandbroadersetsofapplications.\n",
      "           Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\n",
      "           cropped,extremelysmallimages( ,).Sincethentherehas Rumelhartetal.1986a\n",
      "             beenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\n",
      "         objectrecognitionnetworksprocessrichhigh-resolutionphotographsanddonot\n",
      "2 2  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "    1950 198520002015 205610− 210− 1100101102103104105106107108109101 0101 1    Numberofneurons(logarithmicscale)\n",
      "123\n",
      "456\n",
      "78\n",
      "91011\n",
      "121314\n",
      "151617\n",
      "181920\n",
      "SpongeRoundwormLeechAntBeeFrogOctopusHuman\n",
      "            Figure1.11:Increasingneuralnetworksizeovertime.Sincetheintroductionofhidden\n",
      "            units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Biological\n",
      "     neuralnetworksizesfrom (). Wikipedia2015\n",
      "    1.Perceptron( ,,) Rosenblatt19581962\n",
      "       2.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n",
      "   3.Neocognitron(Fukushima1980,)\n",
      "       4.Earlyback-propagationnetwork( ,) Rumelhart e t al .1986b\n",
      "          5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n",
      "         6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al .1991\n",
      "         7.Meanﬁeldsigmoidbeliefnetwork( ,) Saul e t al .1996\n",
      "     8.LeNet-5( ,) LeCun e t al .1998b\n",
      "       9.Echostatenetwork( ,) JaegerandHaas2004\n",
      "       10.Deepbeliefnetwork(Hinton 2006 e t al .,)\n",
      "       11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al .2006\n",
      "       12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n",
      "        13.GPU-accelerateddeepbeliefnetwork( ,) Raina e t al .2009\n",
      "       14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al .2009\n",
      "       15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al .2010\n",
      "      16.OMP-1network( ,) CoatesandNg2011\n",
      "      17.Distributedautoencoder( ,) Le e t al .2012\n",
      "       18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al .2012\n",
      "         19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al .2013\n",
      "     20.GoogLeNet( ,) Szegedy e t al .2014a\n",
      "             havearequirementthatthephotobecroppedneartheobjecttoberecognized\n",
      "          ( ,).Similarly,theearliestnetworkscouldrecognizeonly Krizhevskyetal.2012\n",
      "                twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\n",
      "          object),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent\n",
      "          categoriesofobjects. ThelargestcontestinobjectrecognitionistheImageNet\n",
      "2 3  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "          LargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\n",
      "            momentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\n",
      "              wonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe\n",
      "          state-of-the-arttop-5errorratefrom26.1percentto15.3percent(Krizhevsky\n",
      "            etal.,),meaningthattheconvolutionalnetworkproducesarankedlistof 2012\n",
      "            possiblecategoriesforeachimage,andthecorrectcategoryappearedintheﬁrst\n",
      "               ﬁveentriesofthislistforallbut15.3percentofthetestexamples.Sincethen,\n",
      "            thesecompetitionsareconsistentlywonbydeepconvolutionalnets,andasofthis\n",
      "             writing,advancesindeeplearninghavebroughtthelatesttop-5errorrateinthis\n",
      "         contestdownto3.6percent,asshowninﬁgure.1.12\n",
      "           Deeplearninghasalsohadadramaticimpactonspeechrecognition.After\n",
      "          improvingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\n",
      "             startinginabout2000.Theintroductionofdeeplearning( ,; Dahletal.2010Deng\n",
      "              etal. etal. etal. ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\n",
      "                inasuddendropinerrorrates,withsomeerrorratescutinhalf.Weexplorethis\n",
      "      historyinmoredetailinsection.12.3\n",
      "          Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand\n",
      "            imagesegmentation( ,; Sermanetetal.2013Farabet2013Couprie etal.,; etal.,\n",
      "         2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan\n",
      "  etal.,).2012\n",
      "             Atthesametimethatthescaleandaccuracyofdeepnetworkshaveincreased,\n",
      "     2010 2011 2012 2013 2014 2015000 .005 .010 .015 .020 .025 .030 .  ILSVRC classiﬁcationerrorrate\n",
      "            Figure1.12:Decreasingerrorrateovertime.Sincedeepnetworksreachedthescale\n",
      "           necessarytocompeteintheImageNetLargeScaleVisualRecognitionChallenge,they\n",
      "            haveconsistentlywonthecompetitioneveryyear,yieldinglowerandlowererrorrates\n",
      "            eachtime.DatafromRussakovsky 2014b He 2015 etal.()andetal.().\n",
      "2 4  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "              sohasthecomplexityofthetasksthattheycansolve. () Goodfellowetal.2014d\n",
      "            showedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\n",
      "           transcribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\n",
      "             itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\n",
      "          elementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013\n",
      "            suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\n",
      "          relationshipsbetweensequences sequences andother ratherthanjustﬁxedinputs.\n",
      "          Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\n",
      "          anotherapplication:machinetranslation(Sutskever2014Bahdanau etal.,; etal.,\n",
      "2015).\n",
      "           Thistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\n",
      "            withtheintroductionofneuralTuringmachines(Graves2014etal.,)thatlearn\n",
      "            toreadfrommemory cellsandwritearbitrarycontenttomemory cells.Such\n",
      "           neuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\n",
      "             example,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\n",
      "           sortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\n",
      "          futureitcouldinprinciplebeappliedtonearlyanytask.\n",
      "            Anothercrowningachievementofdeeplearningisitsextensiontothedomainof\n",
      " reinforcementlearning        .Inthecontextofreinforcementlearning,anautonomous\n",
      "              agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\n",
      "         thehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\n",
      "             basedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\n",
      "           human-levelperformanceonmanytasks( ,).Deeplearninghas Mnihetal.2015\n",
      "         alsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics\n",
      "   ( ,). Finnetal.2015\n",
      "           Manyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning\n",
      "         isnowusedbymanytoptechnologycompanies, includingGoogle,Microsoft,\n",
      "        Facebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIA, andNEC.\n",
      "           Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\n",
      "          infrastructure.SoftwarelibrariessuchasTheano( ,; Bergstraetal.2010Bastien\n",
      "            etal. etal. ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobertetal.2011b\n",
      "             DistBelief( ,),Caﬀe(,),MXNet( ,),and Deanetal.2012 Jia2013 Chenetal.2015\n",
      "           TensorFlow( ,)haveallsupportedimportantresearchprojectsor Abadietal.2015\n",
      " commercialproducts.\n",
      "          Deeplearninghasalsomadecontributionstoothersciences.Modernconvolu-\n",
      "           tionalnetworksforobjectrecognitionprovideamodelofvisualprocessingthat\n",
      "          neuroscientistscanstudy( ,).Deeplearningalsoprovidesusefultools DiCarlo2013\n",
      "           forprocessingmassiveamountsofdataandmakingusefulpredictionsinscientiﬁc\n",
      "2 5  C HAP T E R 1 . I NT R O D UC T I O N\n",
      "             ﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteractinorder\n",
      "            tohelppharmaceuticalcompaniesdesignnewdrugs( ,),tosearch Dahletal.2014\n",
      "           forsubatomicparticles( ,),andtoautomaticallyparsemicroscope Baldietal.2014\n",
      "             imagesusedtoconstructa3-Dmapofthehumanbrain( , Knowles-Barleyetal.\n",
      "              2014).Weexpectdeeplearningtoappearinmoreandmorescientiﬁcﬁeldsinthe\n",
      "future.\n",
      "            Insummary,deeplearningisanapproachtomachinelearningthathasdrawn\n",
      "             heavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\n",
      "            developedoverthepastseveraldecades.Inrecentyears,deeplearninghasseen\n",
      "            tremendousgrowthinitspopularityandusefulness,largelyastheresultofmore\n",
      "          powerfulcomputers,largerdatasetsandtechniquestotraindeepernetworks.The\n",
      "            yearsaheadarefullofchallengesandopportunitiestoimprovedeeplearningeven\n",
      "       furtherandtobringittonewfrontiers.\n",
      "2 6 P a rt I\n",
      "   Applie d Math and Mac hi ne\n",
      " Learning B asi cs\n",
      "27           Thispartofthebookintroducesthebasicmathematicalconceptsneededto\n",
      "           understanddeeplearning.Webeginwithgeneralideasfromappliedmaththat\n",
      "             enableustodeﬁnefunctionsofmanyvariables,ﬁndthehighestandlowestpoints\n",
      "       onthesefunctions,andquantifydegreesofbelief.\n",
      "           Next,wedescribethefundamentalgoalsofmachinelearning.Wedescribehow\n",
      "           toaccomplishthesegoalsbyspecifyingamodelthatrepresentscertainbeliefs,\n",
      "           designingacostfunctionthatmeasureshowwellthosebeliefscorrespondwith\n",
      "          reality,andusingatrainingalgorithmtominimizethatcostfunction.\n",
      "            Thiselementaryframeworkisthebasisforabroadvarietyofmachinelearning\n",
      "          algorithms,includingapproachestomachinelearningthatarenotdeep. Inthe\n",
      "           subsequentpartsofthebook,wedevelopdeeplearningalgorithmswithinthis\n",
      "framework.\n",
      "2 8/\n",
      " C h a p t e r 2\n",
      " Li ne ar Al ge b r a\n",
      "            Linearalgebraisabranchofmathematicsthatiswidelyusedthroughoutscience\n",
      "            andengineering.Yetbecauselinearalgebraisaformofcontinuousratherthan\n",
      "          discretemathematics,manycomputerscientistshavelittleexperiencewithit.A\n",
      "          goodunderstandingoflinearalgebraisessentialforunderstandingandworking\n",
      "         withmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\n",
      "           thereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\n",
      "    thekeylinearalgebraprerequisites.\n",
      "              Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\n",
      "           youhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\n",
      "           sheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\n",
      "              Pedersen2006,).Ifyouhavehadnoexposureatalltolinearalgebra,thischapter\n",
      "              willteachyouenoughtoreadthisbook,butwehighlyrecommend thatyoualso\n",
      "          consultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas\n",
      "          Shilov1977().Thischaptercompletelyomitsmanyimportantlinearalgebratopics\n",
      "       thatarenotessentialforunderstandingdeeplearning.\n",
      "     2.1Scalars,Vectors,MatricesandTensors\n",
      "          Thestudyoflinearalgebrainvolvesseveraltypesofmathematicalobjects:\n",
      " • Sc a l a r s              :Ascalarisjustasinglenumber,incontrasttomostoftheother\n",
      "           objectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\n",
      "           Wewritescalarsinitalics.Weusuallygivescalarslowercasevariablenames.\n",
      "            Whenweintroducethem,wespecifywhatkindofnumbertheyare.For\n",
      "29/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "    example,wemightsay“Let  s∈ R        betheslopeoftheline,”whiledeﬁninga\n",
      "   real-valuedscalar,or“Let  n∈ N       bethenumberofunits,”whiledeﬁninga\n",
      "  naturalnumberscalar.\n",
      " •Vectors           : Avectorisanarrayofnumbers.Thenumbersarearrangedin\n",
      "            order.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\n",
      "          Typicallywegivevectorslowercasenamesinboldtypeface,suchasx .The\n",
      "            elementsofthevectorareidentiﬁedbywritingitsnameinitalictypeface,\n",
      "      withasubscript.Theﬁrstelementofxisx 1    ,thesecondelementisx 2 ,and\n",
      "                soon.Wealsoneedtosaywhatkindofnumbersarestoredinthevector.If\n",
      "   eachelementisin R    ,andthevectorhasn      elements,thenthevectorliesin\n",
      "        thesetformedbytakingtheCartesianproductof  Rn  times,denotedas Rn.\n",
      "             Whenweneedtoexplicitlyidentifytheelementsofavector,wewritethem\n",
      "      asacolumnenclosedinsquarebrackets:\n",
      " x =\n",
      "x 1\n",
      "x 2\n",
      "...\n",
      "x n\n",
      " . (2.1)\n",
      "            Wecanthinkofvectorsasidentifyingpointsinspace,witheachelement\n",
      "      givingthecoordinatealongadiﬀerentaxis.\n",
      "               Sometimesweneedtoindexasetofelementsofavector.Inthiscase,we\n",
      "             deﬁneasetcontainingtheindicesandwritethesetasasubscript.For\n",
      "  example,toaccessx 1,x 3andx 6    ,wedeﬁnethesetS ={ 1, 3, 6} andwrite\n",
      "x S   .Weusethe−         signtoindexthecomplementofaset.Forexamplex − 1is\n",
      "     thevectorcontainingallelementsofx exceptforx 1 ,andx − S  isthevector\n",
      "       containingallelementsofexceptforx x 1 ,x 3 andx 6.\n",
      " •Matrices             :Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed\n",
      "           bytwoindicesinsteadofjustone.Weusuallygivematricesuppercase\n",
      "      variablenameswithboldtypeface,suchasA    .Ifareal-valuedmatrixAhas\n",
      "  aheightofm   andawidthofn    ,thenwesaythat  A∈ Rm n × . Weusually\n",
      "              identifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\n",
      "         andtheindicesarelistedwithseparatingcommas.Forexample,A 1 1 , isthe\n",
      "   upperleftentryofAandA m , n       isthebottomrightentry.Wecanidentify\n",
      "     allthenumberswithverticalcoordinatei      bywritinga“”forthehorizontal :\n",
      "  coordinate.Forexample,A i , :      denotesthehorizontalcrosssectionofAwith\n",
      " verticalcoordinatei     .Thisisknownasthei-throwofA .Likewise,A : , iis\n",
      "3 0/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "             the-th of.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\n",
      "           amatrix,wewritethemasanarrayenclosedinsquarebrackets:\n",
      "A 1 1 ,A 1 2 ,\n",
      "A 2 1 ,A 2 2 ,\n",
      " . (2.2)\n",
      "           Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust\n",
      "              asingleletter.Inthiscase,weusesubscriptsaftertheexpressionbutdonot\n",
      "     convertanythingtolowercase.Forexample,f(A) i , j  giveselement( i,j )of\n",
      "         thematrixcomputedbyapplyingthefunctionto.fA\n",
      " •Tensors             :Insomecaseswewillneedanarraywithmorethantwoaxes.\n",
      "              Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\n",
      "              variablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”\n",
      "  withthistypeface:     .Weidentifytheelementof  atcoordinates(  i,j,k)\n",
      "  bywriting i , j , k.\n",
      "      Oneimportantoperationonmatricesisthetranspose    .Thetransposeofa\n",
      "             matrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\n",
      "diagonal             ,runningdownandtotheright,startingfromitsupperleftcorner.See\n",
      "              ﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\n",
      "   matrixasAA      ,anditisdeﬁnedsuchthat\n",
      "(A) i , j= A j , i . (2.3)\n",
      "            Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\n",
      "             transposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n",
      "3 1/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "                deﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,then\n",
      "             usingthetransposeoperatortoturnitintoastandardcolumnvector,forexample\n",
      " x= [x 1 ,x 2 ,x 3].\n",
      "                Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\n",
      "          canseethatascalarisitsowntranspose:aa= .\n",
      "               Wecanaddmatricestoeachother,aslongastheyhavethesameshape,just\n",
      "          byaddingtheircorrespondingelements: where CAB = + C i , j= A i , j +B i , j.\n",
      "                Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just\n",
      "         byperformingthatoperationoneachelementofamatrix:D=  a·B+cwhere\n",
      "D i , j  = aB· i , j +c.\n",
      "             Inthecontextofdeeplearning,wealsousesomelessconventionalnotation.We\n",
      "           allowtheadditionofamatrixandavector,yieldinganothermatrix:C=A+b,\n",
      "whereC i , j=A i , j+b j     .Inotherwords,thevectorb      isaddedtoeachrowofthe\n",
      "          matrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithb copiedinto\n",
      "         eachrowbeforedoingtheaddition.Thisimplicitcopyingofb  tomanylocations\n",
      "  iscalled . broadcasting\n",
      "    2.2MultiplyingMatricesandVectors\n",
      "           Oneofthemostimportantoperationsinvolvingmatricesismultiplicationoftwo\n",
      " matrices.The matrixproduct ofmatricesAandB   isathirdmatrixC .In\n",
      "      orderforthisproducttobedeﬁned,A       musthavethesamenumberofcolumnsas\n",
      "B  hasrows.IfA  isofshape  mn×andB  isofshape  np× ,thenC  isofshape\n",
      "  mp×             .Wecanwritethematrixproductjustbyplacingtwoormorematrices\n",
      "  together,forexample,\n",
      "  CAB= . (2.4)\n",
      "     Theproductoperationisdeﬁnedby\n",
      "C i , j=\n",
      "kA i , kB k , j . (2.5)\n",
      "             Notethatthestandardproductoftwomatricesisjustamatrixcontaining not\n",
      "             theproductoftheindividualelements.Suchanoperationexistsandiscalledthe\n",
      "          element-wiseproductHadamardproduct ,or ,andisdenotedas .AB\n",
      "The dotproduct   betweentwovectorsxandy   ofthesamedimensionality\n",
      "   isthematrixproductxy       .WecanthinkofthematrixproductC=ABas\n",
      " computingC i , j             asthedotproductbetweenrowofandcolumnof. iA jB\n",
      "3 2/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "         Matrixproductoperationshavemanyusefulpropertiesthatmakemathematical\n",
      "  analysis ofmatrices more con venient.For example, matrix multiplication is\n",
      "distributive:\n",
      "     ABCABAC (+) = +. (2.6)\n",
      "   Itisalsoassociative:\n",
      " ABCABC ( ) = ( ). (2.7)\n",
      "      Matrixmultiplicationiscommutative(thecondition not AB=BA doesnot\n",
      "          alwayshold),unlikescalarmultiplication.However,thedotproductbetweentwo\n",
      "  vectorsiscommutative:\n",
      "x yy=  x. (2.8)\n",
      "         Thetransposeofamatrixproducthasasimpleform:\n",
      "( )AB= BA . (2.9)\n",
      "             Thisenablesustodemonstrateequationbyexploitingthefactthatthevalue 2.8\n",
      "             ofsuchaproductisascalarandthereforeequaltoitsowntranspose:\n",
      "x y=\n",
      "xy\n",
      "= y x. (2.10)\n",
      "              Sincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\n",
      "            developacomprehensivelistofusefulpropertiesofthematrixproducthere,but\n",
      "        thereadershouldbeawarethatmanymoreexist.\n",
      "             Wenowknowenoughlinearalgebranotationtowritedownasystemoflinear\n",
      "equations:\n",
      "  Axb= (2.11)\n",
      "where  A∈ Rm n ×   isaknownmatrix,  b∈ Rm    isaknownvector,and  x∈ Rn isa\n",
      "           vectorofunknownvariableswewouldliketosolvefor.Eachelementx iofx isone\n",
      "      oftheseunknownvariables.EachrowofA   andeachelementofb  provideanother\n",
      "      constraint.Wecanrewriteequationas2.11\n",
      "A 1 : , x= b 1 (2.12)\n",
      "A 2 : , x= b 2 (2.13)\n",
      "   ... (2.14)\n",
      "A m , : x= b m (2.15)\n",
      "    orevenmoreexplicitlyas\n",
      "A 1 1 ,x 1 +A 1 2 ,x 2   + +···A 1 , nx n= b 1 (2.16)\n",
      "3 3/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "A 2 1 ,x 1 +A 2 2 ,x 2   + +···A 2 , nx n= b 2 (2.17)\n",
      "   ... (2.18)\n",
      "A m , 1x 1 +A m , 2x 2   + +···A m , nx n= b m . (2.19)\n",
      "        Matrix-vectorproductnotationprovidesamorecompactrepresentationfor\n",
      "   equationsofthisform.\n",
      "    2.3IdentityandInverseMatrices\n",
      "      Linearalgebraoﬀersapowerfultoolcalled matrixinversion   thatenablesusto\n",
      "        analyticallysolveequationformanyvaluesof. 2.11 A\n",
      "            Todescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity\n",
      "matrix              .Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\n",
      "            multiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\n",
      "   n-dimensionalvectorsasI n  .Formally,I n ∈ Rn n × ,and\n",
      "  ∀∈x Rn ,I n  xx= . (2.20)\n",
      "             Thestructureoftheidentitymatrixissimple:alltheentriesalongthemain\n",
      "               diagonalare1,whilealltheotherentriesarezero.Seeﬁgureforanexample. 2.2\n",
      "The matrixinverseofA  isdenotedasA− 1       ,anditisdeﬁnedasthematrix\n",
      " suchthat\n",
      "A− 1 AI= n . (2.21)\n",
      "         Wecannowsolveequationusingthefollowingsteps: 2.11\n",
      "  Axb= (2.22)\n",
      "A− 1 AxA= − 1 b (2.23)\n",
      "I n xA= − 1 b (2.24)\n",
      "\n",
      "  100\n",
      "  010\n",
      "  001\n",
      "\n",
      "I\n",
      "3 4/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      " xA= − 1 b. (2.25)\n",
      "          Ofcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1  .Wediscuss\n",
      "      theconditionsfortheexistenceofA− 1   inthefollowingsection.\n",
      "WhenA− 1          exists,severaldiﬀerentalgorithmscanﬁnditinclosedform.In\n",
      "              theory,thesameinversematrixcanthenbeusedtosolvetheequationmanytimes\n",
      "   fordiﬀerentvaluesofb.A− 1        isprimarilyusefulasatheoreticaltool,however,and\n",
      "           shouldnotactuallybeusedinpracticeformostsoftwareapplications.BecauseA− 1\n",
      "           canberepresentedwithonlylimitedprecisiononadigitalcomputer,algorithms\n",
      "               thatmakeuseofthevalueofcanusuallyobtainmoreaccurateestimatesof. b x\n",
      "    2.4LinearDependenceandSpan\n",
      "ForA− 1            toexist,equationmusthaveexactlyonesolutionforeveryvalueof 2.11\n",
      "               b.Itisalsopossibleforthesystemofequationstohavenosolutionsorinﬁnitely\n",
      "     manysolutionsforsomevaluesofb         .Itisnotpossible,however,tohavemorethan\n",
      "         onebutlessthaninﬁnitelymanysolutionsforaparticularb  ;ifbothxandyare\n",
      " solutions,then\n",
      "     zxy = α+(1 )−α (2.26)\n",
      "       isalsoasolutionforanyreal.α\n",
      "            Toanalyzehowmanysolutionstheequationhas,thinkofthecolumnsofAas\n",
      "        specifyingdiﬀerentdirectionswecantravelinfromtheorigin  (thepointspeciﬁed\n",
      "              bythevectorofallzeros),thendeterminehowmanywaysthereareofreachingb.\n",
      "     Inthisview,eachelementofx          speciﬁeshowfarweshouldtravelineachofthese\n",
      "  directions,withx i           specifyinghowfartomoveinthedirectionofcolumn:i\n",
      " Ax=\n",
      "ix iA : , i . (2.27)\n",
      "        Ingeneral,thiskindofoperationiscalleda linearcombination  .Formally,a\n",
      "      linearcombinationofsomesetofvectors{v(1 )     ,...,v( ) n}   isgivenbymultiplying\n",
      "  eachvectorv( ) i        byacorrespondingscalarcoeﬃcientandaddingtheresults:\n",
      "\n",
      "ic iv( ) i . (2.28)\n",
      "Thespan              ofasetofvectorsisthesetofallpointsobtainablebylinearcombination\n",
      "   oftheoriginalvectors.\n",
      "3 5/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      " DeterminingwhetherAx=b       hasasolutionthusamountstotestingwhetherb\n",
      "       isinthespanofthecolumnsofA       .Thisparticularspanisknownasthecolumn\n",
      "     spacerange ,orthe,of.A\n",
      "    InorderforthesystemAx=b       tohaveasolutionforallvaluesof  b∈ Rm,\n",
      "       wethereforerequirethatthecolumnspaceofA  beallof Rm    .Ifanypointin Rm\n",
      "            isexcludedfromthecolumnspace,thatpointisapotentialvalueofb thathas\n",
      "        nosolution.TherequirementthatthecolumnspaceofA  beallof Rmimplies\n",
      " immediatelythatA   musthaveatleastm   columns,thatis,  nm≥  .Otherwise,the\n",
      "        dimensionalityofthecolumnspacewouldbelessthanm    .Forexample,considera\n",
      "3×   2matrix.Thetargetb  is3-D,butx       isonly2-D,somodifyingthevalueofx\n",
      "          atbestenablesustotraceouta2-Dplanewithin R3     .Theequationhasasolution\n",
      "        ifandonlyifliesonthatplane. b\n",
      "Having  nm≥           isonlyanecessaryconditionforeverypointtohaveasolution.\n",
      "               Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto\n",
      "    beredundant.Considera2×        2matrixwherebothofthecolumnsareidentical.\n",
      "        Thishasthesamecolumnspaceasa2×       1matrixcontainingonlyonecopyofthe\n",
      "               replicatedcolumn.Inotherwords,thecolumnspaceisstilljustalineandfailsto\n",
      "   encompassallof R2      ,eventhoughtherearetwocolumns.\n",
      "       Formally,thiskindofredundancyisknownas lineardependence   .Asetof\n",
      " vectorsis  linearlyindependent          ifnovectorinthesetisalinearcombination\n",
      "                oftheothervectors. Ifweaddavectortoasetthatisalinearcombinationof\n",
      "                theothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s\n",
      "              span.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,\n",
      "        thematrixmustcontainatleastonesetofm    linearlyindependentcolumns.This\n",
      "             conditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11\n",
      "  everyvalueofb           .Notethattherequirementisforasettohaveexactlymlinearly\n",
      "    independentcolumns,notatleastm   .Nosetofm    -dimensionalvectorscanhave\n",
      " morethanm          mutuallylinearlyindependentcolumns,butamatrixwithmorethan\n",
      "        mcolumnsmayhavemorethanonesuchset.\n",
      "             Forthematrixtohaveaninverse,weadditionallyneedtoensurethatequa-\n",
      "          tionhas onesolutionforeachvalueof 2.11atmost b       .Todoso,weneedtomake\n",
      "      certainthatthematrixhasatmostm       columns.Otherwisethereismorethanone\n",
      "    wayofparametrizingeachsolution.\n",
      "       Together,thismeansthatthematrixmustbesquare     ,thatis,werequirethat\n",
      "m=n           andthatallthecolumnsbelinearlyindependent.Asquarematrixwith\n",
      "      linearlydependentcolumnsisknownas .singular\n",
      "IfA             isnotsquareorissquarebutsingular,solvingtheequationisstillpossible,\n",
      "            butwecannotusethemethodofmatrixinversiontoﬁndthesolution.\n",
      "3 6/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "              Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\n",
      "           alsopossibletodeﬁneaninversethatismultipliedontheright:\n",
      "AA− 1 = I. (2.29)\n",
      "          Forsquarematrices,theleftinverseandrightinverseareequal.\n",
      " 2.5Norms\n",
      "              Sometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\n",
      "         measurethesizeofvectorsusingafunctioncalledanorm  .Formally,theLpnorm\n",
      "  isgivenby\n",
      "||||x p=\n",
      "i|x i|p\n",
      "(2.30)\n",
      "      forp,p.∈ R≥1\n",
      "  Norms,includingtheLp      norm,arefunctionsmappingvectorstonon-negative\n",
      "         values.Onanintuitivelevel,thenormofavectorx    measuresthedistancefrom\n",
      "    theorigintothepointx       .Morerigorously,anormisanyfunctionf thatsatisﬁes\n",
      "  thefollowingproperties:\n",
      "    •⇒f() = 0 xx= \n",
      "           •≤f(+) xyff ()+x ()y(thetriangleinequality)\n",
      "    •∀∈|| α R,fα(x) = αf()x\n",
      "TheL2 norm,withp    = 2,isknownasthe  Euclideannorm   ,whichissimply\n",
      "          theEuclideandistancefromtheorigintothepointidentiﬁedbyx .TheL2norm\n",
      "             isusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,\n",
      "              withthesubscriptomitted. Itisalsocommontomeasurethesizeofavector 2\n",
      "   usingthesquaredL2       norm,whichcanbecalculatedsimplyasxx.\n",
      " ThesquaredL2        normismoreconvenienttoworkwithmathematicallyand\n",
      "  computationallythantheL2       normitself.Forexample,eachderivativeofthe\n",
      "squaredL2      normwithrespecttoeachelementofx     dependsonlyonthecorre-\n",
      "  spondingelementofx      ,whileallthederivativesoftheL2   normdependonthe\n",
      "      entirevector.Inmanycontexts,thesquaredL2    normmaybeundesirablebecause\n",
      "            itincreasesveryslowlyneartheorigin.Inseveralmachinelearningapplications,it\n",
      "           isimportanttodiscriminatebetweenelementsthatareexactlyzeroandelements\n",
      "                thataresmallbutnonzero.Inthesecases,weturntoafunctionthatgrowsatthe\n",
      "3 7/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "          samerateinalllocations,butthatretainsmathematicalsimplicity:theL1norm.\n",
      " TheL1    normmaybesimpliﬁedto\n",
      "||||x 1=\n",
      "i|x i |. (2.31)\n",
      " TheL1          normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\n",
      "           zeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\n",
      "      awayfrom0by,theL1   normincreasesby.\n",
      "             Wesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\n",
      "         elements.Someauthorsrefertothisfunctionasthe“L0    norm,”butthisisincorrect\n",
      "            terminology. Thenumberofnonzeroentriesinavectorisnotanorm,because\n",
      "   scalingthevectorbyα        doesnotchangethenumberofnonzeroentries.TheL1\n",
      "            normisoftenusedasasubstituteforthenumberofnonzeroentries.\n",
      "          OneothernormthatcommonlyarisesinmachinelearningistheL∞norm,\n",
      "   alsoknownasthe maxnorm         .Thisnormsimpliﬁestotheabsolutevalueofthe\n",
      "       elementwiththelargestmagnitudeinthevector,\n",
      "||||x ∞= max\n",
      "i|x i |. (2.32)\n",
      "              Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\n",
      "              ofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\n",
      " Frobeniusnorm:\n",
      "||||A F=\n",
      "i , jA2\n",
      "i , j , (2.33)\n",
      "     whichisanalogoustotheL2   normofavector.\n",
      "             Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,\n",
      "x yx= |||| 2||||y 2  cosθ, (2.34)\n",
      "        whereistheanglebetweenand. θ xy\n",
      "      2.6SpecialKindsofMatricesandVectors\n",
      "         Somespecialkindsofmatricesandvectorsareparticularlyuseful.\n",
      "Diagonal           matricesconsistmostlyofzerosandhavenonzeroentriesonlyalong\n",
      "    themaindiagonal. Formally,amatrixD     isdiagonalifandonlyifD i , j  =0for\n",
      "all i=j          . Wehavealreadyseenoneexampleofadiagonalmatrix: theidentity\n",
      "3 8/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "         matrix,whereallthediagonalentriesare1.Wewritediag(v    )todenoteasquare\n",
      "            diagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\n",
      "            Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\n",
      "    iscomputationallyeﬃcient.Tocomputediag(v)x      ,weonlyneedtoscaleeach\n",
      "elementx ibyv i   .Inotherwords,diag(v)x=  vx    .Invertingasquarediagonal\n",
      "             matrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\n",
      "   andinthatcase,diag(v)− 1=diag([1/v 1    ,...,1/v n]     ).Inmanycases,wemay\n",
      "          derivesomegeneralmachinelearningalgorithmintermsofarbitrarymatrices\n",
      "           butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\n",
      "   matricestobediagonal.\n",
      "             Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\n",
      "           diagonalmatrix.Nonsquarediagonalmatricesdonothaveinverses,butwecan\n",
      "         stillmultiplybythemcheaply.ForanonsquarediagonalmatrixD  ,theproduct\n",
      "Dx     willinvolvescalingeachelementofx     andeitherconcatenatingsomezerosto\n",
      "  theresult,ifD             istallerthanitiswide,ordiscardingsomeofthelastelementsof\n",
      "         thevector,ifiswiderthanitistall. D\n",
      "            A matrixisanymatrixthatisequaltoitsowntranspose: symmetric\n",
      " AA=  . (2.35)\n",
      "            Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\n",
      "             twoargumentsthatdoesnotdependontheorderofthearguments.Forexample,\n",
      "ifA      isamatrixofdistancemeasurements,withA i , j    givingthedistancefrompoint\n",
      "     ij topoint,thenA i , j= A j , i     becausedistancefunctionsaresymmetric.\n",
      "        Aunitvector unitnorm isavectorwith :\n",
      "||||x 2 = 1. (2.36)\n",
      " Avectorx  andavectoryareorthogonal   toeachotherifxy  = 0.Ifboth\n",
      "               vectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\n",
      " other.In Rn  ,atmostn        vectorsmaybemutuallyorthogonalwithnonzeronorm.\n",
      "              Ifthevectorsnotonlyareorthogonalbutalsohaveunitnorm,wecallthem\n",
      "orthonormal.\n",
      "An  orthogonalmatrix        isasquarematrixwhoserowsaremutuallyorthonor-\n",
      "      malandwhosecolumnsaremutuallyorthonormal:\n",
      "A AAA=  = I. (2.37)\n",
      "  Thisimpliesthat\n",
      "A− 1= A , (2.38)\n",
      "3 9/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "             soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\n",
      "         Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,\n",
      "            theirrowsarenotmerelyorthogonalbutfullyorthonormal.Thereisnospecial\n",
      "            termforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\n",
      " 2.7Eigendecomposition\n",
      "          Manymathematicalobjectscanbeunderstoodbetterbybreakingtheminto\n",
      "            constituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused\n",
      "       bythewaywechoosetorepresentthem.\n",
      "           Forexample,integerscanbedecomposedintoprimefactors.Thewaywe\n",
      "              representthenumberwillchangedependingonwhetherwewriteitinbaseten 12\n",
      "            orinbinary,butitwillalwaysbetruethat12=2×2×   3.Fromthisrepresentation\n",
      "              wecanconcludeusefulproperties,forexample,thatisnotdivisibleby,and 12 5\n",
      "          thatanyintegermultipleofwillbedivisibleby. 12 3\n",
      "             Muchaswecandiscoversomethingaboutthetruenatureofanintegerby\n",
      "            decomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat\n",
      "            showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\n",
      "        representationofthematrixasanarrayofelements.\n",
      "           Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\n",
      "decomposition            ,inwhichwedecomposeamatrixintoasetofeigenvectorsand\n",
      "eigenvalues.\n",
      "Aneigenvector   ofasquarematrixA   isanonzerovectorv  suchthatmulti-\n",
      "        plicationbyaltersonlythescaleof: A v\n",
      "  Avv= λ. (2.39)\n",
      " Thescalarλ   isknownastheeigenvalue     correspondingtothiseigenvector.(One\n",
      "   canalsoﬁnda lefteigenvector suchthatvA=λv   , butweareusually\n",
      "   concernedwithrighteigenvectors.)\n",
      "Ifv   isaneigenvectorofA      ,thensoisanyrescaledvectorsvfor    s,s∈ R= 0.\n",
      "Moreover,sv           stillhasthesameeigenvalue.Forthisreason,weusuallylookonly\n",
      "  foruniteigenvectors.\n",
      "   SupposethatamatrixAhasn   linearlyindependenteigenvectors{v(1 )    ,...,\n",
      "v( ) n}  withcorrespondingeigenvalues{λ 1     ,...,λ n}    . Wemayconcatenateallthe\n",
      "    eigenvectorstoformamatrixV    withoneeigenvectorpercolumn:V= [v(1 )    ,...,\n",
      "v( ) n          ].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [λ 1    ,...,\n",
      "4 0/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "λ n]        .The ofisthengivenby eigendecompositionA\n",
      "  AVλV = diag()− 1 . (2.40)\n",
      "          Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigen-\n",
      "             vectorsenablesustostretchspaceindesireddirections.Yetweoftenwantto\n",
      "decompose          matricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\n",
      "            usanalyzecertainpropertiesofthematrix,muchasdecomposinganintegerinto\n",
      "           itsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\n",
      "           Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n",
      "          cases,thedecompositionexistsbutinvolvescomplexratherthanrealnumbers.\n",
      "             Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof\n",
      "A v λv\n",
      "λ   u∈ R\n",
      "Au A\n",
      "vλ\n",
      "4 1/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "         matricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric\n",
      "          matrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\n",
      " andeigenvalues:\n",
      " AQQ =  , (2.41)\n",
      "whereQ       isanorthogonalmatrixcomposedofeigenvectorsofA ,and isa\n",
      "    diagonalmatrix.TheeigenvalueΛ i , i      isassociatedwiththeeigenvectorincolumni\n",
      "ofQ  ,denotedasQ : , i .BecauseQ       isanorthogonalmatrix,wecanthinkofAas\n",
      "   scalingspacebyλ i  indirectionv( ) i      .Seeﬁgureforanexample. 2.3\n",
      "    WhileanyrealsymmetricmatrixA     isguaranteedtohaveaneigendecomposi-\n",
      "            tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\n",
      "             sharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\n",
      "           arealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\n",
      "           usingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof\n",
      "          indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\n",
      "     ifalltheeigenvaluesareunique.\n",
      "      Theeigendecomposition ofa matrixtells usmanyuseful factsabout the\n",
      "              matrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\n",
      "            Theeigendecompositionofarealsymmetricmatrixcanalsobeusedtooptimize\n",
      "    quadraticexpressionsoftheformf(x) =xAx  subjectto||||x 2 = 1.Wheneverx\n",
      "     isequaltoaneigenvectorofA,f       takesonthevalueofthecorrespondingeigenvalue.\n",
      "   Themaximumvalueoff       withintheconstraintregionisthemaximumeigenvalue\n",
      "           anditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\n",
      "        Amatrixwhoseeigenvaluesareallpositiveiscalled  positivedeﬁnite .A\n",
      "          matrixwhoseeigenvaluesareallpositiveorzerovaluediscalled  positivesemideﬁ-\n",
      "nite         .Likewise,ifalleigenvaluesarenegative,thematrixis  negativedeﬁnite ,and\n",
      "         ifalleigenvaluesarenegativeorzerovalued,itis  negativesemideﬁnite .Positive\n",
      "       semideﬁnitematricesareinterestingbecausetheyguaranteethat ∀xx, Ax≥0.\n",
      "      Positivedeﬁnitematricesadditionallyguaranteethatx   Axx = 0 ⇒ = .\n",
      "   2.8SingularValueDecomposition\n",
      "             Insection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\n",
      "The   singularvaluedecomposition      (SVD)providesanotherwaytofactorize\n",
      "  amatrix,into  singularvectorsand  singularvalues     . TheSVDenablesusto\n",
      "           discoversomeofthesamekindofinformationastheeigendecompositionreveals;\n",
      "            however,theSVDismoregenerallyapplicable.Everyrealmatrixhasasingular\n",
      "           valuedecomposition,butthesameisnottrueoftheeigenvaluedecomposition.\n",
      "4 2/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "             Forexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,and\n",
      "       wemustuseasingularvaluedecompositioninstead.\n",
      "       RecallthattheeigendecompositioninvolvesanalyzingamatrixA todiscover\n",
      " amatrixV      ofeigenvectorsandavectorofeigenvaluesλ    suchthatwecanrewrite\n",
      " Aas\n",
      "  AVλV = diag()− 1 . (2.42)\n",
      "           Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteA\n",
      "     asaproductofthreematrices:\n",
      "  AUDV =  . (2.43)\n",
      " SupposethatA isan mn×  matrix.ThenU    isdeﬁnedtobean mm×matrix,\n",
      "                D V tobean matrix,and mn× tobean matrix. nn×\n",
      "            Eachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU\n",
      "andV        arebothdeﬁnedtobeorthogonalmatrices.ThematrixD   isdeﬁnedtobe\n",
      "         adiagonalmatrix.Notethatisnotnecessarilysquare. D\n",
      "     TheelementsalongthediagonalofD   areknownasthe  singularvaluesof\n",
      " thematrixA   .ThecolumnsofU   areknownasthe  left-singularvectors .The\n",
      "         columnsofareknownasasthe V right-singularvectors.\n",
      "        WecanactuallyinterpretthesingularvaluedecompositionofA  intermsof\n",
      "    theeigendecompositionoffunctionsofA    .Theleft-singularvectorsofA arethe\n",
      " eigenvectorsofAA    .Theright-singularvectorsofA   aretheeigenvectorsofAA.\n",
      "    ThenonzerosingularvaluesofA       arethesquarerootsoftheeigenvaluesofAA.\n",
      "     ThesameistrueforAA.\n",
      "               PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\n",
      "             generalizematrixinversiontononsquarematrices,aswewillseeinthenextsection.\n",
      "   2.9TheMoore-PenrosePseudoinverse\n",
      "             Matrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant\n",
      "                tomakealeft-inverseofamatrixsothatwecansolvealinearequation B A\n",
      "  Axy= (2.44)\n",
      "     byleft-multiplyingeachsidetoobtain\n",
      "  xBy= . (2.45)\n",
      "4 3/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "              Dependingonthestructureoftheproblem,itmaynotbepossibletodesigna\n",
      "     uniquemappingfromto.AB\n",
      "IfA              istallerthanitiswide,thenitispossibleforthisequationtohave\n",
      "  nosolution.IfA           iswiderthanitistall,thentherecouldbemultiplepossible\n",
      "solutions.\n",
      "The  Moore-Penrosepseudoinv erse       enablesustomakesomeheadwayin\n",
      "          thesecases.Thepseudoinverseofisdeﬁnedasamatrix A\n",
      "A+ =lim\n",
      "α  0(A  AI+α)− 1A . (2.46)\n",
      "           Practicalalgorithmsforcomputingthepseudoinversearebasednotonthisdeﬁni-\n",
      "     tion,butratherontheformula\n",
      "A+ = VD+U , (2.47)\n",
      "whereU,DandV     arethesingularvaluedecompositionofA   ,andthepseudoinverse\n",
      "D+   ofadiagonalmatrixD        isobtainedbytakingthereciprocalofitsnonzero\n",
      "        elementsthentakingthetransposeoftheresultingmatrix.\n",
      "WhenA           hasmorecolumnsthanrows,thensolvingalinearequationusingthe\n",
      "          pseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides\n",
      " thesolutionx=A+y   withminimalEuclideannorm||||x 2  amongallpossible\n",
      "solutions.\n",
      "WhenA             hasmorerowsthancolumns,itispossiblefortheretobenosolution.\n",
      "        Inthiscase,usingthepseudoinversegivesusthex forwhichAx   isascloseas\n",
      "          possibletointermsofEuclideannorm y ||−||Axy 2.\n",
      "   2.10TheTraceOperator\n",
      "             Thetraceoperatorgivesthesumofallthediagonalentriesofamatrix:\n",
      "Tr() =A\n",
      "iA i , i . (2.48)\n",
      "             Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\n",
      "           diﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing\n",
      "           matrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\n",
      "          analternativewayofwritingtheFrobeniusnormofamatrix:\n",
      "||||A F=\n",
      "Tr(AA ). (2.49)\n",
      "4 4/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "            Writinganexpressionintermsofthetraceoperatoropensupopportunitiesto\n",
      "         manipulatetheexpressionusingmanyusefulidentities. Forexample,thetrace\n",
      "      operatorisinvarianttothetransposeoperator:\n",
      "Tr() = Tr(AA ). (2.50)\n",
      "             Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto\n",
      "             movingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding\n",
      "       matricesallowtheresultingproducttobedeﬁned:\n",
      " Tr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\n",
      "  ormoregenerally,\n",
      "Tr(n\n",
      "i =1F( ) i) = Tr(F( ) nn − 1\n",
      "i =1F( ) i ). (2.52)\n",
      "            Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\n",
      "       diﬀerentshape.Forexample,forA∈ Rm n ×   andB∈ Rn m ×  ,wehave\n",
      " Tr( ) = Tr( ) ABBA (2.53)\n",
      "    eventhoughAB∈ Rm m ×   andBA∈ Rn n ×.\n",
      "              Anotherusefulfacttokeepinmindisthatascalarisitsowntrace:a=Tr(a).\n",
      "  2.11TheDeterminant\n",
      "      Thedeterminantofasquarematrix,denoteddet(A     ),isafunctionthatmaps\n",
      "           matricestorealscalars.Thedeterminantisequal totheproduct ofallthe\n",
      "            eigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\n",
      "             ofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\n",
      "             space.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\n",
      "               onedimension,causingittoloseallitsvolume.Ifthedeterminantis1,thenthe\n",
      "  transformationpreservesvolume.\n",
      "    2.12Example: PrincipalComponentsAnalysis\n",
      "    Onesimplemachinelearningalgorithm,   principalcomponentsanalysis(PCA),\n",
      "         canbederivedusingonlyknowledgeofbasiclinearalgebra.\n",
      "4 5/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "        Supposewehaveacollectionofpointsm{x(1 )     ,...,x( ) m  }in Rn  andwewant\n",
      "           toapplylossycompressiontothesepoints.Lossycompressionmeansstoringthe\n",
      "              pointsinawaythatrequireslessmemory butmaylosesomeprecision.Wewant\n",
      "      toloseaslittleprecisionaspossible.\n",
      "            Onewaywecanencodethesepointsistorepresentalower-dimensionalversion\n",
      "    ofthem.Foreachpointx( ) i ∈ Rn      wewillﬁndacorrespondingcodevectorc( ) i ∈ Rl.\n",
      "Ifl  issmallerthann           ,storingthecodepointswilltakelessmemory thanstoringthe\n",
      "             originaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode\n",
      "  foraninput,f(x) =c        ,andadecodingfunctionthatproducesthereconstructed\n",
      "       inputgivenitscode, .xx≈gf(())\n",
      "             PCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe\n",
      "             decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\n",
      " into Rn      .Let ,where g() = cDcD∈ Rn l ×     isthematrixdeﬁningthedecoding.\n",
      "            Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To\n",
      "         keeptheencodingproblemeasy,PCAconstrainsthecolumnsofD  tobeorthogonal\n",
      "    toeachother.(NotethatD       isstillnottechnically“anorthogonalmatrix”unless\n",
      " ln= .)\n",
      "            Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewe\n",
      "    canincreasethescaleofD : , i  ifwedecreasec i      proportionallyforallpoints.Togive\n",
      "          theproblemauniquesolution,weconstrainallthecolumnsofD  tohaveunit\n",
      "norm.\n",
      "              Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst\n",
      "              thingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for\n",
      "  eachinputpointx            .Onewaytodothisistominimizethedistancebetweenthe\n",
      " inputpointx  anditsreconstruction,g(c∗       ).Wecanmeasurethisdistanceusinga\n",
      "         norm.Intheprincipalcomponentsalgorithm,weusetheL2norm:\n",
      "c∗ = argmin  ||−||xg()c 2 . (2.54)\n",
      "     WecanswitchtothesquaredL2    norminsteadofusingtheL2 normitself\n",
      "        becausebothareminimizedbythesamevalueofc     .Bothareminimizedbythe\n",
      "  samevalueofc  becausetheL2       normisnon-negativeandthesquaringoperationis\n",
      "    monotonicallyincreasingfornon-negativearguments.\n",
      "c∗ = argmin  ||−||xg()c2\n",
      "2 . (2.55)\n",
      "     Thefunctionbeingminimizedsimpliﬁesto\n",
      "  ( ())x−gc   ( ())x−gc (2.56)\n",
      "4 6/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "     (bythedeﬁnitionoftheL2  norm,equation)2.30\n",
      "= x  xx− gg ()c−()c  xc+(g) g()c (2.57)\n",
      "   (bythedistributiveproperty)\n",
      "= x  xx−2  gg ()+c ()c g()c (2.58)\n",
      "   (becausethescalarg()c       xisequaltothetransposeofitself).\n",
      "             Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,\n",
      "       sincethistermdoesnotdependon:c\n",
      "c∗ = argmin−2x  gg ()+c ()c g.()c (2.59)\n",
      "           Tomakefurtherprogress,wemustsubstituteinthedeﬁnitionof:g()c\n",
      "c∗ = argmin−2x  Dcc+D Dc (2.60)\n",
      " = argmin−2x  Dcc+I l c (2.61)\n",
      "        (bytheorthogonalityandunitnormconstraintson)D\n",
      " = argmin−2x  Dcc+ c. (2.62)\n",
      "            Wecansolvethisoptimizationproblemusingvectorcalculus(seesectionif4.3\n",
      "       youdonotknowhowtodothis):\n",
      "∇(2−x  Dcc+c) = (2.63)\n",
      " −2D   xc+2= (2.64)\n",
      " cD=  x. (2.65)\n",
      "       Thismakesthealgorithmeﬃcient: wecanoptimallyencodex  usingjusta\n",
      "          matrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\n",
      "f() = xD x. (2.66)\n",
      "           Usingafurthermatrixmultiplication,wecanalsodeﬁnethePCAreconstruction\n",
      "operation:\n",
      "  rgf () = x (()) = xDD x. (2.67)\n",
      "4 7/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "       Next,weneedtochoosetheencodingmatrixD       .Todoso,werevisittheidea\n",
      "  ofminimizingtheL2       distancebetweeninputsandreconstructions.Sincewewill\n",
      "   usethesamematrixD          todecodeallthepoints,wecannolongerconsiderthe\n",
      "            pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\n",
      "        oferrorscomputedoveralldimensionsandallpoints:\n",
      "D∗ = argmin\n",
      "i , j\n",
      "x( ) i\n",
      "j −r(x( ) i) j2\n",
      "  subjecttoD DI= l .(2.68)\n",
      "     ToderivethealgorithmforﬁndingD∗       ,westartbyconsideringthecasewhere\n",
      "l   = 1. Inthiscase,D    isjustasinglevector,d    .Substitutingequationinto2.67\n",
      "          equationandsimplifyinginto,theproblemreducesto 2.68 Dd\n",
      "d∗ = argmin\n",
      "i||x( ) i −ddx( ) i||2\n",
      "2   subjectto||||d 2 = 1.(2.69)\n",
      "            Theaboveformulationisthemostdirectwayofperformingthesubstitutionbut\n",
      "              isnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthescalar\n",
      "valuedx( ) i     ontherightofthevectord    .Scalarcoeﬃcientsareconventionally\n",
      "              writtenontheleftofvectortheyoperateon.Wethereforeusuallywritesucha\n",
      " formulaas\n",
      "d∗ = argmin\n",
      "i||x( ) i −dx( ) id||2\n",
      "2   subjectto||||d 2 = 1,(2.70)\n",
      "           or,exploitingthefactthatascalarisitsowntranspose,as\n",
      "d∗ = argmin\n",
      "i||x( ) i −x( ) i dd||2\n",
      "2   subjectto||||d 2 = 1.(2.71)\n",
      "          Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements.\n",
      "               Atthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\n",
      "            designmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\n",
      "         Thiswillenableustousemorecompactnotation.Let  X∈ Rm n ×  bethematrix\n",
      "          deﬁnedbystackingallthevectorsdescribingthepoints,suchthatX i , :=x( ) i.\n",
      "      Wecannowrewritetheproblemas\n",
      "d∗ = argmin  ||−XXdd||2\n",
      "F   subjecttod  d= 1. (2.72)\n",
      "           Disregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\n",
      "  portionasfollows:\n",
      " argmin  ||−XXdd||2\n",
      "F (2.73)\n",
      "4 8/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      " = argminTr\n",
      "  XXdd− \n",
      "  XXdd−\n",
      "(2.74)\n",
      "  (byequation)2.49\n",
      " = argminTr(X  XX−Xdd −ddX  Xdd+XXdd )(2.75)\n",
      " = argminTr(X X)Tr(−XXdd  )Tr(−ddX  X)+Tr(ddXXdd)\n",
      "(2.76)\n",
      " = argmin−Tr(XXdd  )Tr(−ddX  X)+Tr(ddXXdd )(2.77)\n",
      "          (becausetermsnotinvolvingdonotaﬀectthe ) d argmin\n",
      " = argmin −2Tr(XXdd  )+Tr(ddXXdd ) (2.78)\n",
      "             (becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n",
      " = argmin −2Tr(XXdd  )+Tr(XXdddd ) (2.79)\n",
      "    (usingthesamepropertyagain).\n",
      "      Atthispoint,wereintroducetheconstraint:\n",
      " argmin  −2Tr(XXdd  )+Tr(XXdddd   )subjecttod  d= 1(2.80)\n",
      " = argmin −2Tr(XXdd  )+Tr(XXdd   )subjecttod  d= 1(2.81)\n",
      "   (duetotheconstraint)\n",
      " = argmin−Tr(XXdd   )subjecttod  d= 1 (2.82)\n",
      " = argmaxTr(XXdd   )subjecttod  d= 1 (2.83)\n",
      " = argmaxTr(dX   Xd d )subjectto  d= 1. (2.84)\n",
      "        Thisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,\n",
      " theoptimald     isgivenbytheeigenvectorofXX    correspondingtothelargest\n",
      "eigenvalue.\n",
      "       Thisderivationisspeciﬁctothecaseofl      =1andrecoversonlytheﬁrst\n",
      "            principalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\n",
      "4 9/\n",
      "   C HAP T E R 2 . L I NE AR AL G E B R A\n",
      "  components,thematrix D   isgivenbythe l    eigenvectorscorrespondingtothe\n",
      "           largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\n",
      "     writingthisproofasanexercise.\n",
      "          Linearalgebraisoneofthefundamentalmathematicaldisciplinesnecessaryto\n",
      "          understandingdeeplearning.Anotherkeyareaofmathematicsthatisubiquitous\n",
      "       inmachinelearningisprobabilitytheory,presentednext.\n",
      "5 0 C h a p t e r 3\n",
      "  P r obab i l i t y and I nformation\n",
      "Theory\n",
      "         Inthischapter,wedescribeprobabilitytheoryandinformationtheory.\n",
      "        Probabilitytheoryisamathematicalframeworkforrepresentinguncertain\n",
      "            statements.Itprovidesameansofquantifyinguncertaintyaswellasaxiomsfor\n",
      "         derivingnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuse\n",
      "              probabilitytheoryintwomajorways.First,thelawsofprobabilitytellushowAI\n",
      "           systemsshouldreason,sowedesignouralgorithmstocomputeorapproximate\n",
      "          variousexpressionsderivedusingprobabilitytheory.Second,wecanuseprobability\n",
      "          andstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\n",
      "           Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\n",
      "           engineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\n",
      "          primarilyinsoftwareengineering,withlimitedexposuretoprobabilitytheory,can\n",
      "     understandthematerialinthisbook.\n",
      "           Whileprobabilitytheoryallowsustomakeuncertainstatementsandtoreason\n",
      "            inthepresenceofuncertainty,informationtheoryenablesustoquantifytheamount\n",
      "     ofuncertaintyinaprobabilitydistribution.\n",
      "          Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,\n",
      "             youmaywishtoskipthischapterexceptforsection,whichdescribesthe 3.14\n",
      "           graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\n",
      "           youhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\n",
      "            besuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo\n",
      "          suggestthatyouconsultanadditionalresource,suchasJaynes2003().\n",
      "51     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "  3.1WhyProbability?\n",
      "           Manybranchesofcomputersciencedealmostlywithentitiesthatareentirely\n",
      "            deterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\n",
      "           executeeachmachineinstructionﬂawlessly.Errorsinhardwaredooccurbutare\n",
      "             rareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\n",
      "            forthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\n",
      "           relativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning\n",
      "     makesheavyuseofprobabilitytheory.\n",
      "         Machinelearningmustalwaysdealwithuncertainquantitiesandsometimes\n",
      "       stochastic(nondeterministic)quantities.Uncertaintyandstochasticitycanarise\n",
      "         frommanysources.Researchershavemadecompellingargumentsforquantifying\n",
      "           uncertaintyusingprobabilitysinceatleastthe1980s.Manyofthearguments\n",
      "         presentedherearesummarizedfromorinspiredbyPearl1988().\n",
      "            Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.\n",
      "            Infact,beyondmathematicalstatementsthataretruebydeﬁnition,itisdiﬃcult\n",
      "              tothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\n",
      "  guaranteedtooccur.\n",
      "      Therearethreepossiblesourcesofuncertainty:\n",
      "1.         Inherentstochasticityinthesystembeingmodeled.Forexample, most\n",
      "        interpretationsofquantummechanicsdescribethedynamicsofsubatomic\n",
      "          particlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\n",
      "           wepostulatetohaverandomdynamics,suchasahypotheticalcardgame\n",
      "            whereweassumethatthecardsaretrulyshuﬄedintoarandomorder.\n",
      "2.        Incompleteobservability.Evendeterministicsystemscanappearstochastic\n",
      "            whenwecannotobserveallthevariablesthatdrivethebehaviorofthe\n",
      "            system.Forexample,intheMontyHallproblem,agameshowcontestantis\n",
      "             askedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\n",
      "              door.Twodoorsleadtoagoatwhileathirdleadstoacar. Theoutcome\n",
      "          giventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint\n",
      "     ofview,theoutcomeisuncertain.\n",
      "3.            Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\n",
      "    the informationwehave observed, the discarded informationresults in\n",
      "          uncertaintyinthemodel’spredictions.Forexample,supposewebuilda\n",
      "             robotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n",
      "          robotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\n",
      "5 2     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "         thenthediscretizationmakestherobotimmediatelybecomeuncertainabout\n",
      "           theprecisepositionofobjects:eachobjectcouldbeanywherewithinthe\n",
      "       discretecellthatitwasobservedtooccupy.\n",
      "              Inmanycases,itismorepracticaltouseasimplebutuncertainrulerather\n",
      "              thanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\n",
      "            modelingsystemhastheﬁdelitytoaccommo dateacomplexrule.Forexample,the\n",
      "               simplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule\n",
      "               oftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto\n",
      "               ﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds\n",
      "            includingthecassowary,ostrichandkiwi...”isexpensivetodevelop,maintain\n",
      "             andcommunicateand,afterallthiseﬀort,isstillbrittleandpronetofailure.\n",
      "             Whileitshouldbeclearthatweneedameansofrepresentingandreasoning\n",
      "           aboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide\n",
      "          allthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory\n",
      "             wasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\n",
      "              howprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\n",
      "              cardsinapokergame.Thesekindsofeventsareoftenrepeatable.Whenwesay\n",
      "     thatanoutcomehasaprobability p        ofoccurring,itmeansthatifwerepeatedthe\n",
      "            experiment(e.g.,drawingahandofcards)inﬁnitelymanytimes,thenaproportion\n",
      "p             oftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot\n",
      "           seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\n",
      "              analyzesapatientandsaysthatthepatienthasa40percentchanceofhaving\n",
      "          theﬂu,thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymany\n",
      "              replicasofthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasof\n",
      "           thepatientwouldpresentwiththesamesymptomsyethavevaryingunderlying\n",
      "           conditions. Inthecaseofthedoctordiagnosingthepatient,weuseprobability\n",
      "  torepresenta  degreeofbelief       ,with1indicatingabsolutecertaintythatthe\n",
      "             patienthastheﬂuand0indicatingabsolutecertaintythatthepatientdoesnot\n",
      "              havetheﬂu.Theformerkindofprobability,relateddirectlytotheratesatwhich\n",
      "    eventsoccur,isknownas  frequentistprobability     ,whilethelatter,relatedto\n",
      "        qualitativelevelsofcertainty,isknownasBayesianprobability.\n",
      "           Ifwelistseveralpropertiesthatweexpectcommonsensereasoningabout\n",
      "            uncertaintytohave, thentheonlywaytosatisfythosepropertiesistotreat\n",
      "         Bayesianprobabilitiesasbehavingexactlythesameasfrequentistprobabilities.\n",
      "               Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\n",
      "               gamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\n",
      "              aswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n",
      "             hascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\n",
      "5 3     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "           assumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\n",
      "  see (). Ramsey1926\n",
      "             Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\n",
      "            providesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\n",
      "              betrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\n",
      "            orfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe\n",
      "           likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\n",
      "  3.2RandomVariables\n",
      "A  randomvariable          isavariablethatcantakeondiﬀerentvaluesrandomly.We\n",
      "            typicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\n",
      "            andthevaluesitcantakeonwithlowercasescriptletters.Forexample, x 1and x 2\n",
      "       arebothpossiblevaluesthattherandomvariablex    cantakeon.Forvector-valued\n",
      "       variables,wewouldwritetherandomvariableas x     andoneofitsvaluesas x .On\n",
      "               itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\n",
      "            mustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof\n",
      "  thesestatesare.\n",
      "          Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\n",
      "              isonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese\n",
      "             statesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\n",
      "            arenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\n",
      "    associatedwitharealvalue.\n",
      "  3.3ProbabilityDistributions\n",
      "A  probabilitydistribution         isadescriptionofhowlikelyarandomvariableor\n",
      "               setofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\n",
      "          describeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\n",
      "continuous.\n",
      "      3.3.1DiscreteVariablesandProbabilityMassFunctions\n",
      "          Aprobabilitydistributionoverdiscretevariablesmaybedescribedusingaproba-\n",
      "  bilitymassfunction        (PMF).Wetypicallydenoteprobabilitymassfunctionswith\n",
      " acapital P          .Oftenweassociateeachrandomvariablewithadiﬀerentprobability\n",
      "              massfunctionandthereadermustinferwhichPMFtousebasedontheidentity\n",
      "5 4     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "           oftherandomvariable,ratherthanonthenameofthefunction;P(x  )isusually\n",
      "     notthesameasy.P()\n",
      "            Theprobabilitymassfunctionmapsfromastateofarandomvariableto\n",
      "           theprobabilityofthatrandomvariabletakingonthatstate.Theprobability\n",
      "thatx=x  isdenotedasP(x       ),withaprobabilityof1indicatingthatx=xis\n",
      "       certainandaprobabilityof0indicatingthatx=x  isimpossible.Sometimes\n",
      "             todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\n",
      "explicitly:P(x=x        ).Sometimeswedeﬁneavariableﬁrst,thenuse∼  notationto\n",
      "         specifywhichdistributionitfollowslater:xx.∼P()\n",
      "            Probabilitymassfunctionscanactonmanyvariablesatthesametime.Such\n",
      "         aprobabilitydistributionovermanyvariablesisknownasa jointprobability\n",
      "distribution.P(x= x,y=y    )denotestheprobabilitythatx=xandy=y\n",
      "         simultaneously.Wemayalsowrite forbrevity. Px,y()\n",
      "       TobeaPMFonarandomvariablex  ,afunctionP   mustsatisfythefollowing\n",
      "properties:\n",
      "              •Thedomainofmustbethesetofallpossiblestatesofx. P\n",
      "   •∀∈xx,0 ≤P(x)≤1.        Animpossibleeventhasprobability,andnostate 0\n",
      "            canbelessprobablethanthat.Likewise,aneventthatisguaranteedto\n",
      "             happenhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\n",
      "•\n",
      "x ∈ xP(x       ) = 1.Werefertothispropertyasbeingnormalized .Without\n",
      "          thisproperty,wecouldobtainprobabilitiesgreaterthanonebycomputing\n",
      "       theprobabilityofoneofmanyeventsoccurring.\n",
      "       Forexample,considerasinglediscreterandomvariablexwithkdiﬀerent\n",
      "    states.Wecanplacea  uniformdistributiononx     —thatis,makeeachofits\n",
      "      statesequallylikely—bysettingitsPMFto\n",
      "  Px (= x i) =1\n",
      "k(3.1)\n",
      " foralli             .Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.\n",
      " Thevalue1\n",
      "k           ispositivebecauseisapositiveinteger.Wealsoseethat k\n",
      "\n",
      "i  Px (= x i) =\n",
      "i1\n",
      "k=k\n",
      "k = 1, (3.2)\n",
      "     sothedistributionisproperlynormalized.\n",
      "5 5     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "      3.3.2ContinuousVariablesandProbabilityDensityFunctions\n",
      "         Whenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\n",
      "          butionsusingaprobabilitydensityfunction(PDF)ratherthanaprobability\n",
      "         massfunction.Tobeaprobabilitydensityfunction,afunctionp  mustsatisfythe\n",
      " followingproperties:\n",
      "              •Thedomainofmustbethesetofallpossiblestatesofx. p\n",
      "             •∀∈≥ ≤ xx,px() 0 () . p Notethatwedonotrequirex 1.\n",
      "• pxdx()= 1.\n",
      "   Aprobabilitydensityfunctionp(x        )doesnotgivetheprobabilityofaspeciﬁc\n",
      "           statedirectly;insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\n",
      "     volumeisgivenby . δx pxδx()\n",
      "             Wecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa\n",
      "      setofpoints.Speciﬁcally,theprobabilitythatx   liesinsomeset S   isgivenbythe\n",
      " integralofp(x         )overthatset. Intheunivariateexample,theprobabilitythatx\n",
      "        liesintheintervalisgivenby []a,b\n",
      "[ ] a , bpxdx().\n",
      "            ForanexampleofaPDFcorrespondingtoaspeciﬁcprobabilitydensityover\n",
      "           acontinuousrandomvariable,considerauniformdistributiononanintervalof\n",
      "         therealnumbers.Wecandothiswithafunctionu(x; a,b ),whereaandb arethe\n",
      "    endpointsoftheinterval,with  b>a      .The“;”notationmeans“parametrizedby”;\n",
      " weconsiderx       tobetheargumentofthefunction,whileaandb areparameters\n",
      "             thatdeﬁnethefunction.Toensurethatthereisnoprobabilitymassoutsidethe\n",
      "  interval,wesayu(x; a,b   ) = 0forall x∈[ a,b  ] [ .Within a,b],u(x; a,b) =1\n",
      "b a − .We\n",
      "            canseethatthisisnon-negativeeverywhere.Additionally,itintegratesto1.We\n",
      "  oftendenotethatx      followstheuniformdistributionon[ a,b ] bywriting  x∼U( a,b).\n",
      "  3.4MarginalProbability\n",
      "             Sometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\n",
      "            toknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\n",
      "       distributionoverthesubsetisknownasthe   marginalprobabilitydistribution.\n",
      "       Forexample,supposewehavediscreterandomvariablesxandy   ,andweknow\n",
      "           P,(xy.Wecanﬁndxwiththe : ) P() sumrule\n",
      "     ∀∈xxx,P(= ) =x\n",
      "y     Px,y. (= xy= ) (3.3)\n",
      "5 6     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "          Thename“marginalprobability”comesfromtheprocessofcomputingmarginal\n",
      "      probabilitiesonpaper.WhenthevaluesofP( xy,      )arewritteninagridwith\n",
      "  diﬀerentvaluesofx     inrowsanddiﬀerentvaluesofy     incolumns,itisnaturalto\n",
      "        sumacrossarowofthegrid,thenwriteP(x        )inthemarginofthepaperjustto\n",
      "    therightoftherow.\n",
      "          Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:\n",
      "px() =\n",
      "  px,ydy. () (3.4)\n",
      "  3.5ConditionalProbability\n",
      "              Inmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\n",
      "       othereventhashappened.Thisiscalleda  conditionalprobability  .Wedenote\n",
      "   theconditionalprobabilitythaty=ygivenx=xasP(y=  y|x=x ).This\n",
      "       conditionalprobabilitycanbecomputedwiththeformula\n",
      "     Pyx (= y|x= ) =    Py,x (= yx= )\n",
      "  Px (= x ) . (3.5)\n",
      "      TheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.  Wecannotcompute\n",
      "         theconditionalprobabilityconditionedonaneventthatneverhappens.\n",
      "          Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\n",
      "          wouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\n",
      "              apersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\n",
      "            arandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\n",
      "            doesnotchange.Computingtheconsequencesofanactioniscalledmakingan\n",
      " interventionquery      .Interventionqueriesarethedomainof causalmodeling,\n",
      "       whichwedonotexploreinthisbook.\n",
      "      3.6TheChainRuleofConditionalProbabilities\n",
      "          Anyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\n",
      "      intoconditionaldistributionsoveronlyonevariable:\n",
      " P(x(1 )     ,...,x( ) n ) = (Px(1 ))Πn\n",
      "i =2 P(x( ) i |x(1 )     ,...,x( 1 ) i − ). (3.6)\n",
      "     Thisobservationisknownasthe chainrule ,or  productrule  ,ofprobability.\n",
      "           Itfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5\n",
      "5 7     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "       Forexample,applyingthedeﬁnitiontwice,weget\n",
      "           P,,P,P, (abc)= (ab|c)(bc)\n",
      "        P,PP (bc)= ( )bc| ()c\n",
      "             P,,P,PP. (abc)= (ab|c)( )bc| ()c\n",
      "    3.7IndependenceandConditionalIndependence\n",
      "  Tworandomvariablesxandyareindependent   iftheirprobabilitydistribution\n",
      "           canbeexpressedasaproductoftwofactors,oneinvolvingonlyx  andoneinvolving\n",
      " onlyy:\n",
      "            ∀∈∈xx,yyxy xy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\n",
      "  Tworandomvariablesxandyare  conditionallyindependent   givenarandom\n",
      "variablez     iftheconditionalprobabilitydistributionoverxandy   factorizesinthis\n",
      "     wayforeveryvalueofz:\n",
      "                       ∀∈∈∈ | || xx,yy,zzxy,p(= x,= yz x = ) = (zp= xzy = )(zp= yz= )z.\n",
      "(3.8)\n",
      "      Wecandenote independence andconditional independence withcompact\n",
      "notation:xy⊥  meansthatxandy  areindependent,while  xyz⊥|  meansthatx\n",
      "      andyareconditionallyindependentgivenz.\n",
      "    3.8Expectation,VarianceandCovariance\n",
      "Theexpectation ,or  expectedvalue   ,ofsomefunctionf(x    )withrespecttoa\n",
      " probabilitydistributionP(x       )istheaverage,ormeanvalue,thatf  takesonwhen\n",
      "x  isdrawnfromP          .Fordiscretevariablesthiscanbecomputedwithasummation:\n",
      "E x ∼ P[()] =fx\n",
      "x  Pxfx, ()() (3.9)\n",
      "         whileforcontinuousvariables,itiscomputedwithanintegral:\n",
      "E x ∼ p[()] =fx\n",
      " pxfxdx. ()() (3.10)\n",
      "5 8     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "             Whentheidentityofthedistributionisclearfromthecontext,wemaysimply\n",
      "             writethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\n",
      "              Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\n",
      "   subscriptentirely,asin E[f(x      )].Bydefault,wecanassumethat E[·  ]averagesover\n",
      "             thevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\n",
      "       noambiguity,wemayomitthesquarebrackets.\n",
      "    Expectationsarelinear,forexample,\n",
      "E x   [()+ ()] = αfxβgxα E x   [()]+fxβ E x [()]gx, (3.11)\n",
      "        whenandarenotdependenton. αβ x\n",
      "Thevariance             givesameasureofhowmuchthevaluesofafunctionofarandom\n",
      "variablex      varyaswesamplediﬀerentvaluesofx   fromitsprobabilitydistribution:\n",
      "Var(()) = fx E\n",
      "  (() [()]) fx− Efx2\n",
      " . (3.12)\n",
      "       Whenthevarianceislow,thevaluesoff(x      )clusterneartheirexpectedvalue.The\n",
      "          squarerootofthevarianceisknownasthe . standarddeviation\n",
      "Thecovariance          givessomesenseofhowmuchtwovaluesarelinearlyrelated\n",
      "          toeachother,aswellasthescaleofthesevariables:\n",
      "           Cov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)\n",
      "            Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\n",
      "                andarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\n",
      "            covarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\n",
      "             simultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto\n",
      "               takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\n",
      "        lowvalueandviceversa.Othermeasuressuchascorrelation  normalizethe\n",
      "             contributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\n",
      "            related,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.\n",
      "          Thenotionsofcovarianceanddependencearerelatedbutdistinctconcepts.\n",
      "           Theyarerelatedbecausetwovariablesthatareindependenthavezerocovariance,\n",
      "         andtwovariablesthathavenonzerocovariancearedependent.Independence,\n",
      "            however,isadistinctpropertyfromcovariance.Fortwovariablestohavezero\n",
      "          covariance,theremustbenolineardependencebetweenthem.Independenceis\n",
      "         astrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\n",
      "            nonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\n",
      "          zerocovariance.Forexample,supposeweﬁrstsamplearealnumberx froma\n",
      "     uniformdistributionovertheinterval[−1,      1].Wenextsamplearandomvariables.\n",
      "5 9     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      " Withprobability1\n",
      "2     ,wechoosethevalueofs     tobe. Otherwise,wechoosethe 1\n",
      " valueofs tobe−       1.Wecanthengeneratearandomvariabley byassigning\n",
      "y=sx .Clearly,xandy   arenotindependent,becausex  completelydetermines\n",
      "      themagnitudeof.However, . y Cov( ) = 0x,y\n",
      "The  covariancematrix   ofarandomvector  x∈ Rn isan  nn×  matrix,such\n",
      "that\n",
      "Cov() x i , j= Cov(x i ,x j ). (3.14)\n",
      "        Thediagonalelementsofthecovariancegivethevariance:\n",
      "Cov(x i ,x i) = Var(x i ). (3.15)\n",
      "   3.9Common ProbabilityDistributions\n",
      "          Severalsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\n",
      "learning.\n",
      "  3.9.1BernoulliDistribution\n",
      "The  Bernoullidistribution        isadistributionoverasinglebinaryrandomvariable.\n",
      "      Itiscontrolledbyasingleparameter φ∈[0,      1],whichgivestheprobabilityofthe\n",
      "          randomvariablebeingequalto1.Ithasthefollowingproperties:\n",
      "   P φ (= 1) = x (3.16)\n",
      "     P φ (= 0) = 1 x− (3.17)\n",
      "  Pxφ (= x ) = x  (1 )−φ1 − x(3.18)\n",
      "E x [] = xφ (3.19)\n",
      "Var x    () = (1 ) xφ−φ (3.20)\n",
      "  3.9.2MultinoulliDistribution\n",
      "Themultinoulli ,orcategorical,distribution      isadistributionoverasingledis-\n",
      "  cretevariablewithk   diﬀerentstates,wherek isﬁnite.1 Themultinoullidistribution\n",
      "1             “Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdaandpopularizedby\n",
      "          Murphy2012().Themultinoullidistributionisaspecialcaseofthe  m u l t i n omi a l d i s t r i b u t i on.\n",
      "        Amultinomialdistributionisthedistributionovervectorsin{0      , . . . , n}k  representinghowmany\n",
      "   timeseachofthe k    categoriesisvisitedwhen n       samplesaredrawnfromamultinoullidistribution.\n",
      "            Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\n",
      "         thattheyarereferringonlytothecase. n= 1\n",
      "60     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "    isparametrizedbyavector p∈[0,1]k − 1 ,wherep i    givestheprobabilityofthe\n",
      "i   -thstate.Theﬁnal,k      -thstate’sprobabilityisgivenby1 − 1p. Notethat\n",
      "  wemustconstrain 1 p≤        1. Multinoullidistributionsareoftenusedtoreferto\n",
      "            distributionsovercategoriesofobjects,sowedonotusuallyassumethatstate\n",
      "                1hasnumericalvalue1,andsoon.Forthisreason,wedonotusuallyneedto\n",
      "        computetheexpectationorvarianceofmultinoulli-distributedrandomvariables.\n",
      "          TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\n",
      "           butionovertheirdomain. Theyareabletodescribeanydistributionovertheir\n",
      "            domainnotsomuchbecausetheyareparticularlypowerfulbutratherbecausetheir\n",
      "             domainissimple;theymodeldiscretevariablesforwhichitisfeasibletoenumerate\n",
      "          allthestates.Whendealingwithcontinuousvariables,thereareuncountably\n",
      "            manystates,soanydistributiondescribedbyasmallnumberofparametersmust\n",
      "     imposestrictlimitsonthedistribution.\n",
      "  3.9.3GaussianDistribution\n",
      "         Themostcommonlyuseddistributionoverrealnumbersisthe  normaldistribu-\n",
      "      tion,alsoknownasthe : Gaussiandistribution\n",
      "  N(;xµ,σ2) =\n",
      "1\n",
      "2πσ2exp\n",
      "−1\n",
      "2σ2  ( )xµ−2\n",
      " . (3.21)\n",
      "           Seeﬁgureforaplotofthenormaldistributiondensityfunction. 3.1\n",
      "  Thetwoparameters  µ∈ Rand σ∈(0 ,∞    )controlthenormaldistribution.\n",
      " Theparameterµ            givesthecoordinateofthecentralpeak.Thisisalsothemeanof\n",
      " thedistribution: E[x] =µ         .Thestandarddeviationofthedistributionisgivenby\n",
      "     σ,andthevariancebyσ2.\n",
      "          WhenweevaluatethePDF,weneedtosquareandinvertσ    .Whenweneedto\n",
      "           frequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway\n",
      "        ofparametrizingthedistributionistouseaparameter β∈(0 ,∞   )tocontrolthe\n",
      "      precision,orinversevariance,ofthedistribution:\n",
      "  N(;xµ,β− 1) =\n",
      "β\n",
      "2πexp\n",
      "−1\n",
      "2  βxµ (−)2\n",
      " . (3.22)\n",
      "           Normaldistributionsareasensiblechoiceformanyapplications.Intheabsence\n",
      "            ofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould\n",
      "            take,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\n",
      "6 1     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "        −−−− 20. 15. 10. 05 00 05 10 15 20 ......000.005.010.015.020.025.030.035.040.p(x)   Maximumat= xµ\n",
      "  Inﬂectionpointsat\n",
      "   xµσ = ±\n",
      "       Figure3.1:Thenormaldistribution.ThenormaldistributionN(x; µ,σ2  ) exhibitsaclassic\n",
      "    “bellcurve”shape,withthex       coordinateofitscentralpeakgivenbyµ    ,andthewidthof\n",
      "   itspeakcontrolledbyσ      .Inthisexample,wedepictthe   standardnormaldistribution,\n",
      "     withand. µ= 0σ= 1\n",
      "            First,manydistributionswewishtomodelaretrulyclosetobeingnormal\n",
      " distributions.The  centrallimittheorem      showsthatthesumofmanyindepen-\n",
      "        dentrandomvariablesisapproximatelynormallydistributed. Thismeansthat\n",
      "          inpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\n",
      "            distributednoise,evenifthesystemcanbedecomposedintopartswithmore\n",
      " structuredbehavior.\n",
      "          Second,outofallpossibleprobabilitydistributionswiththesamevariance,\n",
      "          thenormaldistributionencodesthemaximumamountofuncertaintyoverthe\n",
      "             realnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone\n",
      "            thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\n",
      "           andjustifyingthisidearequiresmoremathematicaltoolsandispostponedto\n",
      " section .19.4.2\n",
      "    Thenormaldistributiongeneralizesto Rn        ,inwhichcaseitisknownasthe\n",
      "  multivariatenormaldistribution       .Itmaybeparametrizedwithapositive\n",
      "   deﬁnitesymmetricmatrix: Σ\n",
      "  N(; ) =xµ, Σ\n",
      "1\n",
      "(2)πndet() Σexp\n",
      "−1\n",
      "2  ( )xµ−Σ− 1  ( )xµ−\n",
      " .(3.23)\n",
      "6 2     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      " Theparameterµ          stillgivesthemeanofthedistribution,thoughnowitis\n",
      "   vectorvalued.Theparameter Σ      givesthecovariancematrixofthedistribution.\n",
      "              Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\n",
      "           manydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally\n",
      "          eﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σ toevaluate\n",
      "         thePDF.Wecaninsteadusea : precisionmatrixβ\n",
      "  N(;xµβ,− 1) =\n",
      "det()β\n",
      "(2)πnexp\n",
      "−1\n",
      "2  ( )xµ−  βxµ (−)\n",
      " .(3.24)\n",
      "             Weoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\n",
      "  versionistheisotropic        Gaussiandistribution,whosecovariancematrixisascalar\n",
      "   timestheidentitymatrix.\n",
      "    3.9.4ExponentialandLaplaceDistributions\n",
      "             Inthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\n",
      "    withasharppointatx        =0.Toaccomplishthis,wecanusetheexponential\n",
      "distribution:\n",
      " pxλλ (;) = 1 x ≥ 0    exp( )−λx. (3.25)\n",
      "      Theexponentialdistributionusestheindicatorfunction 1 x ≥ 0  toassignprobability\n",
      "      zerotoallnegativevaluesof.x\n",
      "            Acloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\n",
      "           ofprobabilitymassatanarbitrarypointistheµLaplacedistribution\n",
      "  Laplace(;) =xµ,γ1\n",
      "2γexp\n",
      "−  |−|xµ\n",
      "γ\n",
      " . (3.26)\n",
      "      3.9.5TheDiracDistributionandEmpiricalDistribution\n",
      "              Insomecases,wewishtospecifythatallthemassinaprobabilitydistribution\n",
      "             clustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing\n",
      "    the ,: Diracdeltafunctionδx()\n",
      "  pxδxµ. () = (−) (3.27)\n",
      "             TheDiracdeltafunctionisdeﬁnedsuchthatitiszerovaluedeverywhereexcept\n",
      "              0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\n",
      "  associateseachvaluex          withareal-valuedoutput;insteaditisadiﬀerentkindof\n",
      "6 3     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "   mathematicalobjectcalleda  generalizedfunction     thatisdeﬁnedintermsof\n",
      "             itspropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeing\n",
      "                thelimitpointofaseriesoffunctionsthatputlessandlessdensityonallpoints\n",
      "  otherthanzero.\n",
      " Bydeﬁningp(x  )tobeδ  shiftedby−µ     weobtainaninﬁnitelynarrowand\n",
      "        inﬁnitelyhighpeakofprobabilitydensitywhere.xµ= \n",
      "             AcommonuseoftheDiracdeltadistributionisasacomponentofanempirical\n",
      "distribution,\n",
      "ˆp() =x1\n",
      "mm\n",
      "i =1 δ(xx−( ) i ) (3.28)\n",
      "   whichputsprobabilitymass1\n",
      "m   oneachofthempointsx(1 )     ,...,x( ) m ,forming\n",
      "             agivendatasetorcollectionofsamples.TheDiracdeltadistributionisonly\n",
      "          necessarytodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscrete\n",
      "          variables,thesituationissimpler:anempiricaldistributioncanbeconceptualized\n",
      "          asamultinoullidistribution,withaprobabilityassociatedwitheachpossible\n",
      "       inputvaluethatissimplyequaltothe  empiricalfrequency    ofthatvalueinthe\n",
      " trainingset.\n",
      "           Wecanviewtheempiricaldistributionformedfromadatasetoftraining\n",
      "             examplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\n",
      "         onthisdataset. Another importantperspectiveontheempiricaldistributionis\n",
      "             thatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n",
      "  (seesection).5.5\n",
      "   3.9.6MixturesofDistributions\n",
      "           Itisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler\n",
      "       probabilitydistributions.Onecommon wayofcombiningdistributionsis to\n",
      " constructa  mixturedistribution        .Amixturedistributionismadeupofseveral\n",
      "          componentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\n",
      "           shouldgeneratethesampleisdeterminedbysamplingacomponentidentityfrom\n",
      "  amultinoullidistribution:\n",
      " P() =x\n",
      "i       PiPi, (= c )( = xc| ) (3.29)\n",
      "         wherecisthemultinoullidistributionovercomponentidentities. P()\n",
      "          Wehavealreadyseenoneexampleofamixturedistribution: theempirical\n",
      "          distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\n",
      "    componentforeachtrainingexample.\n",
      "6 4     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "          Themixturemodelisonesimplestrategyforcombiningprobabilitydistributions\n",
      "              tocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\n",
      "       probabilitydistributionsfromsimpleonesinmoredetail.\n",
      "             Themixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof\n",
      "  paramountimportancelater—the latentvariable      .Alatentvariableisarandom\n",
      "         variablethatwecannotobservedirectly.Thecomponentidentityvariablec ofthe\n",
      "          mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\n",
      "     thejointdistribution,inthiscase,P( xc,) =P(  xc|)P(c  ).ThedistributionP(c)\n",
      "      overthelatentvariableandthedistributionP(  xc|    )relatingthelatentvariables\n",
      "         tothevisiblevariablesdeterminestheshapeofthedistributionP(x  ),eventhough\n",
      "    itispossibletodescribeP(x       )withoutreferencetothelatentvariable.Latent\n",
      "      variablesarediscussedfurtherinsection.16.5\n",
      "          Averypowerfulandcommontypeofmixturemodelisthe  Gaussianmixture\n",
      "model    ,inwhichthecomponentsp(  x|c=i     )areGaussians.Eachcomponenthas\n",
      "   aseparatelyparametrizedmean µ( ) i andcovarianceΣ( ) i    .Somemixturescanhave\n",
      "          moreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\n",
      "  viatheconstraintΣ( ) i= Σ,i∀        .AswithasingleGaussiandistribution,themixture\n",
      "           ofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\n",
      "  diagonalorisotropic.\n",
      "            Inadditiontothemeansandcovariances,theparametersofaGaussianmixture\n",
      " specifythe  priorprobabilityα i=P(c=i   ) giventoeachcomponenti  .Theword\n",
      "        “prior”indicatesthatitexpressesthemodel’sbeliefsaboutc   beforeithasobserved\n",
      "x  .Bycomparison,P(  c| x  )isa  posteriorprobability    ,becauseitiscomputed\n",
      "  afterobservationofx      .AGaussianmixturemodelisa  universalapproximator\n",
      "            ofdensities,inthesensethatanysmoothdensitycanbeapproximatedwith\n",
      "            anyspeciﬁcnonzeroamountoferrorbyaGaussianmixturemodelwithenough\n",
      "components.\n",
      "        FigureshowssamplesfromaGaussianmixturemodel. 3.2\n",
      "     3.10UsefulPropertiesofCommon Functions\n",
      "       Certainfunctionsariseoften whileworkingwithprobabilitydistributions,\n",
      "        especiallytheprobabilitydistributionsusedindeeplearningmodels.\n",
      "       Oneofthesefunctionsisthe : logisticsigmoid\n",
      "σx() =1\n",
      "  1+exp()−x . (3.30)\n",
      "6 5     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "x 1x 2\n",
      "           Figure3.2: Sam plesfromaGaussianmixturemodel. Inthisexample,therearethree\n",
      "            components.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,\n",
      "               meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\n",
      "           covariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\n",
      "       direction.Thisexamplehasmorevariancealongthex 2   axisthanalongthex 1 axis.The\n",
      "            thirdcomponenthasafull-rankcovariancematrix,enablingittocontrolthevariance\n",
      "      separatelyalonganarbitrarybasisofdirections.\n",
      "        Thelogisticsigmoidiscommonlyusedtoproducetheφ    parameterofaBernoulli\n",
      "     distributionbecauseitsrangeis(0,        1),whichlieswithinthevalidrangeofvalues\n",
      " fortheφ            parameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3\n",
      " sigmoidfunctionsaturates        whenitsargumentisverypositiveorverynegative,\n",
      "             meaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits\n",
      "input.\n",
      "     Anothercommonlyencounteredfunctionisthe  softplusfunction(Dugas\n",
      "  etal.,):2001\n",
      "      ζx x. ()= log(1+exp()) (3.31)\n",
      "        Thesoftplusfunctioncanbeusefulforproducingtheβorσ    parameterofanormal\n",
      "     distributionbecauseitsrangeis(0 , ∞      ).Italsoarisescommonlywhenmanipulating\n",
      "           expressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\n",
      "         factthatitisasmoothed,or“softened,”versionof\n",
      "x+  = max(0),x. (3.32)\n",
      "         Seeﬁgureforagraphofthesoftplusfunction. 3.4\n",
      "            Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\n",
      "6 6     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "    − − 10 5 0 5 1000 .02 .04 .06 .08 .10 .σ x()\n",
      "     Figure3.3:Thelogisticsigmoidfunction.\n",
      "    − − 10 5 0 5 100246810ζ x()\n",
      "    Figure3.4:Thesoftplusfunction.\n",
      "6 7     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "them:\n",
      "σx() =exp()x\n",
      "  exp()+exp(0)x(3.33)\n",
      "d\n",
      "dx  σxσxσx () = ()(1−()) (3.34)\n",
      "   1 () = ()−σxσ−x (3.35)\n",
      "  log() = () σx−ζ−x (3.36)\n",
      "d\n",
      "dx  ζxσx ()= () (3.37)\n",
      "    ∀∈x(01),,σ− 1() = logxx\n",
      "  1−x\n",
      "(3.38)\n",
      "   ∀x>,ζ0− 1    () = log(exp()1) x x− (3.39)\n",
      " ζx()=x\n",
      "−∞ σydy() (3.40)\n",
      "   ζxζxx ()−(−) = (3.41)\n",
      " Thefunctionσ− 1(x   )iscalledthelogit        instatistics,butthistermisrarelyusedin\n",
      " machinelearning.\n",
      "          Equationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41\n",
      "        functionisintendedasasmoothedversionofthe   positivepartfunction,x+=\n",
      "max{0 ,x}         .Thepositivepartfunctionisthecounterpartofthe  negativepart\n",
      "function,x−=max{0 ,x−}        . Toobtainasmoothfunctionthatisanalogousto\n",
      "     thenegativepart,onecanuseζ(−x  ).Justasx     canberecoveredfromitspositive\n",
      "       partanditsnegativepartviatheidentityx+−x−=x      ,itisalsopossibletorecover\n",
      "x    usingthesamerelationshipbetweenζ(x )andζ(−x     ),asshowninequation.3.41\n",
      "  3.11Bayes’Rule\n",
      "         WeoftenﬁndourselvesinasituationwhereweknowP(  yx|    )andneedtoknow\n",
      "P(  xy|     ).Fortunately,ifwealsoknowP(x      ),wecancomputethedesiredquantity\n",
      "  usingBayes’rule:\n",
      "   P( ) =xy|    PP()x( )yx|\n",
      " P()y . (3.42)\n",
      "  NotethatwhileP(y          )appearsintheformula,itisusuallyfeasibletocompute\n",
      " P() =y\n",
      "x                PxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\n",
      "6 8     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "        Bayes’ruleisstraightforwardtoderivefrom thedeﬁnitionof conditional\n",
      "              probability,butitisusefultoknowthenameofthisformulasincemanytexts\n",
      "              refertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst\n",
      "            discoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\n",
      "    independentlydiscoveredbyPierre-SimonLaplace.\n",
      "     3.12TechnicalDetailsofContinuousVariables\n",
      "         Aproperformalunderstandingofcontinuousrandomvariablesandprobability\n",
      "           densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\n",
      "  mathematicsknownas  measuretheory       .Measuretheoryisbeyondthescopeof\n",
      "              thistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis\n",
      "  employedtoresolve.\n",
      "           Insection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\n",
      "   lyinginsomeset S     isgivenbytheintegralof p(x   )overtheset S  .Somechoices\n",
      " ofset S          canproduceparadoxes.Forexample,itispossibletoconstructtwo\n",
      "sets S 1and S 2 suchthat p(  x∈ S 1) + p(  x∈ S 2) > 1but S 1 ∩ S 2=∅ .These\n",
      "            setsaregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionof\n",
      "            realnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁned\n",
      "      bytransformingthesetofrationalnumbers.2     Oneofthekeycontributionsof\n",
      "              measuretheoryistoprovideacharacterizationofthesetofsetswecancompute\n",
      "           theprobabilityofwithoutencounteringparadoxes.Inthisbook,weintegrateonly\n",
      "            oversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever\n",
      "   becomesarelevantconcern.\n",
      "           Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\n",
      "    applytomostpointsin Rn         butdonotapplytosomecornercases.Measuretheory\n",
      "              providesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\n",
      "     asetissaidtohave  measurezero         .Wedonotformallydeﬁnethisconceptinthis\n",
      "             textbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset\n",
      "             ofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,\n",
      "within R2            ,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.\n",
      "            Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\n",
      "               thateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\n",
      "     numbershasmeasurezero,forinstance).\n",
      "      Anotherusefultermfrommeasuretheoryis almosteverywhere  .Aproperty\n",
      "             thatholdsalmosteverywhereholdsthroughoutallspaceexceptforonasetof\n",
      "2         TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\n",
      "69     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "           measurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\n",
      "           canbesafelyignoredformanyapplications.Someimportantresultsinprobability\n",
      "            theoryholdforalldiscretevaluesbuthold“almosteverywhere”onlyforcontinuous\n",
      "values.\n",
      "         Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\n",
      "           randomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\n",
      "  tworandomvariables, xand y  ,suchthaty=g(x ),whereg   isaninvertible,con-\n",
      "      tinuous,diﬀerentiabletransformation.Onemightexpectthatp y(y) =p x(g− 1(y)).\n",
      "     Thisisactuallynotthecase.\n",
      "         Asasimpleexample,supposewehavescalarrandomvariablesxandy .Suppose\n",
      "y=x\n",
      "2and  x∼U(0,     1).Ifweusetherulep y(y )=p x(2y )thenp y  willbe0\n",
      "    everywhereexcepttheinterval[0,1\n",
      "2          ] 1 ,anditwillbeonthisinterval.Thismeans\n",
      "\n",
      "p y ()=ydy1\n",
      "2 , (3.43)\n",
      "            whichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake.\n",
      "              Theproblemwiththisapproachisthatitfailstoaccountforthedistortionof\n",
      "    spaceintroducedbythefunctiong     .Recallthattheprobabilityofx  lyinginan\n",
      "    inﬁnitesimallysmallregionwithvolumeδx  isgivenbyp(x)δx .Sinceg canexpand\n",
      "      orcontractspace,theinﬁnitesimalvolumesurroundingxinx  spacemayhave\n",
      "    diﬀerentvolumeinspace.y\n",
      "               Toseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\n",
      "  preservetheproperty\n",
      "|p y (())= gxdy||p x ()xdx.| (3.44)\n",
      "    Solvingfromthis,weobtain\n",
      "p y() = yp x(g− 1())y∂x\n",
      "∂y(3.45)\n",
      " orequivalently\n",
      "p x() = xp y(())gx∂gx()\n",
      "∂x . (3.46)\n",
      "          Inhigherdimensions,thederivativegeneralizestothedeterminantofthe J a c o bi a n\n",
      "   m a t r i x—thematrixwithJ i , j=∂ x i\n",
      "∂ y j       .Thus,forreal-valuedvectorsand,xy\n",
      "p x() = xp y(())gxdet∂g()x\n",
      "∂x  . (3.47)\n",
      "7 0     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "  3.13InformationTheory\n",
      "       Informationtheory isa branchofappliedmathematicsthatrevolves around\n",
      "            quantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\n",
      "            tostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\n",
      "          communicationviaradiotransmission.Inthiscontext,informationtheorytellshow\n",
      "            todesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\n",
      "          speciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof\n",
      "          machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\n",
      "            wheresomeofthesemessagelengthinterpretationsdonotapply.Thisﬁeldis\n",
      "           fundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\n",
      "            textbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\n",
      "        probabilitydistributionsortoquantifysimilaritybetweenprobabilitydistributions.\n",
      "            Formoredetailoninformationtheory,seeCoverandThomas2006 MacKay ()or\n",
      "().2003\n",
      "           Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely\n",
      "         eventhas occurredismore informative thanlearningthatalikelyeventhas\n",
      "            occurred.Amessagesaying“thesunrosethismorning”issouninformativeas\n",
      "              tobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis\n",
      "   morning”isveryinformative.\n",
      "            Wewouldliketoquantifyinformationinawaythatformalizesthisintuition.\n",
      "•           Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\n",
      "          eventsthatareguaranteedtohappenshouldhavenoinformationcontent\n",
      "whatsoever.\n",
      "        •Lesslikelyeventsshouldhavehigherinformationcontent.\n",
      "•         Independenteventsshouldhaveadditiveinformation.Forexample,ﬁnding\n",
      "              outthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\n",
      "             muchinformationasﬁndingoutthatatossedcoinhascomeupasheads\n",
      "once.\n",
      "         Tosatisfyallthreeoftheseproperties,wedeﬁnethe s e l f - i nf o r m a t i o n ofan\n",
      "    eventxtobe = x\n",
      "   IxPx. () = log− () (3.48)\n",
      "     Inthisbook,wealwaysuselog      tomeanthenaturallogarithm,withbasee .Our\n",
      " deﬁnitionofI(x      )isthereforewritteninunitsof na t s      .Onenatistheamountof\n",
      "       informationgainedbyobservinganeventofprobability1\n",
      "e    .Othertextsusebase-2\n",
      "7 1     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "   logarithmsandunitscalledbitsorshannons     ;informationmeasuredinbitsis\n",
      "       justarescalingofinformationmeasuredinnats.\n",
      "Whenx          iscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,\n",
      "              butsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\n",
      "             withunitdensitystillhaszeroinformation,despitenotbeinganeventthatis\n",
      "  guaranteedtooccur.\n",
      "           Self-informationdealsonlywithasingleoutcome.Wecanquantifytheamount\n",
      "          ofuncertaintyinanentireprobabilitydistributionusingtheShannonentropy,\n",
      "H() = x E x ∼ P[()] = Ix− E x ∼ P    [log()]Px, (3.49)\n",
      " alsodenotedH(P           ).Inotherwords,theShannonentropyofadistributionisthe\n",
      "            expectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\n",
      "                alowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\n",
      "           arediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.\n",
      "          Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\n",
      "            havelowentropy;distributionsthatareclosertouniformhavehighentropy.See\n",
      "     ﬁgureforademonstration.When 3.5 x     iscontinuous,theShannonentropyis\n",
      "    knownasthediﬀerentialentropy.\n",
      "      IfwehavetwoseparateprobabilitydistributionsP(x )andQ(x   )overthesame\n",
      " randomvariablex          ,wecanmeasurehowdiﬀerentthesetwodistributionsareusing\n",
      "   theKullback-Leibler(KL)divergence:\n",
      "D K L ( ) = PQ E x ∼ P\n",
      "log Px()\n",
      "Qx()\n",
      "= E x ∼ P        [log()log()] Px−Qx.(3.50)\n",
      "             Inthecaseofdiscretevariables,itistheextraamountofinformation(measured\n",
      "               inbitsifweusethebase-logarithm,butinmachinelearningweusuallyusenats 2\n",
      "           andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\n",
      "  fromprobabilitydistributionP          ,whenweuseacodethatwasdesignedtominimize\n",
      "        thelengthofmessagesdrawnfromprobabilitydistribution.Q\n",
      "          TheKLdivergencehasmanyusefulproperties,mostnotablybeingnon-negative.\n",
      "        TheKLdivergenceis0ifandonlyifPandQ     arethesamedistributioninthe\n",
      "            caseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous\n",
      "          variables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence\n",
      "           betweentwodistributions,itisoftenconceptualizedasmeasuringsomesortof\n",
      "            distancebetweenthesedistributions.Itisnotatruedistancemeasurebecauseit\n",
      "  isnotsymmetric:D K L( PQ)=D K L(QP  )forsomePandQ  .Thisasymmetry\n",
      "            meansthatthereareimportantconsequencestothechoiceofwhethertouse\n",
      "D K L   ( )PQorD K L       ( )QP.Seeﬁgureformoredetail. 3.6\n",
      "7 2     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "     00 02 04 06 08 10 . . . . . .00 .01 .02 .03 .04 .05 .06 .07 .   Shannonentropyinnats\n",
      "           Figure3.5: Shannon entropyofabinaryrandomvariable. Thisplotshowshowdistri-\n",
      "           butionsthatareclosertodeterministichavelowShannonentropywhiledistributions\n",
      "              thatareclosetouniformhavehighShannonentropy.Onthehorizontalaxis,weplot\n",
      "p               ,theprobabilityofabinaryrandomvariablebeingequalto.Theentropyisgiven 1\n",
      " by( p−1)log(1 −p)   −pplog .Whenp       isnear0,thedistributionisnearlydeterministic,\n",
      "        becausetherandomvariableisnearlyalways0.Whenp     isnear1,thedistributionis\n",
      "          nearlydeterministic,becausetherandomvariableisnearlyalways1.Whenp= 0. 5,the\n",
      "           entropyismaximal,becausethedistributionisuniformoverthetwooutcomes.\n",
      "           AquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o s s - e n t r o p y\n",
      "H( P,Q) =H(P )+D K L( PQ         ),whichissimilartotheKLdivergencebutlacking\n",
      "    thetermontheleft:\n",
      " HP,Q( ) = − E x ∼ P   log()Qx. (3.51)\n",
      "     Minimizingthecross-entropywithrespecttoQ    isequivalenttominimizingthe\n",
      "          KLdivergence,becausedoesnotparticipateintheomittedterm. Q\n",
      "           Whencomputingmanyofthesequantities,itiscommontoencounterexpres-\n",
      "    sionsoftheform0log         0.Byconvention,inthecontextofinformationtheory,we\n",
      "    treattheseexpressionsaslim x → 0   xxlog= 0.\n",
      "   3.14StructuredProbabilisticModels\n",
      "         Machinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\n",
      "         largenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\n",
      "          directinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n",
      "          describetheentirejointprobabilitydistributioncanbeveryineﬃcient(both\n",
      "  computationallyandstatistically).\n",
      "7 3     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "xProbability Densityq∗= argminq D K L() p q \n",
      "p x()\n",
      "q∗() x\n",
      "xProbability Densityq∗= argminq D K L() q p \n",
      "p x()\n",
      "q∗() x\n",
      "           Figure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistribution p( x )and\n",
      "      wishtoapproximateitwithanotherdistribution q( x      ).Wehavethechoiceofminimizing\n",
      "either D KL( p q )or D KL( q p           ).Weillustratetheeﬀectofthischoiceusingamixtureof\n",
      "  twoGaussiansfor p     ,andasingleGaussianfor q      . Thechoiceofwhichdirectionofthe\n",
      "           KLdivergencetouseisproblemdependent.Someapplicationsrequireanapproximation\n",
      "           thatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\n",
      "          probability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\n",
      "            probabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\n",
      "             directionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach\n",
      "    application. ( L e f t )Theeﬀectofminimizing D KL( p q      ).Inthiscase,weselecta q thathas\n",
      "  highprobabilitywhere p   hashighprobability.When p  hasmultiplemodes, q  choosesto\n",
      "               blurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\n",
      "  eﬀectofminimizing D KL( q p      ).Inthiscase,weselecta q    thathaslowprobabilitywhere p\n",
      "   haslowprobability.When p        hasmultiplemodesthataresuﬃcientlywidelyseparated,as\n",
      "               inthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,toavoidputting\n",
      "        probabilitymassinthelow-probabilityareasbetweenmodesof p   .Here,weillustrate\n",
      "  theoutcomewhen q           ischosentoemphasizetheleftmode.Wecouldalsohaveachieved\n",
      "               anequalvalueoftheKLdivergencebychoosingtherightmode. Ifthemodesarenot\n",
      "            separatedbyasuﬃcientlystronglow-probabilityregion,thenthisdirectionoftheKL\n",
      "       divergencecanstillchoosetoblurthemodes.\n",
      "7 4     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "           Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\n",
      "           cansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\n",
      "       Forexample,supposewehavethreerandomvariables:a,bandc  .Supposethat\n",
      "a    inﬂuencesthevalueofb ,andb    inﬂuencesthevalueofc  ,butthataandcare\n",
      " independentgivenb         .Wecanrepresenttheprobabilitydistributionoverallthree\n",
      "         variablesasaproductofprobabilitydistributionsovertwovariables:\n",
      "       p,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\n",
      "         Thesefactorizationscangreatlyreducethenumberofparametersneeded\n",
      "            todescribethedistribution.Eachfactorusesanumberofparametersthatis\n",
      "              exponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\n",
      "              reducethecostofrepresentingadistributionifweareabletoﬁndafactorization\n",
      "    intodistributionsoverfewervariables.\n",
      "            Wecandescribethesekindsoffactorizationsusinggraphs.Here,weusethe\n",
      "               word“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnected\n",
      "           toeachotherwithedges. Whenwerepresentthefactorizationofaprobability\n",
      "     distributionwith agraph, wecallita   structuredprobabilisticmodel, or\n",
      " graphicalmodel.\n",
      "          Therearetwomainkindsofstructuredprobabilisticmodels:directedand\n",
      "        undirected.BothkindsofgraphicalmodelsuseagraphG   inwhicheachnode\n",
      "        inthegraph corresponds to arandomvariable, andanedge connectingtwo\n",
      "           randomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect\n",
      "     interactionsbetweenthosetworandomvariables.\n",
      "Directed      modelsusegraphs withdirectededges, andthey represent fac-\n",
      "         torizationsintoconditionalprobabilitydistributions,asintheexampleabove.\n",
      "          Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\n",
      "          thedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\n",
      "    giventheparentsofx i   ,denotedPa G(x i):\n",
      "p() = x\n",
      "i p(x i  |Pa G(x i  )). (3.53)\n",
      "              Seeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7\n",
      "  distributionsitrepresents.\n",
      "Undirected         modelsusegraphswithundirectededges,andtheyrepresent\n",
      "            factorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n",
      "              areusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall\n",
      "    connectedtoeachotherinG     iscalledaclique.EachcliqueC( ) i  inanundirected\n",
      "     modelisassociatedwithafactorφ( ) i(C( ) i      ).Thesefactorsarejustfunctions,not\n",
      "7 5     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "aa\n",
      "ccbb\n",
      "eedd\n",
      "        Figure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande  .Thisgraph\n",
      "        correspondstoprobabilitydistributionsthatcanbefactoredas\n",
      "              p,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\n",
      "             Thisgraphicalmodelenablesustoquicklyseesomepropertiesofthedistribution.For\n",
      "              example,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.\n",
      "         probabilitydistributions. Theoutputofeachfactormustbenon-negative,but\n",
      "               thereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\n",
      "distribution.\n",
      "        Theprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o na l tothe\n",
      "           productofallthesefactors—assignmentsthatresultinlargerfactorvaluesare\n",
      "               morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\n",
      "     thereforedividebyanormalizingconstantZ       ,deﬁnedtobethesumorintegral\n",
      "       overallstatesoftheproductoftheφ       functions,inordertoobtainanormalized\n",
      " probabilitydistribution:\n",
      "p() = x1\n",
      "Z\n",
      "iφ( ) i\n",
      "C( ) i\n",
      " . (3.55)\n",
      "             Seeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\n",
      "   probabilitydistributionsitrepresents.\n",
      "     Keep inmind thatthesegraphical representations offactorizations are a\n",
      "         languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\n",
      "           familiesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\n",
      "          ofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o n ofa\n",
      "          probabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\n",
      "ways.\n",
      "            Throughoutpartsandofthisbook,weusestructuredprobabilisticmodels III\n",
      "7 6     C HAP T E R 3 . P R O B AB I L I T Y AND I NF O R M A T I O N T HE O R Y\n",
      "aa\n",
      "ccbb\n",
      "eedd\n",
      "        Figure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande .This\n",
      "         graphcorrespondstoprobabilitydistributionsthatcanbefactoredas\n",
      "    p,,,, (abcde) =1\n",
      "Zφ( 1 )  ( )abc,,φ( 2 ) ()bd,φ( 3 )  ()ce,. (3.56)\n",
      "             Thisgraphicalmodelenablesustoquicklyseesomepropertiesofthedistribution.For\n",
      "              example,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.\n",
      "          merelyasalanguagetodescribewhichdirectprobabilisticrelationshipsdiﬀerent\n",
      "         machinelearningalgorithmschoosetorepresent.Nofurtherunderstandingof\n",
      "           structuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,in\n",
      "           part,whereweexplorestructuredprobabilisticmodelsinmuchgreaterdetail. III\n",
      "           Thischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\n",
      "           mostrelevanttodeeplearning.Onemoresetoffundamentalmathematicaltools\n",
      "  remains:numericalmethods.\n",
      "7 7 C h a p t e r 4\n",
      " N um e r i c al C omput at i on\n",
      "          Machinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\n",
      "          tation.Thistypicallyreferstoalgorithmsthatsolvemathematicalproblemsby\n",
      "           methodsthatupdateestimatesofthesolutionviaaniterativeprocess,rather\n",
      "           thananalyticallyderivingaformulatoprovideasymbolicexpressionforthe\n",
      "          correctsolution.Commonoperationsincludeoptimization(ﬁndingthevalueofan\n",
      "           argumentthatminimizesormaximizesafunction)andsolvingsystemsoflinear\n",
      "           equations.Evenjustevaluatingamathematicalfunctiononadigitalcomputercan\n",
      "           bediﬃcultwhenthefunctioninvolvesrealnumbers,whichcannotberepresented\n",
      "      preciselyusingaﬁniteamountofmemory.\n",
      "   4.1OverﬂowandUnderﬂow\n",
      "          Thefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer\n",
      "             isthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber\n",
      "            ofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursome\n",
      "           approximationerrorwhenwerepresentthenumberinthecomputer.Inmany\n",
      "           cases,thisisjustroundingerror.Roundingerrorisproblematic,especiallywhen\n",
      "           itcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\n",
      "              theorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\n",
      " roundingerror.\n",
      "         Oneformofroundingerrorthatisparticularlydevastatingis under ﬂo w.\n",
      "           Underﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\n",
      "           behavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall\n",
      "           positivenumber. Forexample,weusuallywanttoavoiddivisionbyzero(some\n",
      "78   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "           softwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\n",
      "            resultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\n",
      "   isusuallytreatedas−∞          ,whichthenbecomesnot-a-numberifitisusedformany\n",
      "  furtherarithmeticoperations).\n",
      "       Anotherhighlydamagingformofnumericalerrorisoverﬂow  .Overﬂowoccurs\n",
      "       whennumberswithlargemagnitudeareapproximatedas∞or−∞ .Further\n",
      "         arithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.\n",
      "           Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowand\n",
      "  overﬂowisthe  softmaxfunction        .Thesoftmaxfunctionisoftenusedtopredict\n",
      "         theprobabilitiesassociatedwithamultinoullidistribution.Thesoftmaxfunction\n",
      "   isdeﬁnedtobe\n",
      "softmax()x i=exp(x i)n\n",
      "j =1exp(x j) . (4.1)\n",
      "     Considerwhathappenswhenallthex i    areequaltosomeconstantc .Analytically,\n",
      "          wecanseethatalltheoutputsshouldbeequalto1\n",
      "n   .Numerically,thismay\n",
      "  notoccurwhenc   haslargemagnitude.Ifc   isverynegative,thenexp(c )will\n",
      "             underﬂow.Thismeansthedenominatorofthesoftmaxwillbecome0,sotheﬁnal\n",
      "   resultisundeﬁned.Whenc    isverylargeandpositive,exp(c   )willoverﬂow,again\n",
      "            resultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties\n",
      "     canberesolvedbyinsteadevaluatingsoftmax(z )wherez=  x−max ix i .Simple\n",
      "             algebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\n",
      "         addingorsubtractingascalarfromtheinputvector.Subtractingmax ix iresults\n",
      "    inthelargestargumenttoexp        being0,whichrulesoutthepossibilityofoverﬂow.\n",
      "               Likewise,atleastoneterminthedenominatorhasavalueof1,whichrulesout\n",
      "            thepossibilityofunderﬂowinthedenominatorleadingtoadivisionbyzero.\n",
      "            Thereisstillonesmallproblem.Underﬂowinthenumerator canstillcause\n",
      "              theexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\n",
      " logsoftmax(x           )byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto\n",
      "      thelogfunction,wecoulderroneouslyobtain−∞    .Instead,wemustimplement\n",
      "    aseparatefunctionthatcalculates logsoftmax     inanumericallystableway.The\n",
      " logsoftmax             functioncanbestabilizedusingthesametrickasweusedtostabilize\n",
      "  the function. softmax\n",
      "            Forthemostpart,wedonotexplicitlydetailallthenumericalconsiderations\n",
      "          involvedinimplementingthevariousalgorithmsdescribedinthisbook.Developers\n",
      "          oflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\n",
      "            deeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\n",
      "           levellibrariesthatprovidestableimplementations.Insomecases,itispossible\n",
      "          toimplementanewalgorithmandhavethenewimplementationautomatically\n",
      "7 9   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "            stabilized.Theano( ,; ,)isanexample Bergstraetal.2010Bastienetal.2012\n",
      "          ofasoftwarepackagethatautomaticallydetectsandstabilizesmanycommon\n",
      "          numericallyunstableexpressionsthatariseinthecontextofdeeplearning.\n",
      "  4.2PoorConditioning\n",
      "            Conditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchanges\n",
      "            initsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\n",
      "           canbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs\n",
      "       canresultinlargechangesintheoutput.\n",
      "  Considerthefunctionf(x )=A− 1x .When  A∈ Rn n ×  hasaneigenvalue\n",
      "    decomposition,itsconditionnumberis\n",
      "max\n",
      "i , jλ i\n",
      "λ j . (4.2)\n",
      "             Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\n",
      "             thisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\n",
      "            Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\n",
      "        ofroundingerrorduringmatrixinversion. Poorlyconditionedmatricesamplify\n",
      "            pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\n",
      "            errorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\n",
      "  4.3Gradient-BasedOptimization\n",
      "        Mostdeeplearningalgorithmsinvolveoptimizationofsomesort. Optimization\n",
      "          referstothetaskofeitherminimizingormaximizingsomefunctionf(x ) byaltering\n",
      "x         . Weusuallyphrasemostoptimizationproblemsintermsofminimizingf(x).\n",
      "         Maximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing\n",
      "−f()x.\n",
      "            Thefunctionwewanttominimizeormaximizeiscalledtheobjectivefunc-\n",
      "tion ,orcriterion           .Whenweareminimizingit,wemayalsocallitthecost\n",
      "function, lossfunction ,or errorfunction      . Inthisbook,weusetheseterms\n",
      "        interchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\n",
      "    tosomeoftheseterms.\n",
      "            Weoftendenotethevaluethatminimizesormaximizesafunctionwitha\n",
      "       superscript.Forexample,wemightsay ∗ x∗  = argmin()fx.\n",
      "8 0   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "        − − − − 20. 15. 10. 05 00 05 10 15 20 ......\n",
      "x−20.−15.−10.−05.00.05.10.15.20.\n",
      "    Globalminimumat= 0.x\n",
      " Sincef () = 0,gradient x\n",
      " descent haltshere.\n",
      "      For 0,wehave x< f () 0,x<\n",
      "     sowecandecreasebyf\n",
      "moving rightward.      For 0,wehave x> f () 0,x>\n",
      "     sowecandecreasebyf\n",
      "moving leftward.\n",
      "f x() =1\n",
      "2x2\n",
      "f() = x x\n",
      "            Figure4.1:Gradientdescent.Anillustrationofhowthegradientdescentalgorithmuses\n",
      "            thederivativesofafunctiontofollowthefunctiondownhilltoaminimum.\n",
      "            Weassumethereaderisalreadyfamiliarwithcalculusbutprovideabrief\n",
      "        reviewofhowcalculusconceptsrelatetooptimizationhere.\n",
      "    Supposewehaveafunctiony=f(x  ),wherebothxandy  arerealnumbers.\n",
      "Thederivative     ofthisfunctionisdenotedasf(x  )orasd y\n",
      "d x  .Thederivativef(x)\n",
      "   givestheslopeoff(x   )atthepointx        .Inotherwords,itspeciﬁeshowtoscale\n",
      "             asmallchangeintheinputtoobtainthecorrespondingchangeintheoutput:\n",
      "     fxfxf (+) ≈()+()x.\n",
      "            Thederivativeisthereforeusefulforminimizingafunctionbecauseittellsus\n",
      "  howtochangex       inordertomakeasmallimprovementiny   .Forexample,we\n",
      " knowthatf(   x−sign(f(x   )))islessthanf(x   )forsmallenough   .Wecanthus\n",
      "reducef(x  )bymovingx         insmallstepswiththeoppositesignofthederivative.\n",
      "   Thistechniqueiscalled  gradientdescent       (Cauchy1847,).Seeﬁgureforan4.1\n",
      "   exampleofthistechnique.\n",
      "Whenf(x        ) = 0,thederivativeprovidesnoinformationaboutwhichdirection\n",
      "   tomove.Pointswheref(x     )=0areknownas  criticalpoints ,orstationary\n",
      "points .A localminimum   isapointwheref(x      )islowerthanatallneighboring\n",
      "        points,soitisnolongerpossibletodecreasef(x    )bymakinginﬁnitesimalsteps.\n",
      "A localmaximum   isapointwheref(x       )ishigherthanatallneighboringpoints,\n",
      "8 1   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "   Minimum Maximum Saddlepoint\n",
      "               Figure4.2:Typesofcriticalpoints.Examplesofthethreetypesofcriticalpointsinone\n",
      "                 dimension.Acriticalpointisapointwithzeroslope.Suchapointcaneitherbealocal\n",
      "             minimum,whichislowerthantheneighboringpoints;alocalmaximum,whichishigher\n",
      "              thantheneighboringpoints;orasaddlepoint,whichhasneighborsthatarebothhigher\n",
      "     andlowerthanthepointitself.\n",
      "      soitisnotpossibletoincrease f( x      )bymakinginﬁnitesimalsteps.Somecritical\n",
      "         pointsareneithermaximanorminima.Theseareknownas  saddlepoints .See\n",
      "         ﬁgureforexamplesofeachtypeofcriticalpoint. 4.2\n",
      "        Apointthatobtainstheabsolutelowestvalueof f( x  )isa globalminimum.\n",
      "             Therecanbeonlyoneglobalminimumormultipleglobalminimaofthefunction.\n",
      "                Itisalsopossiblefortheretobelocalminimathatarenotgloballyoptimal.Inthe\n",
      "            contextofdeeplearning,weoptimizefunctionsthatmayhavemanylocalminima\n",
      "             thatarenotoptimalandmanysaddlepointssurroundedbyveryﬂatregions.All\n",
      "            ofthismakesoptimizationdiﬃcult,especiallywhentheinputtothefunctionis\n",
      "         multidimensional.Wethereforeusuallysettleforﬁndingavalueof f  thatisvery\n",
      "              lowbutnotnecessarilyminimalinanyformalsense.Seeﬁgureforanexample. 4.3\n",
      "       Weoftenminimizefunctionsthathavemultipleinputs: f: Rn → R  .Forthe\n",
      "           conceptof“minimization” tomakesense,theremuststillbeonlyone(scalar)\n",
      "output.\n",
      "            Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptofpartial\n",
      "derivatives   .Thepartialderivative∂\n",
      "∂ x if( x  )measureshow f    changesasonlythe\n",
      "variable x i   increasesatpoint x .Thegradient     generalizesthenotionofderivative\n",
      "              tothecasewherethederivativeiswithrespecttoavector:thegradientof fis\n",
      "       thevectorcontainingallthepartialderivatives,denoted∇ x f( x ).Element i ofthe\n",
      "     gradientisthepartialderivativeof f  withrespectto x i   .Inmultipledimensions,\n",
      "8 2   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "xf x()\n",
      "   Ideally,wewouldlike\n",
      "    toarriveattheglobal\n",
      " minimum, butthis\n",
      "  might notbepossible.  Thislocalminimum\n",
      "    performsnearlyaswellas\n",
      "  theglobalone,\n",
      "    soitisanacceptable\n",
      " haltingpoint.\n",
      "   Thislocalminimumperforms\n",
      "    poorlyandshouldbeavoided.\n",
      "           Figure4.3:Approximateminimization.Optimizationalgorithmsmayfailtoﬁndaglobal\n",
      "            minimumwhentherearemultiplelocalminimaorplateauspresent. Inthecontextof\n",
      "             deeplearning,wegenerallyacceptsuchsolutionseventhoughtheyarenottrulyminimal,\n",
      "            solongastheycorrespondtosigniﬁcantlylowvaluesofthecostfunction.\n",
      "             criticalpointsarepointswhereeveryelementofthegradientisequaltozero.\n",
      "             Thedirectionalderivativeindirection(aunitvector)istheslopeofthe u\n",
      "functionf indirectionu         .Inotherwords,thedirectionalderivativeisthederivative\n",
      "  ofthefunctionf(x+αu   )withrespecttoα  ,evaluatedatα   = 0.Usingthechain\n",
      "    rule,wecanseethat∂\n",
      "∂ α     fα (+xu)evaluatestou∇ x    f α ()xwhen= 0.\n",
      " Tominimizef         ,wewouldliketoﬁndthedirectioninwhichf  decreasesthe\n",
      "        fastest.Wecandothisusingthedirectionalderivative:\n",
      "min\n",
      "u u , u =1u∇ x  f()x (4.3)\n",
      " =min\n",
      "u u , u =1||||u 2||∇ xf()x|| 2  cosθ (4.4)\n",
      "whereθ   istheanglebetweenu    andthegradient.Substitutingin||||u 2  = 1and\n",
      "      ignoringfactorsthatdonotdependonu   ,thissimpliﬁestomin u  cosθ  .Thisis\n",
      " minimizedwhenu          pointsintheoppositedirectionasthegradient.Inother\n",
      "           words,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\n",
      "   downhill.Wecandecreasef        bymovinginthedirectionofthenegativegradient.\n",
      "           Thisisknownasthemethodofsteepestdescentgradientdescent ,or .\n",
      "     Steepestdescentproposesanewpoint\n",
      "x  = x−∇ x  f()x (4.5)\n",
      "8 3   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "where isthe  learningrate         ,apositivescalardeterminingthesizeofthestep.\n",
      "  Wecanchoose         inseveraldiﬀerentways.Apopularapproachistoset  toasmall\n",
      "            constant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\n",
      "      derivativevanish.Anotherapproachistoevaluate   f (x−∇ xf())x forseveral\n",
      " valuesof           andchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\n",
      "       Thislaststrategyiscalledalinesearch.\n",
      "            Steepestdescentconvergeswheneveryelementofthegradientiszero(or,in\n",
      "              practice,veryclosetozero).Insomecases,wemaybeabletoavoidrunning\n",
      "             thisiterativealgorithmandjustjumpdirectlytothecriticalpointbysolvingthe\n",
      " equation∇ x   f() = 0xfor.x\n",
      "          Althoughgradientdescentislimitedtooptimizationincontinuousspaces,the\n",
      "            generalconceptofrepeatedlymakingasmallmove(thatisapproximatelythebest\n",
      "          smallmove)towardbetterconﬁgurationscanbegeneralizedtodiscretespaces.\n",
      "        Ascendinganobjectivefunctionofdiscreteparametersiscalled hillclimbing\n",
      "   ( ,). RusselandNorvig2003\n",
      "       4.3.1BeyondtheGradient:JacobianandHessianMatrices\n",
      "             Sometimesweneedtoﬁndallthepartialderivativesofafunctionwhoseinput\n",
      "            andoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\n",
      "  knownasa  Jacobianmatrix      .Speciﬁcally,ifwehaveafunctionf: Rm → Rn,\n",
      "      thentheJacobianmatrixJ∈ Rn m ×      ofisdeﬁnedsuchthat f J i , j=∂\n",
      "∂ x jf()x i.\n",
      "             Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\n",
      " asa secondderivative     .Forexample,forafunctionf: Rn → R  ,thederivative\n",
      "  withrespecttox i   ofthederivativeoff  withrespecttox j  isdenotedas∂2\n",
      "∂ x i ∂ x jf.\n",
      "      Inasingledimension,wecandenoted2\n",
      "d x2fbyf (x    ).Thesecondderivativetells\n",
      "              ushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant\n",
      "              becauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement\n",
      "              aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\n",
      "  derivativeasmeasuringcurvature       .Supposewehaveaquadraticfunction(many\n",
      "            functionsthatariseinpracticearenotquadraticbutcanbeapproximatedwell\n",
      "              asquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\n",
      "                thenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted\n",
      "                usingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \n",
      "          alongthenegativegradient,andthecostfunctionwilldecreaseby   .Ifthesecond\n",
      "           derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\n",
      "    actuallydecreasebymorethan        .Finally,ifthesecondderivativeispositive,the\n",
      "           functioncurvesupward,sothecostfunctioncandecreasebylessthan .See\n",
      "8 4   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "xf x ( ) Negativecurvature\n",
      "xf x ( ) Nocurvature\n",
      "xf x ( ) Positivecurvature\n",
      "             Figure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\n",
      "           quadraticfunctionswithvariouscurvature. Thedashedlineindicatesthevalueofthe\n",
      "              costfunctionwewouldexpectbasedonthegradientinformationaloneaswemakea\n",
      "          gradientstepdownhill.Withnegativecurvature,thecostfunctionactuallydecreases\n",
      "            fasterthanthegradientpredicts.Withnocurvature,thegradientpredictsthedecrease\n",
      "           correctly.Withpositivecurvature,thefunctiondecreasesmoreslowlythanexpectedand\n",
      "              eventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethefunction\n",
      "inadvertently.\n",
      "            ﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4\n",
      "             thevalueofthecostfunctionpredictedbythegradientandthetruevalue.\n",
      "          Whenourfunctionhasmultipleinputdimensions,therearemanysecond\n",
      "           derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\n",
      "         Hessianmatrix.TheHessianmatrix isdeﬁnedsuchthat H x()(f)\n",
      "H x()(f) i , j=∂2\n",
      "∂x i∂x j f.() x (4.6)\n",
      "        Equivalently,theHessianistheJacobianofthegradient.\n",
      "         Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerential\n",
      "         operatorsarecommutative;thatis,theirordercanbeswapped:\n",
      "∂2\n",
      "∂x i∂x jf() = x∂2\n",
      "∂x j∂x i f.() x (4.7)\n",
      "  ThisimpliesthatH i , j=H j , i         ,sotheHessianmatrixissymmetricatsuchpoints.\n",
      "              Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\n",
      "         Hessianalmosteverywhere. BecausetheHessianmatrixisrealandsymmetric,\n",
      "              wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n",
      "8 5   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "           eigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit\n",
      "vectord  isgivenbydHd .Whend   isaneigenvectorofH   ,thesecondderivative\n",
      "            inthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\n",
      "d            ,thedirectionalsecondderivativeisaweightedaverageofalltheeigenvalues,\n",
      "             withweightsbetween0and1,andeigenvectorsthathaveasmalleranglewith\n",
      "d         receivingmoreweight.Themaximumeigenvaluedeterminesthemaximum\n",
      "         secondderivative,andtheminimumeigenvaluedeterminestheminimumsecond\n",
      "derivative.\n",
      "            The(directional)secondderivativetellsushowwellwecanexpectagradient\n",
      "           descentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\n",
      "         tothefunction aroundthecurrentpoint f()x x(0):\n",
      " ff () x≈(x(0)    )+(xx−(0)) g+1\n",
      "2  (xx−(0))  Hxx (−(0) ),(4.8)\n",
      "whereg   isthegradientandH   istheHessianatx(0)     . Ifweusealearningrate\n",
      "of    ,thenthenewpointx   willbegivenbyx(0) −g    .Substitutingthisintoour\n",
      "  approximation,weobtain\n",
      "f(x(0)   −≈g) f(x(0)  )−g g+1\n",
      "22g Hg. (4.9)\n",
      "         Therearethreetermshere:the originalvalue ofthefunction, theexpected\n",
      "             improvementduetotheslopeofthefunction,andthecorrectionwemustapply\n",
      "               toaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\n",
      "       gradientdescentstepcanactuallymoveuphill.WhengHg   iszeroornegative,\n",
      "      theTaylorseriesapproximationpredictsthatincreasing  foreverwilldecreasef\n",
      "            forever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ,so\n",
      "       onemustresorttomoreheuristicchoicesof   inthiscase.WhengHg ispositive,\n",
      "            solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximationof\n",
      "    thefunctionthemostyields\n",
      "∗=gg\n",
      "gHg . (4.10)\n",
      "    Intheworstcase,wheng    alignswiththeeigenvectorofH   correspondingtothe\n",
      " maximaleigenvalueλmax        ,thenthisoptimalstepsizeisgivenby1\n",
      "λ m a x  .Tothe\n",
      "            extentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\n",
      "            function,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\n",
      "rate.\n",
      "            Thesecondderivativecanbeusedtodeterminewhetheracriticalpointis\n",
      "              alocalmaximum,alocalminimum,orasaddlepoint.Recallthatonacritical\n",
      "point,f(x    ) = 0.Whenthesecondderivativef (x)>   0,theﬁrstderivativef(x)\n",
      "8 6   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "                increasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n",
      "f(  x−)< 0andf(x+)>   0forsmallenough      .Inotherwords,aswemove\n",
      "                right,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\n",
      "       beginstopointuphilltotheleft. Thus,whenf(x   )=0andf (x)>  0,wecan\n",
      " concludethatx     isalocalminimum.Similarly,whenf(x  ) = 0andf (x)< 0,we\n",
      "  canconcludethatx        isalocalmaximum.Thisisknownasthe secondderivative\n",
      "test  .Unfortunately,whenf (x       ) = 0,thetestisinconclusive.Inthiscasexmay\n",
      "          beasaddlepointorapartofaﬂatregion.\n",
      "            Inmultipledimensions,weneedtoexamineallthesecondderivativesofthe\n",
      "          function.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\n",
      "           thesecondderivativetesttomultipledimensions.Atacriticalpoint,where\n",
      "∇ xf( x           ) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\n",
      "             thecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\n",
      "             Hessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal\n",
      "           minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\n",
      "            inanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\n",
      "           derivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues\n",
      "             arenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\n",
      "            possibletoﬁndpositiveevidenceofsaddlepointsinsomecases. Whenatleast\n",
      "             oneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\n",
      "x        isalocalmaximumononecrosssectionoff     butalocalminimumonanother\n",
      "           crosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5\n",
      "            derivativetestcanbeinconclusive,justastheunivariateversioncan.Thetest\n",
      "            isinconclusivewheneverallthenonzeroeigenvalueshavethesamesignbutat\n",
      "             leastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\n",
      "         inconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\n",
      "           Inmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection\n",
      "             atasinglepoint.TheconditionnumberoftheHessianatthispointmeasures\n",
      "             howmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa\n",
      "           poorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\n",
      "          direction,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\n",
      "              slowly.Gradientdescentisunawareofthischangeinthederivative,soitdoesnot\n",
      "            knowthatitneedstoexplorepreferentiallyinthedirectionwherethederivative\n",
      "           remainsnegativeforlonger.Poorconditionnumberalsomakeschoosingagood\n",
      "            stepsizediﬃcult.Thestepsizemustbesmallenoughtoavoidovershooting\n",
      "           theminimumandgoinguphillindirectionswithstrongpositivecurvature.This\n",
      "              usuallymeansthatthestepsizeistoosmalltomakesigniﬁcantprogressinother\n",
      "         directionswithlesscurvature.Seeﬁgureforanexample. 4.6\n",
      "8 7   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "  \n",
      "            Figure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\n",
      "   inthisexampleis f( x )= x2\n",
      "1 − x2\n",
      "2     .Alongtheaxiscorrespondingto x 1  ,thefunction\n",
      "              curvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\n",
      "    Alongtheaxiscorrespondingto x 2        ,thefunctioncurvesdownward.Thisdirectionisan\n",
      "            eigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom\n",
      "              thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunctionwith\n",
      "                 asaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalueof0\n",
      "               togetasaddlepoint:itisonlynecessarytohavebothpositiveandnegativeeigenvalues.\n",
      "                Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocalmaximum\n",
      "           withinonecrosssectionandalocalminimumwithinanothercrosssection.\n",
      "8 8   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "     − − − 30201001020\n",
      "x 1−30−20−1001020x 2\n",
      "            Figure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\n",
      "           Hessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunctionf( x) whose\n",
      "             Hessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\n",
      "               hasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\n",
      "     curvatureisinthedirection[1,1]         ,andtheleastcurvatureisinthedirection[1 , −1].\n",
      "             Theredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\n",
      "          functionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\n",
      "              canyonwallsbecausetheyarethesteepestfeature.Sincethestepsizeissomewhattoo\n",
      "                large,ithasatendencytoovershootthebottomofthefunctionandthusneedstodescend\n",
      "              theoppositecanyonwallonthenextiteration.ThelargepositiveeigenvalueoftheHessian\n",
      "           correspondingtotheeigenvectorpointedinthisdirectionindicatesthatthisdirectional\n",
      "            derivativeisrapidlyincreasing,soanoptimizationalgorithmbasedontheHessiancould\n",
      "             predictthatthesteepestdirectionisnotactuallyapromisingsearchdirectioninthis\n",
      "context.\n",
      "8 9   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "             ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n",
      "         thesearch. Thesimplestmethodfordoingsoisknownas  Newton’smethod.\n",
      "           Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto\n",
      "     approximate nearsomepoint f()x x(0 ):\n",
      " ff () x≈(x(0 ))+(xx−(0 ))∇ xf(x(0 ))+1\n",
      "2(xx−(0 ))Hx()(f(0 ))(xx−(0 ) ).(4.11)\n",
      "            Ifwethensolveforthecriticalpointofthisfunction,weobtain\n",
      "x∗= x(0 ) −Hx()(f(0 ))− 1∇ xf(x(0 ) ). (4.12)\n",
      "Whenf         isapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof\n",
      "            applyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\n",
      "Whenf           isnottrulyquadraticbutcanbelocallyapproximatedasapositive\n",
      "         deﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple 4.12\n",
      "         times. Iterativ elyupdatingtheapproximationandjumpingtotheminimumof\n",
      "           theapproximationcanreachthecriticalpointmuchfasterthangradientdescent\n",
      "               would.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\n",
      "            propertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3\n",
      "            onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\n",
      "            oftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\n",
      "      pointsunlessthegradientpointstowardthem.\n",
      "          Optimizationalgorithmsthatuseonlythegradient,suchasgradientdescent,\n",
      " arecalled   ﬁrst-orderoptimizationalgorithms   .Optimizationalgorithmsthat\n",
      "          alsousetheHessianmatrix,suchasNewton’smethod,arecalledsecond-order\n",
      "     optimizationalgorithms(NocedalandWright2006,).\n",
      "     The optimizationalgorithms employedinmost contexts inthis bookare\n",
      "             applicabletoawidevarietyoffunctionsbutcomewithalmostnoguarantees.Deep\n",
      "            learningalgorithmstendtolackguaranteesbecausethefamilyoffunctionsusedin\n",
      "            deeplearningisquitecomplicated.Inmanyotherﬁelds,thedominantapproachto\n",
      "           optimizationistodesignoptimizationalgorithmsforalimitedfamilyoffunctions.\n",
      "            Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-\n",
      "      ingourselvestofunctionsthatareeither  Lipschitzcontinuous  orhaveLipschitz\n",
      "        continuousderivatives.ALipschitzcontinuousfunctionisafunctionf whoserate\n",
      "        ofchangeisboundedbyaLipschitzconstantL:\n",
      "        ∀∀|−|≤L||−|| x,y,f()xf()yxy 2 . (4.13)\n",
      "             Thispropertyisusefulbecauseitenablesustoquantifyourassumptionthata\n",
      "              smallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n",
      "9 0   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "             asmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\n",
      "           andmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\n",
      "   withrelativelyminormodiﬁcations.\n",
      "      Perhapsthemostsuccessfulﬁeld ofspecializedoptimization is convexop-\n",
      "timization         .Convexoptimizationalgorithmsareabletoprovidemanymore\n",
      "         guaranteesbymakingstrongerrestrictions.Thesealgorithmsareapplicableonly\n",
      "          toconvexfunctions—functionsforwhichtheHessianispositivesemideﬁniteev-\n",
      "          erywhere.Suchfunctionsarewell-behavedbecausetheylacksaddlepoints,and\n",
      "           alltheirlocalminimaarenecessarilyglobalminima.However,mostproblemsin\n",
      "           deeplearningarediﬃculttoexpressintermsofconvexoptimization.Convex\n",
      "            optimizationisusedonlyasasubroutineofsomedeeplearningalgorithms.Ideas\n",
      "           fromtheanalysisofconvexoptimizationalgorithmscanbeusefulforproving\n",
      "           theconvergenceofdeeplearningalgorithms,butingeneral,theimportanceof\n",
      "           convexoptimizationisgreatlydiminishedinthecontextofdeeplearning.For\n",
      "         moreinformationaboutconvexoptimization,seeBoydandVandenberghe2004()\n",
      "  orRockafellar1997().\n",
      "  4.4ConstrainedOptimization\n",
      "          Sometimeswewishnotonlytomaximizeorminimizeafunction f( x  )overall\n",
      " possible valuesof x     .Insteadwe may wishto ﬁnd themaximal orminimal\n",
      " valueof f( x  )for valuesof x insome set S   .Thisis knownasconstrained\n",
      "optimization .Points x    thatliewithintheset S arecalledfeasible pointsin\n",
      "  constrainedoptimizationterminology.\n",
      "              Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\n",
      "              approachinsuchsituationsistoimposeanormconstraint,suchas .||||≤ x 1\n",
      "          Onesimpleapproachtoconstrainedoptimizationissimplytomodifygradient\n",
      "             descenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ,\n",
      "           wecanmakegradientdescentsteps,thenprojecttheresultbackinto S   .Ifweuse\n",
      "         alinesearch,wecansearchonlyoverstepsizes   thatyieldnew x  pointsthatare\n",
      "              feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\n",
      "            Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient\n",
      "             intothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\n",
      "    thelinesearch(,). Rosen1960\n",
      "          Amoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-\n",
      "            mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\n",
      "         constrainedoptimizationproblem.Forexample,ifwewanttominimize f( x )for\n",
      "9 1   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "  x∈ R2withx     constrainedtohaveexactlyunitL2    norm,wecaninsteadminimize\n",
      "g(θ) =f([   cossinθ,θ]   )withrespecttoθ   ,thenreturn[   cossinθ,θ   ]asthesolution\n",
      "         totheoriginalproblem.Thisapproachrequirescreativity;thetransformation\n",
      "          betweenoptimizationproblemsmustbedesignedspeciﬁcallyforeachcasewe\n",
      "encounter.\n",
      "TheKarush–Kuhn–Tucker  (KKT)approach1    providesaverygeneralso-\n",
      "          lutiontoconstrainedoptimization.WiththeKKTapproach,weintroducea\n",
      "   newfunctioncalledthe  generalizedLagrangianor  generalizedLagrange\n",
      "function.\n",
      "        TodeﬁnetheLagrangian,weﬁrstneedtodescribe S   intermsofequations\n",
      "     andinequalities. Wewantadescriptionof S  intermsofmfunctionsg( ) iandn\n",
      "functionsh( ) j sothat S=   {|∀xi,g( ) i(x) = 0  and∀j,h( ) j(x)≤0}  .Theequations\n",
      "involvingg( ) i  arecalledthe  equalityconstraints    ,andtheinequalitiesinvolving\n",
      "h( ) j   arecalledinequalityconstraints.\n",
      "   Weintroducenewvariablesλ iandα j      foreachconstraint,thesearecalledthe\n",
      "        KKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas\n",
      "   L,,f (xλα) = ()+x\n",
      "iλ ig( ) i ()+x\n",
      "jα jh( ) j ()x. (4.14)\n",
      "         Wecannowsolveaconstrainedminimizationproblemusingunconstrained\n",
      "            optimizationofthegeneralizedLagrangian.Aslongasatleastonefeasiblepoint\n",
      "          existsandisnotpermittedtohavevalue,then f()x ∞\n",
      "min\n",
      "xmax\n",
      "λmax\n",
      "α α , ≥ 0   L,, (xλα) (4.15)\n",
      "             hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsasx\n",
      "min\n",
      "x ∈ S f.()x (4.16)\n",
      "        Thisfollowsbecauseanytimetheconstraintsaresatisﬁed,\n",
      "max\n",
      "λmax\n",
      "α α , ≥ 0   L,,f, (xλα) = ()x (4.17)\n",
      "      whileanytimeaconstraintisviolated,\n",
      "max\n",
      "λmax\n",
      "α α , ≥ 0   L,,. (xλα) = ∞ (4.18)\n",
      "            Thesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\n",
      "      optimumwithinthefeasiblepointsisunchanged.\n",
      "1      TheKKTapproachgeneralizesthemethodof  L a gran ge m u l t i p l i e r s   ,whichallowsequality\n",
      "    constraintsbutnotinequalityconstraints.\n",
      "92   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "         Toperformconstrainedmaximization,wecanconstructthegeneralizedLa-\n",
      "          grangefunctionof ,whichleadstothisoptimizationproblem: −f()x\n",
      "min\n",
      "xmax\n",
      "λmax\n",
      "α α , ≥ 0 −f()+x\n",
      "iλ ig( ) i ()+x\n",
      "jα jh( ) j ()x. (4.19)\n",
      "             Wemayalsoconvertthistoaproblemwithmaximizationintheouterloop:\n",
      "max\n",
      "xmin\n",
      "λmin\n",
      "α α , ≥ 0 f()+x\n",
      "iλ ig( ) i ()x−\n",
      "jα jh( ) j ()x. (4.20)\n",
      "               Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit\n",
      "             withadditionorsubtractionaswewish,becausetheoptimizationisfreetochoose\n",
      "    anysignforeachλ i.\n",
      "          Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\n",
      "h( ) i(x )is a c t i v eifh( ) i(x∗          ) = 0.Ifaconstraintisnotactive,thenthesolutionto\n",
      "             theproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\n",
      "           thatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes\n",
      "            othersolutions.Forexample,aconvexproblemwithanentireregionofglobally\n",
      "              optimalpoints(awide,ﬂatregionofequalcost)couldhaveasubsetofthis\n",
      "           regioneliminatedbyconstraints,oranonconvexproblemcouldhavebetterlocal\n",
      "            stationarypointsexcludedbyaconstraintthatisinactiveatconvergence.Yetthe\n",
      "            pointfoundatconvergenceremainsastationarypointwhetherornottheinactive\n",
      "     constraintsareincluded.Becauseaninactiveh( ) i    hasnegativevalue,thenthe\n",
      " solutiontomin xmax λmax α α , ≥ 0L(  xλα,,  )willhaveα i    = 0.Wecanthusobserve\n",
      "   thatatthesolution, αh(x) = 0     .Inotherwords,foralli     ,weknowthatatleast\n",
      "   oneoftheconstraintsα i≥ 0orh( ) i(x)≤        0mustbeactiveatthesolution.Togain\n",
      "               someintuitionforthisidea,wecansaythateitherthesolutionisontheboundary\n",
      "             imposedbytheinequalityandwemustuseitsKKTmultipliertoinﬂuencethe\n",
      " solutiontox            ,ortheinequalityhasnoinﬂuenceonthesolutionandwerepresent\n",
      "      thisbyzeroingoutitsKKTmultiplier.\n",
      "           Asimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-\n",
      "        mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\n",
      "          conditions(,; Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\n",
      "             butnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:\n",
      "        •ThegradientofthegeneralizedLagrangianiszero.\n",
      "           •AllconstraintsonbothandtheKKTmultipliersaresatisﬁed. x\n",
      "•     Theinequalityconstraintsexhibit“complementary slackness”: αh(x) = 0.\n",
      "           FormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\n",
      "9 3   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "    4.5Example: LinearLeastSquares\n",
      "          Supposewewanttoﬁndthevalueofthatminimizes x\n",
      "f() =x1\n",
      "2  ||−||Axb2\n",
      "2 . (4.21)\n",
      "         Specializedlinearalgebraalgorithmscansolvethisproblemeﬃciently;however,\n",
      "             wecanalsoexplorehowtosolveitusinggradient-basedoptimizationasasimple\n",
      "     exampleofhowthesetechniqueswork.\n",
      "      First,weneedtoobtainthegradient:\n",
      "∇ xf() = xA  ( ) = Axb−A  AxA− b. (4.22)\n",
      "            Wecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\n",
      " fordetails.\n",
      " Algorithm4.1   Analgorithmtominimizef(x) =1\n",
      "2  ||−||Axb2\n",
      "2  withrespecttox\n",
      "         usinggradientdescent,startingfromanarbitraryvalueof.x\n",
      "           Setthestepsize()andtolerance()tosmall,positivenumbers.  δ\n",
      " while||A  AxA−b|| 2  >δdo\n",
      "    xx←−\n",
      "A  AxA−b\n",
      " endwhile\n",
      "            OnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because\n",
      "          thetruefunctionisquadratic,thequadraticapproximationemployedbyNewton’s\n",
      "             methodisexact,andthealgorithmconvergestotheglobalminimuminasingle\n",
      "step.\n",
      "        Nowsuppose wewishtominimizethesamefunction, but subjectto the\n",
      " constraintx         x≤1.Todoso,weintroducetheLagrangian\n",
      "   L,λfλ (x) = ()+x\n",
      "x  x−1\n",
      " . (4.23)\n",
      "     Wecannowsolvetheproblem\n",
      "min\n",
      "xmax\n",
      "λ , λ ≥ 0  L,λ. (x) (4.24)\n",
      "         Thesmallest-normsolutiontotheunconstrainedleast-squaresproblemmaybe\n",
      "    foundusingtheMoore-Penrosepseudoinverse:x=A+b     .Ifthispointisfeasible,\n",
      "             thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda\n",
      "9 4   C HAP T E R 4 . NUME R I C AL C O M P UT A T I O N\n",
      "          solutionwheretheconstraintisactive.BydiﬀerentiatingtheLagrangianwith\n",
      "      respectto,weobtaintheequation x\n",
      "A  AxA−    bx+2λ= 0. (4.25)\n",
      "         Thistellsusthatthesolutionwilltaketheform\n",
      " xA= (  AI+2λ)− 1A b. (4.26)\n",
      "  Themagnitudeofλ          mustbechosensuchthattheresultobeystheconstraint.We\n",
      "             canﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\n",
      "∂\n",
      "∂λ L,λ(x) = x   x−1. (4.27)\n",
      "   Whenthenormofx           exceeds1,thisderivativeispositive,sotofollowthederivative\n",
      "       uphillandincreasetheLagrangianwithrespecttoλ  ,weincreaseλ  .Becausethe\n",
      "  coeﬃcientonthexx        penaltyhasincreased,solvingthelinearequationforx\n",
      "              willnowyieldasolutionwithasmallernorm.Theprocessofsolvingthelinear\n",
      "  equationandadjustingλ  continuesuntilx      hasthecorrectnormandthederivative\n",
      "   onis0.λ\n",
      "          Thisconcludesthemathematicalpreliminariesthatweusetodevelopmachine\n",
      "           learningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\n",
      " learningsystems.\n",
      "9 5 C h a p t e r 5\n",
      "  Mac hi ne Learning B asi cs\n",
      "            Deeplearningisaspeciﬁckindofmachinelearning.Tounderstanddeeplearning\n",
      "             well,onemusthaveasolidunderstandingofthebasicprinciplesofmachinelearning.\n",
      "            Thischapterprovidesabriefcourseinthemostimportantgeneralprinciplesthat\n",
      "              areappliedthroughouttherestofthebook.Novicereadersorthosewhowanta\n",
      "          widerperspectiveareencouragedtoconsidermachinelearningtextbookswitha\n",
      "           morecomprehensivecoverageofthefundamentals,suchas ()or Murphy2012Bishop\n",
      "             ().Ifyouarealreadyfamiliarwithmachinelearningbasics,feelfreetoskip 2006\n",
      "           aheadtosection.Thatsectioncoverssomeperspectivesontraditionalmachine 5.11\n",
      "          learningtechniquesthathavestronglyinﬂuencedthedevelopmentofdeeplearning\n",
      "algorithms.\n",
      "             Webeginwithadeﬁnitionofwhatalearningalgorithmisandpresentan\n",
      "           example:thelinearregressionalgorithm.Wethenproceedtodescribehowthe\n",
      "            challengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns\n",
      "          thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\n",
      "         called h y p e r p a r a m e t e r s,whichmustbedeterminedoutsidethelearningalgorithm\n",
      "            itself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\n",
      "            essentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\n",
      "         computerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\n",
      "          onprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe\n",
      "         twocentralapproachestostatistics:frequentistestimatorsandBayesianinference.\n",
      "           Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\n",
      "          learningandunsupervisedlearning;wedescribethesecategoriesandgivesome\n",
      "          examplesofsimplelearningalgorithmsfromeachcategory.Mostdeeplearning\n",
      "         algorithmsarebasedonanoptimizationalgorithmcalledstochasticgradient\n",
      "96    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "          descent.Wedescribehowtocombinevariousalgorithmcomponents,suchas\n",
      "             anoptimizationalgorithm,acostfunction,amodel,andadataset,tobuilda\n",
      "           machinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\n",
      "           factorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\n",
      "          Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\n",
      "  overcometheseobstacles.\n",
      "  5.1LearningAlgorithms\n",
      "             Amachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.\n",
      "            Butwhatdowemeanbylearning? ()providesasuccinctdeﬁnition: Mitchell1997\n",
      "        “Acomputerprogramissaidtolearnfromexperience E   withrespecttosome\n",
      "  classoftasks T  andperformancemeasure P      ,ifitsperformanceattasksin T ,as\n",
      " measuredby P   ,improveswithexperience E       .”Onecanimagineawidevarietyof\n",
      "experiences E ,tasks T   ,andperformancemeasures P      ,andwedonotattemptin\n",
      "              thisbooktoformallydeﬁnewhatmaybeusedforeachoftheseentities.Instead,\n",
      "           inthefollowingsections,weprovideintuitivedescriptionsandexamplesofthe\n",
      "           diﬀerentkindsoftasks,performancemeasures,andexperiencesthatcanbeused\n",
      "    toconstructmachinelearningalgorithms.\n",
      "   5.1.1TheTask, T\n",
      "             Machinelearningenablesustotackletasksthataretoodiﬃculttosolvewith\n",
      "           ﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand\n",
      "          philosophicalpointofview,machinelearningisinterestingbecausedevelopingour\n",
      "          understandingofitentailsdevelopingourunderstandingoftheprinciplesthat\n",
      " underlieintelligence.\n",
      "            Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning\n",
      "               itselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe\n",
      "                 task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\n",
      "               Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\n",
      "       aprogramthatspeciﬁeshowtowalkmanually.\n",
      "           Machinelearningtasksareusuallydescribedintermsofhowthemachine\n",
      "    learningsystemshouldprocessan e x a m pl e      .Anexampleisacollectionof f e a t ur e s\n",
      "            thathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\n",
      "            themachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa\n",
      "vector  x∈ Rn  whereeachentry x i       ofthevectorisanotherfeature.Forexample,\n",
      "              thefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\n",
      "9 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             Manykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\n",
      "      commonmachinelearningtasksincludethefollowing:\n",
      " •Classiﬁcation            :Inthistypeoftask,thecomputerprogramisaskedtospecify\n",
      " whichofk           categoriessomeinputbelongsto.Tosolvethistask,thelearning\n",
      "       algorithmisusuallyaskedtoproduceafunctionf : Rn →{ 1     ,...,k} .When\n",
      "y =f ( x        ),themodelassignsaninputdescribedbyvector x  toacategory\n",
      "   identiﬁedbynumericcodey       .Thereareothervariantsoftheclassiﬁcation\n",
      "   task,forexample,wheref      outputsaprobabilitydistributionoverclasses.\n",
      "           Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput\n",
      "             isanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\n",
      "            outputisanumericcodeidentifyingtheobjectintheimage.Forexample,\n",
      "              theWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\n",
      "           diﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\n",
      "          fellow 2010etal.,).Modernobjectrecognitionisbestaccomplishedwith\n",
      "          deeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015\n",
      "          recognitionisthesamebasictechnologythatenablescomputerstorecognize\n",
      "            faces(Taigman 2014etal.,),whichcanbeusedtoautomaticallytagpeople\n",
      "           inphotocollectionsandforcomputerstointeractmorenaturallywiththeir\n",
      "users.\n",
      "    •Classiﬁcationwithmissinginputs    :Classiﬁcationbecomesmorechal-\n",
      "           lengingifthecomputerprogramisnotguaranteedthateverymeasurementin\n",
      "            itsinputvectorwillalwaysbeprovided.Tosolvetheclassiﬁcationtask,the\n",
      "            learningalgorithmonlyhastodeﬁneafunctionmappingfromavector single\n",
      "            inputtoacategoricaloutput.Whensomeoftheinputsmaybemissing,\n",
      "         ratherthanprovidingasingleclassiﬁcationfunction,thelearningalgorithm\n",
      "          mustlearnaoffunctions.Eachfunctioncorrespondstoclassifying set xwith\n",
      "            adiﬀerentsubsetofitsinputsmissing.Thiskindofsituationarisesfrequently\n",
      "           inmedicaldiagnosis,becausemanykindsofmedicaltestsareexpensiveor\n",
      "             invasive.Onewaytoeﬃcientlydeﬁnesuchalargesetoffunctionsisto\n",
      "           learnaprobabilitydistributionoveralltherelevantvariables,thensolvethe\n",
      "        classiﬁcationtaskbymarginalizingoutthemissingvariables.Withninput\n",
      "      variables,wecannowobtainall 2n   diﬀerentclassiﬁcationfunctionsneeded\n",
      "           foreachpossiblesetofmissinginputs,butthecomputerprogramneeds\n",
      "          tolearnonlyasinglefunctiondescribingthejointprobabilitydistribution.\n",
      "            See ()foranexampleofadeepprobabilisticmodel Goodfellowetal.2013b\n",
      "               appliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis\n",
      "          sectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation\n",
      "            withmissinginputsisjustoneexampleofwhatmachinelearningcando.\n",
      "9 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      " •Regression             :Inthistypeoftask,thecomputerprogramisaskedtopredicta\n",
      "           numericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\n",
      "     isaskedtooutputafunction f : Rn → R       .Thistypeoftaskissimilarto\n",
      "           classiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof\n",
      "            aregressiontaskisthepredictionoftheexpectedclaimamountthatan\n",
      "           insuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\n",
      "            offuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\n",
      " algorithmictrading.\n",
      " •Transcription           :Inthistypeoftask,themachinelearningsystemisasked\n",
      "          toobservearelativelyunstructuredrepresentationofsomekindofdata\n",
      "          andtranscribetheinformationintodiscretetextualform.Forexample,in\n",
      "         opticalcharacterrecognition,thecomputerprogramisshownaphotograph\n",
      "               containinganimageoftextandisaskedtoreturnthistextintheformof\n",
      "           asequenceofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreet\n",
      "           Viewusesdeeplearningtoprocessaddressnumbersinthisway(Goodfellow\n",
      "          etal.,).Anotherexampleisspeechrecognition,wherethecomputer 2014d\n",
      "            programisprovidedanaudiowaveformandemitsasequenceofcharactersor\n",
      "            wordIDcodesdescribingthewordsthatwerespokenintheaudiorecording.\n",
      "          Deeplearningisacrucialcomponentofmodernspeechrecognitionsystems\n",
      "         usedatmajorcompanies,includingMicrosoft,IBMandGoogle(Hinton\n",
      "  etal.,).2012b\n",
      "  •Machinetranslation        :Inamachinetranslationtask,theinputalready\n",
      "            consistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram\n",
      "           mustconvertthisintoasequenceofsymbolsinanotherlanguage. Thisis\n",
      "          commonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\n",
      "           French.Deeplearninghasrecentlybeguntohaveanimportantimpacton\n",
      "           thiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\n",
      "  •Structuredoutput        :Structuredoutputtasksinvolveanytaskwherethe\n",
      "           outputisavector(orotherdatastructurecontainingmultiplevalues)with\n",
      "         importantrelationshipsbetweenthediﬀerentelements.Thisisabroad\n",
      "        categoryandsubsumesthetranscriptionandtranslationtasksdescribed\n",
      "           above,aswellasmanyothertasks.Oneexampleisparsing—mappi nga\n",
      "          naturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\n",
      "             bytaggingnodesofthetreesasbeingverbs,nouns,adverbs,andsoon.\n",
      "            See ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\n",
      "        task.Anotherexampleispixel-wisesegmentationofimages, wherethe\n",
      "           computerprogramassignseverypixelinanimagetoaspeciﬁccategory.\n",
      "9 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            Forexample,deeplearningcanbeusedtoannotatethelocationsofroads\n",
      "          inaerialphotographs( ,).Theoutputformneed MnihandHinton2010\n",
      "            notmirrorthestructureoftheinputascloselyasintheseannotation-style\n",
      "          tasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\n",
      "          imageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\n",
      "              etal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\n",
      "             KarpathyandLi2015Fang 2015Xu 2015 ,;etal.,;etal.,).Thesetasks\n",
      "          arecalledstructuredoutputtasksbecausetheprogrammustoutputseveral\n",
      "           valuesthatarealltightlyinterrelated.Forexample,thewordsproducedby\n",
      "        animagecaptioningprogrammustformavalidsentence.\n",
      "  •Anomalydetection        :Inthistypeoftask, thecomputerprogramsifts\n",
      "              throughasetofeventsorobjectsandﬂagssomeofthemasbeingunusual\n",
      "            oratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\n",
      "          detection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\n",
      "              detectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard\n",
      "          information,thethief’spurchaseswilloftencomefromadiﬀerentprobability\n",
      "          distributionoverpurchasetypesthanyourown.Thecreditcardcompany\n",
      "               canpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\n",
      "            beenusedforanuncharacteristicpurchase.See ()fora Chandolaetal.2009\n",
      "    surveyofanomalydetectionmethods.\n",
      "   •Synthesisandsampling         :Inthistypeoftask,themachinelearningal-\n",
      "             gorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\n",
      "         trainingdata. Synthesisandsamplingviamachinelearningcanbeuseful\n",
      "          formediaapplicationswhengeneratinglargevolumesofcontentbyhand\n",
      "           wouldbeexpensive,boring,orrequiretoomuchtime.Forexample,video\n",
      "         gamescanautomaticallygeneratetexturesforlargeobjectsorlandscapes,\n",
      "             ratherthanrequiringanartisttomanuallylabeleachpixel( ,). Luoetal.2013\n",
      "            Insomecases,wewantthesamplingorsynthesisproceduretogeneratea\n",
      "            speciﬁckindofoutputgiventheinput.Forexample,inaspeechsynthesis\n",
      "             task,weprovideawrittensentenceandasktheprogramtoemitanaudio\n",
      "            waveformcontainingaspokenversionofthatsentence.Thisisakindof\n",
      "           structuredoutputtask,butwiththeaddedqualiﬁcationthatthereisno\n",
      "            singlecorrectoutputforeachinput,andweexplicitlydesirealargeamount\n",
      "              ofvariationintheoutput,inorderfortheoutputtoseemmorenaturaland\n",
      "realistic.\n",
      "    •Imputationofmissingvalues        :Inthistypeoftask,themachinelearning\n",
      "     algorithmisgivenanewexample  x∈ Rn    ,butwithsomeentries x iofx\n",
      "1 0 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            missing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\n",
      "entries.\n",
      " •Denoising            :Inthistypeoftask,themachinelearningalgorithmisgivenas\n",
      "   inputacorruptedexample˜  x∈ Rn     obtainedbyanunknowncorruptionprocess\n",
      "   fromacleanexample  x∈ Rn       .Thelearnermustpredictthecleanexample\n",
      "x   fromitscorruptedversion˜x      ,ormoregenerallypredicttheconditional\n",
      "   probabilitydistributionp(x|˜x).\n",
      "  •Densityestimationor    probabilitymassfunctionestimation  :Inthe\n",
      "           densityestimationproblem,themachinelearningalgorithmisaskedtolearna\n",
      "functionp m o d e l: Rn → R ,wherep m o d e l(x     ) canbeinterpretedasaprobability\n",
      "  densityfunction(if x       iscontinuous)oraprobabilitymassfunction(if xis\n",
      "              discrete)onthespacethattheexamplesweredrawnfrom.Todosuchatask\n",
      "           well(wewillspecifyexactlywhatthatmeanswhenwediscussperformance\n",
      "measuresP             ),thealgorithmneedstolearnthestructureofthedataithasseen.\n",
      "            Itmustknowwhereexamplesclustertightlyandwheretheyareunlikelyto\n",
      "            occur.Mostofthetasksdescribedaboverequirethelearningalgorithmtoat\n",
      "         leastimplicitlycapturethestructureoftheprobabilitydistribution.Density\n",
      "         estimationenablesustoexplicitlycapturethatdistribution.Inprinciple,\n",
      "           wecanthenperformcomputationsonthatdistributiontosolvetheother\n",
      "            tasksaswell.Forexample,ifwehaveperformeddensityestimationtoobtain\n",
      "  aprobabilitydistributionp(x        ),wecanusethatdistributiontosolvethe\n",
      "      missingvalueimputationtask.Ifavaluex i     ismissing,andalltheother\n",
      " values,denotedx − i           ,aregiven,thenweknowthedistributionoveritisgiven\n",
      "byp(x i |x − i          ).Inpractice,densityestimationdoesnotalwaysenableusto\n",
      "           solvealltheserelatedtasks,becauseinmanycasestherequiredoperations\n",
      "    onarecomputationallyintractable. p()x\n",
      "              Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\n",
      "             welisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\n",
      "        do,nottodeﬁnearigidtaxonomyoftasks.\n",
      "    5.1.2ThePerformanceMeasure, P\n",
      "       Toevaluate theabilities of amachine learningalgorithm,wemust designa\n",
      "        quantitativemeasureofitsperformance.UsuallythisperformancemeasurePis\n",
      "          speciﬁctothetaskbeingcarriedoutbythesystem. T\n",
      "          Fortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-\n",
      "    scription,weoftenmeasuretheaccuracy      ofthemodel.Accuracyisjustthe\n",
      "1 0 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            proportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n",
      "      alsoobtainequivalentinformationbymeasuringthe errorrate  ,theproportion\n",
      "             ofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\n",
      "                theerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\n",
      "                ifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,\n",
      "               itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\n",
      "            loss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel\n",
      "           acontinuous-valuedscoreforeachexample.Themostcommonapproachisto\n",
      "         reporttheaveragelog-probabilitythemodelassignstosomeexamples.\n",
      "           Usuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\n",
      "                ondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\n",
      "           deployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\n",
      "a testset            ofdatathatisseparatefromthedatausedfortrainingthemachine\n",
      " learningsystem.\n",
      "         Thechoiceofperformancemeasuremayseemstraightforwardandobjective,\n",
      "             butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto\n",
      "     thedesiredbehaviorofthesystem.\n",
      "              Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.\n",
      "           Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\n",
      "             ofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained\n",
      "           performancemeasurethatgivespartialcreditforgettingsomeelementsofthe\n",
      "          sequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe\n",
      "            systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\n",
      "           verylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\n",
      "             Inothercases,weknowwhatquantitywewouldideallyliketomeasure,but\n",
      "            measuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\n",
      "         densityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\n",
      "         distributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\n",
      "              aspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one\n",
      "           mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\n",
      "        ordesignagoodapproximationtothedesiredcriterion.\n",
      "   5.1.3TheExperience, E\n",
      "       Machinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\n",
      "supervised          bywhatkindofexperiencetheyare allowedtohaveduringthe\n",
      " learningprocess.\n",
      "             Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\n",
      "1 0 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   toexperienceanentiredataset         .Adatasetisacollectionofmanyexamples,as\n",
      "         deﬁnedinsection.Sometimeswecallexamples . 5.1.1 datapoints\n",
      "           Oneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\n",
      "            searchersistheIrisdataset(,).Itisacollectionofmeasurements Fisher1936\n",
      "            ofdiﬀerentpartsof150irisplants.Eachindividualplantcorrespondstoone\n",
      "          example. Thefeatureswithineachexamplearethemeasurementsofeachpart\n",
      "            oftheplant: thesepallength,sepalwidth,petallengthandpetalwidth.The\n",
      "           datasetalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspecies\n",
      "    arerepresentedinthedataset.\n",
      "  Unsupervisedlearningalgorithms     experienceadatasetcontainingmany\n",
      "             features,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\n",
      "            ofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\n",
      "           generatedadataset,whetherexplicitly,asindensityestimation,orimplicitly,for\n",
      "         taskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms\n",
      "           performotherroles,likeclustering,whichconsistsofdividingthedatasetinto\n",
      "   clustersofsimilarexamples.\n",
      "  Supervisedlearningalgorithms     experienceadatasetcontainingfeatures,\n",
      "       buteachexampleisalsoassociatedwithalabelortarget    .Forexample,theIris\n",
      "            datasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\n",
      "             algorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\n",
      "     diﬀerentspeciesbasedontheirmeasurements.\n",
      "       Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples\n",
      "   ofarandomvectorx        andattemptingtoimplicitlyorexplicitlylearntheproba-\n",
      " bilitydistributionp(x        ),orsomeinterestingpropertiesofthatdistribution;while\n",
      "         supervisedlearninginvolvesobservingseveralexamplesofarandomvectorxand\n",
      "    anassociatedvalueorvectory    ,thenlearningtopredictyfromx  ,usuallyby\n",
      "estimatingp(  yx|  ).Theterm  supervisedlearning     originatesfromtheviewof\n",
      " thetargety          beingprovidedbyaninstructororteacherwhoshowsthemachine\n",
      "            learningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror\n",
      "              teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\n",
      "         Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.\n",
      "           Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\n",
      "             beusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\n",
      "      thatforavectorx∈ Rn       ,thejointdistributioncanbedecomposedas\n",
      "p() =xn\n",
      "i =1p(x i |x 1     ,...,x i − 1 ). (5.1)\n",
      "           Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof\n",
      "1 0 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "modelingp( x   ) bysplittingitinton     supervisedlearningproblems.Alternatively,we\n",
      "       cansolvethesupervisedlearningproblemoflearningp(  y| x   )byusingtraditional\n",
      "       unsupervisedlearningtechnologiestolearnthejointdistributionp( x,y ),then\n",
      "inferring\n",
      "  py(| x) = p,y( x)\n",
      "y p,y( x) . (5.2)\n",
      "         Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformal\n",
      "              ordistinctconcepts,theydohelproughlycategorizesomeofthethingswedowith\n",
      "        machinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation\n",
      "         andstructuredoutputproblemsassupervisedlearning.Densityestimationin\n",
      "        supportofothertasksisusuallyconsideredunsupervisedlearning.\n",
      "           Othervariantsofthelearningparadigmarepossible.Forexample,insemi-\n",
      "          supervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\n",
      "           not.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\n",
      "            containingornotcontaininganexampleofaclass,buttheindividualmembers\n",
      "            ofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning\n",
      "       withdeepmodels,see (). Kotziasetal.2015\n",
      "           Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\n",
      "example,  reinforcementlearning      algorithmsinteractwithanenvironment,so\n",
      "           thereisafeedbackloopbetweenthelearningsystemanditsexperiences. Such\n",
      "             algorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\n",
      "         orBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\n",
      "            and ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013\n",
      "          Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\n",
      "              bedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\n",
      "      whichareinturncollectionsoffeatures.\n",
      "             Onecommonwayofdescribingadatasetiswitha .Adesign designmatrix\n",
      "              matrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe\n",
      "           matrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains\n",
      "            150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent\n",
      "     thedatasetwithadesignmatrix  X∈ R1 5 0 4 × ,whereX i , 1    isthesepallengthof\n",
      "planti,X i , 2     isthesepalwidthofplanti       ,etc.Wedescribemostofthelearning\n",
      "             algorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\n",
      "              Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\n",
      "               describeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\n",
      "             Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\n",
      "          withdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent\n",
      "             numbersofpixels,sonotallthephotographsmaybedescribedwiththesame\n",
      "1 0 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "              lengthofvector.InSectionandchapter,wedescribehowtohandlediﬀerent 9.7 10\n",
      "            typesofsuchheterogeneousdata.Incaseslikethese,ratherthandescribingthe\n",
      "    datasetasamatrixwithm       rows,wedescribeitasasetcontainingmelements:\n",
      "{x(1 ) ,x(2 )     ,...,x( ) m}          .Thisnotationdoesnotimplythatanytwoexamplevectors\n",
      "x( ) i andx( ) j   havethesamesize.\n",
      "             Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\n",
      "               wellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\n",
      "           toperformobjectrecognitionfromphotographs,weneedtospecifywhichobject\n",
      "               appearsineachofthephotos.Wemightdothiswithanumericcode,with0\n",
      "               signifyingaperson,1signifyingacar,2signifyingacat,andsoforth.Oftenwhen\n",
      "          workingwithadatasetcontainingadesignmatrixoffeatureobservationsX ,we\n",
      "        alsoprovideavectoroflabels,withyy i      providingthelabelforexample.i\n",
      "             Ofcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\n",
      "            example,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\n",
      "            sentences,thenthelabelforeachexamplesentenceisasequenceofwords.\n",
      "           Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,\n",
      "            thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\n",
      "              covermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\n",
      "   5.1.4Example:LinearRegression\n",
      "            Ourdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable\n",
      "           ofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis\n",
      "            somewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa\n",
      "   simplemachinelearningalgorithm: linearregression     .Wewillreturntothis\n",
      "           examplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\n",
      "   understandthealgorithm’sbehavior.\n",
      "          Asthenameimplies,linearregressionsolvesaregressionproblem. Inother\n",
      "            words,thegoalistobuildasystemthatcantakeavector  x∈ Rn  asinputand\n",
      "     predictthevalueofascalar  y∈ R        asitsoutput.Theoutputoflinearregressionis\n",
      "      alinearfunctionoftheinput.Letˆy      bethevaluethatourmodelpredictsyshould\n",
      "       takeon.Wedeﬁnetheoutputtobe\n",
      " ˆy= w x, (5.3)\n",
      "   wherew∈ Rn    isavectorof . parameters\n",
      "            Parametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\n",
      "      thecoeﬃcientthatwemultiplybyfeaturex i    beforesummingupthecontributions\n",
      "       fromallthefeatures.Wecanthinkofw   asasetofweights  thatdeterminehow\n",
      "1 0 5    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "      eachfeatureaﬀectstheprediction. Ifafeaturex i    receivesapositiveweightw i,\n",
      "            thenincreasingthevalueofthatfeatureincreasesthevalueofourpredictionˆy.\n",
      "             Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\n",
      "             decreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,\n",
      "                 thenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno\n",
      "   eﬀectontheprediction.\n",
      "       WethushaveadeﬁnitionofourtaskT : topredictyfromx byoutputting\n",
      " ˆy= w           x.Nextweneedadeﬁnitionofourperformancemeasure,.P\n",
      "       Supposethatwehaveadesignmatrixofm      exampleinputsthatwewillnot\n",
      "             usefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\n",
      "         avectorofregressiontargetsprovidingthecorrectvalueofy   foreachofthese\n",
      "              examples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\n",
      "         set.WerefertothedesignmatrixofinputsasX( ) te s t    andthevectorofregression\n",
      "  targetsasy( ) te s t.\n",
      "            Onewayofmeasuringtheperformanceofthemodelistocomputethemean\n",
      " squarederror       ofthemodelonthetestset.Ifˆy( ) te s t    givesthepredictionsofthe\n",
      "            modelonthetestset,thenthemeansquarederrorisgivenby\n",
      "MSE te s t=1\n",
      "m\n",
      "i(ˆy( ) te s t −y( ) te s t)2\n",
      "i . (5.4)\n",
      "           Intuitively,onecanseethatthiserrormeasuredecreasesto0whenˆy( ) te s t=y( ) te s t.\n",
      "    Wecanalsoseethat\n",
      "MSE te s t=1\n",
      "m||ˆy( ) te s t −y( ) te s t||2\n",
      "2 , (5.5)\n",
      "          sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\n",
      "   andthetargetsincreases.\n",
      "            Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\n",
      "   willimprovetheweightsw    inawaythatreducesMSE te s t  whenthealgorithm\n",
      "          isallowedtogainexperiencebyobservingatrainingset(X( ) tr a i n ,y( ) tr a i n ).One\n",
      "              intuitivewayofdoingthis(whichwejustifylater,insection)isjustto 5.5.1\n",
      "         minimizethemeansquarederroronthetrainingset,MSE tr a i n.\n",
      "  TominimizeMSE tr a i n          ,wecansimplysolveforwhereitsgradientis: 0\n",
      "∇ wMSE tr a i n  = 0 (5.6)\n",
      " ⇒∇ w1\n",
      "m||ˆy( ) tr a i n −y( ) tr a i n||2\n",
      "2  = 0 (5.7)\n",
      "1 0 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "⇒1\n",
      "m∇ w||X( ) tr a i n  wy−( ) tr a i n||2\n",
      "2  = 0 (5.8)\n",
      " ⇒∇ w\n",
      "X( ) tr a i n  wy−( ) tr a i n \n",
      "X( ) tr a i n  wy−( ) tr a i n\n",
      " = 0(5.9)\n",
      " ⇒∇ w\n",
      "wX( ) tr a i n X( ) tr a i n  ww−2X( ) tr a i n y( ) tr a i n +y( ) tr a i n y( ) tr a i n\n",
      "= 0\n",
      "(5.10)\n",
      " ⇒2X( ) tr a i n X( ) tr a i n  wX−2( ) tr a i n y( ) tr a i n = 0 (5.11)\n",
      "  ⇒w=\n",
      "X( ) tr a i n X( ) tr a i n− 1\n",
      "X( ) tr a i n y( ) tr a i n(5.12)\n",
      "             Thesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\n",
      "the  normalequations       .Evaluatingequationconstitutesasimplelearning 5.12\n",
      "           algorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\n",
      "  seeﬁgure.5.1\n",
      "      Itisworthnotingthattheterm linearregression     isoftenusedtoreferto\n",
      "         aslightlymoresophisticatedmodelwithoneadditionalparameter—anintercept\n",
      "    term.Inthismodel b\n",
      " ˆy= w   x+b, (5.13)\n",
      "             sothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\n",
      "            mappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto\n",
      "    − −10 . 05000510 . . . .\n",
      "x1−3−2−10123y  Linearregressionexample\n",
      "  05 10 15 . . .\n",
      "w1020 .025 .030 .035 .040 .045 .050 .055 .MSE(train)  Optimizationof w\n",
      "              Figure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\n",
      "            eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\n",
      "      containsonlyasingleparametertolearn,w 1     . ( L e f t )Observethatlinearregressionlearns\n",
      " tosetw 1   suchthattheliney=w 1x         comesascloseaspossibletopassingthroughallthe\n",
      "        trainingpoints. Theplottedpointindicatesthevalueof ( R i g h t ) w 1   foundbythenormal\n",
      "             equations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\n",
      "1 0 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             aﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea\n",
      "              line,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n",
      "b            ,onecancontinuetousethemodelwithonlyweightsbutaugmentx withan\n",
      "               extraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\n",
      "             playstheroleofthebiasparameter.Wefrequentlyusetheterm“linear”when\n",
      "      referringtoaﬃnefunctionsthroughoutthisbook.\n",
      "  Theintercepttermb   isoftencalledthebias     parameteroftheaﬃnetransfor-\n",
      "             mation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\n",
      "    transformationisbiasedtowardbeingb      intheabsenceofanyinput. Thisterm\n",
      "             isdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation\n",
      "            algorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.\n",
      "           Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\n",
      "             butitprovidesanexampleofhowalearningalgorithmcanwork.Insubsequent\n",
      "          sectionswedescribesomeofthebasicprinciplesunderlyinglearningalgorithm\n",
      "            designanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\n",
      " learningalgorithms.\n",
      "    5.2Capacity,OverﬁttingandUnderﬁtting\n",
      "           Thecentralchallengeinmachinelearningisthatouralgorithmmustperform\n",
      "            wellonnew,previouslyunseeninputs—notjustthoseonwhichourmodelwas\n",
      "           trained.Theabilitytoperformwellonpreviouslyunobservedinputsiscalled\n",
      "generalization.\n",
      "            Typically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\n",
      "            set;wecancomputesomeerrormeasureonthetrainingset,calledthetraining\n",
      "error              ;andwereducethistrainingerror.Sofar,whatwehavedescribedissimply\n",
      "         anoptimizationproblem.Whatseparatesmachinelearningfromoptimizationis\n",
      "   thatwewantthe  generalizationerror   ,alsocalledthe testerror    ,tobelowas\n",
      "             well. Thegeneralizationerrorisdeﬁnedastheexpectedvalueoftheerrorona\n",
      "           newinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn\n",
      "            fromthedistributionofinputsweexpectthesystemtoencounterinpractice.\n",
      "           Wetypicallyestimatethegeneralizationerrorofamachinelearningmodelby\n",
      "    measuringitsperformanceona testset     ofexamplesthatwerecollectedseparately\n",
      "   fromthetrainingset.\n",
      "           Inourlinearregressionexample,wetrainedthemodelbyminimizingthe\n",
      " trainingerror,\n",
      "1\n",
      "m( ) tr a i n||X( ) tr a i n  wy−( ) tr a i n||2\n",
      "2 , (5.14)\n",
      "1 0 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "       butweactuallycareaboutthetesterror,1\n",
      "m( ) t e st||X() te s t  wy−() te s t||2\n",
      "2.\n",
      "              Howcanweaﬀectperformanceonthetestsetwhenwecanobserveonlythe\n",
      "    trainingset?Theﬁeldof   statisticallearningtheory    providessomeanswers.If\n",
      "              thetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\n",
      "              do.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\n",
      "        setarecollected,thenwecanmakesomeprogress.\n",
      "           Thetrainingandtestdataaregeneratedbyaprobabilitydistributionover\n",
      "  datasetscalledthe  data-generatingprocess       .Wetypicallymakeasetofas-\n",
      "    sumptionsknowncollectivelyasthe i.i.d.assumptions   .Theseassumptionsare\n",
      "      thattheexamplesineachdatasetareindependent    fromeachother,andthat\n",
      "      thetrainingsetandtestsetare  identicallydistributed    ,drawnfromthesame\n",
      "          probabilitydistributionaseachother.Thisassumptionenablesustodescribe\n",
      "          thedata-generatingprocesswithaprobabilitydistributionoverasingleexample.\n",
      "             Thesamedistributionisthenusedtogenerateeverytrainexampleandeverytest\n",
      "       example.Wecallthatsharedunderlyingdistributionthe  data-generatingdis-\n",
      "tribution ,denotedpdata       .Thisprobabilisticframeworkandthei.i.d.assumptions\n",
      "          enablesustomathematicallystudytherelationshipbetweentrainingerrorand\n",
      " testerror.\n",
      "           Oneimmediateconnectionwecanobservebetweentrainingerrorandtesterror\n",
      "              isthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\n",
      "           expectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\n",
      "p( x,y              )andwesamplefromitrepeatedlytogeneratethetrainingsetandthetest\n",
      "    set.Forsomeﬁxedvaluew          ,theexpectedtrainingseterrorisexactlythesameas\n",
      "            theexpectedtestseterror,becausebothexpectationsareformedusingthesame\n",
      "           datasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthe\n",
      "       nameweassigntothedatasetwesample.\n",
      "        Ofcourse, when we useamachinelearningalgorithm, wedonotﬁx the\n",
      "            parametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\n",
      "              thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\n",
      "              testset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\n",
      "            theexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\n",
      "       learningalgorithmwillperformareitsabilityto\n",
      "    1. Makethetrainingerrorsmall.\n",
      "        2. Makethegapbetweentrainingandtesterrorsmall.\n",
      "           Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\n",
      "underﬁttingandoverﬁtting         .Underﬁttingoccurswhenthemodelisnotableto\n",
      "1 0 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            obtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhen\n",
      "           thegapbetweenthetrainingerrorandtesterroristoolarge.\n",
      "              Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering\n",
      "itscapacity             .Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof\n",
      "            functions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models\n",
      "             withhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo\n",
      "       notservethemwellonthetestset.\n",
      "             Onewaytocontrolthecapacityofalearningalgorithmisbychoosingits\n",
      "            hypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\n",
      "            selectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\n",
      "              setofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize\n",
      "           linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\n",
      "       hypothesisspace.Doingsoincreasesthemodel’scapacity.\n",
      "             Apolynomialofdegree1givesusthelinearregressionmodelwithwhichwe\n",
      "     arealreadyfamiliar,withtheprediction\n",
      "    ˆybwx. = + (5.15)\n",
      " Byintroducingx2         asanotherfeatureprovidedtothelinearregressionmodel,we\n",
      "           canlearnamodelthatisquadraticasafunctionof:x\n",
      "   ˆybw = + 1  xw+ 2x2 . (5.16)\n",
      "            Thoughthismodelimplementsaquadraticfunctionofits,theoutputis i n p u t\n",
      "              stillalinearfunctionofthe p a r a m e t e r s,sowecanstillusethenormalequations\n",
      "             totrainthemodelinclosedform. Wecancontinuetoaddmorepowersofxas\n",
      "          additionalfeatures,forexample,toobtainapolynomialofdegree9:\n",
      "  ˆyb= +9 \n",
      "i =1w ixi . (5.17)\n",
      "         Machinelearningalgorithmswillgenerallyperformbestwhentheircapacity\n",
      "              isappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\n",
      "           amountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity\n",
      "            areunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex\n",
      "              tasks,butwhentheircapacityishigherthanneededtosolvethepresenttask,they\n",
      " mayoverﬁt.\n",
      "           Figureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\n",
      "           anddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying\n",
      "1 1 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "\n",
      "              Figure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas\n",
      "    generatedsynthetically,byrandomlysampling x  valuesandchoosing ydeterministically\n",
      "            byevaluatingaquadraticfunction. ( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom\n",
      "           underﬁtting—itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\n",
      "               quadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom\n",
      "           asigniﬁcantamountofoverﬁttingorunderﬁtting. ( R i g h t )Apolynomialofdegree9ﬁt\n",
      "            tothedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinverseto\n",
      "           solvetheunderdeterminednormalequations.Thesolutionpassesthroughallthetraining\n",
      "               pointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\n",
      "                Itnowhasadeepvalleybetweentwotrainingpointsthatdoesnotappearinthetrue\n",
      "               underlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\n",
      "    functiondecreasesinthisarea.\n",
      "           functionisquadratic. Thelinearfunctionisunabletocapturethecurvaturein\n",
      "            thetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof\n",
      "           representingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely\n",
      "           manyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n",
      "           havemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\n",
      "            asolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In\n",
      "            thisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\n",
      "        thetask,soitgeneralizeswelltonewdata.\n",
      "             Sofarwehavedescribedonlyonewayofchangingamodel’scapacity:by\n",
      "           changingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew\n",
      "            parametersassociatedwiththosefeatures.Thereareinfactmanywaystochange\n",
      "             amodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\n",
      "           modelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\n",
      "             whenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\n",
      "the  representationalcapacity        ofthemodel.Inmanycases,ﬁndingthebest\n",
      "1 1 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "           functionwithinthisfamilyisadiﬃcultoptimizationproblem.Inpractice,the\n",
      "            learningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyonethat\n",
      "          signiﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchasthe\n",
      "         imperfectionoftheoptimizationalgorithm,meanthatthelearningalgorithm’s\n",
      " eﬀectivecapacity         maybelessthantherepresentationalcapacityofthemodel\n",
      "family.\n",
      "         Ourmodernideasaboutimprovingthegeneralizationofmachinelearning\n",
      "             modelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearlyas\n",
      "            Ptolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnowmost\n",
      "  widelyknownas  Occam’srazor      (c.1287–1347). Thisprinciplestatesthatamong\n",
      "         competinghypothesesthatexplainknownobservationsequallywell,weshould\n",
      "            choosethe“simplest”one.Thisideawasformalizedandmademoreprecisein\n",
      "           thetwentiethcenturybythefoundersofstatisticallearningtheory(Vapnikand\n",
      "         Chervonenkis1971Vapnik1982Blumer 1989Vapnik1995 ,;,; etal.,;,).\n",
      "         Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\n",
      "       Amongthese,themostwellknownisthe  Vapnik-Chervonenkisdimension ,or\n",
      "            VCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The\n",
      "          VCdimensionisdeﬁnedasbeingthelargestpossiblevalueof m  forwhichthere\n",
      "    existsatrainingsetof mdiﬀerent x       pointsthattheclassiﬁercanlabelarbitrarily.\n",
      "          Quantifyingthecapacityofthemodelenablesstatisticallearningtheoryto\n",
      "         makequantitativepredictions.Themostimportantresultsinstatisticallearning\n",
      "          theoryshowthatthediscrepancybetweentrainingerrorandgeneralizationerror\n",
      "              isboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\n",
      "          shrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,\n",
      "           1971Vapnik1982Blumer 1989Vapnik1995 ; ,; etal.,; ,).Theseboundsprovide\n",
      "          intellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare\n",
      "            rarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\n",
      "               partbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\n",
      "          diﬃculttodeterminethecapacityofdeeplearningalgorithms. Theproblemof\n",
      "           determiningthecapacityofadeeplearningmodelisespeciallydiﬃcultbecause\n",
      "           theeﬀectivecapacityislimitedbythecapabilitiesoftheoptimizationalgorithm,\n",
      "          andwehavelittletheoreticalunderstandingofthegeneralnonconvexoptimization\n",
      "    problemsinvolvedindeeplearning.\n",
      "           Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n",
      "              (tohaveasmallgapbetweentrainingandtesterror),wemuststillchoosea\n",
      "         suﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\n",
      "            errordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\n",
      "          capacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,\n",
      "1 1 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "  0 OptimalCapacity\n",
      "CapacityError   UnderﬁttingzoneOverﬁttingzone\n",
      " Generalizationgap Trainingerror\n",
      " Generalizationerror\n",
      "           Figure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\n",
      "             behavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\n",
      "     arebothhigh.Thisisthe  underﬁttingregime      .Asweincreasecapacity,trainingerror\n",
      "          decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\n",
      "              thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting\n",
      "         regime,wherecapacityistoolarge,abovetheoptimalcapacity.\n",
      "             generalizationerrorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\n",
      "   illustratedinﬁgure.5.3\n",
      "          Toreachthemostextremecaseofarbitrarilyhighcapacity, weintroduce\n",
      "  theconceptof  nonparametricmodels       .Sofar,wehaveseenonlyparametric\n",
      "          models,suchaslinearregression.Parametricmodelslearnafunctiondescribed\n",
      "              byaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.\n",
      "     Nonparametricmodelshavenosuchlimitation.\n",
      "        Sometimes,nonparametricmodelsarejusttheoreticalabstractions(suchas\n",
      "          analgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\n",
      "          beimplementedinpractice.However,wecanalsodesignpracticalnonparametric\n",
      "             modelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\n",
      "    ofsuchanalgorithmis   nearestneighborregression   .Unlikelinearregression,\n",
      "           whichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel\n",
      "  simplystorestheXandy        fromthetrainingset. Whenaskedtoclassifyatest\n",
      "pointx              ,themodellooksupthenearestentryinthetrainingsetandreturnsthe\n",
      "     associatedregressiontarget.Inotherwords,ˆ y= y iwhere i=   argmin||X i , :−||x2\n",
      "2.\n",
      "           Thealgorithmcanalsobegeneralizedtodistancemetricsotherthanthe L2norm,\n",
      "            suchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\n",
      "      allowedtobreaktiesbyaveragingthe y i  valuesforallX i , :    thataretiedfornearest,\n",
      "            thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which\n",
      "1 1 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent\n",
      "    outputs)onanyregressiondataset.\n",
      "           Finally,wecanalsocreateanonparametriclearningalgorithmbywrappinga\n",
      "         parametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\n",
      "             ofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\n",
      "              thatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\n",
      "    polynomialexpansionoftheinput.\n",
      "            Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\n",
      "             thatgeneratesthedata. Evensuchamodelwillstillincursomeerroronmany\n",
      "             problems,becausetheremaystillbesomenoiseinthedistribution.Inthecase\n",
      "     ofsupervisedlearning,themappingfrom xtoy   maybeinherentlystochastic,\n",
      "ory          maybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\n",
      " includedin x           .Theerrorincurredbyanoraclemakingpredictionsfromthetrue\n",
      "       distribution iscalledthe p,y( x) Bayeserror.\n",
      "            Trainingandgeneralizationerrorvaryasthesizeofthetrainingsetvaries.\n",
      "           Expectedgeneralizationerrorcanneverincreaseasthenumberoftrainingexamples\n",
      "         increases.Fornonparametricmodels,moredatayieldbettergeneralizationuntil\n",
      "            thebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan\n",
      "            optimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.\n",
      "               Seeﬁgureforanillustration.Notethatitispossibleforthemodeltohave 5.4\n",
      "            optimalcapacityandyetstillhavealargegapbetweentrainingandgeneralization\n",
      "              errors.Inthissituation,wemaybeabletoreducethisgapbygatheringmore\n",
      " trainingexamples.\n",
      "     5.2.1TheNoFreeLunchTheorem\n",
      "           Learningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom\n",
      "             aﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\n",
      "            logic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\n",
      "              isnotlogicallyvalid.Tologicallyinferaruledescribingeverymemberofaset,\n",
      "         onemusthaveinformationabouteverymemberofthatset.\n",
      "           Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,\n",
      "          ratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machine\n",
      "            learningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\n",
      "   thesettheyconcern.\n",
      "         Unfortunately,eventhisdoesnotresolvetheentireproblem.The nofree\n",
      " lunchtheorem        formachinelearning(Wolpert1996,)statesthat,averagedover\n",
      "        allpossibledata-generatingdistributions,everyclassiﬁcationalgorithmhasthe\n",
      "1 1 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "                 Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\n",
      "            ontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\n",
      "              addingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\n",
      "              andthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40\n",
      "             diﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.\n",
      "              ( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,\n",
      "                 andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For\n",
      "              thequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\n",
      "             Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,\n",
      "           becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\n",
      "               modeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\n",
      "              ahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The\n",
      "               trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\n",
      "              tomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,\n",
      "              thetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat\n",
      "           leasttheBayeserror. Asthetrainingsetsizeincreases,theoptimalcapacity ( Bo t t o m )\n",
      "            (shownhereasthedegreeoftheoptimalpolynomialregressor)increases.Theoptimal\n",
      "         capacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.\n",
      "1 1 5    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "          sameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\n",
      "            insomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\n",
      "            other.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n",
      "           performance(overallpossibletasks)asmerelypredictingthateverypointbelongs\n",
      "   tothesameclass.\n",
      "           Fortunately,theseresultsholdonlywhenweaverageoverpossibledata- all\n",
      "          generatingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\n",
      "          distributionsweencounterinreal-worldapplications,thenwecandesignlearning\n",
      "      algorithmsthatperformwellonthesedistributions.\n",
      "              Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversal\n",
      "            learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\n",
      "             understandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI\n",
      "           agentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\n",
      "          datadrawnfromthekindsofdata-generatingdistributionswecareabout.\n",
      " 5.2.2Regularization\n",
      "            Thenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\n",
      "               algorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof\n",
      "          preferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith\n",
      "            thelearningproblemsthatweaskthealgorithmtosolve,itperformsbetter.\n",
      "             Sofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\n",
      "           concretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding\n",
      "           orremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\n",
      "             isabletochoosefrom.Wegavethespeciﬁcexampleofincreasingordecreasing\n",
      "             thedegreeofapolynomialforaregressionproblem.Theviewwehavedescribed\n",
      "   sofarisoversimpliﬁed.\n",
      "             Thebehaviorofouralgorithmisstronglyaﬀected notjustbyhowlargewe\n",
      "              makethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity\n",
      "            ofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\n",
      "              hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\n",
      "           linearfunctionscanbeusefulforproblemswheretherelationshipbetweeninputs\n",
      "               andoutputstrulyisclosetolinear.Theyarelessusefulforproblemsthatbehavein\n",
      "            averynonlinearfashion.Forexample,linearregressionwouldnotperformwellif\n",
      "      wetriedtouseittopredictsin( x )from x       .Wecanthuscontroltheperformanceof\n",
      "             ouralgorithmsbychoosingwhatkindoffunctionsweallowthemtodrawsolutions\n",
      "          from,aswellasbycontrollingtheamountofthesefunctions.\n",
      "             Wecanalsogivealearningalgorithmapreferenceforonesolutionoveranother\n",
      "1 1 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             initshypothesisspace.Thismeansthatbothfunctionsareeligible,butoneis\n",
      "             preferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetrainingdata\n",
      "     signiﬁcantlybetterthanthepreferredsolution.\n",
      "            Forexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\n",
      " weightdecay           .Toperformlinearregressionwithweightdecay,weminimizeasum\n",
      "J( w             )comprisingboththemeansquarederroronthetrainingandacriterionthat\n",
      "         expressesapreferencefortheweightstohavesmallersquaredL2 norm.Speciﬁcally,\n",
      "J() = wMSE tr a i n +λ w w, (5.18)\n",
      "whereλ             isavaluechosenaheadoftimethatcontrolsthestrengthofourpreference\n",
      "   forsmallerweights.Whenλ      = 0,weimposenopreference,andlargerλ forcesthe\n",
      "    weightstobecomesmaller.MinimizingJ( w       )resultsinachoiceofweightsthat\n",
      "             makeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus\n",
      "              solutionsthathaveasmallerslope,orthatputweightonfewerofthefeatures.\n",
      "              Asanexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁt\n",
      "           viaweightdecay,wecantrainahigh-degreepolynomialregressionmodelwith\n",
      "         diﬀerentvaluesof.Seeﬁgurefortheresults. λ 5.5\n",
      "          Moregenerally,wecanregularizeamodelthatlearnsafunctionf( x; θ )by\n",
      "    addingapenaltycalledaregularizer        tothecostfunction.Inthecaseofweight\n",
      "    decay,theregularizerisΩ( w) = ww         .Inchapter,wewillseethatmanyother 7\n",
      "  regularizersarepossible.\n",
      "           Expressingpreferencesforonefunctionoveranotherisamoregeneralway\n",
      "           ofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe\n",
      "             hypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas\n",
      "       expressinganinﬁnitelystrongpreferenceagainstthatfunction.\n",
      "           Inourweightdecayexample,weexpressedourpreferenceforlinearfunctions\n",
      "           deﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionwe\n",
      "         minimize.Therearemanyotherwaysofexpressing preferencesfordiﬀerent\n",
      "       solutions,bothimplicitlyandexplicitly.Together, thesediﬀerentapproaches\n",
      "  areknownasregularization        . Regularizationisanymodiﬁcationwemaketoa\n",
      "            learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\n",
      "             trainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine\n",
      "       learning,rivaledinitsimportanceonlybyoptimization.\n",
      "              Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\n",
      "          learningalgorithm,and,inparticular,nobestformofregularization.Instead\n",
      "              wemustchooseaformofregularizationthatiswellsuitedtotheparticulartask\n",
      "              wewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\n",
      "              particularisthatawiderangeoftasks(suchasalltheintellectualtasksthat\n",
      "1 1 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            peoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeformsof\n",
      "regularization.\n",
      "    5.3HyperparametersandValidationSets\n",
      "         Mostmachinelearningalgorithmshavehyperparameters, settingsthatwecan\n",
      "           usetocontrolthealgorithm’sbehavior.Thevaluesofhyperparametersarenot\n",
      "            adaptedbythelearningalgorithmitself(thoughwecandesignanestedlearning\n",
      "          procedureinwhichonelearningalgorithmlearnsthebesthyperparameters for\n",
      "  anotherlearningalgorithm).\n",
      "          Thepolynomialregressionexampleinﬁgurehasasinglehyperparameter: 5.2\n",
      "        thedegreeofthepolynomial,whichactsasa c a pa c i t y  hyperparameter.The\n",
      "λ             valueusedtocontrolthestrengthofweightdecayisanotherexampleofa\n",
      "hyperparameter.\n",
      "\n",
      "\n",
      "    \n",
      "             Figure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset\n",
      "                fromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\n",
      "             Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.\n",
      "  ( L e f t )Withverylarge λ             ,wecanforcethemodeltolearnafunctionwithnoslopeat\n",
      "            all.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction. Witha ( C e n t e r )\n",
      "              mediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. λ\n",
      "            Eventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\n",
      "             shapes,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\n",
      "         coeﬃcients. Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\n",
      "         pseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the\n",
      "         degree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2\n",
      "1 1 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            Sometimesasettingischosentobeahyperparameterthatthelearningalgo-\n",
      "            rithmdoesnotlearnbecausethesettingisdiﬃculttooptimize.Morefrequently,\n",
      "             thesettingmustbeahyperparameterbecauseitisnotappropriatetolearnthat\n",
      "          hyperparameteronthetrainingset.Thisappliestoallhyperparameters that\n",
      "           controlmodelcapacity.Iflearnedonthetrainingset,suchhyperparameterswould\n",
      "          alwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer\n",
      "             toﬁgure).Forexample, wecanalwaysﬁtthetrainingsetbetterwitha 5.3\n",
      "       higher-degreepolynomialandaweightdecaysettingof λ     = 0thanwecouldwith\n",
      "        alower-degreepolynomialandapositiveweightdecaysetting.\n",
      "      Tosolvethisproblem,weneeda  validationset    ofexamplesthatthetraining\n",
      "   algorithmdoesnotobserve.\n",
      "            Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\n",
      "             thesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\n",
      "              errorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\n",
      "              testexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\n",
      "            itshyperparameters. Forthisreason,noexamplefromthetestsetcanbeused\n",
      "            inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\n",
      "           trainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.\n",
      "              Oneofthesesubsetsisusedtolearntheparameters.Theothersubsetisour\n",
      "           validationset,usedtoestimatethegeneralizationerrorduringoraftertraining,\n",
      "           allowingforthehyperparameters tobeupdatedaccordingly.Thesubsetofdata\n",
      "             usedtolearntheparametersisstilltypicallycalledthetrainingset,eventhough\n",
      "              thismaybeconfusedwiththelargerpoolofdatausedfortheentiretraining\n",
      "            process.Thesubsetofdatausedtoguidetheselectionofhyperparametersis\n",
      "            calledthevalidationset.Typically,oneusesabout80percentofthetraining\n",
      "             datafortrainingand20percentforvalidation.Sincethevalidationsetisused\n",
      "          to“train”thehyperparameters, thevalidationseterrorwillunderestimatethe\n",
      "           generalizationerror,thoughtypicallybyasmalleramountthanthetrainingerror\n",
      "         does.Afterallhyperparameteroptimizationiscomplete,thegeneralizationerror\n",
      "      maybeestimatedusingthetestset.\n",
      "           Inpractice, whenthesametestsethasbeenusedrepeatedlytoevaluate\n",
      "           performanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider\n",
      "           alltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-\n",
      "            the-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\n",
      "               thetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe\n",
      "           trueﬁeldperformanceofatrainedsystem.Thankfully,thecommunitytendsto\n",
      "           moveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\n",
      "1 1 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      " 5.3.1Cross-Validation\n",
      "               Dividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic\n",
      "               ifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\n",
      "            aroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm\n",
      "         A B worksbetterthanalgorithmonthegiventask.\n",
      "             Whenthedatasethashundredsofthousandsofexamplesormore,thisisnot\n",
      "            aseriousissue.Whenthedatasetistoosmall,alternativeproceduresenableone\n",
      "                tousealltheexamplesintheestimationofthemeantesterror,atthepriceof\n",
      "           increasedcomputationalcost.Theseproceduresarebasedontheideaofrepeating\n",
      "           thetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits\n",
      "          oftheoriginaldataset.Themostcommonoftheseisthek -foldcross-validation\n",
      "              procedure,showninalgorithm,inwhichapartitionofthedatasetisformedby 5.1\n",
      "  splittingitintok         nonoverlappingsubsets.Thetesterrormaythenbeestimated\n",
      "      bytakingtheaveragetesterroracrossk  trials.Ontriali ,thei   -thsubsetofthe\n",
      "                  dataisusedasthetestset,andtherestofthedataisusedasthetrainingset.\n",
      "             Oneproblemisthatnounbiasedestimatorsofthevarianceofsuchaverageerror\n",
      "         estimatorsexist(BengioandGrandvalet2004,),butapproximations aretypically\n",
      "used.\n",
      "    5.4Estimators,BiasandVariance\n",
      "              Theﬁeldofstatisticsgivesusmanytoolstoachievethemachinelearninggoalof\n",
      "            solvingatasknotonlyonthetrainingsetbutalsotogeneralize. Foundational\n",
      "           conceptssuchasparameterestimation,biasandvarianceareusefultoformally\n",
      "      characterizenotionsofgeneralization,underﬁttingandoverﬁtting.\n",
      "  5.4.1PointEstimation\n",
      "            Pointestimationistheattempttoprovidethesingle“best”predictionofsome\n",
      "             quantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\n",
      "              oravectorofparametersinsomeparametricmodel,suchastheweightsinour\n",
      "             linearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\n",
      "          Todistinguishestimatesofparametersfromtheirtruevalue,ourconvention\n",
      "           willbetodenoteapointestimateofaparameterbyθˆθ.\n",
      "Let{x(1 )     ,...,x( ) m}   beasetofm    independentandidenticallydistributed\n",
      "1 2 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      " Algorithm5.1Thek        -foldcross-validationalgorithm.Itcanbeusedtoestimate\n",
      "     generalizationerrorofalearningalgorithmA   whenthegivendataset D istoo\n",
      "            smallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\n",
      "       generalizationerror,becausethemeanofalossL       onasmalltestsetmayhavetoo\n",
      "    highavariance.Thedataset D      containsaselementstheabstractexamples z( ) i(for\n",
      "thei        -thexample),whichcouldstandforan(input,target)pair z( ) i= ( x( ) i ,y( ) i)\n",
      "          inthecaseofsupervisedlearning,orforjustaninput z( ) i= x( ) i  inthecase\n",
      "        ofunsupervisedlearning. Thealgorithmreturnsthevectoroferrors e foreach\n",
      " examplein D         ,whosemeanistheestimatedgeneralizationerror. Theerrorson\n",
      "           individualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthe\n",
      "          mean(equation).Thoughtheseconﬁdenceintervalsarenotwelljustiﬁed 5.47\n",
      "              aftertheuseofcross-validation,itisstillcommonpracticetousethemtodeclare\n",
      " thatalgorithmA   isbetterthanalgorithmB      onlyiftheconﬁdenceintervalofthe\n",
      "  errorofalgorithmA         liesbelowanddoesnotintersecttheconﬁdenceintervalof\n",
      " algorithm.B\n",
      "    DeﬁneKFoldXV( ): D,A,L,k\n",
      "       Require: D,thegivendataset,withelements z( ) i\n",
      " Require:A            ,thelearningalgorithm,seenasafunctionthattakesadatasetas\n",
      "     inputandoutputsalearnedfunction\n",
      " Require:L           ,thelossfunction,seenasafunctionfromalearnedfunctionfand\n",
      "  anexample z( ) i      ∈∈ Dtoascalar R\n",
      "     Require:k,thenumberoffolds\n",
      "       Splitintomutuallyexclusivesubsets Dk D i    ,whoseunionis D\n",
      "      for do i k fromto1\n",
      "f i= (A D D\\ i)\n",
      " for z( ) j in D ido\n",
      "e j= (Lf i , z( ) j)\n",
      " endfor\n",
      " endfor\n",
      " Return e\n",
      "             (i.i.d.)datapoints.A or isanyfunctionofthedata: pointestimatorstatistic\n",
      "ˆ θ m= (g x(1 )     ,..., x( ) m ). (5.19)\n",
      "     Thedeﬁnitiondoesnotrequirethatg        returnavaluethatisclosetothetrue θ\n",
      "     oreventhattherangeofg         bethesameasthesetofallowablevaluesof θ .This\n",
      "              deﬁnitionofapointestimatorisverygeneralandwouldenablethedesignerofan\n",
      "           estimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,\n",
      "1 2 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             agoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat\n",
      "   generatedthetrainingdata.\n",
      "            Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\n",
      "    thatthetrueparametervalueθ       isﬁxedbutunknown,whilethepointestimate\n",
      "ˆθ               isafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\n",
      "      functionofthedataisrandom.Thereforeˆ    θisarandomvariable.\n",
      "           Pointestimationcanalsorefertotheestimationoftherelationshipbetween\n",
      "             inputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\n",
      "estimators.\n",
      "Function Estimation   Sometimes we areinterested inperforming function\n",
      "           estimation(orfunctionapproximation). Here,wearetryingtopredictavariable\n",
      "y   givenaninputvectorx       .Weassumethatthereisafunctionf(x  )thatdescribes\n",
      "   theapproximaterelationshipbetweenyandx     .Forexample,wemayassume\n",
      "thaty=f(x )+ ,where    standsforthepartofy    thatisnotpredictablefrom\n",
      "x        .Infunctionestimation,weareinterestedinapproximatingf   withamodelor\n",
      "estimate ˆf           .Functionestimationisreallyjustthesameasestimatingaparameter\n",
      "θ   ;thefunctionestimator ˆf        issimplyapointestimatorinfunctionspace.The\n",
      "          linearregressionexample(discussedinsection)andthepolynomialregression 5.1.4\n",
      "           example(discussedinsection)bothillustratescenariosthatmaybeinterpreted 5.2\n",
      "    aseitherestimatingaparameterw   orestimatingafunctionˆf  mappingfromx\n",
      " to.y\n",
      "           Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\n",
      "       discusswhattheytellusabouttheseestimators.\n",
      " 5.4.2Bias\n",
      "       Thebiasofanestimatorisdeﬁnedas\n",
      "bias(ˆθ m) = ( Eˆθ m   )−θ, (5.20)\n",
      "             wheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\n",
      "andθ     isthetrueunderlyingvalueofθ     usedtodeﬁnethedata-generatingdistri-\n",
      "  bution.Anestimator ˆθ m   issaidtobeunbiasedifbias(ˆθ m) = 0  ,whichimplies\n",
      "that E(ˆθ m )=θ  .Anestimator ˆθ m   issaidtobe  asymptoticallyunbiasedif\n",
      "lim m → ∞bias(ˆθ m    ) = lim 0,whichimpliesthat m → ∞ E(ˆθ m) = θ.\n",
      "  Example:BernoulliDistribution     Considerasetofsamples{x(1 )     ,...,x( ) m}\n",
      "          thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\n",
      "1 2 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   butionwithmean:θ\n",
      " Px(( ) i ;) = θθx( ) i  (1 )−θ(1 − x( ) i) . (5.21)\n",
      "    Acommonestimatorfortheθ         parameterofthisdistributionisthemeanofthe\n",
      " trainingsamples:\n",
      "ˆθ m=1\n",
      "mm\n",
      "i =1x( ) i . (5.22)\n",
      "           Todeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\n",
      "  intoequation:5.20\n",
      "bias(ˆθ m) = [ Eˆθ m   ]−θ (5.23)\n",
      "= E\n",
      "1\n",
      "mm\n",
      "i =1x( ) i\n",
      "  −θ (5.24)\n",
      "=1\n",
      "mm\n",
      "i =1E\n",
      "x( ) i\n",
      "  −θ (5.25)\n",
      "=1\n",
      "mm\n",
      "i =11\n",
      "x( ) i=0\n",
      "x( ) iθx( ) i  (1 )−θ(1 − x( ) i)\n",
      "  −θ (5.26)\n",
      "=1\n",
      "mm\n",
      "i =1   ()θ−θ (5.27)\n",
      "    = = 0θθ− (5.28)\n",
      " Sincebias(ˆ     θ) = 0,wesaythatourestimator ˆ  θisunbiased.\n",
      "      Example:GaussianDistributionEstimatoroftheMean  Now,consider\n",
      "   asetofsamples{x(1 )     ,...,x( ) m}     thatareindependentlyandidenticallydistributed\n",
      "    accordingtoaGaussiandistributionp(x( ) i) =N(x( ) i; µ,σ2 ),where  i∈{1     ,...,m}.\n",
      "         RecallthattheGaussianprobabilitydensityfunctionisgivenby\n",
      "px(( ) i  ;µ,σ2) =1√\n",
      "2πσ2exp\n",
      "−1\n",
      "2(x( ) i −µ)2\n",
      "σ2\n",
      " . (5.29)\n",
      "           AcommonestimatoroftheGaussianmeanparameterisknownasthesample\n",
      "mean:\n",
      "ˆµ m=1\n",
      "mm\n",
      "i =1x( ) i(5.30)\n",
      "1 2 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             Todeterminethebiasofthesamplemean,weareagaininterestedincalculating\n",
      " itsexpectation:\n",
      "bias(ˆµ m) = [ˆ Eµ m   ]−µ (5.31)\n",
      "= E\n",
      "1\n",
      "mm\n",
      "i =1x( ) i\n",
      "  −µ (5.32)\n",
      "=\n",
      "1\n",
      "mm\n",
      "i =1E\n",
      "x( ) i\n",
      "  −µ (5.33)\n",
      "=\n",
      "1\n",
      "mm\n",
      "i =1µ\n",
      "  −µ (5.34)\n",
      "    = = 0µµ− (5.35)\n",
      "             ThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean\n",
      "parameter.\n",
      "        Example:EstimatorsoftheVarianceofaGaussianDistributionFor\n",
      "          thisexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2of\n",
      "            aGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\n",
      "    Theﬁrstestimatorofσ2       weconsiderisknownasthesamplevariance\n",
      "ˆσ2\n",
      "m=1\n",
      "mm\n",
      "i =1\n",
      "x( ) i −ˆµ m2\n",
      " , (5.36)\n",
      " whereˆµ m          isthesamplemean.Moreformally,weareinterestedincomputing\n",
      "bias(ˆσ2\n",
      "m) = [ˆ Eσ2\n",
      "m  ]−σ2 . (5.37)\n",
      "      Webeginbyevaluatingtheterm E[ˆσ2\n",
      "m]:\n",
      "E[ˆσ2\n",
      "m] = E\n",
      "1\n",
      "mm\n",
      "i =1\n",
      "x( ) i −ˆµ m2\n",
      "(5.38)\n",
      "=  m−1\n",
      "mσ2(5.39)\n",
      "         Returningtoequation,weconcludethatthebiasof 5.37 ˆσ2\n",
      "mis−σ2/m .Therefore,\n",
      "      thesamplevarianceisabiasedestimator.\n",
      "1 2 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "    Theunbiasedsamplevarianceestimator\n",
      "˜σ2\n",
      "m=1\n",
      "  m−1m \n",
      "i =1\n",
      "x( ) i −ˆµ m2\n",
      "(5.40)\n",
      "           providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\n",
      "     Thatis,weﬁndthat E[˜σ2\n",
      "m] = σ2:\n",
      "E[˜σ2\n",
      "m] = E\n",
      "1\n",
      "  m−1m \n",
      "i =1\n",
      "x( ) i −ˆµ m2\n",
      "(5.41)\n",
      "=m\n",
      "  m−1E[ˆσ2\n",
      "m ] (5.42)\n",
      "=m\n",
      "  m−1  m−1\n",
      "mσ2\n",
      "(5.43)\n",
      "= σ2 . (5.44)\n",
      "             Wehavetwoestimators:oneisbiased,andtheotherisnot.Whileunbiased\n",
      "            estimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe\n",
      "           willseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\n",
      "    5.4.3VarianceandStandardError\n",
      "             Anotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\n",
      "                weexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\n",
      "            expectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.\n",
      "        Thevarianceofanestimatorissimplythevariance\n",
      "Var(ˆ θ) (5.45)\n",
      "             wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\n",
      "       varianceiscalledthe ,denoted standarderror SE(ˆθ).\n",
      "             Thevariance,orthestandarderror,ofanestimatorprovidesameasureofhow\n",
      "             wewouldexpecttheestimatewecomputefromdatatovaryasweindependently\n",
      "          resamplethedatasetfromtheunderlyingdata-generatingprocess.Justaswe\n",
      "               mightlikeanestimatortoexhibitlowbias,wewouldalsolikeittohaverelatively\n",
      " lowvariance.\n",
      "            Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate\n",
      "             ofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave\n",
      "           obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\n",
      "1 2 5    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             beendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof\n",
      "     errorthatwewanttoquantify.\n",
      "        Thestandarderrorofthemeanisgivenby\n",
      "SE(ˆµ m) =Var\n",
      "1\n",
      "mm \n",
      "i =1x( ) i\n",
      "=σ√m , (5.46)\n",
      "whereσ2      isthetruevarianceofthesamplesxi     .Thestandarderrorisoften\n",
      "     estimatedbyusinganestimateofσ      .Unfortunately,neitherthesquarerootof\n",
      "             thesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance\n",
      "          provideanunbiasedestimateofthestandarddeviation.Bothapproachestend\n",
      "            tounderestimatethetruestandarddeviationbutarestillusedinpractice.The\n",
      "             squarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\n",
      "       Forlarge,theapproximationisquitereasonable. m\n",
      "            Thestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\n",
      "            Weoftenestimatethegeneralizationerrorbycomputingthesamplemeanofthe\n",
      "              erroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe\n",
      "           accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\n",
      "            tellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\n",
      "             wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\n",
      "            fallsinanychoseninterval.Forexample,the95percentconﬁdenceintervalcentered\n",
      "   onthemean ˆµ mis\n",
      "(ˆµ m −196SE( ˆ.µ m )ˆ,µ m +196SE( ˆ.µ m )), (5.47)\n",
      "     underthenormaldistributionwithmean ˆµ m andvarianceSE(ˆµ m)2  .Inmachine\n",
      "        learningexperiments,itiscommontosaythatalgorithmA   isbetterthanalgorithm\n",
      "B              iftheupperboundofthe95percentconﬁdenceintervalfortheerrorofalgorithm\n",
      "A              islessthanthelowerboundofthe95percentconﬁdenceintervalfortheerror\n",
      "  ofalgorithm.B\n",
      " Example: BernoulliDistribution       Weonceagainconsiderasetofsamples\n",
      "{x(1 )     ,...,x( ) m}       drawnindependentlyandidenticallyfromaBernoullidistribution\n",
      "(recallP(x( ) i;θ) =θx( ) i(1 −θ)(1 − x( ) i)       ).Thistimeweareinterestedincomputing\n",
      "    thevarianceoftheestimator ˆθ m=1\n",
      "mm\n",
      "i =1x( ) i.\n",
      "Var\n",
      "ˆθ m\n",
      "= Var\n",
      "1\n",
      "mm \n",
      "i =1x( ) i\n",
      "(5.48)\n",
      "1 2 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "=1\n",
      "m2m \n",
      "i =1Var\n",
      "x( ) i\n",
      "(5.49)\n",
      "=1\n",
      "m2m \n",
      "i =1   θ θ (1−) (5.50)\n",
      "=1\n",
      "m2   m θ θ (1−) (5.51)\n",
      "=1\n",
      "m   θ θ (1−) (5.52)\n",
      "         Thevarianceoftheestimatordecreasesasafunctionof m    ,thenumberofexamples\n",
      "             inthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\n",
      "        returntowhenwediscussconsistency(seesection).5.4.5\n",
      "         5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared\n",
      "Error\n",
      "            Biasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias\n",
      "            measurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\n",
      "             Varianceontheotherhand,providesameasureofthedeviationfromtheexpected\n",
      "            estimatorvaluethatanyparticularsamplingofthedataislikelytocause.\n",
      "            Whathappenswhenwearegivenachoicebetweentwoestimators,onewith\n",
      "             morebiasandonewithmorevariance?Howdowechoosebetweenthem?For\n",
      "           example,imaginethatweareinterestedinapproximatingthefunctionshownin\n",
      "               ﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2\n",
      "           onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?\n",
      "           Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation.\n",
      "         Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\n",
      "     natively,wecanalsocomparethe  meansquarederror   (MSE)oftheestimates:\n",
      "MSE = [( Eˆθ m − θ)2 ] (5.53)\n",
      "= Bias(ˆθ m)2 +Var(ˆ θ m ) (5.54)\n",
      "          TheMSEmeasurestheoverallexpecteddeviation—inasquarederrorsense—\n",
      "         betweentheestimatorandthetruevalueoftheparameter θ    .Asisclearfrom\n",
      "           equation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\n",
      "           DesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\n",
      "          managetokeepboththeirbiasandvariancesomewhatincheck.\n",
      "           Therelationshipbetweenbiasandvarianceistightlylinkedtothemachine\n",
      "        learningconceptsofcapacity,underﬁttingandoverﬁtting.Whengeneralization\n",
      "1 2 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "CapacityBias Generalization\n",
      "errorVariance\n",
      "Optimal\n",
      "capacityOverﬁtting zone Underﬁtting zone\n",
      "     Figure5.6:Ascapacityincreases(x       -axis),bias(dotted)tendstodecreaseandvariance\n",
      "           (dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\n",
      "              curve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting\n",
      "              whenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship\n",
      "           issimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin\n",
      "    sectionandﬁgure. 5.2 5.3\n",
      "            errorismeasuredbytheMSE(wherebiasandvariancearemeaningfulcomponents\n",
      "          ofgeneralizationerror),increasingcapacitytendstoincreasevarianceanddecrease\n",
      "              bias.Thisisillustratedinﬁgure,whereweseeagaintheU-shapedcurveof 5.6\n",
      "      generalizationerrorasafunctionofcapacity.\n",
      " 5.4.5Consistency\n",
      "              Sofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\n",
      "              ﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe\n",
      "             amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\n",
      "  ofdatapointsm          inourdatasetincreases,ourpointestimatesconvergetothetrue\n",
      "          valueofthecorrespondingparameters.Moreformally,wewouldlikethat\n",
      "plimm → ∞ˆθ m = θ. (5.55)\n",
      " Thesymbolplim        indicatesconvergenceinprobability,meaningthatforany >0,\n",
      "P(|ˆθ m   −|θ>)→ 0as  m→∞       .Theconditiondescribedbyequationis5.55\n",
      " knownasconsistency         .Itissometimesreferredtoasweakconsistency,with\n",
      "    strongconsistencyreferringtothe almostsure  convergenceofˆθtoθ.Almost\n",
      "1 2 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      " sureconvergence     ofasequenceofrandomvariables x(1 ) , x(2 )   ,...  toavaluex\n",
      "  occurswhenp(lim m → ∞ x( ) m= ) = 1x.\n",
      "           Consistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\n",
      "          numberofdataexamplesgrows.However,thereverseisnottrue—asymptotic\n",
      "        unbiasednessdoesnotimplyconsistency. Forexample,considerestimatingthe\n",
      " meanparameterµ   ofanormaldistributionN(x; µ,σ2    ),withadatasetconsisting\n",
      "ofmsamples:{x(1 )     ,...,x( ) m}      .Wecouldusetheﬁrstsamplex(1 )  ofthedataset\n",
      "   asanunbiasedestimator: ˆθ=x(1 )   .Inthatcase, E(ˆθ m )=θ   ,sotheestimator\n",
      "             isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\n",
      "           thattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\n",
      "       estimatorasitisthecasethat not ˆθ m     →→∞θmas .\n",
      "   5.5MaximumLikelihoodEstimation\n",
      "           Wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzedtheirproperties.\n",
      "           Butwheredidtheseestimatorscomefrom?Ratherthanguessingthatsome\n",
      "            functionmightmakeagoodestimatorandthenanalyzingitsbiasandvariance,\n",
      "             wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁcfunctions\n",
      "      thataregoodestimatorsfordiﬀerentmodels.\n",
      "         Themostcommonsuchprincipleisthemaximumlikelihoodprinciple.\n",
      "   Considerasetofmexamples X={x(1 )     ,...,x( ) m}  drawnindependentlyfrom\n",
      "      thetruebutunknowndata-generatingdistributionp d a ta() x.\n",
      "Letp m o d e l( x;θ         )beaparametricfamilyofprobabilitydistributionsoverthe\n",
      "   samespaceindexedbyθ   .Inotherwords,p m o d e l(x;θ   )mapsanyconﬁgurationx\n",
      "        toarealnumberestimatingthetrueprobabilityp d a ta()x.\n",
      "         Themaximumlikelihoodestimatorforisthendeﬁnedas θ\n",
      "θ M L  = argmax\n",
      "θp m o d e l  (;) Xθ, (5.56)\n",
      " = argmax\n",
      "θm \n",
      "i =1p m o d e l(x( ) i  ;)θ. (5.57)\n",
      "          Thisproductovermanyprobabilitiescanbeinconvenientforvariousreasons.\n",
      "            Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient\n",
      "           butequivalentoptimizationproblem,weobservethattakingthelogarithmofthe\n",
      "    likelihooddoesnotchangeits  argmax     butdoesconvenientlytransformaproduct\n",
      "1 2 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "  intoasum:\n",
      "θ M L  = argmax\n",
      "θm \n",
      "i =1 logp m o d e l(x( ) i  ;)θ. (5.58)\n",
      " Becausethe  argmax          doesnotchangewhenwerescalethecostfunction,wecan\n",
      " dividebym            toobtainaversionofthecriterionthatisexpressedasanexpectation\n",
      "      withrespecttotheempiricaldistributionˆp d a ta     deﬁnedbythetrainingdata:\n",
      "θ M L  = argmax\n",
      "θE x ∼ ˆ p d a t a logp m o d e l  (;)xθ. (5.59)\n",
      "            Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\n",
      "      thedissimilaritybetweentheempiricaldistributionˆp d a ta    ,deﬁnedbythetraining\n",
      "            setandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\n",
      "          measuredbytheKLdivergence.TheKLdivergenceisgivenby\n",
      "D K L(ˆp d a tap m o d e l) = E x ∼ ˆ p d a t a[log ˆp d a ta    ()logx−p m o d e l  ()]x. (5.60)\n",
      "              Thetermontheleftisafunctiononlyofthedata-generatingprocess,notthe\n",
      "             model.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\n",
      "  needonlyminimize\n",
      " − E x ∼ ˆ p d a t a  [logp m o d e l  ()]x, (5.61)\n",
      "           whichisofcoursethesameasthemaximizationinequation.5.59\n",
      "         MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\n",
      "          entropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to\n",
      "          identifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,\n",
      "              butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\n",
      "           entropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe\n",
      "           probabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe\n",
      "        cross-entropybetweentheempiricaldistributionandaGaussianmodel.\n",
      "             Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\n",
      "    tributionmatchtheempiricaldistributionˆp d a ta      .Ideally,wewouldliketomatch\n",
      "   thetruedata-generatingdistributionp d a ta        ,butwehavenodirectaccesstothis\n",
      "distribution.\n",
      "  Whiletheoptimalθ         isthesameregardlessofwhetherwearemaximizingthe\n",
      "           likelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n",
      "            arediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.\n",
      "        Maximumlikelihoodthusbecomesminimizationofthenegativelog-likelihood\n",
      "         (NLL),orequivalently,minimizationofthecross-entropy.Theperspectiveof\n",
      "          maximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\n",
      "            becausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\n",
      "        log-likelihoodcanactuallybecomenegativewhenisreal-valued. x\n",
      "1 3 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "      5.5.1ConditionalLog-LikelihoodandMeanSquaredError\n",
      "           Themaximumlikelihoodestimatorcanreadilybegeneralizedtoestimateacondi-\n",
      " tionalprobabilityP( y x|;θ    )inordertopredictygivenx    .Thisisactuallythe\n",
      "            mostcommonsituationbecauseitformsthebasisformostsupervisedlearning.If\n",
      "X     representsallourinputsandY      allourobservedtargets,thentheconditional\n",
      "   maximumlikelihoodestimatoris\n",
      "θ M L  = argmax\n",
      "θ     P . ( ;)YX|θ (5.62)\n",
      "             Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\n",
      "θ M L  = argmax\n",
      "θm \n",
      "i =1  log(Py( ) i |x( ) i  ;)θ. (5.63)\n",
      "     Example:LinearRegressionasMaximumLikelihood  Linearregression,\n",
      "           introducedinsection,maybejustiﬁedasamaximumlikelihoodprocedure. 5.1.4\n",
      "            Previously,wemotivatedlinearregressionasanalgorithmthatlearnstotakean\n",
      "inputx    andproduceanoutputvalueˆy  . Themappingfromxtoˆy  ischosento\n",
      "            minimizemeansquarederror,acriterionthatweintroducedmoreorlessarbitrarily.\n",
      "            Wenowrevisitlinearregressionfromthepointofviewofmaximumlikelihood\n",
      "      estimation.Insteadofproducingasinglepredictionˆy      ,wenowthinkofthemodel\n",
      "    asproducingaconditionaldistributionp(  y|x      ).Wecanimaginethatwithan\n",
      "            inﬁnitelylargetrainingset,wemightseeseveraltrainingexampleswiththesame\n",
      " inputvaluex   butdiﬀerentvaluesofy        .Thegoalofthelearningalgorithmisnow\n",
      "   toﬁtthedistributionp(  y|x    )toallthosediﬀerenty    valuesthatareallcompatible\n",
      "withx           .Toderivethesamelinearregressionalgorithmweobtainedbefore,we\n",
      "deﬁnep(  y|x) =N(y;ˆy(x;w) ,σ2  ).Thefunctionˆy(x;w    )givesthepredictionof\n",
      "               themeanoftheGaussian.Inthisexample,weassumethatthevarianceisﬁxedto\n",
      " someconstantσ2            chosenbytheuser.Wewillseethatthischoiceofthefunctional\n",
      " formofp(  y|x         )causesthemaximumlikelihoodestimationproceduretoyieldthe\n",
      "           samelearningalgorithmaswedevelopedbefore.Sincetheexamplesareassumed\n",
      "          tobei.i.d.,theconditionallog-likelihood(equation)isgivenby 5.63\n",
      "m\n",
      "i =1 log(py( ) i |x( ) i  ;)θ (5.64)\n",
      "     = log−mσ−m\n",
      "2 log(2)π−m\n",
      "i =1ˆy( ) i −y( ) i2\n",
      "2σ2 , (5.65)\n",
      "1 3 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "where ˆy( ) i        istheoutputofthelinearregressiononthei -thinputx( ) iandm isthe\n",
      "          numberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\n",
      " squarederror,\n",
      "MSE tr a i n =1\n",
      "mm \n",
      "i =1|| ˆy( ) i −y( ) i||2 , (5.66)\n",
      "         weimmediatelyseethatmaximizingthelog-likelihoodwithrespecttowyields\n",
      "     thesameestimateoftheparametersw      asdoesminimizingthemeansquarederror.\n",
      "             Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This\n",
      "             justiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\n",
      "         willsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\n",
      "    5.5.2PropertiesofMaximumLikelihood\n",
      "              Themainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\n",
      "         bethebestestimatorasymptotically,asthenumberofexamples  m→∞  ,interms\n",
      "       ofitsrateofconvergenceasincreases. m\n",
      "    Underappropriateconditions, the maximum likelihoodestimator hasthe\n",
      "            propertyofconsistency(seesection),meaningthatasthenumberoftraining 5.4.5\n",
      "         examplesapproachesinﬁnity,themaximumlikelihoodestimateofaparameter\n",
      "            convergestothetruevalueoftheparameter.Theseconditionsareasfollows:\n",
      "•  Thetruedistributionp d a ta     mustliewithinthemodelfamilyp m o d e l (· ;θ ).\n",
      "     Otherwise,noestimatorcanrecoverp d a ta.\n",
      "•  Thetruedistributionp d a ta      mustcorrespondtoexactlyonevalueofθ .Oth-\n",
      "      erwise,maximumlikelihoodcanrecoverthecorrectp d a ta    butwillnotbeable\n",
      "           todeterminewhichvalueofwasusedbythedata-generatingprocess. θ\n",
      "         Thereareotherinductiveprinciplesbesidesthemaximumlikelihoodestimator,\n",
      "          manyofwhichsharethepropertyofbeingconsistentestimators.Consistent\n",
      "     estimatorscandiﬀer,however,intheir  statisticaleﬃciency   ,meaningthatone\n",
      "           consistentestimatormayobtainlowergeneralizationerrorforaﬁxednumberof\n",
      "samplesm            ,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof\n",
      " generalizationerror.\n",
      "      Statisticaleﬃciencyistypicallystudiedinthe  parametriccase  (asinlinear\n",
      "             regression),whereourgoalistoestimatethevalueofaparameter(assumingit\n",
      "               ispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto\n",
      "              measurehowclosewearetothetrueparameterisbytheexpectedmeansquared\n",
      "          error,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter\n",
      "1 3 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "     values,wheretheexpectationisoverm     trainingsamplesfromthedata-generating\n",
      "       distribution.Thatparametricmeansquarederrordecreasesasm  increases,and\n",
      "form          large,theCramér-Raolowerbound(,; ,)showsthat Rao1945Cramér1946\n",
      "           noconsistentestimatorhasalowerMSEthanthemaximumlikelihoodestimator.\n",
      "         Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften\n",
      "           consideredthepreferredestimatortouseformachinelearning.Whenthenumber\n",
      "          ofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies\n",
      "              suchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\n",
      "        thathaslessvariancewhentrainingdataislimited.\n",
      "  5.6BayesianStatistics\n",
      "    Sofarwehavediscussed  frequentiststatistics    andapproachesbasedonestimat-\n",
      "    ingasinglevalueofθ         ,thenmakingallpredictionsthereafterbasedonthatone\n",
      "         estimate.Anotherapproachistoconsiderallpossiblevaluesofθ  whenmakinga\n",
      "        prediction.ThelatteristhedomainofBayesianstatistics.\n",
      "       Asdiscussedinsection, the frequentistperspectiveis thatthe true 5.4.1\n",
      " parametervalueθ       isﬁxedbutunknown,whilethepointestimate ˆθ  isarandom\n",
      "               variableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\n",
      "         TheBayesianperspectiveonstatisticsisquitediﬀerent. TheBayesianuses\n",
      "            probabilitytoreﬂectdegreesofcertaintyinstatesofknowledge.Thedatasetis\n",
      "             directlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ\n",
      "           isunknownoruncertainandthusisrepresentedasarandomvariable.\n",
      "        Beforeobservingthedata,werepresentourknowledgeofθ usingtheprior\n",
      " probabilitydistribution,p(θ       )(sometimesreferredtoassimply“theprior”).\n",
      "          Generally,themachinelearningpractitionerselectsapriordistributionthatis\n",
      "              quitebroad(i.e.,withhighentropy)toreﬂectahighdegreeofuncertaintyinthe\n",
      " valueofθ           beforeobservinganydata.Forexample,onemightassumeapriorithat\n",
      "θ           liesinsomeﬁniterangeorvolume,withauniformdistribution. Manypriors\n",
      "         insteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitude\n",
      "         coeﬃcients,orafunctionthatisclosertobeingconstant).\n",
      "         Nowconsiderthatwehaveasetofdatasamples{x(1 )     ,...,x( ) m}  .Wecan\n",
      "        recovertheeﬀectofdataonourbeliefaboutθ    bycombiningthedatalikelihood\n",
      "px((1 )     ,...,x( ) m       |θ)withthepriorviaBayes’rule:\n",
      "  px(θ|(1 )     ,...,x( ) m) =px((1 )     ,...,x( ) m |θθ)(p)\n",
      "px((1 )     ,...,x( ) m)(5.67)\n",
      "1 3 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             InthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\n",
      "          relativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\n",
      "             ofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\n",
      "      fewhighlylikelyvaluesoftheparameters.\n",
      "        Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo\n",
      "         importantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes\n",
      "     predictionsusingapointestimateofθ       ,theBayesianapproachistomakepredictions\n",
      "    usingafulldistributionoverθ    .Forexample,afterobservingm  examples,the\n",
      "       predicteddistributionoverthenextdatasample,x( +1 ) m   ,isgivenby\n",
      "px(( +1 ) m |x(1 )     ,...,x( ) m) =\n",
      "px(( +1 ) m   ||θθ)(px(1 )     ,...,x( ) m  )d.θ(5.68)\n",
      "   Hereeachvalueofθ       withpositiveprobabilitydensitycontributestotheprediction\n",
      "            ofthenextexample,withthecontributionweightedbytheposteriordensityitself.\n",
      "  Afterhavingobserved{x(1 )     ,...,x( ) m}        ,ifwearestillquiteuncertainaboutthe\n",
      " valueofθ          ,thenthisuncertaintyisincorporateddirectlyintoanypredictionswe\n",
      " mightmake.\n",
      "           Insection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\n",
      "      taintyinagivenpointestimateofθ      byevaluatingitsvariance.Thevarianceof\n",
      "            theestimatorisanassessmentofhowtheestimatemightchangewithalternative\n",
      "              samplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\n",
      "              withtheuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto\n",
      "           protectwellagainstoverﬁtting. Thisintegralisofcoursejustanapplicationof\n",
      "            thelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\n",
      "            frequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\n",
      "            decisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\n",
      "estimate.\n",
      "         ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation\n",
      "            andthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n",
      "           priordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity\n",
      "            towardsregionsoftheparameterspacethatarepreferredapriori.Inpractice,\n",
      "             theprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\n",
      "             CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\n",
      "   judgmentaﬀectingthepredictions.\n",
      "         Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\n",
      "            isavailablebuttypicallysuﬀerfromhighcomputationalcostwhenthenumberof\n",
      "   trainingexamplesislarge.\n",
      "1 3 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   Example:BayesianLinearRegression     HereweconsidertheBayesianesti-\n",
      "          mationapproachtolearningthelinearregressionparameters.Inlinearregression,\n",
      "        welearnalinearmappingfromaninputvector  x∈ Rn     topredictthevalueofa\n",
      "             scalar.Thepredictionisparametrizedbythevector y∈ R w∈ Rn:\n",
      " ˆy= w x. (5.69)\n",
      "   Givenasetofm   trainingsamples(X( ) tr a i n ,y( ) tr a i n     ),wecanexpresstheprediction\n",
      "       ofovertheentiretrainingsetas y\n",
      "ˆy( ) tr a i n= X( ) tr a i n w. (5.70)\n",
      "       ExpressedasaGaussianconditionaldistributionony( ) tr a i n  ,wehave\n",
      "p(y( ) tr a i n |X( ) tr a i n ,wy ) = (N( ) tr a i n ;X( ) tr a i n  wI,) (5.71)\n",
      " ∝exp\n",
      "−1\n",
      "2(y( ) tr a i n −X( ) tr a i nw)(y( ) tr a i n −X( ) tr a i nw)\n",
      ",\n",
      "(5.72)\n",
      "           wherewefollowthestandardMSEformulationinassumingthattheGaussian\n",
      " varianceony            isone.Inwhatfollows,toreducethenotationalburden,wereferto\n",
      "(X( ) tr a i n ,y( ) tr a i n    ) ( ) assimplyXy,.\n",
      "         Todeterminetheposteriordistributionoverthemodelparametervectorw ,we\n",
      "             ﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief\n",
      "            aboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural\n",
      "               toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\n",
      "           typicallyassumeafairlybroaddistribution,expressingahighdegreeofuncertainty\n",
      "aboutθ            . Forreal-valuedparametersitiscommontouseaGaussianasaprior\n",
      "distribution,\n",
      " p() = (;wNwµ 0 , Λ 0 ) exp∝\n",
      "−1\n",
      "2  (wµ− 0)Λ− 1\n",
      "0  (wµ− 0)\n",
      " ,(5.73)\n",
      "whereµ 0and Λ 0        arethepriordistributionmeanvectorandcovariancematrix\n",
      "respectively.1\n",
      "           Withthepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior\n",
      "    distributionoverthemodelparameters:\n",
      "        p,p,p (wX|y) ∝(yX|w)()w (5.74)\n",
      "1              Unlessthereisareasontouseaparticularcovariancestructure,wetypicallyassumea\n",
      "   diagonalcovariancematrix Λ 0= diag( λ 0).\n",
      "135    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      " ∝exp\n",
      "−1\n",
      "2  ( )yXw−  ( )yXw−\n",
      "exp\n",
      "−1\n",
      "2  (wµ− 0)Λ− 1\n",
      "0  (wµ− 0)\n",
      "(5.75)\n",
      " ∝exp\n",
      "−1\n",
      "2\n",
      "−2y  Xww+X  Xww+Λ− 1\n",
      "0  wµ−2\n",
      "0 Λ− 1\n",
      "0w\n",
      ".\n",
      "(5.76)\n",
      "  Wenowdeﬁne Λ m=\n",
      "X  X+ Λ− 1\n",
      "0− 1andµ m= Λ m\n",
      "X  y+ Λ− 1\n",
      "0µ 0\n",
      " .Us-\n",
      "              ingthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian\n",
      "distribution:\n",
      "    p, (wX|y) exp∝\n",
      "−1\n",
      "2  (wµ− m)Λ− 1\n",
      "m  (wµ− m )+1\n",
      "2µ\n",
      "m Λ− 1\n",
      "mµ m\n",
      "(5.77)\n",
      " ∝exp\n",
      "−1\n",
      "2  (wµ− m)Λ− 1\n",
      "m  (wµ− m)\n",
      " . (5.78)\n",
      "        Alltermsthatdonotincludetheparametervectorw   havebeenomitted;they\n",
      "              areimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\n",
      "         EquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\n",
      "           Examiningthisposteriordistributionenablesustogainsomeintuitionforthe\n",
      "        eﬀectofBayesianinference.Inmostsituations,wesetµ 0to 0   .Ifweset Λ 0=1\n",
      "αI,\n",
      "thenµ m    givesthesameestimateofw      asdoesfrequentistlinearregressionwitha\n",
      "   weightdecaypenaltyofαww        .OnediﬀerenceisthattheBayesianestimateis\n",
      " undeﬁnedifα           issettozero—wearenotallowedtobegintheBayesianlearning\n",
      "      processwithaninﬁnitelywideprioronw      .Themoreimportantdiﬀerenceisthat\n",
      "           theBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\n",
      "           diﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m.\n",
      "     5.6.1MaximumaPosteriori(MAP)Estimation\n",
      "            WhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\n",
      "    posteriordistributionovertheparameterθ        ,itisstilloftendesirabletohavea\n",
      "           singlepointestimate. Onecommonreasonfordesiringapointestimateisthat\n",
      "          mostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\n",
      "          intractable,andapointestimateoﬀersatractableapproximation.Ratherthan\n",
      "            simplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\n",
      "             thebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice\n",
      "             ofthepointestimate.Onerationalwaytodothisistochoosethemaximum\n",
      " aposteriori          (MAP)pointestimate.TheMAPestimatechoosesthepointof\n",
      "1 3 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "          maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\n",
      "   caseofcontinuous):θ\n",
      "θMAP  = argmax\n",
      "θ   p( ) = argmax θx|\n",
      "θ       log( )+log() pxθ|pθ. (5.79)\n",
      "     Werecognize,ontherighthandside, logp(  xθ|     ),thatis,thestandardlog-\n",
      "         likelihoodterm,and ,correspondingtothepriordistribution. log()pθ\n",
      "            Asanexample,consideralinearregressionmodelwithaGaussianprioron\n",
      " theweightsw      .IfthispriorisgivenbyN(w; 0,1\n",
      "λI2     ),thenthelog-priortermin\n",
      "      equationisproportionaltothefamiliar 5.79 λww     weightdecaypenalty,plusa\n",
      "     termthatdoesnotdependonw       anddoesnotaﬀectthelearningprocess.MAP\n",
      "            BayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\n",
      "decay.\n",
      "           AswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\n",
      "             leveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\n",
      "           trainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\n",
      "             MAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\n",
      "    thepriceofincreasedbias.\n",
      "        Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearning\n",
      "           regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\n",
      "           tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\n",
      "          addinganextratermtotheobjectivefunctionthatcorrespondsto  logp(θ ).Not\n",
      "         allregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\n",
      "           someregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\n",
      "            Otherregularizationtermsdependonthedata,whichofcourseapriorprobability\n",
      "     distributionisnotallowedtodo.\n",
      "         MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\n",
      "         yetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\n",
      "              termcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\n",
      "       distribution,astheprior(NowlanandHinton1992,).\n",
      "   5.7SupervisedLearningAlgorithms\n",
      "          Recallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\n",
      "            learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\n",
      "     trainingsetofexamplesofinputsx andoutputsy    . Inmanycasestheoutputs\n",
      "y            maybediﬃculttocollectautomaticallyandmustbeprovidedbyahuman\n",
      "1 3 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            “supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere\n",
      " collectedautomatically.\n",
      "   5.7.1ProbabilisticSupervisedLearning\n",
      "  Most supervised learning algorithms inthis book are b asedon estimating a\n",
      " probabilitydistributionp(  y| x        ).Wecandothissimplybyusingmaximum\n",
      "       likelihoodestimationtoﬁndthebestparametervector θ   foraparametricfamily\n",
      "     ofdistributions .py(| x θ;)\n",
      "          Wehavealreadyseenthatlinearregressioncorrespondstothefamily\n",
      "    py y (|N x θ;) = (; θ  x I,). (5.80)\n",
      "           Wecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga\n",
      "            diﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and\n",
      "              class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\n",
      "             probabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\n",
      "    mustaddupto1.\n",
      "          Thenormaldistributionoverreal-valuednumbersthatweusedforlinear\n",
      "              regressionisparametrizedintermsofamean.Anyvaluewesupplyforthismean\n",
      "            isvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\n",
      "                 itsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\n",
      "             thelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe\n",
      "         interval(0,1)andinterpretthatvalueasaprobability:\n",
      "   py σ (= 1 ;) = | x θ ( θ x). (5.81)\n",
      "    Thisapproachisknownas  logisticregression    (asomewhatstrangenamesince\n",
      "        weusethemodelforclassiﬁcationratherthanregression).\n",
      "              Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby\n",
      "          solvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There\n",
      "            isnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\n",
      "            thembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative\n",
      "   log-likelihoodusinggradientdescent.\n",
      "           Thissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\n",
      "          bywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\n",
      "       therightkindofinputandoutputvariables.\n",
      "1 3 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   5.7.2SupportVectorMachines\n",
      "            Oneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector\n",
      "             machine( ,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\n",
      "          logisticregressioninthatitisdrivenbyalinearfunctionwx+b  .Unlikelogistic\n",
      "          regression,thesupportvectormachinedoesnotprovideprobabilities,butonly\n",
      "             outputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\n",
      "wx+b           ispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\n",
      "w    x+bisnegative.\n",
      "         Onekeyinnovationassociatedwithsupportvectormachinesisthe k e r nel\n",
      "t r i c k           .Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\n",
      "            canbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\n",
      "              itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan\n",
      "  bere-writtenas\n",
      "w    x+= +bbm\n",
      "i =1α ixx( ) i , (5.82)\n",
      "wherex( ) i    isatrainingexample,andα      isavectorofcoeﬃcients.Rewritingthe\n",
      "       learningalgorithmthiswayenablesustoreplacex      withtheoutputofagivenfeature\n",
      "functionφ(x      ) andthedotproductwithafunctionk( xx,( ) i) =φ(x) ·φ(x( ) i) called\n",
      "a k e r nel .The ·       operatorrepresentsaninnerproductanalogoustoφ(x)φ(x( ) i).\n",
      "             Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\n",
      "             someinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for\n",
      "          example,innerproductsbasedonintegrationratherthansummation.Acomplete\n",
      "             developmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\n",
      "          Afterreplacingdotproductswithkernelevaluations,wecanmakepredictions\n",
      "  usingthefunction\n",
      " fb () = x +\n",
      "iα i k,(xx( ) i ). (5.83)\n",
      "      Thisfunctionisnonlinearwithrespecttox    ,buttherelationshipbetweenφ(x)\n",
      "andf(x      )islinear.Also,therelationshipbetweenαandf(x   )islinear.The\n",
      "          kernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying\n",
      "             φ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\n",
      "              Thekerneltrickispowerfulfortworeasons.First,itenablesustolearnmodels\n",
      "      thatarenonlinearasafunctionofx    usingconvexoptimizationtechniquesthat\n",
      "          areguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxed\n",
      "  andoptimizeonlyα         ,thatis,theoptimizationalgorithmcanviewthedecision\n",
      "           functionasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoften\n",
      "1 3 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "         admitsanimplementationthatissigniﬁcantlymorecomputationallyeﬃcientthan\n",
      "          naivelyconstructingtwovectorsandexplicitlytakingtheirdotproduct. φ()x\n",
      "  Insomecases,φ(x         )canevenbeinﬁnitedimensional,whichwouldresultin\n",
      "           aninﬁnitecomputationalcostforthenaive,explicitapproach.Inmanycases,\n",
      "k( xx,      )isanonlinear,tractablefunctionofx evenwhenφ(x   )isintractable.As\n",
      "           anexampleofaninﬁnite-dimensionalfeaturespacewithatractablekernel,we\n",
      "   constructafeaturemappingφ(x    )overthenonnegativeintegersx  .Supposethat\n",
      "     thismappingreturnsavectorcontainingx     onesfollowedbyinﬁnitelymanyzeros.\n",
      "     Wecanwriteakernelfunctionk( x,x( ) i) =min( x,x( ) i    )thatisexactlyequivalent\n",
      "     tothecorrespondinginﬁnite-dimensionaldotproduct.\n",
      "        Themostcommonlyusedkernelisthe , Gaussiankernel\n",
      "     k, ,σ (uvuv ) = (N−;02 I), (5.84)\n",
      "whereN(x; µ, Σ           )isthestandardnormaldensity.Thiskernelisalsoknownas\n",
      "the  radialbasisfunction        (RBF)kernel,becauseitsvaluedecreasesalonglines\n",
      "inv   spaceradiatingoutwardfromu       .TheGaussiankernelcorrespondstoadot\n",
      "            productinaninﬁnite-dimensionalspace,butthederivationofthisspaceisless\n",
      "           straightforwardthaninourexampleofthekernelovertheintegers. min\n",
      "           WecanthinkoftheGaussiankernelasperformingakindof  templatematch-\n",
      "ing   .Atrainingexamplex    associatedwithtraininglabely   becomesatemplate\n",
      " forclassy    .Whenatestpointx isnearx     accordingtoEuclideandistance,the\n",
      "       Gaussiankernelhasalargeresponse,indicatingthatx    isverysimilartothex\n",
      "            template.Themodelthenputsalargeweightontheassociatedtraininglabely.\n",
      "           Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\n",
      "     similarityofthecorrespondingtrainingexamples.\n",
      "           Supportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\n",
      "              usingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\n",
      "          categoryofalgorithmsthatemploythekerneltrickisknownas kernelmachines,\n",
      "          or ( ,; kernelmethodsWilliamsandRasmussen1996Schölkopf1999etal.,).\n",
      "             Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\n",
      "          functionislinearinthenumberoftrainingexamples,becausethei -thexample\n",
      "  contributesatermα ik( xx,( ) i       )tothedecisionfunction.Supportvectormachines\n",
      "       areabletomitigatethisbylearninganα     vectorthatcontainsmostlyzeros.\n",
      "           Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\n",
      "     thetrainingexamplesthathavenonzeroα i     .Thesetrainingexamplesareknown\n",
      "  assupportvectors.\n",
      "           Kernelmachinesalsosuﬀerfromahighcomputationalcostoftrainingwhen\n",
      "             thedatasetislarge.Werevisitthisideainsection.Kernelmachineswith 5.9\n",
      "1 4 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            generickernelsstruggletogeneralizewell.Weexplainwhyinsection.The 5.11\n",
      "           modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\n",
      "           kernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\n",
      "           ()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\n",
      "   ontheMNISTbenchmark.\n",
      "     5.7.3OtherSimpleSupervisedLearningAlgorithms\n",
      "        Wehavealreadybrieﬂyencounteredanothernonprobabilisticsupervisedlearning\n",
      "     algorithm,nearestneighborregression.Moregenerally, k   -nearestneighborsis\n",
      "             afamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa\n",
      "  nonparametriclearningalgorithm, k        -nearestneighborsisnotrestrictedtoaﬁxed\n",
      "       numberofparameters.Weusuallythinkofthe k   -nearestneighborsalgorithm\n",
      "            asnothavinganyparametersbutratherimplementingasimplefunctionofthe\n",
      "              trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\n",
      "          Instead,attesttime,whenwewanttoproduceanoutput y    foranewtestinputx,\n",
      "  weﬁndthe k   -nearestneighborstox   inthetrainingdataX    .Wethenreturnthe\n",
      "   averageofthecorresponding y        valuesinthetrainingset.Thisworksforessentially\n",
      "           anykindofsupervisedlearningwherewecandeﬁneanaverageover y values.In\n",
      "          thecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwith c y= 1\n",
      "and c i      = 0forallothervaluesof i        .Wecantheninterprettheaverageoverthese\n",
      "           one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanonparametric\n",
      " learningalgorithm, k         -nearestneighborcanachieveveryhighcapacity.Forexample,\n",
      "           supposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1\n",
      "             loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\n",
      "            numberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes\n",
      "           errorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\n",
      "           distantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx\n",
      "             willhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\n",
      "            algorithmtousealltheseneighborstovote,ratherthanrandomlychoosingone\n",
      "            ofthem,theprocedureconvergestotheBayeserrorrate. Thehighcapacityof\n",
      "k             -nearestneighborsenablesittoobtainhighaccuracygivenalargetrainingset.\n",
      "             Itdoessoathighcomputationalcost,however,anditmaygeneralizeverybadly\n",
      "       givenasmallﬁnitetrainingset. Oneweaknessof k     -nearestneighborsisthatit\n",
      "           cannotlearnthatonefeatureismorediscriminativethananother.Forexample,\n",
      "      imaginewehavearegressiontaskwith  x∈ R1 0 0    drawnfromanisotropicGaussian\n",
      "     distribution,butonlyasinglevariable x 1     isrelevanttotheoutput.Suppose\n",
      "         furtherthatthisfeaturesimplyencodestheoutputdirectly,that y= x 1 inall\n",
      "            cases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\n",
      "1 4 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "     Thenearestneighborofmostpoints x       willbedeterminedbythelargenumberof\n",
      "featuresx 2throughx 1 0 0     ,notbythelonefeaturex 1    . Thustheoutputonsmall\n",
      "     trainingsetswillessentiallyberandom.\n",
      "            Anothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\n",
      "        andhasseparateparametersforeachregionisthe  decisiontree   ( , Breimanetal.\n",
      "              1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7\n",
      "              treeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\n",
      "             regionintoonesubregionforeachchildofthenode(typicallyusinganaxis-aligned\n",
      "          cut).Spaceisthussubdividedintononoverlappingregions,withaone-to-one\n",
      "           correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\n",
      "             everypointinitsinputregiontothesameoutput.Decisiontreesareusually\n",
      "            trainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\n",
      "             learningalgorithmcanbeconsiderednonparametricifitisallowedtolearnatree\n",
      "           ofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\n",
      "            thatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\n",
      "          typicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,\n",
      "            struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\n",
      "            example,ifwehaveatwo-classproblem,andthepositiveclassoccurswherever\n",
      "x 2 >x 1            ,thedecisionboundaryisnotaxisaligned.Thedecisiontreewillthus\n",
      "           needtoapproximatethedecisionboundarywithmanynodes,implementingastep\n",
      "           functionthatconstantlywalksbackandforthacrossthetruedecisionfunction\n",
      "  withaxis-alignedsteps.\n",
      "           Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\n",
      "        limitations.Nonetheless,theyareusefullearningalgorithmswhencomputational\n",
      "          resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\n",
      "         learningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween\n",
      "        sophisticatedalgorithmsand-nearestneighborsordecisiontreebaselines. k\n",
      "          SeeMurphy2012Bishop2006Hastie 2001 (), (), etal.()orothermachine\n",
      "         learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\n",
      "   5.8UnsupervisedLearningAlgorithms\n",
      "          Recallfromsection thatunsupervisedalgorithmsarethosethatexperience 5.1.3\n",
      "          only“features”butnotasupervisionsignal.Thedistinctionbetweensupervised\n",
      "            andunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno\n",
      "              objectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\n",
      "          asupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\n",
      "           informationfromadistributionthatdonotrequirehumanlabortoannotate\n",
      "1 4 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "0\n",
      "101\n",
      "111 0 1\n",
      "011\n",
      "1111 1110110\n",
      "10010\n",
      "00 1110 11111101001 00\n",
      " 010 01111\n",
      "111\n",
      "11\n",
      "             Figure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\n",
      "                  choosestosendtheinputexampletothechildnodeontheleft(0)ortothechildnode\n",
      "               ontheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Each\n",
      "              nodeisdisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,\n",
      "                 obtainedbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=choose\n",
      "            rightorbottom). Thetreedividesspaceintoregions.The2-Dplaneshows ( Bo t t o m )\n",
      "     howadecisiontreemightdivide R2          .Thenodesofthetreeareplottedinthisplane,\n",
      "             witheachinternalnodedrawnalongthedividinglineitusestocategorizeexamples,\n",
      "               andleafnodesdrawninthecenteroftheregionofexamplestheyreceive.Theresult\n",
      "              isapiecewise-constantfunction,withonepieceperleaf.Eachleafrequiresatleastone\n",
      "                trainingexampletodeﬁne,soitisnotpossibleforthedecisiontreetolearnafunction\n",
      "          thathasmorelocalmaximathanthenumberoftrainingexamples.\n",
      "1 4 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "          examples.Thetermisusuallyassociatedwithdensityestimation,learningto\n",
      "           drawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,\n",
      "              ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n",
      " relatedexamples.\n",
      "            Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe\n",
      "             data.By“best”wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking\n",
      "        forarepresentationthatpreservesasmuchinformationaboutx  aspossiblewhile\n",
      "           obeyingsomepenaltyorconstraintaimedatkeepingtherepresentation orsimpler\n",
      "    moreaccessiblethanitself.x\n",
      "           Therearemultiplewaysofdeﬁningasimplerrepresentation.Threeofthe\n",
      "      mostcommonincludelower-dimensionalrepresentations,sparserepresentations,\n",
      "      andindependentrepresentations.Low-dimensionalrepresentationsattemptto\n",
      "    compressasmuchinformationabout x     aspossibleinasmallerrepresentation.\n",
      "         Sparserepresentations(Barlow1989OlshausenandField1996Hintonand ,; ,;\n",
      "          Ghahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\n",
      "           mostlyzerosformostinputs.Theuseofsparserepresentationstypicallyrequires\n",
      "         increasingthedimensionalityoftherepresentation,sothattherepresentation\n",
      "            becomingmostlyzerosdoesnotdiscardtoomuchinformation.Thisresultsinan\n",
      "            overallstructureoftherepresentationthattendstodistributedataalongtheaxes\n",
      "        oftherepresentationspace.Independentrepresentationsattempttodisentangle\n",
      "           thesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\n",
      "     oftherepresentationarestatisticallyindependent.\n",
      "     Of coursethese threecriteria arecertainly notmutually exclusive.Low-\n",
      "          dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\n",
      "           pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto\n",
      "            reducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying\n",
      "         andremovingmoreredundancyenablesthedimensionalityreductionalgorithmto\n",
      "      achievemorecompressionwhilediscardinglessinformation.\n",
      "             Thenotionofrepresentationisoneofthecentralthemesofdeeplearningand\n",
      "              thereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\n",
      "        simpleexamplesofrepresentationlearningalgorithms.Together,theseexample\n",
      "             algorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe\n",
      "       remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\n",
      "         developthesecriteriaindiﬀerentwaysorintroduceothercriteria.\n",
      "1 4 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "    − −20 100 10 20\n",
      "x 1−20−1001020x 2\n",
      "    − −20 100 10 20\n",
      "z 1−20−1001020z 2\n",
      "              Figure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariancewith\n",
      "            theaxesofthenewspace. ( L e f t )Theoriginaldataconsistofsamplesofx    .Inthisspace,the\n",
      "           variancemightoccuralongdirectionsthatarenotaxisaligned. ( R i g h t )Thetransformed\n",
      "dataz =xW     nowvariesmostalongtheaxisz 1     .Thedirectionofsecond-mostvariance\n",
      "   isnowalongz 2.\n",
      "   5.8.1PrincipalComponentsAnalysis\n",
      "           Insection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\n",
      "             ameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\n",
      "           algorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\n",
      "            twoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n",
      "           representationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\n",
      "           arepresentationwhoseelementshavenolinearcorrelationwitheachother.This\n",
      "            isaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare\n",
      "        statisticallyindependent.Toachievefullindependence,arepresentationlearning\n",
      "        algorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\n",
      "           PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan\n",
      "inputx  toarepresentationz          asshowninﬁgure.Insection,wesawthat 5.8 2.12\n",
      "          wecouldlearnaone-dimensionalrepresentationthatbestreconstructstheoriginal\n",
      "            data(inthesenseofmeansquarederror)andthatthisrepresentationactually\n",
      "             correspondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA\n",
      "           asasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch\n",
      "            oftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\n",
      "           reconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation\n",
      "     decorrelatestheoriginaldatarepresentation.X\n",
      "   Letusconsiderthe  mn×  designmatrixX       .Wewillassumethatthedatahas\n",
      "1 4 5    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   ameanofzero, E[x] = 0            .Ifthisisnotthecase,thedatacaneasilybecentered\n",
      "          bysubtractingthemeanfromallexamplesinapreprocessingstep.\n",
      "          TheunbiasedsamplecovariancematrixassociatedwithisgivenbyX\n",
      "Var[] =x1\n",
      "  m−1X X. (5.85)\n",
      "      PCAﬁndsarepresentation(throughlineartransformation)z=Wx ,where\n",
      "  Var[]zisdiagonal.\n",
      "            Insection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\n",
      "      aregivenbytheeigenvectorsofX   X.Fromthisview,\n",
      "X  XWW = Λ . (5.86)\n",
      "           Inthissection,weexploitanalternativederivationoftheprincipalcomponents.\n",
      "          Theprincipalcomponentsmayalsobeobtainedviasingularvaluedecomposition\n",
      "        (SVD).Speciﬁcally,theyaretherightsingularvectorsofX    .Toseethis,letWbe\n",
      "      therightsingularvectorsinthedecompositionX=UW Σ  . Wethenrecover\n",
      "         theoriginaleigenvectorequationwithastheeigenvectorbasis: W\n",
      "X X=\n",
      "UW Σ\n",
      "UW Σ = W Σ2W . (5.87)\n",
      "           TheSVDishelpfultoshowthatPCAresultsinadiagonalVar[z  ].Usingthe\n",
      "          SVDof,wecanexpressthevarianceofas: X X\n",
      "Var[] =x1\n",
      "  m−1X X (5.88)\n",
      "=1\n",
      "  m−1(UW Σ)UW Σ(5.89)\n",
      "=1\n",
      "  m−1 W ΣUUW Σ(5.90)\n",
      "=1\n",
      "  m−1 W Σ2W , (5.91)\n",
      "     whereweusethefactthatUU=I  becausetheU     matrixofthesingularvalue\n",
      "           decompositionisdeﬁnedtobeorthogonal.Thisshowsthatthecovarianceofzis\n",
      "  diagonalasrequired:\n",
      "Var[] =z1\n",
      "  m−1Z Z (5.92)\n",
      "=1\n",
      "  m−1WX XW (5.93)\n",
      "1 4 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "=1\n",
      "  m−1W W Σ2W W (5.94)\n",
      "=1\n",
      "  m−1Σ2 , (5.95)\n",
      "       wherethistimeweusethefactthatWW=I      ,againfromthedeﬁnitionofthe\n",
      "SVD.\n",
      "         Theaboveanalysisshowsthatwhenweprojectthedataxtoz   ,viathelinear\n",
      "transformationW        ,theresultingrepresentationhasadiagonalcovariancematrix\n",
      "  (asgivenby Σ2        ),whichimmediatelyimpliesthattheindividualelementsofzare\n",
      " mutuallyuncorrelated.\n",
      "            ThisabilityofPCAtotransformdataintoarepresentationwheretheelements\n",
      "             aremutuallyuncorrelatedisaveryimportantpropertyofPCA.Itisasimple\n",
      "           exampleofarepresentationthatattemptstodisentangletheunknownfactorsof\n",
      "           variationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakesthe\n",
      "          formofﬁndingarotationoftheinputspace(describedbyW   )thatalignsthe\n",
      "            principalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\n",
      " with.z\n",
      "          Whilecorrelationisanimportantcategoryofdependencybetweenelementsof\n",
      "           thedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\n",
      "            complicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\n",
      "       canbedonewithasimplelineartransformation.\n",
      "  5.8.2-meansClustering k\n",
      "        Anotherexampleofasimplerepresentationlearningalgorithmisk -meansclustering.\n",
      "Thek       -meansclusteringalgorithmdividesthetrainingsetintok  diﬀerentclusters\n",
      "              ofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\n",
      " providingak    -dimensionalone-hotcodevectorh   representinganinputx .Ifx\n",
      "  belongstoclusteri ,thenh i       = 1,andallotherentriesoftherepresentationhare\n",
      "zero.\n",
      "    Theone-hotcodeprovidedbyk       -meansclusteringisanexampleofasparse\n",
      "            representation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\n",
      "          wedevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,where\n",
      "         morethanoneentrycanbenonzeroforeachinputx     .One-hotcodesareanextreme\n",
      "            exampleofsparserepresentationsthatlosemanyofthebeneﬁtsofadistributed\n",
      "         representation.Theone-hotcodestillconferssomestatisticaladvantages(it\n",
      "              naturallyconveystheideathatallexamplesinthesameclusteraresimilartoeach\n",
      "          other),anditconfersthecomputationaladvantagethattheentirerepresentation\n",
      "1 4 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "      maybecapturedbyasingleinteger.\n",
      "Thek    -meansalgorithmworksbyinitializingk  diﬀerentcentroids{ µ(1 )     ,..., µ( ) k}\n",
      "          todiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.\n",
      "         Inonestep,eachtrainingexampleisassignedtoclusteri ,wherei   istheindexof\n",
      "  thenearestcentroid µ( ) i      .Intheotherstep,eachcentroid µ( ) i   isupdatedtothe\n",
      "     meanofalltrainingexamples x( ) j   assignedtocluster.i\n",
      "           Onediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently\n",
      "               illposed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\n",
      "            clusteringofthedatacorrespondstotherealworld.Wecanmeasureproperties\n",
      "            oftheclustering,suchastheaverageEuclideandistancefromaclustercentroid\n",
      "                tothemembersofthecluster.Thisenablesustotellhowwellweareableto\n",
      "            reconstructthetrainingdatafromtheclusterassignments.Wedonotknowhow\n",
      "           welltheclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,\n",
      "            theremaybemanydiﬀerentclusteringsthatallcorrespondwelltosomeproperty\n",
      "                oftherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut\n",
      "             obtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For\n",
      "            example,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\n",
      "               imagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\n",
      "             cars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay\n",
      "                ﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof\n",
      "              redvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\n",
      "            algorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\n",
      "              theexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\n",
      "            newclusteringnowatleastcapturesinformationaboutbothattributes,butithas\n",
      "            lostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray\n",
      "              cars,justastheyareinadiﬀerentclusterfromgraytrucks. Theoutputofthe\n",
      "              clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\n",
      "               thantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall\n",
      " weknow.\n",
      "            Theseissuesillustratesomeofthereasonsthatwemaypreferadistributed\n",
      "         representationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\n",
      "          twoattributesforeachvehicle—onerepresentingitscolorandonerepresenting\n",
      "                whetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\n",
      "          distributedrepresentationis(howcanthelearningalgorithmknowwhetherthe\n",
      "           twoattributesweareinterestedinarecolorandcar-versus-truckratherthan\n",
      "           manufacturerandage?),buthavingmanyattributesreducestheburdenonthe\n",
      "             algorithmtoguesswhichsingleattributewecareabout,andgivesustheability\n",
      "           tomeasuresimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmany\n",
      "1 4 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "        attributesinsteadofjusttestingwhetheroneattributematches.\n",
      "   5.9StochasticGradientDescent\n",
      "           Nearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\n",
      " gradientdescent       (SGD).Stochasticgradientdescentisan extensionof the\n",
      "      gradientdescentalgorithmintroducedinsection.4.3\n",
      "            Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\n",
      "          forgoodgeneralization,butlargetrainingsetsarealsomorecomputationally\n",
      "expensive.\n",
      "            Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\n",
      "           sumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\n",
      "          negativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\n",
      "J() = θ E x ,y ∼ ˆ p d a t a  L,y,(xθ) =1\n",
      "mm \n",
      "i =1L(x( ) i ,y( ) i  ,,θ) (5.96)\n",
      "            whereistheper-exampleloss L L,y, py. (xθ) = log− (|xθ;)\n",
      "        Fortheseadditivecostfunctions,gradientdescentrequirescomputing\n",
      "∇ θJ() =θ1\n",
      "mm\n",
      "i =1∇ θL(x( ) i ,y( ) i  ,.θ) (5.97)\n",
      "      ThecomputationalcostofthisoperationisO(m       ).Asthetrainingsetsizegrowsto\n",
      "            billionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\n",
      "long.\n",
      "             TheinsightofSGDisthatthegradientisanexpectation.Theexpectationmay\n",
      "            beapproximatelyestimatedusingasmallsetofsamples.Speciﬁcally,oneachstep\n",
      "      ofthealgorithm,wecansampleaminibatch ofexamples B={x(1 )     ,...,x( m)}\n",
      "        drawnuniformlyfromthetrainingset.Theminibatchsizem  istypicallychosen\n",
      "              tobearelativelysmallnumberofexamples,rangingfromonetoafewhundred.\n",
      "Crucially,m        isusuallyheldﬁxedasthetrainingsetsizem     grows.Wemayﬁta\n",
      "            trainingsetwithbillionsofexamplesusingupdatescomputedononlyahundred\n",
      "examples.\n",
      "       Theestimateofthegradientisformedas\n",
      " g=1\n",
      "m∇ θm\n",
      "i =1L(x( ) i ,y( ) i  ,θ) (5.98)\n",
      "1 4 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "          usingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\n",
      "     thenfollowstheestimatedgradientdownhill:\n",
      "     θθg←−, (5.99)\n",
      "     whereisthelearningrate. \n",
      "            Gradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\n",
      "          thepast,theapplicationofgradientdescenttononconvexoptimizationproblems\n",
      "           wasregardedasfoolhardyorunprincipled.Today,weknowthatthemachine\n",
      "            learningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\n",
      "            descent.Theoptimizationalgorithmmaynotbeguaranteedtoarriveatevena\n",
      "               localminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\n",
      "        ofthecostfunctionquicklyenoughtobeuseful.\n",
      "          Stochasticgradientdescenthasmanyimportantusesoutsidethecontextof\n",
      "              deeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\n",
      "               datasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\n",
      "  trainingsetsizem             .Inpractice,weoftenusealargermodelasthetrainingsetsize\n",
      "               increases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\n",
      "       convergenceusuallyincreaseswithtrainingsetsize. However,asmapproaches\n",
      "            inﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\n",
      "         SGDhassampledeveryexampleinthetrainingset.Increasingm   furtherwillnot\n",
      "             extendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\n",
      "              error.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\n",
      "          amodelwithSGDisasafunctionof. O(1) m\n",
      "             Priortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\n",
      "              wastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\n",
      "   algorithmsrequireconstructinganmm×matrixG i , j=k(x( ) i ,x( ) j ).Constructing\n",
      "    thismatrixhascomputationalcostO(m2      ),whichisclearlyundesirablefordatasets\n",
      "     with billionsof examp les.Inacademia,starting in2006, deep learning was\n",
      "           initiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\n",
      "          thancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\n",
      "          thousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\n",
      "            industrybecauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\n",
      "datasets.\n",
      "          Stochasticgradientdescentandmanyenhancementstoitaredescribedfurther\n",
      "  inchapter.8\n",
      "1 5 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "     5.10BuildingaMachineLearningAlgorithm\n",
      "           Nearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\n",
      "            afairlysimplerecipe: combineaspeciﬁcationofadataset,acostfunction,an\n",
      "    optimizationprocedureandamodel.\n",
      "          Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n",
      "     Xyand,thecostfunction\n",
      " J,b(w) = − E x ,y ∼ ˆ p d a t a logp m o d e l   ( )y|x, (5.100)\n",
      "  themodelspeciﬁcationp m o d e l(  y|x) =N(y;xw+b,     1),and,inmostcases,the\n",
      "             optimizationalgorithmdeﬁnedbysolvingforwherethegradientofthecostiszero\n",
      "   usingthenormalequations.\n",
      "           Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\n",
      "          fromtheothers,wecanobtainawiderangeofalgorithms.\n",
      "            Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearning\n",
      "           processtoperformstatisticalestimation.Themostcommoncostfunctionisthe\n",
      "         negativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\n",
      " likelihoodestimation.\n",
      "          Thecostfunctionmayalsoincludeadditionalterms,suchasregularization\n",
      "             terms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\n",
      " toobtain\n",
      " J,bλ (w) = ||||w2\n",
      "2 − E x ,y ∼ ˆ p d a t a logp m o d e l   ( )y|x. (5.101)\n",
      "     Thisstillallowsclosedformoptimization.\n",
      "              Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger\n",
      "            beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\n",
      "     optimizationprocedure,suchasgradientdescent.\n",
      "           Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\n",
      "        optimizationalgorithmssupportsbothsupervisedandunsupervisedlearning.The\n",
      "         linearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\n",
      "          learningcanbesupportedbydeﬁningadatasetthatcontainsonlyX andproviding\n",
      "            anappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst\n",
      "        PCAvectorbyspecifyingthatourlossfunctionis\n",
      "J() = w E x ∼ ˆ p d a t a   ||−||xr(;)xw2\n",
      "2 (5.102)\n",
      "      whileourmodelisdeﬁnedtohavew     withnormoneandreconstructionfunction\n",
      "r() = xwxw.\n",
      "1 5 1    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             Insomecases,thecostfunctionmaybeafunctionthatwecannotactually\n",
      "          evaluate,forcomputationalreasons.Inthesecases,wecanstillapproximately\n",
      "             minimizeitusingiterativenumericaloptimization,aslongaswehavesomewayof\n",
      "  approximatingitsgradients.\n",
      "            Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot\n",
      "           beimmediatelyobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n",
      "            handdesigned,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\n",
      "     models,suchasdecisiontreesand k     -means,requirespecial-caseoptimizersbecause\n",
      "           theircostfunctionshaveﬂatregionsthatmaketheminappropriateforminimization\n",
      "        bygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\n",
      "               canbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa\n",
      "            taxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather\n",
      "           thanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.\n",
      "    5.11ChallengesMotivatingDeepLearning\n",
      "            Thesimplemachinelearningalgorithmsdescribedinthischapterworkwellona\n",
      "           widevarietyofimportantproblems.Theyhavenotsucceeded,however,insolving\n",
      "           thecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\n",
      "            Thedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\n",
      "        traditionalalgorithmstogeneralizewellonsuchAItasks.\n",
      "            Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes\n",
      "         exponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow\n",
      "         themechanismsusedtoachievegeneralizationintraditionalmachinelearning\n",
      "         areinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\n",
      "           spacesalsooftenimposehighcomputationalcosts.Deeplearningwasdesignedto\n",
      "    overcometheseandotherobstacles.\n",
      "    5.11.1TheCurseofDimensionality\n",
      "         Manymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber\n",
      "            ofdimensionsinthedataishigh.Thisphenomenonisknownasthe curseof\n",
      "dimensionality          .Ofparticularconcernisthatthenumberofpossibledistinct\n",
      "            conﬁgurationsofasetofvariablesincreasesexponentiallyasthenumberofvariables\n",
      "increases.\n",
      "           Thecurseofdimensionalityarisesinmanyplacesincomputerscience,especially\n",
      "  inmachinelearning.\n",
      "1 5 2    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "              Figure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\n",
      "           right),thenumberofconﬁgurationsofinterestmaygrowexponentially. ( L e f t )Inthis\n",
      "             one-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\n",
      "             regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\n",
      "            correspondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\n",
      "              Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin\n",
      "         eachregion(andpossiblyinterpolatebetweenneighboringregions). Withtwo ( C e n t e r )\n",
      "              dimensions,itismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable.Weneed\n",
      "      tokeeptrackofupto10 ×           10 = 100 regions,andweneedatleastthatmanyexamplesto\n",
      "          coverallthoseregions. Withthreedimensions,thisgrowsto ( R i g h t ) 103= 1 , 000regions\n",
      "      andatleastthatmanyexamples.For d  dimensionsand v    valuestobedistinguishedalong\n",
      "     eachaxis,weseemtoneed O( vd          )regionsandexamples.Thisisaninstanceofthecurse\n",
      "       ofdimensionality.FiguregraciouslyprovidedbyNicolasChapados.\n",
      "           Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\n",
      "            Asillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9\n",
      "  possibleconﬁgurationsof x        ismuchlargerthanthenumberoftrainingexamples.\n",
      "              Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\n",
      "             grid,asintheﬁgure.Wecandescribelow-dimensionalspacewithasmallnumber\n",
      "               ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\n",
      "             point,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\n",
      "               thatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\n",
      "   densityatsomepoint x          ,wecanjustreturnthenumberoftrainingexamplesin\n",
      "     thesameunitvolumecellas x        ,dividedbythetotalnumberoftrainingexamples.\n",
      "               Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\n",
      "              examplesinthesamecell.Ifwearedoingregression,wecanaveragethetarget\n",
      "              valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\n",
      "           wehaveseennoexample?Becauseinhigh-dimensionalspaces,thenumberof\n",
      "             conﬁgurationsishuge,muchlargerthanournumberofexamples,atypicalgridcell\n",
      "            hasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n",
      "        meaningfulaboutthesenewconﬁgurations?Manytraditionalmachinelearning\n",
      "1 5 3    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "            algorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\n",
      "         thesameastheoutputatthenearesttrainingpoint.\n",
      "     5.11.2LocalConstancyandSmoothnessRegularization\n",
      "            Togeneralizewell,machinelearningalgorithmsneedtobeguidedbypriorbeliefs\n",
      "             aboutwhatkindoffunctiontheyshouldlearn.Wehaveseenthesepriorsincorpo-\n",
      "            ratedasexplicitbeliefsintheformofprobabilitydistributionsoverparametersof\n",
      "            themodel.Moreinformally,wemayalsodiscusspriorbeliefsasdirectlyinﬂuencing\n",
      "             the itselfandinﬂuencingtheparametersonlyindirectly,asaresultofthe function\n",
      "         relationshipbetweentheparametersandthefunction.Additionally,weinformally\n",
      "          discusspriorbeliefsasbeingexpressedimplicitlybychoosingalgorithmsthat\n",
      "           arebiasedtowardchoosingsomeclassoffunctionsoveranother,eventhough\n",
      "               thesebiasesmaynotbeexpressed(orevenbepossibletoexpress)intermsofa\n",
      "         probabilitydistributionrepresentingourdegreeofbeliefinvariousfunctions.\n",
      "         Amongthemostwidelyusedoftheseimplicit“priors” isthesmoothness\n",
      "prior ,or  localconstancyprior        .Thispriorstatesthatthefunctionwelearn\n",
      "        shouldnotchangeverymuchwithinasmallregion.\n",
      "           Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\n",
      "              asaresult,theyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\n",
      "          leveltasks.Throughoutthisbook,wedescribehowdeeplearningintroduces\n",
      "       additional(explicit andimplicit)priors inordertoreducethe generalization\n",
      "            erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\n",
      "   insuﬃcientforthesetasks.\n",
      "            Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief\n",
      "            thatthelearnedfunctionshouldbesmoothorlocallyconstant.Allthesediﬀerent\n",
      "           methodsaredesignedtoencouragethelearningprocesstolearnafunction f∗that\n",
      "  satisﬁesthecondition\n",
      "f∗ () x≈ f∗   (+)x  (5.103)\n",
      "  formostconﬁgurationsx  andsmallchange         .Inotherwords,ifweknowagood\n",
      "   answerforaninputx  (forexample,ifx      isalabeledtrainingexample),thenthat\n",
      "       answerisprobablygoodintheneighborhoodofx      .Ifwehaveseveralgoodanswers\n",
      "            insomeneighborhood,wewouldcombinethem(bysomeformofaveragingor\n",
      "              interpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\n",
      "possible.\n",
      "         Anextremeexampleofthelocalconstancyapproachisthe k  -nearestneighbors\n",
      "          familyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n",
      "    regioncontainingallthepointsx     thathavethesamesetof k   nearestneighborsin\n",
      "1 5 4    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "   thetrainingset.Fork        = 1,thenumberofdistinguishableregionscannotbemore\n",
      "     thanthenumberoftrainingexamples.\n",
      " Whilethek         -nearestneighborsalgorithmcopiestheoutputfromnearbytraining\n",
      "         examples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\n",
      "            withnearbytrainingexamples.Animportantclassofkernelsisthefamilyof l o c a l\n",
      "k e r nel s ,wherek( u v,   )islargewhen u= v  anddecreasesas uand v growfurther\n",
      "              apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\n",
      "          thatperformstemplatematching,bymeasuringhowcloselyatestexample x\n",
      "   resembleseachtrainingexample x( ) i      . Muchofthemodernmotivationfordeep\n",
      "           learningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\n",
      "             howdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n",
      "   ( ,). Bengioetal.2006b\n",
      "         Decisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based\n",
      "             learning,becausetheybreaktheinputspaceintoasmanyregionsasthereare\n",
      "           leavesanduseaseparateparameter(orsometimesmanyparametersforextensions\n",
      "              ofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\n",
      "leastn       leavestoberepresentedaccurately,thenatleastn   trainingexamplesare\n",
      "       requiredtoﬁtthetree.Amultipleofn       isneededtoachievesomelevelofstatistical\n",
      "    conﬁdenceinthepredictedoutput.\n",
      "   Ingeneral,todistinguishO(k        ) regionsininputspace,allthesemethodsrequire\n",
      "O(k   )examples.Typicallythere areO(k  )parameters,withO  (1)parameters\n",
      "    associatedwitheachoftheO(k       )regions.Thenearestneighborscenario,inwhich\n",
      "              eachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustratedin\n",
      " ﬁgure.5.10\n",
      "              Isthereawaytorepresentacomplexfunctionthathasmanymoreregionsto\n",
      "          bedistinguishedthanthenumberoftrainingexamples?Clearly,assumingonly\n",
      "             smoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.For\n",
      "            example,imaginethatthetargetfunctionisakindofcheckerboard.Acheckerboard\n",
      "            containsmanyvariations,butthereisasimplestructuretothem.Imaginewhat\n",
      "           happenswhenthenumberoftrainingexamplesissubstantiallysmallerthanthe\n",
      "            numberofblackandwhitesquaresonthecheckerboard.Basedononlylocal\n",
      "           generalizationandthesmoothnessorlocalconstancyprior,thelearnerwouldbe\n",
      "               guaranteedtocorrectlyguessthecolorofanewpointifitlaywithinthesame\n",
      "           checkerboardsquareasatrainingexample.Thereisnoguarantee,however,that\n",
      "           thelearnercouldcorrectlyextendthecheckerboardpatterntopointslyingin\n",
      "            squaresthatdonotcontaintrainingexamples.Withthisprioralone,theonly\n",
      "                informationthatanexampletellsusisthecolorofitssquare,andtheonlywayto\n",
      "                getthecolorsoftheentirecheckerboardrightistocovereachofitscellswithat\n",
      "1 5 5    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "  leastoneexample.\n",
      "        Thesmoothnessassumptionandtheassociatednonparametriclearningalgo-\n",
      "             rithmsworkextremelywellaslongasthereareenoughexamplesforthelearning\n",
      "             algorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\n",
      "             ofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\n",
      "            functiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\n",
      "             Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina\n",
      "          diﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently\n",
      "             invariousregions,itcanbecomeextremelycomplicatedtodescribewithasetof\n",
      "            trainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\n",
      "             numberofregionscomparedtothenumberofexamples),isthereanyhopeto\n",
      " generalizewell?\n",
      "           Theanswertobothofthesequestions—whether itispossibletorepresent\n",
      "           acomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated\n",
      "               functiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthataverylarge\n",
      "    numberofregions,suchas O(2k    ),canbedeﬁnedwith O( k     ) examples,solongaswe\n",
      "             Figure5.10:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\n",
      "             intoregions.Anexample(representedherebyacircle)withineachregiondeﬁnesthe\n",
      "       regionboundary(representedherebythelines).The y    valueassociatedwitheachexample\n",
      "            deﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. The\n",
      "            regionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi\n",
      "            diagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber\n",
      "            oftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor\n",
      "          algorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\n",
      "          localsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\n",
      "           onlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\n",
      "  surroundingthatexample.\n",
      "1 5 6    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "        introducesomedependenciesbetweentheregionsthroughadditionalassumptions\n",
      "          abouttheunderlyingdata-generatingdistribution.Inthisway,wecanactually\n",
      "          generalizenonlocally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\n",
      "          diﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\n",
      "             reasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\n",
      "         Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-\n",
      "           sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\n",
      "             theassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\n",
      "          strong,task-speciﬁcassumptionsinneuralnetworkssothattheycangeneralize\n",
      "              toamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\n",
      "           complextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,\n",
      "         sowewantlearningalgorithmsthatembodymoregeneral-purposeassumptions.\n",
      "               Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby\n",
      "            thecompositionoffactors,orfeatures,potentiallyatmultiplelevelsinahierar-\n",
      "          chy.Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearning\n",
      "          algorithms.Theseapparentlymildassumptionsallowanexponentialgaininthe\n",
      "            relationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\n",
      "          bedistinguished.Wedescribetheseexponentialgainsmorepreciselyinsections\n",
      "            6.4.115.415.5 ,and.Theexponentialadvantagesconferredbytheuseofdeep\n",
      "         distributedrepresentationscountertheexponentialchallengesposedbythecurse\n",
      " ofdimensionality.\n",
      "  5.11.3ManifoldLearning\n",
      "            Animportantconceptunderlyingmanyideasinmachinelearningisthatofa\n",
      "manifold.\n",
      "A m a ni f o l d      isaconnectedregion.Mathematically, it isa setof points\n",
      "           associatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\n",
      "            manifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\n",
      "                 thesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n",
      " 3-Dspace.\n",
      "           Theconceptofaneighborhoodsurroundingeachpointimpliestheexistenceof\n",
      "             transformationsthatcanbeappliedtomoveonthemanifoldfromonepositionto\n",
      "              aneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan\n",
      "     walknorth,south,east,orwest.\n",
      "           Althoughthereisaformalmathematicalmeaningtotheterm“manifold,”in\n",
      "             machinelearningittendstobeusedmorelooselytodesignateaconnectedset\n",
      "             ofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\n",
      "1 5 7    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "       05 10 15 20 25 30 35 40 . . . . . . . .−10 .−05 .00 .05 .10 .15 .20 .25 .\n",
      "             Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\n",
      "            concentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\n",
      "       theunderlyingmanifoldthatthelearnershouldinfer.\n",
      "          degreesoffreedom,ordimensions,embeddedinahigher-dimensionalspace.Each\n",
      "            dimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan 5.11\n",
      "           exampleoftrainingdatalyingnearaone-dimensionalmanifoldembeddedintwo-\n",
      "           dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\n",
      "              ofthemanifoldtovaryfromonepointtoanother.Thisoftenhappenswhena\n",
      "              manifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle\n",
      "            dimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\n",
      "          Manymachinelearningproblemsseemhopelessifweexpectthemachine\n",
      "          learningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\n",
      " Manifoldlearning        algorithmssurmountthisobstaclebyassumingthatmost\n",
      "of Rn          consistsofinvalidinputs,andthatinterestinginputsoccuronlyalong\n",
      "           acollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\n",
      "           variationsintheoutputofthelearnedfunctionoccurringonlyalongdirections\n",
      "            thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\n",
      "            movefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\n",
      "          ofcontinuous-valueddataandintheunsupervisedlearningsetting,althoughthis\n",
      "           probabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\n",
      "          supervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\n",
      " highlyconcentrated.\n",
      "           Theassumptionthatthedataliesalongalow-dimensionalmanifoldmaynot\n",
      "               alwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas\n",
      "           thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\n",
      "1 5 8    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "           Figure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\n",
      "             accordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisa\n",
      "              nonzeroprobabilityofgeneratinganimageofafaceorofanyotherobjectfrequently\n",
      "            encounteredinAIapplications,weneveractuallyobservethishappeninginpractice.This\n",
      "            suggeststhattheimagesencounteredinAIapplicationsoccupyanegligibleproportionof\n",
      "    thevolumeofimagespace.\n",
      "           atleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\n",
      "    oftwocategoriesofobservations.\n",
      "            Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-\n",
      "1 5 9    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "             bilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\n",
      "        highlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\n",
      "         fromthesedomains. Figure showshow,instead,uniformlysampledpoints 5.12\n",
      "              looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\n",
      "            isavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\n",
      "           random,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\n",
      "             text?Almostzero,again,becausemostofthelongsequencesoflettersdonot\n",
      "          correspondtoanaturallanguagesequence:thedistributionofnaturallanguage\n",
      "             sequencesoccupiesaverylittlevolumeinthetotalspaceofsequencesofletters.\n",
      "          Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshowthat\n",
      "             thedataliesonareasonablysmallnumberofmanifolds.Wemustalsoestablish\n",
      "            thattheexamplesweencounterareconnectedtoeachotherbyotherexamples,\n",
      "            witheachexamplesurroundedbyotherhighlysimilarexamplesthatcanbereached\n",
      "         byapplyingtransformationstotraversethemanifold. Thesecondargumentin\n",
      "            favorofthemanifoldhypothesisisthatwecanimaginesuchneighborhoodsand\n",
      "            transformations,atleastinformally.Inthecaseofimages,wecancertainlythink\n",
      "             ofmanypossibletransformationsthatallowustotraceoutamanifoldinimage\n",
      "            space:wecangraduallydimorbrightenthelights,graduallymoveorrotate\n",
      "              objectsintheimage,graduallyalterthecolorsonthesurfacesofobjects,andso\n",
      "          forth.Multiplemanifoldsarelikelyinvolvedinmostapplications.Forexample,\n",
      "              themanifoldofhumanfaceimagesmaynotbeconnectedtothemanifoldofcat\n",
      " faceimages.\n",
      "         Thesethoughtexperimentsconveysomeintuitivereasonssupportingthemani-\n",
      "         foldhypothesis.Morerigorousexperiments(,; Cayton2005NarayananandMitter,\n",
      "             2010Schölkopf1998RoweisandSaul2000Tenenbaum 2000Brand ; etal.,; ,; etal.,;,\n",
      "           2003BelkinandNiyogi2003DonohoandGrimes2003WeinbergerandSaul ; ,; ,; ,\n",
      "              2004)clearlysupportthehypothesisforalargeclassofdatasetsofinterestinAI.\n",
      "             Whenthedataliesonalow-dimensionalmanifold,itcanbemostnaturalfor\n",
      "            machinelearningalgorithmstorepresentthedataintermsofcoordinatesonthe\n",
      "       manifold,ratherthanintermsofcoordinatesin Rn      .Ineverydaylife,wecanthink\n",
      "             ofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionstospeciﬁc\n",
      "             addressesintermsofaddressnumbersalongthese1-Droads,notintermsof\n",
      "          coordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallengingbut\n",
      "          holdsthepromiseofimprovingmanymachinelearningalgorithms.Thisgeneral\n",
      "            principleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\n",
      "               adatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\n",
      "             methodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6\n",
      "         howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\n",
      "1 6 0    C HAP T E R 5 . M A C HI NE L E ARNI N G B AS I C S\n",
      "           Thisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\n",
      "           andmachinelearningthatareemployedthroughouttheremainingpartsofthe\n",
      "            book.Youarenowpreparedtoembarkonyourstudyofdeeplearning.\n",
      "           Figure5.13: TrainingexamplesfromtheQMULMultiviewFaceDataset( , Gongetal.\n",
      "                 2000),forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-\n",
      "           dimensionalmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearning\n",
      "            algorithmstobeabletodiscoveranddisentanglesuchmanifoldcoordinates.Figure20.6\n",
      "   illustratessuchafeat.\n",
      "1 6 1 P a rt I I\n",
      "  D e e p N e t w ork s : Mo dern\n",
      "P r ac t i ces\n",
      "162              Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\n",
      "    usedtosolvepracticalapplications.\n",
      "          Deeplearninghasalonghistoryandmanyaspirations.Severalproposed\n",
      "             approacheshaveyettoentirelybearfruit.Severalambitiousgoalshaveyettobe\n",
      "            realized.Theseless-developedbranchesofdeeplearningappearintheﬁnalpartof\n",
      " thebook.\n",
      "           Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\n",
      "       nologiesthatarealreadyusedheavilyinindustry.\n",
      "         Moderndeeplearningprovidesapowerfulframeworkforsupervisedlearning.\n",
      "              Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcanrepresent\n",
      "           functionsofincreasingcomplexity.Mosttasksthatconsistofmappinganinput\n",
      "                vectortoanoutputvector,andthatareeasyforapersontodorapidly,canbe\n",
      "         accomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃciently\n",
      "           largedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed\n",
      "             asassociatingonevectortoanother,orthatarediﬃcultenoughthataperson\n",
      "             wouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remain\n",
      "       beyondthescopeofdeeplearningfornow.\n",
      "          Thispartofthebookdescribesthecoreparametricfunctionapproximation\n",
      "           technologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\n",
      "       Webegin by describingthefeedforwarddeep networkmodel thatis usedto\n",
      "         representthesefunctions.Next,wepresentadvancedtechniquesforregularization\n",
      "             andoptimizationofsuchmodels.Scalingthesemodelstolargeinputssuchashigh-\n",
      "         resolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\n",
      "           theconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\n",
      "         networkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\n",
      "          forthepracticalmethodologyinvolvedindesigning,building,andconﬁguringan\n",
      "         applicationinvolvingdeeplearningandreviewsomeofitsapplications.\n",
      "          Thesechaptersarethemostimportantforapractitioner—someonewhowants\n",
      "          tobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\n",
      " problemstoday.\n",
      "1 6 3 C h a p t e r 6\n",
      "  D e e p F e e dforw ard N e t w ork s\n",
      "  Deepfeedforwardnetworks  ,alsocalled   feedforwardneuralnetworks ,or\n",
      " multilayerperceptrons       (MLPs),arethequintessentialdeeplearningmodels.\n",
      "          Thegoalofafeedforwardnetworkistoapproximatesomefunction f∗  .Forexample,\n",
      "  foraclassiﬁer, y= f∗( x   )mapsaninput x  toacategory y   .Afeedforwardnetwork\n",
      "  deﬁnesamapping y= f( x; θ       )andlearnsthevalueoftheparameters θ thatresult\n",
      "    inthebestfunctionapproximation.\n",
      "   Thesemodelsarecalledfeedforward     becauseinformationﬂowsthroughthe\n",
      "   functionbeingevaluatedfrom x      ,throughtheintermediatecomputationsusedto\n",
      "deﬁne f     ,andﬁnallytotheoutput y   .Therearenofeedback   connectionsinwhich\n",
      "            outputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\n",
      "        areextendedtoincludefeedbackconnections,theyarecalled  recurrentneural\n",
      "     networks,aspresentedinchapter.10\n",
      "         Feedforwardnetworksareofextremeimportancetomachinelearningpracti-\n",
      "          tioners.Theyformthebasisofmanyimportantcommerci alapplications.For\n",
      "           example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\n",
      "         specializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\n",
      "           steppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\n",
      " languageapplications.\n",
      "    Feedforwardneuralnetworksarecallednetworks    becausetheyaretypically\n",
      "          representedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-\n",
      "           ciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed\n",
      "       together.Forexample,wemighthavethreefunctions f(1 ), f(2 ) ,and f(3 )connected\n",
      "    inachain,toform f( x )= f(3 )( f(2 )( f(1 )( x     ))).Thesechainstructuresarethe\n",
      "         mostcommonlyusedstructuresofneuralnetworks.Inthiscase, f(1 ) iscalled\n",
      "the ﬁrstlayer  ofthenetwork, f(2 )  iscalledthe secondlayer   ,andsoon. The\n",
      "164    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "      overalllengthofthechaingivesthedepth      ofthemodel.Thename“deeplearning”\n",
      "             arosefromthisterminology.Theﬁnallayerofafeedforwardnetworkiscalledthe\n",
      " outputlayer      .Duringneuralnetworktraining,wedrivef( x  )tomatchf∗( x).\n",
      "         Thetrainingdataprovidesuswithnoisy,approximateexamplesoff∗( x) evaluated\n",
      "     atdiﬀerenttrainingpoints.Eachexample x    isaccompaniedbyalabel  yf≈∗( x).\n",
      "             Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint\n",
      "x         ;itmustproduceavaluethatisclosetoy       .Thebehavioroftheotherlayersis\n",
      "           notdirectlyspeciﬁedbythetrainingdata.Thelearningalgorithmmustdecide\n",
      "              howtousethoselayerstoproducethedesiredoutput,butthetrainingdatado\n",
      "            notsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\n",
      "           decidehowtousetheselayerstobestimplementanapproximationoff∗ .Because\n",
      "              thetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,they\n",
      "   arecalledhiddenlayers.\n",
      "           Finally,thesenetworksarecalledneuralbecausetheyarelooselyinspiredby\n",
      "           neuroscience.Eachhiddenlayerofthenetworkistypicallyvectorvalued.The\n",
      "      dimensionalityofthesehiddenlayersdeterminesthewidth   ofthemodel.Each\n",
      "              elementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\n",
      "           Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\n",
      "          wecanalsothinkofthelayerasconsistingofmanyunits   thatactinparallel,\n",
      "          eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\n",
      "             thesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\n",
      "          activationvalue.Theideaofusingmanylayersofvector-valuedrepresentations\n",
      "        isdrawnfromneuroscience.Thechoiceofthefunctionsf( ) i( x   )usedtocompute\n",
      "         theserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout\n",
      "         thefunctionsthatbiologicalneuronscompute.Modernneuralnetworkresearch,\n",
      "          however,isguidedbymanymathematicalandengineeringdisciplines,andthe\n",
      "                goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\n",
      "         feedforwardnetworksasfunctionapproximationmachinesthataredesignedto\n",
      "         achievestatisticalgeneralization,occasionallydrawingsomeinsightsfromwhatwe\n",
      "          knowaboutthebrain,ratherthanasmodelsofbrainfunction.\n",
      "           Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\n",
      "          andconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogistic\n",
      "           regressionandlinearregression,areappealingbecausetheycanbeﬁteﬃciently\n",
      "            andreliably,eitherinclosedformorwithconvexoptimization.Linearmodelsalso\n",
      "             havetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\n",
      "          themodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\n",
      "        Toextendlinearmodelstorepresentnonlinearfunctionsof x   ,wecanapply\n",
      "    thelinearmodelnotto x     itselfbuttoatransformedinputφ( x ),whereφ isa\n",
      "1 6 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "          nonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\n",
      "           section,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\n",
      "theφ     mapping.Wecanthinkofφ      asprovidingasetoffeaturesdescribing x ,or\n",
      "      asprovidinganewrepresentationfor. x\n",
      "         Thequestionisthenhowtochoosethemapping.φ\n",
      "1.       Oneoptionistouseaverygenericφ    ,suchastheinﬁnite-dimensionalφthat\n",
      "          isimplicitlyusedbykernelmachinesbasedontheRBFkernel. Ifφ( x )is\n",
      "            ofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe\n",
      "           trainingset,butgeneralizationtothetestsetoftenremainspoor.Very\n",
      "           genericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\n",
      "          smoothnessanddonotencodeenoughpriorinformationtosolveadvanced\n",
      "problems.\n",
      "2.      Anotheroptionistomanuallyengineerφ      .Untiltheadventofdeeplearning,\n",
      "           thiswasthedominantapproach.Itrequiresdecadesofhumaneﬀortfor\n",
      "         eachseparatetask,withpractitionersspecializingindiﬀerentdomains,such\n",
      "          asspeechrecognitionorcomputervision,andwithlittletransferbetween\n",
      "domains.\n",
      "3.       Thestrategyofdeeplearningistolearnφ       .Inthisapproach,wehaveamodel\n",
      "y=f( x; θ w,) =φ( x; θ)w    .Wenowhaveparameters θ    thatweusetolearn\n",
      "φ       fromabroadclassoffunctions,andparameters w  thatmapfromφ( x )to\n",
      "            thedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\n",
      "φ              deﬁningahiddenlayer.Thisapproachistheonlyoneofthethreethat\n",
      "            givesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh\n",
      "         theharms.Inthisapproach,weparametrizetherepresentationasφ( x; θ)\n",
      "       andusetheoptimizationalgorithmtoﬁndthe θ    thatcorrespondstoagood\n",
      "            representation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst\n",
      "            approachbybeinghighlygeneric—wedosobyusingaverybroadfamily\n",
      "φ( x; θ           ).Deeplearningcanalsocapturethebeneﬁtofthesecondapproach.\n",
      "         Humanpractitionerscanencodetheirknowledgetohelpgeneralizationby\n",
      " designingfamiliesφ( x; θ        )thattheyexpectwillperformwell.Theadvantage\n",
      "            isthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction\n",
      "       familyratherthanﬁndingpreciselytherightfunction.\n",
      "          Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\n",
      "            thefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeof\n",
      "            deeplearningthatappliestoallthekindsofmodelsdescribedthroughoutthis\n",
      "         book.Feedforwardnetworks aretheapplicationofthisprincipletolearning\n",
      "1 6 6    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "  deterministicmappingsfrom xto y     thatlackfeedbackconnections.Othermodels,\n",
      "         presentedlater,applytheseprinciplestolearningstochasticmappings,functions\n",
      "        withfeedback,andprobabilitydistributionsoverasinglevector.\n",
      "            Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\n",
      "            weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\n",
      "           First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign\n",
      "            decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\n",
      "             function,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\n",
      "            learning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\n",
      "          tofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\n",
      "        hiddenlayer,andthisrequiresustochoosethe  activationfunctions thatwill\n",
      "             beusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\n",
      "            ofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese\n",
      "         layersshouldbeconnected toeachother, and how manyunitsshouldbein\n",
      "          eachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\n",
      "     ofcomplicatedfunctions.Wepresenttheback-propagation   algorithmandits\n",
      "          moderngeneralizations,whichcanbeusedtoeﬃcientlycomputethesegradients.\n",
      "      Finally,weclosewithsomehistoricalperspective.\n",
      "   6.1Example: LearningXOR\n",
      "             Tomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\n",
      "            exampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning\n",
      "  theXORfunction.\n",
      "           TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1\n",
      "and x 2              .Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\n",
      "            returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\n",
      "y= f∗( x          )thatwewanttolearn.Ourmodelprovidesafunction y= f( x; θ ),and\n",
      "      ourlearningalgorithmwilladapttheparameters θ tomake f   assimilaraspossible\n",
      " to f∗.\n",
      "           Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.\n",
      "          Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0] ,[0 ,1],\n",
      "[1 ,0]  ,and[1 ,1]}          . Wewilltrainthenetworkonallfourofthesepoints. The\n",
      "       onlychallengeistoﬁtthetrainingset.\n",
      "             Wecantreatthisproblemasaregressionproblemanduseameansquared\n",
      "             errorlossfunction.Wehavechosenthislossfunctiontosimplifythemathfor\n",
      "             thisexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n",
      "1 6 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "         appropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\n",
      "    aredescribedinsection .6.2.2.2\n",
      "          Evaluatedonourwholetrainingset,theMSElossfunctionis\n",
      "J() =θ1\n",
      "4\n",
      "x ∈ X(f∗   () (;))x−fxθ2 . (6.1)\n",
      "        Nowwemustchoosetheformofourmodel,f(x;θ    ).Supposethatwechoose\n",
      "               alinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θ wb\n",
      "  f,b (;xw) = x   w+b. (6.2)\n",
      "  WecanminimizeJ(θ      )inclosedformwithrespecttowandb  usingthenormal\n",
      "equations.\n",
      "      Aftersolvingthenormalequations,weobtainw= 0andb=1\n",
      "2  .Thelinear\n",
      "   modelsimplyoutputs0.        5everywhere.Whydoesthishappen?Figureshows 6.1\n",
      "               howalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\n",
      "               thisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha\n",
      "       linearmodelisabletorepresentthesolution.\n",
      "          Speciﬁcally,wewillintroduceasimplefeedforwardnetworkwithonehidden\n",
      "             layercontainingtwohiddenunits.Seeﬁgureforanillustrationofthismodel. 6.2\n",
      "        Thisfeedforwardnetworkhasavectorofhiddenunitsh    thatarecomputedbya\n",
      "functionf(1 )(x;  Wc,            ).Thevaluesofthesehiddenunitsarethenusedastheinput\n",
      "               forasecondlayer.Thesecondlayeristheoutputlayerofthenetwork.Theoutput\n",
      "             layerisstilljustalinearregressionmodel,butnowitisappliedtoh  ratherthantox.\n",
      "       Thenetworknowcontainstwofunctionschainedtogether,h=f(1 )(x;  Wc, )and\n",
      "y=f(2 )(h; w,b     ),withthecompletemodelbeingf(x;    Wcw,,,b) =f(2 )(f(1 )(x)).\n",
      "  Whatfunctionshouldf(1 )        compute?Linearmodelshaveserveduswellsofar,\n",
      "      anditmaybetemptingtomakef(1 )   linearaswell. Unfortunately,iff(1 )were\n",
      "             linear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\n",
      "         itsinput.Ignoringtheintercepttermsforthemoment,supposef(1 )(x) =Wx\n",
      "andf(2 )(h) =hw .Thenf(x) =x Ww      .Wecouldrepresentthisfunctionas\n",
      "f() = xxw wherew = Ww.\n",
      "            Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural\n",
      "          networksdosousinganaﬃnetransformationcontrolledbylearnedparameters,\n",
      "            followedbyaﬁxednonlinearfunctioncalledanactivationfunction.Weusethat\n",
      "   strategyhere,bydeﬁningh=g(Wx+c),whereW     providestheweightsofa\n",
      "  lineartransformationandc       thebiases.Previously,todescribealinearregression\n",
      "              model,weusedavectorofweightsandascalarbiasparametertodescribean\n",
      "1 6 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      " 0 1\n",
      "x 101x 2  Originalspacex\n",
      "  0 1 2\n",
      "h 101h 2  Learnedspaceh\n",
      "            Figure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\n",
      "               printedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\n",
      "             (Left)AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\n",
      " function.When x1      = 0,themodel’soutputmustincreaseas x2  increases.When x1= 1,\n",
      "     themodel’soutputmustdecreaseas x 2        increases.Alinearmodelmustapplyaﬁxed\n",
      "coeﬃcient w 2to x 2         .Thelinearmodelthereforecannotusethevalueof x 1 tochange\n",
      "  thecoeﬃcienton x 2         andcannotsolvethisproblem. Inthetransformedspace (Right)\n",
      "              representedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\n",
      "              theproblem. Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\n",
      "              collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\n",
      " mappedboth x= [1 ,0]and x= [0 ,1]      toasinglepointinfeaturespace, h= [1 ,0].\n",
      "          Thelinearmodelcannowdescribethefunctionasincreasingin h1  anddecreasingin h2.\n",
      "               Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\n",
      "              capacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\n",
      "       representationscanalsohelpthemodeltogeneralize.\n",
      "1 6 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "yy\n",
      "hh\n",
      "x xWwyy\n",
      "h 1 h 1\n",
      "x 1 x 1h 2 h 2\n",
      "x 2 x 2\n",
      "             Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,\n",
      "                thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\n",
      "                 layercontainingtwounits.(Left)Inthisstyle,wedraweveryunitasanodeinthegraph.\n",
      "              Thisstyleisexplicitandunambiguous,butfornetworkslargerthanthisexample,itcan\n",
      "                 consumetoomuchspace.(Right)Inthisstyle,wedrawanodeinthegraphforeachentire\n",
      "           vectorrepresentingalayer’sactivations.Thisstyleismuchmorecompact.Sometimes\n",
      "               weannotatetheedgesinthisgraphwiththenameoftheparametersthatdescribethe\n",
      "         relationshipbetweentwolayers.Here,weindicatethatamatrix W   describesthemapping\n",
      "from xto h   ,andavector w    describesthemappingfrom hto y    .Wetypicallyomitthe\n",
      "           interceptparametersassociatedwitheachlayerwhenlabelingthiskindofdrawing.\n",
      "0\n",
      "z0  g z ( ) = m a x 0{ , z}\n",
      "            Figure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault\n",
      "          activationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\n",
      "            thisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\n",
      "               Thefunctionremainsveryclosetolinear,however,inthesensethatitisapiecewise\n",
      "            linearfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,\n",
      "             theypreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewith\n",
      "           gradient-basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinear\n",
      "           modelsgeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwe\n",
      "           canbuildcomplicatedsystemsfromminimalcomponents.MuchasaTuringmachine’s\n",
      "                 memoryneedsonlytobeabletostore0or1states,wecanbuildauniversalfunction\n",
      "    approximatorfromrectiﬁedlinearfunctions.1 7 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "            aﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\n",
      "     anaﬃnetransformationfromavectorx  toavectorh      ,soanentirevectorofbias\n",
      "     parametersisneeded.Theactivationfunctiong      istypicallychosentobeafunction\n",
      "    thatisappliedelement-wise,withh i=g(xW : , i+c i    ).Inmodernneuralnetworks,\n",
      "      thedefaultrecommendati onistousethe   rectiﬁedlinearunit   ,orReLU(Jarrett\n",
      "              etal. etal. ,; ,; 2009NairandHinton2010Glorot,),deﬁnedbytheactivation 2011a\n",
      "      function ,depictedinﬁgure. gz ,z () = max0{} 6.3\n",
      "       Wecannowspecifyourcompletenetworkas\n",
      "     f,,,b (;xWcw) = w max0{,W     xc+}+b. (6.3)\n",
      "          WecanthenspecifyasolutiontotheXORproblem.Let\n",
      " W=\n",
      " 11\n",
      " 11\n",
      " , (6.4)\n",
      " c=\n",
      "0\n",
      "−1\n",
      " , (6.5)\n",
      " w=1\n",
      "−2\n",
      " , (6.6)\n",
      "  and.b= 0\n",
      "             Wecannowwalkthroughhowthemodelprocessesabatchofinputs.LetX\n",
      "              bethedesignmatrixcontainingallfourpointsinthebinaryinputspace,withone\n",
      "  exampleperrow:\n",
      " X=\n",
      " 00\n",
      " 01\n",
      " 10\n",
      " 11\n",
      " . (6.7)\n",
      "               Theﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst\n",
      "  layer’sweightmatrix:\n",
      " XW=\n",
      " 00\n",
      " 11\n",
      " 11\n",
      " 22\n",
      " . (6.8)\n",
      "        Next,weaddthebiasvector,toobtainc\n",
      "\n",
      " 0 1−\n",
      " 10\n",
      " 10\n",
      " 21\n",
      " . (6.9)\n",
      "1 7 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "                 Inthisspace,alltheexamplesliealongalinewithslope.Aswemovealongthis 1\n",
      "                  line,theoutputneedstobeginat,thenriseto,thendropbackdownto.A 0 1 0\n",
      "            linearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalueof\n",
      "         hforeachexample,weapplytherectiﬁedlineartransformation:\n",
      "\n",
      " 00\n",
      " 10\n",
      " 10\n",
      " 21\n",
      " . (6.10)\n",
      "          Thistransformationhaschangedtherelationshipbetweentheexamples.Theyno\n",
      "                  longerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1\n",
      "     linearmodelcansolvetheproblem.\n",
      "        Weﬁnishwithmultiplyingbytheweightvector:w\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      " . (6.11)\n",
      "             Theneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\n",
      "            Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\n",
      "            zeroerror. Inarealsituation,theremightbebillionsofmodelparametersand\n",
      "             billionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\n",
      "         here.Instead,agradient-basedoptimizationalgorithmcanﬁndparametersthat\n",
      "              produceverylittleerror.ThesolutionwedescribedtotheXORproblemisata\n",
      "            globalminimumofthelossfunction,sogradientdescentcouldconvergetothis\n",
      "           point.ThereareotherequivalentsolutionstotheXORproblemthatgradient\n",
      "            descentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe\n",
      "           initialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\n",
      "          ﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\n",
      "here.\n",
      "  6.2Gradient-BasedLearning\n",
      "            Designingandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany\n",
      "           othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\n",
      "           howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\n",
      "      acostfunction,andamodelfamily.\n",
      "1 7 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural\n",
      "            networksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\n",
      "          functionstobecomenonconvex.Thismeansthatneuralnetworksareusually\n",
      "          trainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost\n",
      "              functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\n",
      "          linearregressionmodelsortheconvexoptimizationalgorithmswithglobalconver-\n",
      "          genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\n",
      "           convergesstartingfromanyinitialparameters(intheory—inpracticeitisrobust\n",
      "         butcanencounternumericalproblems).Stochasticgradientdescentappliedto\n",
      "            nonconvexlossfunctionshasnosuchconvergenceguaranteeandissensitivetothe\n",
      "            valuesoftheinitialparameters.Forfeedforwardneuralnetworks,itisimportantto\n",
      "             initializeallweightstosmallrandomvalues.Thebiasesmaybeinitializedtozero\n",
      "         ortosmallpositivevalues.Theiterativegradient-basedoptimizationalgorithms\n",
      "            usedtotrainfeedforwardnetworksandalmostallotherdeepmodelsaredescribed\n",
      "           indetailinchapter,withparameterinitializationinparticulardiscussedin 8\n",
      "            section.Forthemoment,itsuﬃcestounderstandthatthetrainingalgorithm 8.4\n",
      "              isalmostalwaysbasedonusingthegradienttodescendthecostfunctioninone\n",
      "          wayoranother.Thespeciﬁcalgorithmsareimprovementsandreﬁnementson\n",
      "            theideasofgradientdescent,introducedinsection,and,morespeciﬁcally,are 4.3\n",
      "         mostoftenimprovementsofthestochasticgradientdescentalgorithm,introduced\n",
      "  insection.5.9\n",
      "            Wecanofcoursetrainmodelssuchaslinearregressionandsupportvector\n",
      "             machineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\n",
      "              setisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\n",
      "           muchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly\n",
      "             morecomplicatedforaneuralnetworkbutcanstillbedoneeﬃcientlyandexactly.\n",
      "            InSectionwedescribehowtoobtainthegradientusingtheback-propagation 6.5\n",
      "       algorithmandmoderngeneralizationsoftheback-propagationalgorithm.\n",
      "          Aswithothermachinelearningmodels,toapplygradient-basedlearningwe\n",
      "              mustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\n",
      "           themodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\n",
      "   theneuralnetworksscenario.\n",
      "  6.2.1CostFunctions\n",
      "               Animportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe\n",
      "            costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\n",
      "1 7 3    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           thesameasthoseforotherparametricmodels,suchaslinearmodels.\n",
      "        Inmostcases,ourparametricmodeldeﬁnesadistributionp(  yx|;θ )and\n",
      "        wesimplyuse theprinciple ofmaximumlikelihood.This meansweuse the\n",
      "           cross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost\n",
      "function.\n",
      "           Sometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\n",
      "  probabilitydistributionovery      ,wemerelypredictsomestatisticofyconditioned\n",
      "             on.Specializedlossfunctionsenableustotrainapredictoroftheseestimates. x\n",
      "             Thetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\n",
      "            oftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\n",
      "           alreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\n",
      "           section. Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\n",
      "           applicabletodeepneuralnetworksandisamongthemostpopularregulariza-\n",
      "         tionstrategies.Moreadvancedregularizationstrategiesforneuralnetworksare\n",
      "   describedinchapter.7\n",
      "      6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\n",
      "          Mostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\n",
      "          thatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n",
      "           asthecross-entropybetweenthetrainingdataandthemodeldistribution.This\n",
      "    costfunctionisgivenby\n",
      "J() = θ− E x y , ∼ ˆ p d a t a logp m o d e l   ( )yx|. (6.12)\n",
      "            Thespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending\n",
      "    onthespeciﬁcformof  logp m o d e l       .Theexpansionoftheaboveequationtypically\n",
      "              yieldssometermsthatdonotdependonthemodelparametersandmaybedis-\n",
      "         carded.Forexample,aswesawinsection,if5.5.1p m o d e l(  yx|) =N(y;f(x;θ) ,I),\n",
      "       thenwerecoverthemeansquarederrorcost,\n",
      "Jθ() =1\n",
      "2E x y , ∼ ˆ p d a t a   ||−||yf(;)xθ2  +const, (6.13)\n",
      "     uptoascalingfactorof1\n",
      "2          andatermthatdoesnotdependon.Thediscarded θ\n",
      "             constantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\n",
      "           wechosenottoparametrize.Previously,wesawthattheequivalencebetween\n",
      "         maximumlikelihoodestimationwithanoutputdistributionandminimizationof\n",
      "1 7 4    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             meansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\n",
      "            regardlessofthe usedtopredictthemeanoftheGaussian. f(;)xθ\n",
      "           Anadvantageofthisapproachofderivingthecostfunctionfrommaximum\n",
      "             likelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\n",
      "  Specifyingamodel p(  yx|     )automaticallydeterminesacostfunction  log p(  yx|).\n",
      "           Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\n",
      "              thecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\n",
      "          forthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine\n",
      "            thisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases\n",
      "            thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\n",
      "           hiddenunitsortheoutputunitssaturate.Thenegativelog-likelihoodhelpsto\n",
      "          avoidthisproblemformanymodels.Severaloutputunitsinvolveanexpfunction\n",
      "         thatcansaturatewhenitsargumentisverynegative.Thelog   functioninthe\n",
      "     negativelog-likelihoodcostfunctionundoestheexp     ofsomeoutputunits.Wewill\n",
      "             discusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\n",
      " section.6.2.2\n",
      "          Oneunusualpropertyofthecross-entropycostusedtoperformmaximum\n",
      "             likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\n",
      "           tothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\n",
      "            modelsareparametrizedinsuchawaythattheycannotrepresentaprobability\n",
      "             ofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\n",
      "             isanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n",
      "            cancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\n",
      "          varianceparameterofaGaussianoutputdistribution)thenitbecomespossible\n",
      "            toassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\n",
      "      cross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed\n",
      "            inchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7\n",
      "         thatthemodelcannotreapunlimitedrewardinthisway.\n",
      "   6.2.1.2LearningConditionalStatistics\n",
      "      Insteadoflearningafullprobabilitydistribution p(  yx|;θ    ),weoftenwantto\n",
      "        learnjustoneconditionalstatisticofgiven.yx\n",
      "      Forexample,wemayhaveapredictor f(x;θ      ) thatwewishtoemploytopredict\n",
      "   themeanof.y\n",
      "             Ifweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural\n",
      "       networkasbeingabletorepresentanyfunction f     fromawideclassoffunctions,\n",
      "            withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\n",
      "1 7 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             ratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we\n",
      "       canviewthecostfunctionasbeingafunctional     ratherthanjustafunction.A\n",
      "             functionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\n",
      "            learningaschoosingafunctionratherthanmerelychoosingasetofparameters.\n",
      "             Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\n",
      "             functionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\n",
      "      minimumlieonthefunctionthatmaps x    totheexpectedvalueof ygiven x.\n",
      "           Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical\n",
      " toolcalled   calculusofvariations        ,describedinsection .Itisnotnecessary 19.4.2\n",
      "            tounderstandcalculusofvariationstounderstandthecontentofthischapter.At\n",
      "             themoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\n",
      "      usedtoderivethefollowingtworesults.\n",
      "1 7 6    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "            Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\n",
      " tionproblem\n",
      "f∗ = argmin\n",
      "fE x y ,∼ p d a t a  ||−||yf()x2(6.14)\n",
      "yields\n",
      "f∗() = x E y∼ p d a t a ( ) y x| []y, (6.15)\n",
      "                solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe\n",
      "           couldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,\n",
      "            minimizingthemeansquarederrorcostfunctionwouldgiveafunctionthatpredicts\n",
      "        themeanofforeachvalueof. y x\n",
      "          Diﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing\n",
      "    calculusofvariationsisthat\n",
      "f∗ = argmin\n",
      "fE x y ,∼ p d a t a  ||−||yf()x 1 (6.16)\n",
      "        yieldsafunctionthatpredictsthemedianvalueofy foreachx     ,aslongassucha\n",
      "             functionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\n",
      "      functioniscommonlycalled . meanabsoluteerror\n",
      "           Unfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\n",
      "         resultswhenusedwithgradient-basedoptimization.Someoutputunitsthat\n",
      "          saturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\n",
      "             Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\n",
      "              squarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\n",
      "    entiredistribution .p( )yx|\n",
      "  6.2.2OutputUnits\n",
      "              Thechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\n",
      "             ofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\n",
      "          modeldistribution. Thechoiceofhowtorepresenttheoutputthendetermines\n",
      "     theformofthecross-entropyfunction.\n",
      "               Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\n",
      "                 usedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\n",
      "              model,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\n",
      "           withadditionaldetailabouttheiruseashiddenunitsinsection.6.3\n",
      "          Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\n",
      "     setofhiddenfeaturesdeﬁnedbyh=f(x;θ        ).Theroleoftheoutputlayeristhen\n",
      "           toprovidesomeadditionaltransformationfromthefeaturestocompletethetask\n",
      "    thatthenetworkmustperform.\n",
      "1 7 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "      6.2.2.1LinearUnitsforGaussianOutputDistributions\n",
      "             Onesimplekindofoutputunitisbasedonanaﬃnetransformationwithno\n",
      "       nonlinearity.Theseareoftenjustcalledlinearunits.\n",
      " Givenfeaturesh         ,alayeroflinearoutputunitsproducesavectorˆy=Wh+b.\n",
      "            Linearoutputlayersareoftenusedtoproducethemeanofaconditional\n",
      " Gaussiandistribution:\n",
      "  p( ) = (; yx|Nyˆ  yI,). (6.17)\n",
      "          Maximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\n",
      "error.\n",
      "         Themaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\n",
      "              covarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\n",
      "             functionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\n",
      "              deﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear\n",
      "            outputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.\n",
      "          Approachestomodelingthecovariancearedescribedshortly,insection .6.2.2.4\n",
      "           Becauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-\n",
      "            basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\n",
      "algorithms.\n",
      "      6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\n",
      "         Manytasksrequirepredictingthevalueofabinaryvariabley .Classiﬁcation\n",
      "         problemswithtwoclassescanbecastinthisform.\n",
      "          ThemaximumlikelihoodapproachistodeﬁneaBernoullidistributionovery\n",
      "  conditionedon.x\n",
      "            ABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet\n",
      "   needstopredictonlyP(y= 1 |x         ).Forthisnumbertobeavalidprobability,it\n",
      "      mustlieintheinterval[0,1].\n",
      "          Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere\n",
      "             tousealinearunitandthresholditsvaluetoobtainavalidprobability:\n",
      "   Py(= 1 ) = max|x\n",
      " 0min,\n",
      " 1,w  h+b \n",
      " . (6.18)\n",
      "             Thiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable\n",
      "          totrainitveryeﬀectivelywithgradientdescent.Anytimethatwh+bstrayed\n",
      "1 7 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "              outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\n",
      "   itsparameterswouldbe 0   .Agradientof 0    istypicallyproblematicbecausethe\n",
      "            learningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\n",
      "parameters.\n",
      "              Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa\n",
      "            stronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\n",
      "        onusingsigmoidoutputunitscombinedwithmaximumlikelihood.\n",
      "      Asigmoidoutputunitisdeﬁnedby\n",
      " ˆyσ= \n",
      "w  h+b\n",
      " , (6.19)\n",
      "          whereisthelogisticsigmoidfunctiondescribedinsection. σ 3.10\n",
      "             Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\n",
      "     usesalinearlayertocomputez=wh+b      .Next,itusesthesigmoidactivation\n",
      "      functiontoconvertintoaprobability. z\n",
      "    Weomitthedependenceonx        forthemomenttodiscusshowtodeﬁnea\n",
      "  probabilitydistributionovery  usingthevaluez     .Thesigmoidcanbemotivated\n",
      "     byconstructinganunnormalizedprobabilitydistribution˜P(y   ),whichdoesnot\n",
      "              sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\n",
      "           probabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalizedlog\n",
      "   probabilitiesarelinearinyandz       ,wecanexponentiatetoobtaintheunnormalized\n",
      "           probabilities.WethennormalizetoseethatthisyieldsaBernoullidistribution\n",
      "      controlledbyasigmoidaltransformationof:z\n",
      "log˜  Pyyz, () = (6.20)\n",
      "˜  Pyyz, () = exp() (6.21)\n",
      " Py() =exp()yz1\n",
      "y =0exp(yz,)(6.22)\n",
      "      Pyσyz. () = ((2−1)) (6.23)\n",
      "        Probabilitydistributionsbasedonexponentiationandnormalizationarecommon\n",
      "     throughoutthestatisticalmodelingliterature.Thez    variabledeﬁningsucha\n",
      "       distributionoverbinaryvariablesiscalleda. l o g i t\n",
      "            Thisapproachtopredictingtheprobabilitiesinlogspaceisnaturaltouse\n",
      "          withmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\n",
      " likelihoodis  −logP(  y|x ),thelog     inthecostfunctionundoestheexp ofthe\n",
      "           sigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-\n",
      "     based learningfrom maki nggood progress.The lossfunction formaximum\n",
      "1 7 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "         likelihoodlearningofaBernoulliparametrizedbyasigmoidis\n",
      "     J Py () = log θ− (| x) (6.24)\n",
      "     = log((2 1)) −σy−z (6.25)\n",
      "     = ((12)) ζ−yz. (6.26)\n",
      "           Thisderivationmakesuseofsomepropertiesfromsection.Byrewriting 3.10\n",
      "               thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n",
      "(1−2y)z          isverynegative.Saturationthusoccursonlywhenthemodelalready\n",
      "   hastherightanswer—wheny  = 1andz   isverypositive,ory  = 0andz isvery\n",
      " negative.Whenz         hasthewrongsign,theargumenttothesoftplusfunction,\n",
      "(1−2y)z    ,maybesimpliﬁedto||z .As||z   becomeslargewhilez   hasthewrongsign,\n",
      "        thesoftplusfunctionasymptotestowardsimplyreturningitsargument||z .The\n",
      "   derivativewithrespecttoz  asymptotestosign(z      ),so,inthelimitofextremely\n",
      "incorrectz            ,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\n",
      "            isusefulbecauseitmeansthatgradient-basedlearningcanacttoquicklycorrect\n",
      "  amistaken.z\n",
      "             Whenweuseotherlossfunctions,suchasmeansquarederror,thelosscan\n",
      " saturateanytimeσ(z        )saturates.Thesigmoidactivationfunctionsaturatesto0\n",
      "whenz        becomesverynegativeandsaturatestowhen 1z   becomesverypositive.\n",
      "             Thegradientcanshrinktoosmalltobeusefulforlearningwhenthishappens,\n",
      "             whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\n",
      "          maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\n",
      " outputunits.\n",
      "           Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because\n",
      "         thesigmoidreturnsvaluesrestrictedtotheopeninterval(0,   1),ratherthanusing\n",
      "       theentireclosedintervalofvalidprobabilities[0,   1].Insoftwareimplementations,\n",
      "             toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\n",
      " functionofz      ,ratherthanasafunctionofˆy=σ(z    ).Ifthesigmoidfunction\n",
      "           underﬂowstozero,thentakingthelogarithmofˆyyieldsnegativeinﬁnity.\n",
      "      6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\n",
      "            Anytimewewishtorepresentaprobabilitydistributionoveradiscretevariable\n",
      "withn              possiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\n",
      "           generalizationofthesigmoidfunction,whichwasusedtorepresentaprobability\n",
      "    distributionoverabinaryvariable.\n",
      "             Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent\n",
      "   theprobabilitydistributionovern      diﬀerentclasses.Morerarely,softmaxfunctions\n",
      "1 8 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "                canbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\n",
      "      ndiﬀerentoptionsforsomeinternalvariable.\n",
      "            Inthecaseofbinaryvariables,wewishedtoproduceasinglenumber\n",
      "     ˆyPy. = (= 1 )|x (6.27)\n",
      "              Becausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\n",
      "           logarithmofthenumbertobewellbehavedforgradient-basedoptimizationof\n",
      "        thelog-likelihood,wechosetoinsteadpredictanumberz=log˜P(y =1 |x).\n",
      "          ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\n",
      " sigmoidfunction.\n",
      "         Togeneralizetothecaseofadiscretevariablewithn    values,wenowneed\n",
      "   toproduceavectorˆy ,withˆy i=P(y=  i|x      ).Werequirenotonlythateach\n",
      " elementofˆy i               bebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\n",
      "           itrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\n",
      "          theBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\n",
      "    layerpredictsunnormalizedlogprobabilities:\n",
      " zW=    hb+, (6.28)\n",
      "wherez i=log˜P(y=  i|x).      Thesoftmaxfunctioncanthenexponentiateand\n",
      "     normalizetoobtainthedesired z ˆ       y.Formally,thesoftmaxfunctionisgivenby\n",
      "softmax()z i=exp(z i)\n",
      "jexp(z j) . (6.29)\n",
      "        Aswiththelogisticsigmoid,theuseoftheexp    functionworkswellwhen\n",
      "       trainingthesoftmaxtooutputatargetvaluey   usingmaximumlog-likelihood.In\n",
      "     thiscase,wewishtomaximize  logP(y=i;z )=  logsoftmax(z) i  .Deﬁningthe\n",
      "   softmaxintermsofexp   isnaturalbecausethelog    inthelog-likelihoodcanundo\n",
      "    theofthesoftmax: exp\n",
      " logsoftmax()z i= z i −log\n",
      "jexp(z j ). (6.30)\n",
      "         Theﬁrsttermofequationshowsthattheinput 6.30 z i   alwayshasadirect\n",
      "           contributiontothecostfunction.Becausethistermcannotsaturate,weknow\n",
      "        thatlearningcanproceed,evenifthecontributionofz i    tothesecondtermof\n",
      "          equationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30\n",
      " termencouragesz i          tobepushedup,whilethesecondtermencouragesallofz tobe\n",
      "         pusheddown.Togainsomeintuitionforthesecondterm,log\n",
      "jexp(z j ),observe\n",
      "1 8 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "       thatthistermcanberoughlyapproximatedbymax jz j   .Thisapproximationis\n",
      "    basedontheideathatexp(z k   ) isinsigniﬁcantforanyz k    thatisnoticeablylessthan\n",
      "max jz j            .Theintuitionwecangainfromthisapproximationisthatthenegative\n",
      "         log-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\n",
      "             prediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\n",
      "the−z i  termandthelog\n",
      "jexp(z j) ≈max jz j=z i   termswillroughlycancel.\n",
      "             Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\n",
      "         dominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.\n",
      "           Sofarwehavediscussedonlyasingleexample.Overall,unregularizedmaximum\n",
      "             likelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n",
      "           thefractionofcountsofeachoutcomeobservedinthetrainingset:\n",
      " softmax((;))zxθ i≈m\n",
      "j =1 1y( ) j = i , x( ) j = xm\n",
      "j =1 1x( ) j = x . (6.31)\n",
      "           Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\n",
      "             aslongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\n",
      "          practice,limitedmodelcapacityandimperfectoptimizationwillmeanthatthe\n",
      "       modelisonlyabletoapproximatethesefractions.\n",
      "           Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell\n",
      "           withthesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto\n",
      " undotheexp          ofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\n",
      "             verynegative,causingthegradienttovanish.Inparticular,squarederrorisapoor\n",
      "               lossfunctionforsoftmaxunitsandcanfailtotrainthemodeltochangeitsoutput,\n",
      "          evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,). Bridle1990\n",
      "             Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexaminethe\n",
      "  softmaxfunctionitself.\n",
      "          Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunction\n",
      "             hasasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\n",
      "           positive.Thesoftmaxhasmultipleoutputvalues.Theseoutputvaluescansaturate\n",
      "         whenthediﬀerencesbetweeninputvaluesbecomeextreme. Whenthesoftmax\n",
      "            saturates,manycostfunctionsbasedonthesoftmaxalsosaturate,unlesstheyare\n",
      "      abletoinvertthesaturatingactivatingfunction.\n",
      "            Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,\n",
      "              observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallits\n",
      "inputs:\n",
      "   softmax() = softmax(+) z zc. (6.32)\n",
      "            Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\n",
      "  softmax() = softmax( max z z−\n",
      "iz i ). (6.33)\n",
      "1 8 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           Thereformulatedversionenablesustoevaluatesoftmaxwithonlysmallnumerical\n",
      " errors, evenwhenz    containsextremelylarge orextremely negative numbers.\n",
      "           Examiningthenumericallystablevariant,weseethatthesoftmaxfunctionis\n",
      "         drivenbytheamountthatitsargumentsdeviatefrommax iz i.\n",
      " Anoutputsoftmax( z) i        saturatestowhenthecorrespondinginputismaximal 1 \n",
      "(z i=max iz i )andz i         ismuchgreaterthanalltheotherinputs.Theoutput\n",
      "softmax( z) i     canalsosaturatetowhen 0z i      isnotmaximalandthemaximumis\n",
      "             muchgreater.Thisisageneralizationofthewaythatsigmoidunitssaturateand\n",
      "             cancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto\n",
      "  compensateforit.\n",
      " Theargument z          tothesoftmaxfunctioncanbeproducedintwodiﬀerentways.\n",
      "              Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\n",
      "  everyelementof z       ,asdescribedaboveusingthelinearlayer z= Wh+ b .While\n",
      "       straightforward,thisapproachactuallyoverparametrizes thedistribution.The\n",
      "  constraintthatthen        outputsmustsumtomeansthatonly 1  n−  1parametersare\n",
      "    necessary;theprobabilityofthen       -thvaluemaybeobtainedbysubtractingthe\n",
      "ﬁrstn−            1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\n",
      "of z       beﬁxed.Forexample,wecanrequirethatz n     =0.Indeed,thisisexactly\n",
      "     whatthesigmoidunitdoes.DeﬁningP(y= 1 | x) =σ(z    )isequivalenttodeﬁning\n",
      "P(y= 1 | x) =softmax( z) 1  withatwo-dimensional zandz 1  = 0.Boththe n−1\n",
      "  argumentandthen         argumentapproachestothesoftmaxcandescribethesame\n",
      "          setofprobabilitydistributionsbuthavediﬀerentlearningdynamics.Inpractice,\n",
      "           thereisrarelymuchdiﬀerencebetweenusingtheoverparametrizedversionorthe\n",
      "          restrictedversion,anditissimplertoimplementtheoverparametrizedversion.\n",
      "              Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas\n",
      "               awaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\n",
      "               softmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\n",
      "              correspondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\n",
      "             inhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe\n",
      "      extreme(whenthediﬀerencebetweenthemaximala i     andtheothersislargein\n",
      "     magnitude)itbecomesaformof wi nner - t a k e - a l l      (oneoftheoutputsisnearly1,\n",
      "     andtheothersarenearly0).\n",
      "           Thename“softmax” canbesomewhatconfusing.Thefunctionismoreclosely\n",
      "  relatedtothe  argmax        functionthanthemaxfunction. Theterm“soft”derives\n",
      "           fromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable.The\n",
      " argmax            function,withitsresultrepresentedasaone-hotvector,isnotcontinuous\n",
      "           ordiﬀerentiable.Thesoftmaxfunctionthusprovidesa“softened”versionofthe\n",
      " argmax         .Thecorrespondingsoftversionofthemaximumfunctionissoftmax( z)z.\n",
      "1 8 3    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax ,” butthe\n",
      "     currentnameisanentrenchedconvention.\n",
      "   6.2.2.4OtherOutputTypes\n",
      "        Thelinear, sigmoid, andsoftmax o utputunitsdescribedabovearethemost\n",
      "            common.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\n",
      "             wewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n",
      "          agoodcostfunctionfornearlyanykindofoutputlayer.\n",
      "       Ingeneral,ifwedeﬁneaconditionaldistributionp(  yx|;θ   ),theprincipleof\n",
      "             maximumlikelihoodsuggestsweuse asourcostfunction. −| log(pyxθ;)\n",
      "            Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunctionf(x;θ).\n",
      "           Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey .Instead,\n",
      "f(x;θ) =ω       providestheparametersforadistributionovery   .Ourlossfunction\n",
      "       canthenbeinterpretedas .−log(;())pyωx\n",
      "             Forexample,wemaywishtolearnthevarianceofaconditionalGaussianfory,\n",
      "givenx       .Inthesimplecase,wherethevarianceσ2      isaconstant,thereisaclosed\n",
      "           formexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\n",
      "       empiricalmeanofthesquareddiﬀerencebetweenobservationsy  andtheirexpected\n",
      "          value.Acomputationallymoreexpensiveapproachthatdoesnotrequirewriting\n",
      "              special-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe\n",
      "distributionp(  y|x    )thatiscontrolledbyω=f(x;θ   ).Thenegativelog-likelihood\n",
      " −logp(y;ω(x          ))willthenprovideacostfunctionwiththeappropriateterms\n",
      "          necessarytomakeouroptimizationprocedureincrementallylearnthevariance.In\n",
      "             thesimplecasewherethestandarddeviationdoesnotdependontheinput,we\n",
      "            canmakeanewparameterinthenetworkthatiscopieddirectlyintoω  .Thisnew\n",
      "  parametermightbeσ     itselforcouldbeaparametervrepresentingσ2  oritcould\n",
      "  beaparameterβrepresenting1\n",
      "σ2       ,dependingonhowwechoosetoparametrize\n",
      "             thedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance\n",
      "iny   fordiﬀerentvaluesofx    .Thisiscalledaheteroscedastic   model.Inthe\n",
      "            heteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof\n",
      "   thevaluesoutputbyf(x;θ           ).AtypicalwaytodothisistoformulatetheGaussian\n",
      "          distributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\n",
      "             Inthemultivariatecase,itismostcommontouseadiagonalprecisionmatrix\n",
      " diag (6.34) ()β.\n",
      "           Thisformulationworkswellwithgradientdescentbecausetheformulaforthe\n",
      "      log-likelihoodoftheGaussiandistributionparametrizedbyβ   involvesonlymul-\n",
      " tiplicationbyβ i  andadditionof  logβ i     .Thegradientofmultiplication,addition,\n",
      "1 8 4    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           andlogarithmoperationsiswellbehaved.Bycomparison,ifweparametrizedthe\n",
      "             outputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\n",
      "           becomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,arbi-\n",
      "            trarilylargegradientsusuallyresultininstability.Ifweparametrizedtheoutputin\n",
      "           termsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivisionaswell\n",
      "          assquaring. Thegradientthroughthesquaringoperationcanvanishnearzero,\n",
      "            makingitdiﬃculttolearnparametersthataresquared.Regardlessofwhetherwe\n",
      "           usestandarddeviation,variance,orprecision,wemustensurethatthecovariance\n",
      "            matrixoftheGaussianispositivedeﬁnite.Becausetheeigenvaluesoftheprecision\n",
      "           matrixarethereciprocalsoftheeigenvaluesofthecovariancematrix, thisis\n",
      "            equivalenttoensuringthattheprecisionmatrixispositivedeﬁnite. Ifweusea\n",
      "            diagonalmatrix,orascalartimesthediagonalmatrix,thentheonlycondition\n",
      "                weneedtoenforceontheoutputofthemodelispositivity.Ifwesupposethata\n",
      "             istherawactivationofthemodelusedtodeterminethediagonalprecision,we\n",
      "          canusethesoftplusfunctiontoobtainapositiveprecisionvector:β=ζ(a).This\n",
      "           samestrategyappliesequallyifusingvarianceorstandarddeviationratherthan\n",
      "           precisionorifusingascalartimesidentityratherthandiagonalmatrix.\n",
      "             Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\n",
      "          diagonal. Ifthecovarianceisfullandconditional,thenaparametrizationmust\n",
      "          bechosenthatguaranteespositivedeﬁnitenessofthepredictedcovariancematrix.\n",
      "     Thiscanbeachievedbywriting Σ() = ()xBxB()x ,whereB  isanunconstrained\n",
      "              squarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\n",
      "    likelihoodisexpensive,witha  dd×  matrixrequiringO(d3   )computationforthe\n",
      "   determinantandinverseof Σ(x       )(orequivalently,andmorecommonlydone,its\n",
      "    eigendecompositionorthatof).Bx()\n",
      "    We oftenwantto perform multimodal regressi on, thatis, to predictreal\n",
      "    valuesfromaconditionaldistributionp(  yx|     )thatcanhaveseveraldiﬀerent\n",
      " peaksiny     spaceforthesamevalueofx       .Inthiscase,aGaussianmixtureis\n",
      "           anaturalrepresentationfortheoutput( ,; ,). Jacobsetal.1991Bishop1994\n",
      "          NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\n",
      " densitynetworks     .AGaussianmixtureoutputwithn    componentsisdeﬁnedby\n",
      "   theconditionalprobabilitydistribution:\n",
      "  p( ) =yx|n \n",
      "i =1    pi (= c|Nx)(;yµ( ) i ()x, Σ( ) i ())x. (6.35)\n",
      "         Theneuralnetworkmusthavethreeoutputs:avectordeﬁningp(c=  i|x ),a\n",
      " matrixprovidingµ( ) i(x  )foralli    ,andatensorproviding Σ( ) i(x  )foralli .These\n",
      "    outputsmustsatisfydiﬀerentconstraints:\n",
      "1 8 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "1.  Mixturecomponentsp (c =  i| x     ):theseformamultinoullidistribution\n",
      " overthen      diﬀerentcomponentsassociatedwithlatentvariable1c  ,andcan\n",
      "       typicallybeobtainedbyasoftmaxoverann    -dimensionalvector,toguarantee\n",
      "        thattheseoutputsarepositiveandsumto1.\n",
      "2.Means µ( ) i( x         ):theseindicatethecenterormeanassociatedwiththei-th\n",
      "         Gaussiancomponentandareunconstrained(typicallywithnononlinearityat\n",
      "     allfortheseoutputunits).If y isad      -vector,thenthenetworkmustoutput\n",
      "an  nd×   matrixcontainingalln ofthesed   -dimensionalvectors.Learning\n",
      "         thesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan\n",
      "           learningthemeansofadistributionwithonlyoneoutputmode. Weonly\n",
      "           wanttoupdatethemeanforthecomponentthatactuallyproducedthe\n",
      "          observation.Inpractice,wedonotknowwhichcomponentproducedeach\n",
      "        observation.Theexpressionforthenegativelog-likelihoodnaturallyweights\n",
      "           eachexample’scontributiontothelossforeachcomponentbytheprobability\n",
      "     thatthecomponentproducedtheexample.\n",
      "3.Covariances Σ( ) i( x        ):thesespecifythecovariancematrixforeachcomponent\n",
      "i            .AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal\n",
      "           matrixtoavoidneedingtocomputedeterminants.Aswithlearningthemeans\n",
      "          ofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\n",
      "         partialresponsibilityforeachpointtoeachmixturecomponent.Gradient\n",
      "          descentwillautomaticallyfollowthecorrectprocessifgiventhecorrect\n",
      "        speciﬁcationofthenegativelog-likelihoodunderthemixturemodel.\n",
      "         Ithasbeenreportedthatgradient-basedoptimizationofconditionalGaussian\n",
      "             mixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone\n",
      "           getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\n",
      "            variancegetstobesmallforaparticularexample,yieldingverylargegradients).\n",
      "   Onesolutionisto clipgradients       (seesection ),whileanotheristoscale 10.11.1\n",
      "      thegradientsheuristically( ,). Uriaetal.2014\n",
      "         Gaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof\n",
      "          speech(Schuster1999,)andmovementsofphysicalobjects(Graves2013,).The\n",
      "            mixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput\n",
      "             modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\n",
      "1 Weconsiderc             tobelatentbecausewedonotobserveitinthedata:giveninput x andtarget\n",
      "y              ,itisnotpossibletoknowwithcertaintywhichGaussiancomponentwasresponsiblefor y ,but\n",
      "   wecanimaginethat y            wasgeneratedbypickingoneofthem,andwecanmakethatunobserved\n",
      "   choicearandomvariable.\n",
      "186    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "xy\n",
      "             Figure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\n",
      " Theinputx        issampledfromauniformdistribution,andtheoutputy  issampledfrom\n",
      "p m o d e l (  yx|             ).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\n",
      "          theparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\n",
      "             governingwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\n",
      "          parametersforeachmixturecomponent.EachmixturecomponentisGaussianwith\n",
      "              predictedmeanandvariance.Alltheseaspectsoftheoutputdistributionareabletovary\n",
      "            withrespecttotheinput,andtodosoinnonlinearways. x\n",
      "             ahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\n",
      "      densitynetworkisshowninﬁgure.6.4\n",
      "          Ingeneral,wemaywishtocontinuetomodellargervectors y  containingmore\n",
      "            variables,andtoimposericherandricherstructuresontheseoutputvariables.For\n",
      "             example,ifwewantourneuralnetworktooutputasequenceofcharactersthat\n",
      "            formsasentence,wemightcontinuetousetheprincipleofmaximumlikelihood\n",
      "   appliedtoourmodelp ( y ; ω ( x         ) ).Inthiscase,themodelweusetodescribe ywould\n",
      "              becomecomplexenoughtobebeyondthescopeofthischapter.InChapterwe10\n",
      "            describehowtouserecurrentneuralnetworkstodeﬁnesuchmodelsoversequences,\n",
      "           andinpartwedescribeadvancedtechniquesformodelingarbitraryprobability III\n",
      "distributions.\n",
      "  6.3HiddenUnits\n",
      "             Sofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\n",
      "          arecommontomostparametricmachinelearningmodelstrainedwithgradient-\n",
      "             basedoptimization.Nowweturntoanissuethatisuniquetofeedforwardneural\n",
      "1 8 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "                networks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\n",
      "model.\n",
      "              Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\n",
      "      yethavemanydeﬁnitiveguidingtheoreticalprinciples.\n",
      "            Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\n",
      "              typesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse\n",
      "          whichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice). We\n",
      "            describeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunit.\n",
      "             Theseintuitionscanhelpdecidewhentotryoutwhichunit.Predictinginadvance\n",
      "            whichwillworkbestisusuallyimpossible.Thedesignprocessconsistsoftrial\n",
      "              anderror,intuitingthatakindofhiddenunitmayworkwell,andthentraining\n",
      "             anetworkwiththatkindofhiddenunitandevaluatingitsperformanceona\n",
      " validationset.\n",
      "            Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiable\n",
      "         atallinputpoints.Forexample,therectiﬁedlinearfunctiong(z )=max{0 ,z}\n",
      "   isnotdiﬀerentiableatz       =0.Thismayseemlikeitinvalidatesg  forusewith\n",
      "         agradient-basedlearningalgorithm.Inpractice,gradientdescentstillperforms\n",
      "              wellenoughforthesemodelstobeusedformachinelearningtasks.Thisisin\n",
      "            partbecauseneuralnetworktrainingalgorithmsdonotusuallyarriveatalocal\n",
      "            minimumofthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,as\n",
      "              showninﬁgure.(Theseideasaredescribedfurtherinchapter.)Becausewedo 4.3 8\n",
      "           notexpecttrainingtoactuallyreachapointwherethegradientis 0   ,itisacceptable\n",
      "             fortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.\n",
      "           Hiddenunitsthatarenotdiﬀerentiableareusuallynondiﬀerentiableatonlya\n",
      "       smallnumberofpoints.Ingeneral,afunctiong(z     )hasaleftderivativedeﬁned\n",
      "          bytheslopeofthefunctionimmediatelytotheleftofz   andarightderivative\n",
      "           deﬁnedbytheslopeofthefunctionimmediatelytotherightofz  .Afunction\n",
      "  isdiﬀerentiableatz          onlyifboththeleftderivativeandtherightderivativeare\n",
      "             deﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural\n",
      "           networksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe\n",
      " caseofg(z) =max{0 ,z}    ,theleftderivativeatz       = 00is,andtherightderivative\n",
      "           is.Softwareimplementationsofneuralnetworktrainingusuallyreturnoneof 1\n",
      "           theone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor\n",
      "          raisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\n",
      "           basedoptimizationonadigitalcomputerissubjecttonumericalerroranyway.\n",
      "      Whenafunctionisaskedtoevaluateg       (0),itisveryunlikelythattheunderlying\n",
      "            valuetrulywas.Instead,itwaslikelytobesomesmallvalue 0   thatwasrounded\n",
      "           to.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0\n",
      "1 8 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "            theseusuallydonotapplytoneuralnetworktraining.Theimportantpointis\n",
      "            thatinpracticeonecansafelydisregardthenondiﬀerentiabilityofthehiddenunit\n",
      "   activationfunctionsdescribedbelow.\n",
      "          Unlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting\n",
      "   avectorofinputsx    ,computinganaﬃnetransformationz=Wx+b ,and\n",
      "     thenapplyinganelement-wisenonlinearfunctiong(z    ).Mosthiddenunitsare\n",
      "             distinguishedfromeachotheronlybythechoiceoftheformoftheactivation\n",
      " function.g()z\n",
      "      6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\n",
      "        Rectiﬁedlinearunitsusetheactivationfunction . gz ,z () = max0{}\n",
      "              Theseunitsareeasytooptimizebecausetheyaresosimilartolinearunits.The\n",
      "              onlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitisthatarectiﬁed\n",
      "            linearunitoutputszeroacrosshalfitsdomain.Thismakesthederivativesthrough\n",
      "            arectiﬁedlinearunitremainlargewhenevertheunitisactive.Thegradients\n",
      "            arenotonlylargebutalsoconsistent.Thesecondderivativeoftherectifying\n",
      "            operationisalmosteverywhere,andthederivativeoftherectifyingoperationis 0\n",
      "              1everywherethattheunitisactive.Thismeansthatthegradientdirectionisfar\n",
      "            moreusefulforlearningthanitwouldbewithactivationfunctionsthatintroduce\n",
      " second-ordereﬀects.\n",
      "           Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\n",
      " hW= (g   xb+). (6.36)\n",
      "            Wheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood\n",
      "     practicetosetallelementsofb       toasmallpositivevalue,suchas0.  1. Doingso\n",
      "              makesitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformost\n",
      "           inputsinthetrainingsetandallowthederivativestopassthrough.\n",
      "          Severalgeneralizationsofrectiﬁedlinearunitsexist.Mostofthesegeneral-\n",
      "         izationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform\n",
      "better.\n",
      "            Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-\n",
      "           basedmethodsonexamplesforwhichtheiractivationiszero.Variousgeneraliza-\n",
      "          tionsofrectiﬁedlinearunitsguaranteethattheyreceivegradienteverywhere.\n",
      "           Threegeneralizationsofrectiﬁedlinearunitsarebasedonusinganonzero\n",
      "slopeα iwhenz i<0:h i=g( zα,) i=max(0 ,z i )+α imin(0 ,z i).  Absolutevalue\n",
      "1 8 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "rectiﬁcationﬁxesα i=−  1toobtaing(z) =||z      .Itisusedforobjectrecognition\n",
      "              fromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrettetal.2009\n",
      "          invariantunderapolarityreversaloftheinputillumination.Othergeneralizations\n",
      "        ofrectiﬁedlinearunitsaremorebroadlyapplicable.A leakyReLU   ( , Maasetal.\n",
      " 2013)ﬁxesα i       toasmallvaluelike0.01,whilea  parametricReLU ,orPReLU,\n",
      " treatsα i       asalearnableparameter( ,). Heetal.2015\n",
      " Maxoutunits       ( ,)generalizerectiﬁedlinearunits Goodfellowet al.2013a\n",
      "      further.Insteadofapplyinganelement-wisefunctiong(z   ),maxoutunitsdividez\n",
      "  intogroupsofk          values.Eachmaxoutunitthenoutputsthemaximumelementof\n",
      "   oneofthesegroups:\n",
      "g()z i =max\n",
      "j ∈ G( ) iz j , (6.37)\n",
      "where G( ) i         isthesetofindicesintotheinputsforgroupi,{( i−1)k +1     ,...,ik}.\n",
      "             Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\n",
      "     directionsintheinputspace.x\n",
      "            Amaxoutunitcanlearnapiecewiselinear,convexfunctionwithuptokpieces.\n",
      "             Maxoutunitscanthusbeseenaslearningtheactivationfunctionitselfratherthan\n",
      "       justtherelationshipbetweenunits.Withlargeenoughk     ,amaxoutunitcanlearn\n",
      "           toapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,amaxout\n",
      "             layerwithtwopiecescanlearntoimplementthesamefunctionoftheinputx\n",
      "           asatraditionallayerusingtherectiﬁedlinearactivationfunction,theabsolute\n",
      "             valuerectiﬁcationfunction,ortheleakyorparametricReLU,oritcanlearnto\n",
      "           implementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse\n",
      "            beparametrizeddiﬀerentlyfromanyoftheseotherlayertypes,sothelearning\n",
      "             dynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe\n",
      "          samefunctionofasoneoftheotherlayertypes. x\n",
      "      Eachmaxoutunitisnowparametrizedbyk     weightvectorsinsteadofjustone,\n",
      "           somaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They\n",
      "              canworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\n",
      "         piecesperunitiskeptlow( ,). Caietal.2013\n",
      "              Maxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-\n",
      "        tisticalandcomputationaladvantagesbyrequiringfewerparameters.Speciﬁcally,\n",
      "    ifthefeaturescapturedbyn       diﬀerentlinearﬁlterscanbesummarizedwithout\n",
      "         losinginformationbytakingthemaxovereachgroupofk    features,thenthenext\n",
      "        layercangetbywithtimesfewerweights. k\n",
      "            Becauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-\n",
      "       dancythathelpsthemresistaphenomenoncalled  catastrophicforgetting ,in\n",
      "             whichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\n",
      "1 9 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "     thepast( ,). Goodfellowetal.2014a\n",
      "            Rectiﬁedlinearunitsandallthesegeneralizationsofthemarebasedonthe\n",
      "             principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\n",
      "           Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\n",
      "           alsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\n",
      "            learnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\n",
      "            them,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\n",
      "          easierwhensomelinearcomputations(withsomedirectionalderivativesbeingof\n",
      "          magnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork\n",
      "        architectures,theLSTM,propagatesinformationthroughtimeviasummation—a\n",
      "         particularstraightforwardkindoflinearactivation. Thisisdiscussedfurtherin\n",
      " section.10.10\n",
      "     6.3.2LogisticSigmoidandHyperbolicTangent\n",
      "            Priortotheintroductionofrectiﬁedlinearunits,mostneuralnetworksusedthe\n",
      "   logisticsigmoidactivationfunction\n",
      " gzσz () = () (6.38)\n",
      "     orthehyperbolictangentactivationfunction\n",
      " gzz. () = tanh( ) (6.39)\n",
      "         Theseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 zσz−\n",
      "        We havealready seensigmoidunits asoutputunits,usedto predictthe\n",
      "           probabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\n",
      "            unitssaturateacrossmostoftheirdomain—theysaturatetoahighvaluewhen\n",
      "z        isverypositive,saturatetoalowvaluewhenz     isverynegative,andareonly\n",
      "     stronglysensitivetotheirinputwhenz      isnear0.Thewidespreadsaturationof\n",
      "          sigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,\n",
      "            theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\n",
      "            asoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\n",
      "            appropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\n",
      "layer.\n",
      "          Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\n",
      "          activationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\n",
      "        theidentityfunctionmoreclosely,inthesensethattanh  (0) = 0whileσ(0) =1\n",
      "2.\n",
      "Becausetanh           issimilartotheidentityfunctionnear,trainingadeepneural 0\n",
      "networkˆy= wtanh( Utanh( Vx     ))resemblestrainingalinearmodelˆy=\n",
      "1 9 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "wUVx            aslongastheactivationsofthenetworkcanbekeptsmall.This\n",
      "     makestrainingthenetworkeasier. tanh\n",
      "          Sigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\n",
      "       forwardnetworks.Recurrentnetworks,manyprobabilisticmodels, andsome\n",
      "          autoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise\n",
      "          linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\n",
      "  drawbacksofsaturation.\n",
      "   6.3.3OtherHiddenUnits\n",
      "            Manyothertypesofhiddenunitsarepossiblebutareusedlessfrequently.\n",
      "          Ingeneral,awidevarietyofdiﬀerentiablefunctionsperformperfectlywell.\n",
      "            Manyunpublishedactivationfunctionsperformjustaswellasthepopularones.To\n",
      "         provideaconcreteexample,wetestedafeedforwardnetworkusingh=cos( Wx+b)\n",
      "               ontheMNISTdatasetandobtainedanerrorrateoflessthan1percent,whichis\n",
      "        competitivewithresultsobtainedusingmoreconventionalactivationfunctions.\n",
      "            Duringresearchanddevelopmentofnewtechniques,itiscommontotestmany\n",
      "          diﬀerentactivationfunctionsandﬁndthatseveralvariationsonstandardpractice\n",
      "           performcomparably.Thismeansthatusuallynewhiddenunittypesarepublished\n",
      "           onlyiftheyareclearlydemonstratedtoprovideasigniﬁcantimprovement.New\n",
      "            hiddenunittypesthatperformroughlycomparablytoknowntypesaresocommon\n",
      "   astobeuninteresting.\n",
      "              Itwouldbeimpracticaltolistallthehiddenunittypesthathaveappearedin\n",
      "          theliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\n",
      "       Onepossibilityistonothaveanactivation g( z       )atall.Onecanalsothinkof\n",
      "            thisasusingtheidentityfunctionastheactivationfunction.Wehavealready\n",
      "               seenthatalinearunitcanbeusefulastheoutputofaneuralnetwork. Itmay\n",
      "                alsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\n",
      "            lineartransformations,thenthenetworkasawholewillbelinear.However,it\n",
      "             isacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider\n",
      "    aneuralnetworklayerwith n inputsand poutputs,h= g(Wx+b  ).Wemay\n",
      "          replacethiswithtwolayers,withonelayerusingweightmatrixU  andtheother\n",
      "  usingweightmatrixV           .Iftheﬁrstlayerhasnoactivationfunction,thenwehave\n",
      "          essentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW .The\n",
      "    factoredapproachistocomputeh= g(VUx+b ).IfUproduces qoutputs,\n",
      "thenUandV    togethercontainonly( n+ p) q  parameters,whileWcontains n p\n",
      "  parameters.Forsmall q         ,thiscanbeaconsiderablesavinginparameters.It\n",
      "             comesatthecostofconstrainingthelineartransformationtobelowrank,but\n",
      "1 9 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           theselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran\n",
      "          eﬀectivewayofreducingthenumberofparametersinanetwork.\n",
      "              Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\n",
      "             describedinsection )butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\n",
      "          unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewithk\n",
      "               possiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden\n",
      "            unitsareusuallyonlyusedinmoreadvancedarchitecturesthatexplicitlylearnto\n",
      "      manipulatememory,asdescribedinsection.10.12\n",
      "        Afewotherreasonablycommonhiddenunittypesinclude\n",
      "   •Radialbasisfunction(RBF),unit:h i=exp\n",
      "−1\n",
      "σ2\n",
      "i|| W : , i −|| x2\n",
      " .This\n",
      "    functionbecomesmoreactiveas x   approachesatemplate W : , i  .Becauseit\n",
      "           saturatestoformost,itcanbediﬃculttooptimize. 0 x\n",
      " •Softplus:g(a) =ζ(a) =log(1+ea        ).Thisisasmoothversionoftherectiﬁer,\n",
      "           introducedby ()forfunctionapproximationandby Dugasetal.2001 Nair\n",
      "         andHinton2010()fortheconditionaldistributionsofundirectedprobabilistic\n",
      "           models. ()comparedthesoftplusandrectiﬁerandfound Glorotetal.2011a\n",
      "            betterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\n",
      "          Thesoftplusdemonstratesthattheperformanceofhiddenunittypescan\n",
      "          beverycounterintuitive—onemightexpectittohaveanadvantageover\n",
      "           therectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless\n",
      "     completely,butempiricallyitdoesnot.\n",
      "  •Hardtanh      .Thisisshapedsimilarlytothetanh    andtherectiﬁer,butunlike\n",
      "    thelatter,itisbounded,g(a )=max(−1 ,min(1 ,a   )).Itwasintroduced\n",
      "  by (). Collobert2004\n",
      "            Hiddenunitdesignremainsanactiveareaofresearch,andmanyusefulhidden\n",
      "     unittypesremaintobediscovered.\n",
      "  6.4ArchitectureDesign\n",
      "          Anotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\n",
      " Thewordarchitecture         referstotheoverallstructureofthenetwork:howmany\n",
      "             unitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\n",
      "          Mostneuralnetworksareorganizedintogroupsofunitscalledlayers. Most\n",
      "           neuralnetworkarchitecturesarrangetheselayersinachainstructure,witheach\n",
      "1 9 3    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "               layerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer\n",
      "  isgivenby\n",
      "h(1 )= g(1 )\n",
      "W(1 )   xb+(1 )\n",
      " ; (6.40)\n",
      "     thesecondlayerisgivenby\n",
      "h(2 )= g(2 )\n",
      "W(2 ) h(1 ) +b(2 )\n",
      " ; (6.41)\n",
      "  andsoon.\n",
      "        Inthesechain-basedarchitectures,themainarchitecturalconsiderationsare\n",
      "        choosingthe depthofthe networkand thewidthof each layer.Aswe will\n",
      "              see,anetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.\n",
      "              Deepernetworksareoftenabletousefarfewerunitsperlayerandfarfewer\n",
      "              parameters,aswellasfrequentlygeneralizingtothetestset,buttheyalsotendto\n",
      "             behardertooptimize.Theidealnetworkarchitectureforataskmustbefound\n",
      "        viaexperimentationguidedbymonitoringthevalidationseterror.\n",
      "     6.4.1UniversalApproximationPropertiesandDepth\n",
      "           Alinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication,can\n",
      "             bydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\n",
      "          trainbecausemanylossfunctionsresultinconvexoptimizationproblemswhen\n",
      "          appliedtolinearmodels.Unfortunately, weoftenwantoursystemstolearn\n",
      " nonlinearfunctions.\n",
      "           Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\n",
      "             designingaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.\n",
      "         Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\n",
      "   mationframework.Speciﬁcally,the   universalapproximationtheorem(Hornik\n",
      "             etal.,; ,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\n",
      "            layerandatleastonehiddenlayerwithany“squashing”activationfunction(such\n",
      "          asthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\n",
      "          functionfromoneﬁnite-dimensionalspacetoanotherwithanydesirednonzero\n",
      "            amountoferror,providedthatthenetworkisgivenenoughhiddenunits.The\n",
      "           derivativesofthefeedforwardnetworkcanalsoapproximatethederivativesofthe\n",
      "           functionarbitrarilywell( ,).TheconceptofBorelmeasurability Horniketal.1990\n",
      "              isbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatany\n",
      "        continuousfunctiononaclosedandboundedsubsetof Rn  isBorelmeasurable\n",
      "            andthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\n",
      "          alsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace\n",
      "1 9 4    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             toanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith\n",
      "           activationfunctionsthatsaturateforbothverynegativeandverypositiveargu-\n",
      "           ments,universalapproximationtheoremshavealsobeenprovedforawiderclass\n",
      "          ofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear\n",
      "    unit( ,). Leshnoetal.1993\n",
      "         Theuniversalapproximationtheoremmeansthatregardlessofwhatfunction\n",
      "                wearetryingtolearn,weknowthatalargeMLPwillbeabletorepresentthis\n",
      "           function.Wearenotguaranteed,however,thatthetrainingalgorithmwillbe\n",
      "              abletolearnthatfunction.EveniftheMLPisabletorepresentthefunction,\n",
      "           learningcanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmused\n",
      "              fortrainingmaynotbeabletoﬁndthevalueoftheparametersthatcorresponds\n",
      "           tothedesiredfunction.Second,thetrainingalgorithmmightchoosethewrong\n",
      "              functionasaresultofoverﬁtting.Recallfromsection thatthenofreelunch 5.2.1\n",
      "          theoremshowsthatthereisnouniversallysuperiormachinelearningalgorithm.\n",
      "          Feedforwardnetworksprovideauniversalsystemforrepresentingfunctionsinthe\n",
      "           sensethat,givenafunction,thereexistsafeedforwardnetworkthatapproximates\n",
      "            thefunction.Thereisnouniversalprocedureforexaminingatrainingsetof\n",
      "             speciﬁcexamplesandchoosingafunctionthatwillgeneralizetopointsnotinthe\n",
      " trainingset.\n",
      "          Accordingtotheuniversalapproximationtheorem,thereexistsanetworklarge\n",
      "             enoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\n",
      "             sayhowlargethisnetworkwillbe. ()providessomeboundsonthe Barron1993\n",
      "            sizeofasingle-layernetworkneededtoapproximateabroadclassoffunctions.\n",
      "           Unfortunately,intheworstcase,anexponentialnumberofhiddenunits(possibly\n",
      "            withonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe\n",
      "             distinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\n",
      "      numberofpossiblebinaryfunctionsonvectors  v∈{0 ,1}n is22n andselecting\n",
      "    onesuchfunctionrequires2n     bits,whichwillingeneralrequire O(2n  )degreesof\n",
      "freedom.\n",
      "            Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent\n",
      "              anyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\n",
      "          generalizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe\n",
      "            numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\n",
      "   amountofgeneralizationerror.\n",
      "          Variousfamiliesoffunctionscanbeapproximatedeﬃcientlybyanarchitecture\n",
      "     withdepthgreaterthansomevalue d        ,buttheyrequireamuchlargermodelif\n",
      "         depthisrestrictedtobelessthanorequalto d     .Inmanycases,thenumber\n",
      "          ofhiddenunitsrequiredbytheshallowmodelisexponentialin n . Suchresults\n",
      "1 9 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "           Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\n",
      "            rectiﬁernetworksformallyby (). Montufaretal.2014(Left)Anabsolutevaluerectiﬁcation\n",
      "                unithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\n",
      "                ofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A\n",
      "               functioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\n",
      "              ofasimplerpatternacrossthataxisofsymmetry. Thefunctioncanbeobtained (Center)\n",
      "             byfoldingthespacearoundtheaxisofsymmetry. Anotherrepeatingpatterncan (Right)\n",
      "              befoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry\n",
      "            (whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\n",
      "     permissionfromMontufar 2014etal.().\n",
      "           wereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable\n",
      "            neuralnetworksusedformachinelearningbuthavesincebeenextendedtothese\n",
      "            models.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986\n",
      "          workextendedtheseresultstolinearthresholdunitswithnonnegativeweights\n",
      "            ( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnaletal.1993\n",
      "        continuous-valuedactivations(,; ,). Manymodern Maass1992Maassetal.1994\n",
      "          neuralnetworksuserectiﬁedlinearunits. ()demonstrated Leshnoetal.1993\n",
      "          thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\n",
      "         includingrectiﬁedlinearunits,haveuniversalapproximationproperties,butthese\n",
      "            resultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat\n",
      "           asuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufaretal.\n",
      "           ()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\n",
      "            anexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\n",
      "           Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\n",
      "           fromrectiﬁernonlinearitiesormaxoutunits)canrepresentfunctionswithanumber\n",
      "              ofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\n",
      "           anetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction\n",
      "              computedontopofsomehiddenunit,withrespecttotheinputofthathidden\n",
      "              unit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate\n",
      "           mirrorresponses(onbothsidesoftheabsolutevaluenonlinearity).Bycomposing\n",
      "          thesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\n",
      "           linearregionsthatcancaptureallkindsofregular(e.g.,repeating)patterns.\n",
      "1 9 6    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             Themaintheoremin ()statesthatthenumberoflinear Montufaretal.2014\n",
      "        regionscarvedoutbyadeeprectiﬁernetworkwithd  inputs,depthl ,andnunits\n",
      "   perhiddenlayeris\n",
      "On\n",
      "dd l ( − 1 )\n",
      "nd\n",
      " , (6.42)\n",
      "    thatis,exponentialindepthl       .Inthecaseofmaxoutnetworkswithk ﬁltersper\n",
      "      unit,thenumberoflinearregionsis\n",
      "O\n",
      "k( 1 )+ l − d\n",
      " . (6.43)\n",
      "               Ofcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\n",
      "            applicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\n",
      "            Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytime\n",
      "           wechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome\n",
      "             setofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\n",
      "             learn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe\n",
      "            wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\n",
      "            interpretedfromarepresentationlearningpointofviewassayingthatwebelieve\n",
      "            thelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\n",
      "             thatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\n",
      "            variation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\n",
      "              abeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\n",
      "           multiplesteps,whereeachstepmakesuseofthepreviousstep’soutput. These\n",
      "           intermediateoutputsarenotnecessarilyfactorsofvariationbutcaninsteadbe\n",
      "            analogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\n",
      "          processing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\n",
      "               forawidevarietyoftasks( ,; ,;,; Bengioetal.2007Erhanetal.2009Bengio2009\n",
      "              Mesnil2011Ciresan 2012Krizhevsky2012Sermanet etal.,; etal.,; etal.,; etal.,\n",
      "             2013Farabet2013Couprie2013Kahou 2013Goodfellow ; etal.,; etal.,; etal.,;\n",
      "               etal. etal. ,;2014dSzegedy,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7\n",
      "           someoftheseempiricalresults.Theseresultssuggestthatusingdeeparchitectures\n",
      "             doesindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\n",
      "   6.4.2OtherArchitecturalConsiderations\n",
      "              Sofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\n",
      "             mainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\n",
      "       Inpractice,neuralnetworksshowconsiderablymorediversity.\n",
      "1 9 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "        3 4 5 6 7 8 9 10 11920 .925 .930 .935 .940 .945 .950 .955 .960 .965 .  Testaccuracy(percent)\n",
      "           Figure6.6:Eﬀectofdepth.Empiricalresultsshowingthatdeepernetworksgeneralize\n",
      "           betterwhenusedtotranscribemultidigitnumbersfromphotographsofaddresses.Data\n",
      "            fromGoodfellow 2014detal.().Thetestsetaccuracyconsistentlyincreaseswithincreasing\n",
      "             depth.Seeﬁgureforacontrolexperimentdemonstratingthatotherincreasestothe 6.7\n",
      "       modelsizedonotyieldthesameeﬀect.\n",
      "         Manyneuralnetworkarchitectureshavebeendevelopedforspeciﬁctasks.\n",
      "        Specializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\n",
      "           describedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\n",
      "          recurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\n",
      "    havetheirownarchitecturalconsiderations.\n",
      "               Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\n",
      "            mostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra\n",
      "          architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer i tolayer\n",
      "i               +2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom\n",
      "      outputlayerstolayersnearertheinput.\n",
      "           Anotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\n",
      "               pairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\n",
      "   transformationviaamatrix W        ,everyinputunitisconnectedtoeveryoutput\n",
      "           unit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\n",
      "                 thateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsinthe\n",
      "         outputlayer. Thesestrategiesfordecreasingthenumberofconnectionsreduce\n",
      "           thenumberofparametersandtheamountofcomputationrequiredtoevaluate\n",
      "          thenetworkbutareoftenhighlyproblemdependent.Forexample,convolutional\n",
      "1 9 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "     00 02 04 06 08 10 . . . . . .\n",
      "  Numberofparameters ×10891929394959697  Testaccuracy(percent)  3,convolutional\n",
      "  3,fullyconnect ed\n",
      " 11,convolutional\n",
      "            Figure6.7:Eﬀectofnumberofparameters.Deepermodelstendtoperformbetter.\n",
      "              Thisisnotmerelybecausethemodelislarger.ThisexperimentfromGoodfellowetal.\n",
      "            ()showsthatincreasingthenumberofparametersinlayersofconvolutionalnetworks 2014d\n",
      "             withoutincreasingtheirdepthisnotnearlyaseﬀectiveatincreasingtestsetperformance,\n",
      "              asillustratedinthisﬁgure.Thelegendindicatesthedepthofnetworkusedtomake\n",
      "             eachcurveandwhetherthecurverepresentsvariationinthesizeoftheconvolutional\n",
      "              orthefullyconnectedlayers.Weobservethatshallowmodelsinthiscontextoverﬁtat\n",
      "             around20millionparameterswhiledeeponescanbeneﬁtfromhavingover60million.\n",
      "              Thissuggeststhatusingadeepmodelexpressesausefulpreferenceoverthespaceof\n",
      "             functionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthefunctionshould\n",
      "            consistofmanysimplerfunctionscomposedtogether.Thiscouldresulteitherinlearning\n",
      "            arepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,cornersdeﬁned\n",
      "              intermsofedges)orinlearningaprogramwithsequentiallydependentsteps(e.g.,ﬁrst\n",
      "             locateasetofobjects,thensegmentthemfromeachother,thenrecognizethem).\n",
      "1 9 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "          networks,describedinchapter,usespecializedpatternsofsparseconnections 9\n",
      "             thatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult\n",
      "            togivemorespeciﬁcadviceconcerningthearchitectureofagenericneuralnetwork.\n",
      "          Insubsequentchapterswedeveloptheparticulararchitecturalstrategiesthathave\n",
      "        beenfoundtoworkwellfordiﬀerentapplicationdomains.\n",
      "    6.5Back-PropagationandOtherDiﬀerentiation\n",
      "Algorithms\n",
      "          Whenweuseafeedforwardneuralnetworktoacceptaninput x  andproducean\n",
      "outputˆ y        ,informationﬂowsforwardthroughthenetwork.Theinput xprovides\n",
      "             theinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\n",
      "  andﬁnallyproducesˆ y   .Thisiscalled  forwardpropagation  .Duringtraining,\n",
      "          forwardpropagationcancontinueonwarduntilitproducesascalarcostJ( θ).\n",
      "Theback-propagation        algorithm( ,),oftensimplycalled Rumelhartetal.1986a\n",
      "backprop           ,allowstheinformationfromthecosttothenﬂowbackwardthrough\n",
      "       thenetworkinordertocomputethegradient.\n",
      "         Computingananalyticalexpressionforthegradientisstraightforward,but\n",
      "         numericallyevaluatingsuchanexpressioncanbecomputationallyexpensive.The\n",
      "         back-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\n",
      "         Thetermback-propagationisoftenmisunderstoodasmeaningthewhole\n",
      "        learningalgorithmformultilayerneuralnetworks.Actually,back-propagation\n",
      "           refersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\n",
      "            suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\n",
      "         Furthermore,back-propagationisoftenmisunderstoodasbeingspeciﬁctomulti-\n",
      "            layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n",
      "             (forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\n",
      "           functionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient\n",
      "∇ xf( x y,    )foranarbitraryfunctionf ,where x      isasetofvariableswhosederivatives\n",
      "  aredesired,and y           isanadditionalsetofvariablesthatareinputstothefunction\n",
      "            butwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\n",
      "             oftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\n",
      "∇ θJ( θ         ).Manymachinelearningtasksinvolvecomputingotherderivatives,either\n",
      "      aspartof thelearning process,  or toanalyze thelearned model.The back-\n",
      "              propagationalgorithmcanbeappliedtothesetasksaswellandisnotrestrictedto\n",
      "            computingthegradientofthecostfunctionwithrespecttotheparameters.The\n",
      "          ideaofcomputingderivativesbypropagatinginformationthroughanetworkis\n",
      "               verygeneralandcanbeusedtocomputevaluessuchastheJacobianofafunction\n",
      "2 0 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "f           withmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\n",
      "       usedcase,wherehasasingleoutput. f\n",
      "  6.5.1ComputationalGraphs\n",
      "            Sofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\n",
      "            Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\n",
      "    moreprecise language. computationalgraph\n",
      "        Manywaysofformalizingcomputationasgraphsarepossible.\n",
      "              Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\n",
      "            beascalar,vector,matrix,tensor,orevenavariableofanothertype.\n",
      "            Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.\n",
      "             Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\n",
      "          isaccompaniedbyasetofallowableoperations.Functionsmorecomplicated\n",
      "            thantheoperationsinthissetmaybedescribedbycomposingmanyoperations\n",
      "together.\n",
      "           Withoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingle\n",
      "            outputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\n",
      "         multipleentries,suchasavector.Softwareimplementationsofback-propagation\n",
      "            usuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour\n",
      "           descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\n",
      " conceptualunderstanding.\n",
      "  Ifavariabley        iscomputedbyapplyinganoperationtoavariablex ,then\n",
      "     wedrawadirectededgefromxtoy     . Wesometimesannotatetheoutputnode\n",
      "              withthenameoftheoperationapplied,andothertimesomitthislabelwhenthe\n",
      "    operationisclearfromcontext.\n",
      "        Examplesofcomputationalgraphsareshowninﬁgure.6.8\n",
      "    6.5.2ChainRuleofCalculus\n",
      "               Thechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is\n",
      "           usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\n",
      "          whosederivativesareknown.Back-propagationisanalgorithmthatcomputesthe\n",
      "           chainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.\n",
      "Letx     bearealnumber,andletfandg      bothbefunctionsmappingfromareal\n",
      "      numbertoarealnumber.Supposethaty=g(x )andz=f(g(x)) =f(y ).Then\n",
      "2 0 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "z z\n",
      "xx yy\n",
      "( a)×\n",
      "xxww\n",
      "( b )u( 1 )u( 1 )\n",
      "d o t\n",
      "bbu( 2 )u( 2 )\n",
      "+ˆ y ˆ y\n",
      "σ\n",
      "( c )XXWWU( 1 )U( 1 )\n",
      "m a t m u l\n",
      "bbU( 2 )U( 2 )\n",
      "+HH\n",
      "r e l u\n",
      "xx ww\n",
      "( d )ˆ yˆ y\n",
      "d o t\n",
      "λ λu( 1 )u( 1 )\n",
      "s q ru( 2 )u( 2 )\n",
      "s u mu( 3 )u( 3 )\n",
      "×\n",
      "          Figure6.8:Examplesofcomputationalgraphs.Thegraphusingthe (a) ×  operationto\n",
      "computez=xy       . Thegraphforthelogisticregressionprediction (b) ˆy=σ\n",
      "x  w+b\n",
      ".\n",
      "            Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpression\n",
      "         butneednamesinthegraph.Wesimplynamethei  -thsuchvariableu( ) i  .The(c)\n",
      "    computationalgraphfortheexpressionH=max{0 ,XW+b}    ,whichcomputesadesign\n",
      "     matrixofrectiﬁedlinearunitactivationsH      givenadesignmatrixcontainingaminibatch\n",
      " ofinputsX             .Examplesa–cappliedatmostoneoperationtoeachvariable,butit (d)\n",
      "              ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\n",
      "       appliesmorethanoneoperationtotheweightsw     ofalinearregressionmodel.The\n",
      "              weightsareusedtomakeboththepredictionˆyandtheweightdecaypenaltyλ\n",
      "iw2\n",
      "i.\n",
      "2 0 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "    thechainrulestatesthatdz\n",
      "dx=dz\n",
      "dydy\n",
      "dx . (6.44)\n",
      "         Wecangeneralizethisbeyondthescalarcase.Supposethat  x∈ Rm,  y∈ Rn,\n",
      "g mapsfrom Rmto Rn ,andf mapsfrom Rnto R .Ify=g(x) andz=f(y ),then\n",
      "∂z\n",
      "∂x i=\n",
      "j∂z\n",
      "∂y j∂y j\n",
      "∂x i . (6.45)\n",
      "        Invectornotation,thismaybeequivalentlywrittenas\n",
      "∇ x z=∂y\n",
      "∂x\n",
      "∇ y z, (6.46)\n",
      "where∂ y\n",
      "∂ x        isthe Jacobianmatrixof. nm× g\n",
      "         Fromthisweseethatthegradientofavariablex    canbeobtainedbymultiplying\n",
      "  aJacobianmatrix∂ y\n",
      "∂ x  byagradient∇ yz    .Theback-propagationalgorithmconsists\n",
      "           ofperformingsuchaJacobian-gradientproductforeachoperationinthegraph.\n",
      "          Usuallyweapplytheback-propagationalgorithmtotensorsofarbitrarydi-\n",
      "           mensionality,notmerelytovectors.Conceptually,thisisexactlythesameas\n",
      "           back-propagationwithvectors.Theonlydiﬀerenceishowthenumbersarear-\n",
      "              rangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensorinto\n",
      "         avectorbeforewerunback-propagation, computingavector-valuedgradient,\n",
      "            andthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\n",
      "       back-propagationisstilljustmultiplyingJacobiansbygradients.\n",
      "      Todenotethegradientofavaluez    withrespecttoatensor X  ,wewrite∇ Xz,\n",
      "  justasif X     wereavector.Theindicesinto X   nowhavemultiplecoordinates—for\n",
      "             example,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\n",
      "    byusingasinglevariablei        torepresentthecompletetupleofindices.Forall\n",
      "  possibleindextuplesi ,(∇ Xz) igives∂ z\n",
      "∂ X i         .Thisisexactlythesameashowforall\n",
      "  possibleintegerindicesi   intoavector,(∇ xz) igives∂ z\n",
      "∂ x i    .Usingthisnotation,we\n",
      "                canwritethechainruleasitappliestotensors.If and ,then Y X= (g)zf= () Y\n",
      "∇ X z=\n",
      "j(∇ X Y j)∂z\n",
      "∂ Y j . (6.47)\n",
      "2 0 3    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "        6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\n",
      "             Usingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\n",
      "              thegradientofascalarwithrespecttoanynodeinthecomputationalgraphthat\n",
      "          producedthatscalar.Actuallyevaluatingthatexpressioninacomputer,however,\n",
      "   introducessomeextraconsiderations.\n",
      "         Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe\n",
      "          overallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n",
      "            willneedtochoosewhethertostorethesesubexpressionsortorecomputethem\n",
      "            severaltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\n",
      "           ﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\n",
      "          bewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\n",
      "          wastedcomputations,makinganaiveimplementationofthechainruleinfeasible.\n",
      "             Inothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\n",
      "        reducememory consumptionatthecostofhigherruntime.\n",
      "           Webeginwithaversionoftheback-propagationalgorithmthatspeciﬁesthe\n",
      "           actualgradientcomputationdirectly(algorithmalongwithalgorithmforthe 6.2 6.1\n",
      "            associatedforwardcomputation),intheorderitwillactuallybedoneandaccording\n",
      "            totherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\n",
      "           computationsorviewthedescriptionofthealgorithmasasymbolicspeciﬁcation\n",
      "         ofthecomputationalgraphforcomputingtheback-propagation.However,this\n",
      "           formulationdoesnotmakeexplicitthemanipulationandtheconstructionofthe\n",
      "         symbolicgraphthatperformsthegradientcomputation. Suchaformulationis\n",
      "            presentedinsection,withalgorithm,wherewealsogeneralizetonodes 6.5.6 6.5\n",
      "   thatcontainarbitrarytensors.\n",
      "           Firstconsideracomputationalgraphdescribinghowtocomputeasinglescalar\n",
      "u( ) n           (say, thelossonatrainingexample).Thisscalaristhequantitywhose\n",
      "        gradientwewanttoobtain,withrespecttothen i inputnodesu(1 )tou( n i ). In\n",
      "     otherwords,wewishtocompute∂ u( ) n\n",
      "∂ u( ) i forall  i∈{1,2     ,...,n i}   .Intheapplication\n",
      "         ofback-propagationtocomputinggradientsforgradientdescentoverparameters,\n",
      "u( ) n           willbethecostassociatedwithanexampleoraminibatch,whileu(1 )tou( n i )\n",
      "      correspondtotheparametersofthemodel.\n",
      "               Wewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\n",
      "           thatwecancomputetheiroutputoneaftertheother,startingatu( n i +1 )and\n",
      "  goinguptou( ) n       .Asdeﬁnedinalgorithm,eachnode 6.1 u( ) i   isassociatedwithan\n",
      " operationf( ) i      andiscomputedbyevaluatingthefunction\n",
      "u( ) i= (f A( ) i ), (6.48)\n",
      " where A( ) i          isthesetofallnodesthatareparentsofu( ) i.\n",
      "2 0 4    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "z z\n",
      "xxyy\n",
      "w wfff\n",
      "           Figure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\n",
      "  thegradient.Let  w∈ R          betheinputtothegraph.Weusethesamefunctionf:   R R→\n",
      "           astheoperationthatweapplyateverystepofachain:x=f(w),y=f(x),z=f(y).\n",
      " Tocompute∂ z\n",
      "∂ w      ,weapplyequation andobtain: 6.44\n",
      "∂z\n",
      "∂w(6.49)\n",
      "=∂z\n",
      "∂y∂y\n",
      "∂x∂x\n",
      "∂w(6.50)\n",
      "=f()yf()xf ()w (6.51)\n",
      "=f((()))ffwf(())fwf ()w. (6.52)\n",
      "           Equation suggestsanimplementationinwhichwecomputethevalueof 6.51 f(w )only\n",
      "      onceandstoreitinthevariablex        .Thisistheapproachtakenbytheback-propagation\n",
      "           algorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.52\n",
      "f(w        )appearsmorethanonce.Inthealternativeapproach,f(w    )isrecomputedeachtime\n",
      "                itisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\n",
      "           back-propagationapproachofequation isclearlypreferablebecauseofitsreduced 6.51\n",
      "              runtime.However,equation isalsoavalidimplementationofthechainruleandis 6.52\n",
      "    usefulwhenmemoryislimited.\n",
      "2 0 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "         Thatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould\n",
      "   putinagraphG        .Toperformback-propagation, wecanconstructacomputational\n",
      "   graphthatdependsonG           andaddstoitanextrasetofnodes.Theseforma\n",
      "subgraphB     withonenodepernodeofG  .ComputationinB   proceedsinexactly\n",
      "       thereverseoftheorderofcomputationinG    ,andeachnodeofB  computesthe\n",
      "derivative∂u( ) n\n",
      "∂u( ) i      associatedwiththeforwardgraphnodeu( )i    .Thisisdoneusing\n",
      "        thechainrulewithrespecttoscalaroutputu( )n:\n",
      "∂u( )n\n",
      "∂u( )j=\n",
      " ijPau : ∈ (( ) i )∂u( )n\n",
      "∂u( )i∂u( )i\n",
      "∂u( )j(6.53)\n",
      "      asspeciﬁedbyalgorithm.Thesubgraph 6.2 B      containsexactlyoneedgeforeach\n",
      "  edgefromnodeu( )j tonodeu( )iofG   .Theedgefromu( )jtou( )i  isassociatedwith\n",
      "  thecomputationof∂u( ) i\n",
      "∂u( ) j          .Inaddition,adotproductisperformedforeachnode,\n",
      "        betweenthegradientalreadycomputedwithrespecttonodesu( )i  thatarechildren\n",
      "ofu( )j      andthevectorcontainingthepartialderivatives∂u( ) i\n",
      "∂u( ) j   forthesamechildren\n",
      "nodesu( )i         .Tosummarize,theamountofcomputationrequiredforperforming\n",
      "         theback-propagationscaleslinearlywiththenumberofedgesinG  ,wherethe\n",
      "           computationforeachedgecorrespondstocomputingapartialderivative(ofone\n",
      "             nodewithrespecttooneofitsparents)aswellasperformingonemultiplication\n",
      "           andoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\n",
      "               isjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\n",
      " Algorithm6.1      Aprocedurethatperformsthecomputationsmappingniinputs\n",
      "u(1 )tou(n i )  toanoutputu( )n        .Thisdeﬁnesacomputationalgraphwhereeachnode\n",
      "  computesnumericalvalueu( )i   byapplyingafunctionf( )i    tothesetofarguments\n",
      "A( )i      thatcomprisesthevaluesofpreviousnodesu( )j,  j<i ,with   jPa∈ (u( )i ).The\n",
      "       inputtothecomputationalgraphisthevector x      ,andissetintotheﬁrstninodes\n",
      "u(1 )tou(n i )            .Theoutputofthecomputationalgraphisreadoﬀthelast(output)\n",
      " nodeu( )n.\n",
      "       fori,...,n = 1ido\n",
      "u( )i ←xi\n",
      " endfor\n",
      "  forin= i       +1,...,ndo\n",
      "A( )i ←{u( )j    |∈jPau(( )i)}\n",
      "u( )i ←f( )i( A( )i)\n",
      " endfor\n",
      " returnu( )n\n",
      "2 0 6    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      " eﬃcientimplementations.\n",
      "          Theback-propagationalgorithmisdesignedtoreducethenumberofcommon\n",
      "          subexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder\n",
      "              ofoneJacobianproductpernodeinthegraph. Thiscanbeseenfromthefact\n",
      "        thatbackprop(algorithm)visitseachedgefromnode 6.2 u( )j tonodeu( )iof\n",
      "           thegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂u( ) i\n",
      "∂u( ) j.\n",
      "        Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpres-\n",
      "           sions.Otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\n",
      "           simpliﬁcationsonthecomputationalgraph,ormaybeabletoconservememory\n",
      "          byrecomputingratherthanstoringsomesubexpressions.Werevisittheseideas\n",
      "     afterdescribingtheback-propagationalgorithmitself.\n",
      " Algorithm6.2        Simpliﬁedversionoftheback-propagationalgorithmforcomputing\n",
      "  thederivativesofu( )n          withrespecttothevariablesinthegraph.Thisexampleis\n",
      "           intendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables\n",
      "           arescalars,andwewishtocomputethederivativeswithrespecttou(1 )     ,...,u(n i ).\n",
      "            Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.The\n",
      "            computationalcostofthisalgorithmisproportionaltothenumberofedgesin\n",
      "           thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\n",
      "              aconstanttime.Thisisofthesameorderasthenumberofcomputationsfor\n",
      "   theforwardpropagation. Each∂u( ) i\n",
      "∂u( ) j     isafunctionoftheparentsu( )jofu( )i ,thus\n",
      "            linkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\n",
      "graph.\n",
      "           Runforwardpropagation(algorithmforthisexample)toobtaintheactiva- 6.1\n",
      "   tionsofthenetwork.\n",
      "Initializegrad_table          ,adatastructurethatwillstorethederivativesthathave\n",
      "   beencomputed.Theentry g r a d t a b l e_ [u( )i      ]willstorethecomputedvalueof\n",
      "∂u( ) n\n",
      "∂u( ) i.\n",
      "g r a d t a b l e_ [u( )n ] 1←\n",
      "        for do jn= −1downto1\n",
      "   Thenextlinecomputes∂u( ) n\n",
      "∂u( ) j=\n",
      " ijPau : ∈ (( ) i )∂u( ) n\n",
      "∂u( ) i∂u( ) i\n",
      "∂u( ) j  usingstoredvalues:\n",
      "g r a d t a b l e_ [u( )j] ←\n",
      " ijPau : ∈ (( ) i ) g r a d t a b l e_ [u( )i]∂u( ) i\n",
      "∂u( ) j\n",
      " endfor\n",
      " return{ g r a d t a b l e_ [u( )i       ] = 1|i,...,ni}\n",
      "2 0 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "      6.5.4Back-Propaga tionComputationinFullyConnectedMLP\n",
      "           Toclarifytheabovedeﬁnitionoftheback-propagationcomputation,letusconsider\n",
      "         thespeciﬁcgraphassociatedwithafully-connectedmultilayerMLP.\n",
      "          Algorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3\n",
      "  thesupervisedlossL(ˆ yy,       )associatedwithasingle(input,target)trainingexample\n",
      "  ( )xy,,withˆ            ytheoutputoftheneuralnetworkwhenisprovidedininput. x\n",
      "    Algorithm then shows thecorresponding computati onto bedone for 6.4\n",
      "      applyingtheback-propagationalgorithmtothisgraph.\n",
      "           Algorithmsandaredemonstrationschosentobesimpleandstraightfor- 6.36.4\n",
      "          wardtounderstand.However,theyarespecializedtoonespeciﬁcproblem.\n",
      "          Modernsoftwareimplementationsarebasedonthegeneralizedformofback-\n",
      "          propagationdescribedinsection below,whichcanaccommodate anycompu- 6.5.6\n",
      "          tationalgraphbyexplicitlymanipulatingadatastructureforrepresentingsymbolic\n",
      "computation.\n",
      " Algorithm6.3         Forwardpropagationthroughatypicaldeepneuralnetworkand\n",
      "       thecomputationofthecostfunction.ThelossL(ˆ yy,    )dependsontheoutput\n",
      "ˆy   andonthetargety        (seesection forexamplesoflossfunctions).To 6.2.1.1\n",
      "   obtainthetotalcostJ         ,thelossmaybeaddedtoaregularizerΩ(θ ),whereθ\n",
      "           containsalltheparameters(weightsandbiases).Algorithmshowshowto 6.4\n",
      "  computegradientsofJ   withrespecttoparametersWandb   .Forsimplicity,this\n",
      "      demonstrationusesonlyasingleinputexamplex   .Practicalapplicationsshould\n",
      "          useaminibatch.Seesection foramorerealisticdemonstration. 6.5.7\n",
      "   Require:Networkdepth,l\n",
      " Require:W( ) i              ,i,...,l,∈{1}theweightmatricesofthemodel\n",
      " Require:b( ) i              ,i,...,l,∈{1}thebiasparametersofthemodel\n",
      "     Require:x,theinputtoprocess\n",
      "    Require:y,thetargetoutput\n",
      "h(0 )= x\n",
      "        for do k,...,l = 1\n",
      "a( ) k= b( ) k +W( ) kh( 1 ) k −\n",
      "h( ) k= (fa( ) k)\n",
      " endfor\n",
      "ˆ yh= ( ) l\n",
      " JL= (ˆ   yy,)+Ω()λθ\n",
      "2 0 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "Algorithm 6.4     Backwardcomputation forthe deepneural network ofalgo-\n",
      "        rithm,whichuses,inadditiontotheinput 6.3 x  ,atargety  .Thiscomputation\n",
      "     yieldsthegradientsontheactivationsa() k  foreachlayerk   ,startingfromthe\n",
      "            outputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,\n",
      "             whichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange\n",
      "              toreduceerror,onecanobtainthegradientontheparametersofeachlayer.The\n",
      "             gradientsonweightsandbiasescanbeimmediatelyusedaspartofastochas-\n",
      "           ticgradientupdate(performingtheupdaterightafterthegradientshavebeen\n",
      "       computed)orusedwithothergradient-basedoptimizationmethods.\n",
      "          Aftertheforwardcomputation,computethegradientontheoutputlayer:\n",
      "  g←∇ˆ y J= ∇ˆ yL(ˆ yy,)\n",
      "           for do kl,l,..., = −1 1\n",
      "          Convert thegradientonthelayer’soutputinto agradientonthepre-\n",
      "  nonlinearity activation (element-wisemultiplicationiff iselement-wise):\n",
      "  g←∇a( ) k   Jf = g(a() k)\n",
      "         Computegradientsonweightsandbiases(includingtheregularizationterm,\n",
      " whereneeded):\n",
      "∇b( ) k   Jλ = +g∇b( ) kΩ()θ\n",
      "∇W( ) k  J= gh(1) k −  +λ∇W( ) kΩ()θ\n",
      "         Propagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:\n",
      "  g←∇h( 1 ) k − J= W() k g\n",
      " endfor\n",
      "  6.5.5Symbol-to-SymbolDerivatives\n",
      "       Algebraicexpressionsandcomputationalgraphsbothoperateonsymbols ,or\n",
      "    variables thatdo not havespeciﬁc values.These algebraicand graph-based\n",
      "  representationsarecalled  symbolicrepresentations    .Whenweactuallyuse\n",
      "             ortrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We\n",
      "      replaceasymbolicinputtothenetworkx  withaspeciﬁcnumeric  value,suchas\n",
      "  [123765 18] .,.,−..\n",
      "           Someapproachestoback-propagationtakeacomputationalgraphandasetof\n",
      "              numericalvaluesfortheinputstothegraph,thenreturnasetofnumericalvalues\n",
      "          describingthegradientatthoseinputvalues.Wecallthisapproachsymbol-to-\n",
      " numberdiﬀerentiation         . ThisistheapproachusedbylibrariessuchasTorch\n",
      "       ( ,)andCaﬀe(,). Collobertetal.2011b Jia2013\n",
      "           Anotherapproachistotakeacomputationalgraphandaddadditionalnodes\n",
      "2 0 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "z z\n",
      "xxyy\n",
      "w wfffz z\n",
      "xxyy\n",
      "w wfff\n",
      "d z\n",
      "d yd z\n",
      "d yf\n",
      "d y\n",
      "d xd y\n",
      "d xf\n",
      "d z\n",
      "d xd z\n",
      "d x×\n",
      "d x\n",
      "d wd x\n",
      "d wf\n",
      "d z\n",
      "d wd z\n",
      "d w×\n",
      "           Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\n",
      "            thisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\n",
      "            speciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\n",
      "            tocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe\n",
      "              derivativesforanyspeciﬁcnumericvalues.(Left)Inthisexample,webeginwithagraph\n",
      "representing z = f ( f ( f ( w       ) ) ). Weruntheback-propagationalgorithm,instructing (Right)\n",
      "         ittoconstructthegraphfortheexpressioncorrespondingtod z\n",
      "d w     .Inthisexample,wedo\n",
      "            notexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\n",
      "             whatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\n",
      "derivative.\n",
      "            tothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\n",
      "              istheapproachtakenbyTheano( ,; ,)and Bergstraetal.2010Bastienetal.2012\n",
      "             TensorFlow( ,).Anexampleofhowitworksisillustratedin Abadietal.2015\n",
      "           ﬁgure. Theprimaryadvantageofthisapproachisthatthederivativesare 6.10\n",
      "           describedinthesamelanguageastheoriginalexpression.Becausethederivatives\n",
      "         arejustanothercomputationalgraph, itispossibletorunback-propagation\n",
      "         again,diﬀerentiatingthederivativestoobtainhigherderivatives.(Computationof\n",
      "      higher-orderderivativesisdescribedinsection .)6.5.10\n",
      "           Wewillusethelatterapproachanddescribetheback-propagationalgorithmin\n",
      "            termsofconstructingacomputationalgraphforthederivatives.Anysubsetofthe\n",
      "             graphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This\n",
      "           allowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\n",
      "             Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\n",
      "   parents’valuesareavailable.\n",
      "         Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\n",
      "2 1 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "        to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas\n",
      "             performingexactlythesamecomputationsasaredoneinthegraphbuiltbythe\n",
      "        symbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number\n",
      "     approachdoesnotexposethegraph.\n",
      "  6.5.6GeneralBack-Propaga tion\n",
      "           Theback-propagationalgorithmisverysimple.Tocomputethegradientofsome\n",
      "scalarz      withrespecttooneofitsancestors x      inthegraph,webeginbyobserving\n",
      "     thatthegradientwithrespecttoz  isgivenbyd z\n",
      "d z     =1.Wecanthencompute\n",
      "       thegradientwithrespecttoeachparentofz     inthegraphbymultiplyingthe\n",
      "         currentgradientbytheJacobianoftheoperationthatproducedz  .Wecontinue\n",
      "           multiplyingbyJacobians,travelingbackwardthroughthegraphinthiswayuntil\n",
      " wereach x           .Foranynodethatmaybereachedbygoingbackwardfromzthrough\n",
      "             twoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat\n",
      " thatnode.\n",
      "      Moreformally,eachnodeinthegraphG      correspondstoavariable.Toachieve\n",
      "         maximumgenerality,wedescribethisvariableasbeingatensor V  .Tensorsin\n",
      "           generalcanhaveanynumberofdimensions.Theysubsumescalars,vectors,and\n",
      "matrices.\n",
      "           Weassumethateachvariableisassociatedwiththefollowingsubroutines: V\n",
      " • g e t o p e r a t i o n _ ( V      ):Thisreturnstheoperationthatcomputes V ,repre-\n",
      "     sentedbytheedgescominginto V     inthecomputationalgraph.Forexample,\n",
      "           theremaybeaPythonorC++classrepresentingthematrixmultiplication\n",
      "  operation,andtheget_operation       function.Supposewehaveavariablethat\n",
      "    iscreatedbymatrixmultiplication, C= A B .Then g e t o p e r a t i o n _ ( V)\n",
      "          returnsapointertoaninstanceofthecorrespondingC++class.\n",
      " • g e t c o n s u m e r s _ ( V,G          ):Thisreturnsthelistofvariablesthatarechildrenof\n",
      "     Vinthecomputationalgraph.G\n",
      "  • G g e t i n p u t s_ ( V,)          :Thisreturnsthelistofvariablesthatareparentsof V\n",
      "    inthecomputationalgraph.G\n",
      " Eachoperationop    isalsoassociatedwithabprop  operation.Thisbprop\n",
      "          operationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\n",
      "           Thisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\n",
      "          Eachoperationisresponsibleforknowinghowtoback-propagatethroughthe\n",
      "              edgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\n",
      "2 1 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "     multiplicationoperationtocreateavariable C= A B    .Supposethatthegradient\n",
      "  ofascalarz  withrespectto C  isgivenby G    .Thematrixmultiplicationoperation\n",
      "            isresponsiblefordeﬁningtwoback-propagationrules,oneforeachofitsinput\n",
      "    arguments.Ifwecallthebprop        methodtorequestthegradientwithrespectto\n",
      "A       giventhatthegradientontheoutputis G  ,thenthe b p r o p   methodofthe\n",
      "          matrixmultiplicationoperationmuststatethatthegradientwithrespectto A\n",
      "  isgivenby G B     .Likewise,ifwecallthe b p r o p     methodtorequestthegradient\n",
      "  withrespectto B         ,thenthematrixoperationisresponsibleforimplementingthe\n",
      "b p r o p          methodandspecifyingthatthedesiredgradientisgivenby AG .The\n",
      "           back-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiationrules.It\n",
      "     onlyneedstocalleachoperation’sbprop     ruleswiththerightarguments.Formally,\n",
      "    o p b p r o p i n p u t s . (,, X G)mustreturn\n",
      "\n",
      "i( ∇ X o p f i n p u t s.( ) i ) G i , (6.54)\n",
      "             whichisjustanimplementationofthechainruleasexpressedinequation.6.47\n",
      "Here, i n p u t s          isalistofinputsthataresuppliedtotheoperation,op.f isthe\n",
      "     mathematicalfunctionthattheoperationimplements, X    istheinputwhosegradient\n",
      "              wewishtocompute,andisthegradientontheoutputoftheoperation. G\n",
      "Theop.bprop          methodshouldalwayspretendthatallitsinputsaredistinct\n",
      "           fromeachother,eveniftheyarenot.Forexample,ifthemul   operatorispassed\n",
      "  twocopiesofx tocomputex2 ,theop.bprop    methodshouldstillreturnx asthe\n",
      "          derivativewithrespecttobothinputs.Theback-propagationalgorithmwilllater\n",
      "        addbothoftheseargumentstogethertoobtain2x     ,whichisthecorrecttotal\n",
      "  derivativeon.x\n",
      "        Softwareimplementationsofback-propagationusuallyprovideboththeopera-\n",
      "  tionsandtheirbprop          methods,sothatusersofdeeplearningsoftwarelibrariesare\n",
      "          abletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\n",
      "          multiplication,exponents,logarithms,andsoon.Softwareengineerswhobuilda\n",
      "           newimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\n",
      "         ownoperationtoanexistinglibrarymustusuallyderivetheop.bprop  methodfor\n",
      "   anynewoperationsmanually.\n",
      "        Theback-propagationalgorithmisformallydescribedinalgorithm.6.5\n",
      "           Insection,weexplainedthatback-propagationwasdevelopedinorderto 6.5.2\n",
      "            avoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\n",
      "         algorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\n",
      "           Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits\n",
      "           computationalcost.Ifweassumethateachoperationevaluationhasroughlythe\n",
      "2 1 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      " Algorithm6.5       Theoutermostskeletonoftheback-propagationalgorithm.This\n",
      "            portiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\n",
      "      inthe subroutineofalgorithm b u i l d _ g r a d 6.6.\n",
      "           Require: T,thetargetsetofvariableswhosegradientsmustbecomputed.\n",
      "    Require:G,thecomputationalgraph\n",
      "      Require:z,thevariabletobediﬀerentiated\n",
      "LetGbeG         prunedtocontainonlynodesthatareancestorsofz anddescendents\n",
      "   ofnodesin. T\n",
      "         Initialize ,adatastructureassociatingtensorstotheirgradients g r a d _ t a b l e\n",
      " g r a d t a b l e_ [] 1z←\n",
      "    fordo Vin T\n",
      "  b u i l d g r a d_ ( V,,GG , g r a d t a b l e_ )\n",
      " endfor\n",
      "    Return restrictedto g r a d _ t a b l e T\n",
      " Algorithm6.6   Theinnerloopsubroutine b u i l d g r a d_ (  V,,GG , g r a d t a b l e_  )of\n",
      "        theback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned\n",
      "  inalgorithm.6.5\n",
      "            Require: V,thevariablewhosegradientshouldbeaddedtoandG g r a d _ t a b l e\n",
      "     Require:G,thegraphtomodify\n",
      " Require:G           ,therestrictionoftonodesthatparticipateinthegradient G\n",
      "         Require: g r a d _ t a b l e,adatastructuremappingnodestotheirgradients\n",
      "     if then Visin g r a d _ t a b l e\n",
      " Return_ g r a d t a b l e[] V\n",
      " endif\n",
      "  i←1\n",
      "    for C V in_ g e t c o n s u m e r s(,G )do\n",
      "  o p g e t o p e r a t i o n ←_ () C\n",
      "    D C← b u i l d g r a d_ (,,GG , g r a d t a b l e_ )\n",
      "G( ) i  ← G o p b p r o p g e t i n p u t s . (_ ( C,  ) ),, V D\n",
      "    ii←+1\n",
      " endfor\n",
      " G←\n",
      "i G( ) i\n",
      "g r a d t a b l e_ [] = V G\n",
      "        Insertandtheoperationscreatingitinto G G\n",
      " Return G\n",
      "2 1 3    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             samecost,thenwemayanalyzethecomputationalcostintermsofthenumber\n",
      "             ofoperationsexecuted. Keepinmindherethatwerefertoanoperationasthe\n",
      "          fundamentalunitofourcomputationalgraph,whichmightactuallyconsistof\n",
      "            severalarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\n",
      "           multiplicationasasingleoperation).Computingagradientinagraphwithnnodes\n",
      "    willneverexecutemorethanO(n2        )operationsorstoretheoutputofmorethan\n",
      "O(n2          ) operations.Herewearecountingoperationsinthecomputationalgraph,not\n",
      "           individualoperationsexecutedbytheunderlyinghardware,soitisimportantto\n",
      "            rememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\n",
      "           multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\n",
      "              asingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresat\n",
      "mostO(n2         ) operationsbecausetheforwardpropagationstagewillatworstexecute\n",
      "alln            nodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,\n",
      "           wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\n",
      "        addsoneJacobian-vectorproduct,whichshouldbeexpressedwithO   (1)nodes,per\n",
      "            edgeintheoriginalgraph.Becausethecomputationalgraphisadirectedacyclic\n",
      "    graphithasatmostO(n2          )edges.Forthekindsofgraphsthatarecommonlyused\n",
      "            inpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n",
      "     roughlychain-structured,causingback-propagationtohaveO(n    )cost.Thisisfar\n",
      "           betterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany\n",
      "           nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\n",
      "      therecursivechainrule(equation)nonrecursively: 6.53\n",
      "∂u( )n\n",
      "∂u( )j=\n",
      " p a th (u( π 1 ),u( π 2 ),...,u( π t )),\n",
      " f r o mπ 1  = tojπ t =nt \n",
      "k =2∂u(π k )\n",
      "∂u(πk − 1 ) . (6.55)\n",
      "      Sincethenumberofpathsfromnodej tonoden    cangrowexponentiallyinthe\n",
      "               lengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\n",
      "            ofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation\n",
      "           graph.Thislargecostwouldbeincurredbecausethesamecomputationfor\n",
      "∂u( ) i\n",
      "∂u( ) j          wouldberedonemanytimes. Toavoidsuchrecomputation,wecanthink\n",
      "          ofback-propagationasatable-ﬁllingalgorithmthattakesadvantageofstoring\n",
      " intermediateresults∂u( ) n\n",
      "∂u( ) i           .Eachnodeinthegraphhasacorrespondingslotina\n",
      "               tabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,\n",
      "       back-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling\n",
      "     strategyissometimescalled . dynamicprogramming\n",
      "2 1 4    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "     6.5.7Example:Back-Propaga tionforMLPTraining\n",
      "             Asanexample,wewalkthroughtheback-propagationalgorithmasitisusedto\n",
      "   trainamultilayerperceptron.\n",
      "           Herewedevelopaverysimplemultilayerperceptronwithasinglehidden\n",
      "           layer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\n",
      "             Theback-propagationalgorithmisusedtocomputethegradientofthecostona\n",
      "           singleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining\n",
      "     setformattedasadesignmatrixX      andavectorofassociatedclasslabelsy.\n",
      "       ThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW(1 )} .To\n",
      "              simplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\n",
      "   graphlanguageincludesarelu    operationthatcancomputemax{0 ,Z}element-\n",
      "           wise.Thepredictionsoftheunnormalizedlogprobabilitiesoverclassesarethen\n",
      " givenbyHW(2 )        .Weassumethatourgraphlanguageincludesacross_entropy\n",
      "       operationthatcomputesthecross-entropybetweenthetargetsy  andtheprobability\n",
      "         distributiondeﬁnedbytheseunnormalizedlogprobabilities.Theresultingcross-\n",
      "   entropydeﬁnesthecostJ M L E     .Minimizingthiscross-entropyperformsmaximum\n",
      "           likelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,\n",
      "        wealsoincludearegularizationterm.Thetotalcost\n",
      " JJ= M L E +λ\n",
      "\n",
      "i , j\n",
      "W(1 )\n",
      "i , j2\n",
      "+\n",
      "i , j\n",
      "W(2 )\n",
      "i , j2\n",
      " (6.56)\n",
      "          consistsofthecross-entropyandaweightdecaytermwithcoeﬃcientλ .The\n",
      "      computationalgraphisillustratedinﬁgure.6.11\n",
      "            Thecomputationalgraphforthegradientofthisexampleislargeenoughthat\n",
      "              itwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts\n",
      "          oftheback-propagationalgorithm,whichisthatitcanautomaticallygenerate\n",
      "           gradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\n",
      " derivemanually.\n",
      "          Wecanroughlytraceoutthebehavioroftheback-propagationalgorithm\n",
      "             bylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11\n",
      "  tocomputeboth∇W( 1 )Jand∇W( 2 )J      .Therearetwodiﬀerentpathsleading\n",
      " backwardfromJ         totheweights:onethroughthecross-entropycost,andone\n",
      "             throughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\n",
      "  alwayscontribute2λW( ) i    tothegradientonW( ) i.\n",
      "          Theotherpaththroughthecross-entropycostisslightlymorecomplicated.\n",
      "LetG       bethegradientontheunnormalizedlogprobabilitiesU(2 ) providedby\n",
      "thecross_entropy       operation.Theback-propagationalgorithmnowneedsto\n",
      "2 1 5    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "XXW( 1 )W( 1 )U( 1 )U( 1 )\n",
      "m a t m u lHH\n",
      "r e l u\n",
      "U( 3 )U( 3 )\n",
      "s q ru( 4 )u( 4 )\n",
      "s u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\n",
      "m a t m u ly yJ M L E J M L E\n",
      "c r o s s _ e n t r o p y\n",
      "U( 5 )U( 5 )\n",
      "s q ru( 6 )u( 6 )\n",
      "s u mu( 8 )u( 8 )J J\n",
      "+\n",
      "×\n",
      "+\n",
      "               Figure6.11:Thecomputationalgraphusedtocomputethecosttotrainourexampleofa\n",
      "        single-layerMLPusingthecross-entropylossandweightdecay.\n",
      "         exploretwodiﬀerentbranches.Ontheshorterbranch,itaddsHG tothe\n",
      " gradientonW(2 )         ,usingtheback-propagationruleforthesecondargumentto\n",
      "          thematrixmultiplicationoperation.Theotherbranchcorrespondstothelonger\n",
      "         chaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\n",
      "computes ∇ H J=GW(2 )        usingtheback-propagationrulefortheﬁrstargument\n",
      "      tothematrixmultiplicationoperation.Next,the r e l u    operationusesitsback-\n",
      "           propagationruletozerooutcomponentsofthegradientcorrespondingtoentries\n",
      "ofU(1 )         thatarelessthan.Lettheresultbecalled 0 G     .Thelaststepofthe\n",
      "          back-propagationalgorithmistousetheback-propagationruleforthesecond\n",
      "       argumentofthe operationtoadd m a t m u l XG    tothegradientonW(1 ).\n",
      "          Afterthesegradientshavebeencomputed,thegradientdescentalgorithm,or\n",
      "         anotheroptimizationalgorithm,usesthesegradientstoupdatetheparameters.\n",
      "            FortheMLP,thecomputationalcostisdominatedbythecostofmatrix\n",
      "          multiplication.Duringtheforwardpropagationstage,wemultiplybyeachweight\n",
      "  matrix,resultingin O( w  ) multiply-adds,where w     isthenumberofweights.During\n",
      "           thebackwardpropagationstage,wemultiplybythetransposeofeachweight\n",
      "            matrix,whichhasthesamecomputationalcost.Themainmemory costofthe\n",
      "               algorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\n",
      "2 1 6    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "              Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\n",
      "         returnedtothesamepoint.Thememory costisthus O ( m n h ),where m isthe\n",
      "       numberofexamplesintheminibatchand n h     isthenumberofhiddenunits.\n",
      " 6.5.8Complications\n",
      "           Ourdescriptionoftheback-propagationalgorithmhereissimplerthantheimple-\n",
      "    mentationsactuallyusedinpractice.\n",
      "             Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea\n",
      "          functionthatreturnsasingletensor.Mostsoftwareimplementationsneedto\n",
      "             supportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\n",
      "                tocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis\n",
      "               besttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto\n",
      "         implementthisprocedureasasingleoperationwithtwooutputs.\n",
      "     We havenotdescribed how tocontrol thememory consump tionof back-\n",
      "        propagation. Back-propagationofteninvolvessummationofmanytensorstogether.\n",
      "            Inthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\n",
      "               allofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\n",
      "            highmemory bottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand\n",
      "         addingeachvaluetothatbuﬀerasitiscomputed.\n",
      "        Real-worldimplementationsofback-propagationalsoneedtohandlevarious\n",
      "            datatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.\n",
      "            Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\n",
      "           Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthese\n",
      "           casesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.\n",
      "       Variousothertechnicalitiesmakereal-worlddiﬀerentiationmorecomplicated.\n",
      "           Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\n",
      "            intellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\n",
      "    thatmanymoresubtletiesexist.\n",
      "      6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity\n",
      "    The deeplearning communityhas been somewhat i solatedfrom thebroader\n",
      "          computersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n",
      "         concerninghowtoperformdiﬀerentiation.Moregenerally,theﬁeldof a ut o m a t i c\n",
      "di ﬀ e r e n t i a t i o n       isconcernedwithhowtocomputederivativesalgorithmically.\n",
      "          Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\n",
      "            diﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalled r e v e r s e\n",
      "2 1 7    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      " modeaccumulation        .Otherapproachesevaluatethesubexpressionsofthechain\n",
      "          ruleindiﬀerentorders.Ingeneral, determiningtheorderofevaluationthat\n",
      "            resultsinthelowestcomputationalcostisadiﬃcultproblem.Findingtheoptimal\n",
      "          sequenceofoperationstocomputethegradientisNP-complete( ,), Naumann2008\n",
      "            inthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\n",
      " expensiveform.\n",
      "     Forexample,supposewehavevariablesp 1 ,p 2     ,...,p n  representingprobabilities,\n",
      " andvariablesz 1 ,z 2     ,...,z n     representingunnormalizedlogprobabilities.Suppose\n",
      " wedeﬁne\n",
      "q i=exp(z i)\n",
      "iexp(z i) , (6.57)\n",
      "           wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\n",
      " operations, and construct a cross-entropy lossJ= −\n",
      "ip i logq i .A human\n",
      "      mathematiciancanobservethatthederivativeofJ  withrespecttoz i  takesavery\n",
      " simpleform:q i −p i        .Theback-propagationalgorithmisnotcapableofsimplifying\n",
      "           thegradientthiswayandwillinsteadexplicitlypropagategradientsthroughall\n",
      "          thelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware\n",
      "              librariessuchasTheano( ,; ,)areableto Bergstraetal.2010Bastienetal.2012\n",
      "           performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\n",
      "    bythepureback-propagationalgorithm.\n",
      "   Whentheforwardgraph G        hasasingleoutputnodeandeachpartialderivative\n",
      "∂ u( ) i\n",
      "∂ u( ) j         canbecomputedwithaconstantamountofcomputation,back-propagation\n",
      "           guaranteesthatthenumberofcomputationsforthegradientcomputationisof\n",
      "            thesameorderasthenumberofcomputationsfortheforwardcomputation:this\n",
      "          canbeseeninalgorithm,becauseeachlocalpartialderivative 6.2∂ u( ) i\n",
      "∂ u( ) j needsto\n",
      "           becomputedonlyoncealongwithanassociatedmultiplicationandadditionfor\n",
      "         therecursivechain-ruleformulation(equation).Theoverallcomputationis 6.53\n",
      "thereforeO          (#edges).Itcanpotentiallybereduced,however,bysimplifyingthe\n",
      "         computationalgraphconstructedbyback-propagation, andthisisanNP-complete\n",
      "         task. ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon\n",
      "          matchingknownsimpliﬁcationpatternstoiterativelyattempttosimplifythegraph.\n",
      "          Wedeﬁnedback-propagationonlyforcomputingascalaroutputgradient,but\n",
      "         back-propagationcanbeextendedtocomputeaJacobian(eitherofkdiﬀerent\n",
      "          scalarnodesinthegraph,orofatensor-valuednodecontainingk  values).A\n",
      "    naiveimplementationmaythenneedk     timesmorecomputation:foreachscalar\n",
      "          internalnodeintheoriginalforwardgraph,thenaiveimplementationcomputesk\n",
      "              gradientsinsteadofasinglegradient.Whenthenumberofoutputsofthegraphis\n",
      "              largerthanthenumberofinputs,itissometimespreferabletouseanotherformof\n",
      "  automaticdiﬀerentiationcalled   forwardmodeaccumulation  .Forwardmode\n",
      "2 1 8    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "         accumulationhasbeenproposedforobtainingreal-timecomputationofgradients\n",
      "           inrecurrentnetworks,forexample( ,).Thisapproachalso WilliamsandZipser1989\n",
      "              avoidstheneedtostorethevaluesandgradientsforthewholegraph,tradingoﬀ\n",
      "         computationaleﬃciencyformemory.Therelationshipbetweenforwardmodeand\n",
      "         backwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\n",
      "      right-multiplyingasequenceofmatrices,suchas\n",
      " A B C D, (6.58)\n",
      "           wherethematricescanbethoughtofasJacobian.Forexample,if D  isacolumn\n",
      " vectorwhile A            hasmanyrows,thegraphwillhaveasingleoutputandmanyinputs,\n",
      "           andstartingthemultiplicationsfromtheendandgoingbackwardrequiresonly\n",
      "         matrix-vectorproducts.Thisordercorrespondstothebackwardmode.Instead,\n",
      "            startingtomultiplyfromtheleftwouldinvolveaseriesofmatrix-matrixproducts,\n",
      "        whichmakesthewholecomputationmuchmoreexpensive.If A  hasfewerrows\n",
      "than D          hascolumns,however,itischeapertorunthemultiplicationsleft-to-right,\n",
      "    correspondingtotheforwardmode.\n",
      "           Inmanycommunitiesoutsidemachinelearning,itismorecommontoimplement\n",
      "        diﬀerentiationsoftwarethatactsdirectlyontraditionalprogramminglanguage\n",
      "        code, such asPythonorCcode, andautomaticallygeneratesprogramsthat\n",
      "          diﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcommunity,\n",
      "         computationalgraphsareusuallyrepresentedbyexplicitdatastructurescreated\n",
      "          byspecializedlibraries.Thespecializedapproachhasthedrawbackofrequiring\n",
      "     thelibrarydevelopertodeﬁnethebprop      methodsforeveryoperationandlimiting\n",
      "              theuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.Yetthe\n",
      "         specializedapproachalsohasthebeneﬁtofallowingcustomizedback-propagation\n",
      "            rulestobedevelopedforeachoperation,enablingthedevelopertoimprovespeed\n",
      "           orstabilityinnonobviouswaysthatanautomaticprocedurewouldpresumablybe\n",
      "  unabletoreplicate.\n",
      "            Back-propagationisthereforenottheonlywayortheoptimalwayofcomputing\n",
      "              thegradient,butitisapracticalmethodthatcontinuestoservethedeeplearning\n",
      "          communitywell.Inthefuture,diﬀerentiationtechnologyfordeepnetworksmay\n",
      "           improveasdeeplearningpractitionersbecomemoreawareofadvancesinthe\n",
      "    broaderﬁeldofautomaticdiﬀerentiation.\n",
      "  6.5.10Higher-OrderDerivatives\n",
      "          Somesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\n",
      "          deeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\n",
      "2 1 9    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "             Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\n",
      "           derivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This\n",
      "          meansthatthesymbolicdiﬀerentiationmachinerycanbeappliedtoderivatives.\n",
      "              Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivative\n",
      "             ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\n",
      "     matrix.Ifwehaveafunctionf: Rn → R       ,thentheHessianmatrixisofsize nn×.\n",
      "    Intypicaldeeplearningapplications,n       willbethenumberofparametersinthe\n",
      "            model,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\n",
      "    thusinfeasibletoevenrepresent.\n",
      "          InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\n",
      "  istouse Krylovmethods         .Krylovmethodsareasetofiterativetechniquesfor\n",
      "          performingvariousoperations,suchasapproximatelyinvertingamatrixorﬁnding\n",
      "         approximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\n",
      "   otherthanmatrix-vectorproducts.\n",
      "           TouseKrylovmethodson theHessian, weonlyneedto beabletocom-\n",
      "      putetheproductbetweentheHessianmatrixH   andanarbitraryvectorv .A\n",
      "         straightforwardtechnique( ,)fordoingsoistocompute Christianson1992\n",
      " Hv= ∇ x\n",
      "(∇ xfx())v\n",
      " . (6.59)\n",
      "          Bothgradientcomputationsinthisexpressionmaybecomputedautomaticallyby\n",
      "           theappropriatesoftwarelibrary.Notethattheoutergradientexpressiontakesthe\n",
      "        gradientofafunctionoftheinnergradientexpression.\n",
      "Ifv            isitselfavectorproducedbyacomputationalgraph,itisimportantto\n",
      "         specifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough\n",
      "    thegraphthatproduced.v\n",
      "             WhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\n",
      "     Hessianvectorproducts.OnesimplycomputesHe( ) i foralli= 1     ,...,n,where\n",
      "e( ) i     istheone-hotvectorwithe( ) i\n",
      "i         = 1andallotherentriesareequalto0.\n",
      "  6.6HistoricalNotes\n",
      "         Feedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators\n",
      "            basedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\n",
      "            Fromthispointofview,themodernfeedforwardnetworkistheculminationof\n",
      "        centuriesofprogressonthegeneralfunctionapproximationtask.\n",
      "          Thechainrulethatunderliestheback-propagationalgorithmwasinventedin\n",
      "        theseventeenthcentury(,; ,). Calculusandalgebra Leibniz1676L’Hôpital1696\n",
      "2 2 0    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "            havelongbeenusedtosolveoptimizationproblemsinclosedform,butgradient\n",
      "           descentwasnotintroducedasatechniqueforiterativelyapproximatingthesolution\n",
      "        tooptimizationproblemsuntilthenineteenthcentury(Cauchy1847,).\n",
      "          Beginninginthe1940s,thesefunctionapproximationtechniqueswereusedto\n",
      "          motivatemachinelearningmodelssuchastheperceptron.However,theearliest\n",
      "           modelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\n",
      "               severaloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\n",
      "            XORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\n",
      "         Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\n",
      "            ceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient\n",
      "           applicationsofthechainrulebasedondynamicprogrammingbegantoappear\n",
      "            inthe1960sand1970s,mostlyforcontrolapplications(,; Kelley1960Brysonand\n",
      "            Denham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,; ,; ,; ,)butalsofor\n",
      "        sensitivityanalysis( ,). Linnainmaa1976Werbos1981()proposedapplyingthese\n",
      "          techniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped\n",
      "          inpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,; LeCun1985\n",
      "       Parker1985Rumelhart1986a ,; etal.,).Thebook   ParallelDistributedPro-\n",
      "cessing           presentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith\n",
      "          back-propagationinachapter( ,)thatcontributedgreatly Rumelhartetal.1986b\n",
      "            tothepopularizationofback-propagationandinitiatedaveryactiveperiodofre-\n",
      "             searchinmultilayerneuralnetworks.Theideasputforwardbytheauthorsofthat\n",
      "         book,particularlybyRumelhartandHinton,gomuchbeyondback-propagation.\n",
      "         Theyincludecrucialideasaboutthepossiblecomputationalimplementationof\n",
      "           severalcentralaspectsofcognitionandlearning,whichcameunderthename\n",
      "           “connectionism”becauseoftheimportancethisschoolofthoughtplacesonthe\n",
      "           connectionsbetweenneuronsasthelocusoflearningandmemory.Inparticular,\n",
      "           theseideasincludethenotionofdistributedrepresentation(Hinton 1986etal.,).\n",
      "         Followingthesuccessofback-propagation, neuralnetworkresearchgainedpop-\n",
      "            ularityandreachedapeakintheearly1990s.Afterward s,othermachinelearning\n",
      "          techniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\n",
      "  beganin2006.\n",
      "          Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\n",
      "         stantiallysincethe1980s. Thesameback-propagationalgorithmandthesame\n",
      "             approachestogradientdescentarestillinuse.Mostoftheimprovementinneural\n",
      "            networkperformancefrom1986to2015canbeattributedtotwofactors.First,\n",
      "           largerdatasetshavereducedthedegreetowhichstatisticalgeneralizationisa\n",
      "          challengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\n",
      "          becauseofmorepowerfulcomputersandbettersoftwareinfrastructure.Asmall\n",
      "2 2 1    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "          numberofalgorithmicchangeshavealsoimprovedtheperformanceofneural\n",
      " networksnoticeably.\n",
      "           Oneofthesealgorithmicchangeswasthereplacementofmeansquarederror\n",
      "            withthecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\n",
      "            the1980sand1990sbutwasgraduallyreplacedbycross-entropylossesandthe\n",
      "          principleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\n",
      "          andthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\n",
      "          improvedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\n",
      "           hadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean\n",
      "  squarederrorloss.\n",
      "          Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\n",
      "           offeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\n",
      "          linearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 ,z}\n",
      "              functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleastasfar\n",
      "           asthecognitronandneocognitron(Fukushima19751980,,).Theseearlymodelsdid\n",
      "           notuserectiﬁedlinearunitsbutinsteadappliedrectiﬁcationtononlinearfunctions.\n",
      "           Despitetheearlypopularityofrectiﬁcation,itwaslargelyreplacedbysigmoids\n",
      "           inthe1980s,perhapsbecausesigmoidsperformbetterwhenneuralnetworksare\n",
      "            verysmall. Asoftheearly2000s,rectiﬁedlinearunitswereavoidedbecauseof\n",
      "        asomewhatsuperstitiousbeliefthatactivationfunctionswithnondiﬀerentiable\n",
      "              pointsmustbeavoided.Thisbegantochangeinabout2009. () Jarrettetal.2009\n",
      "           observedthat“usingarectifyingnonlinearityisthesinglemostimportantfactor\n",
      "          inimprovingtheperformanceofarecognitionsystem,”amongseveraldiﬀerent\n",
      "     factorsofneuralnetworkarchitecturedesign.\n",
      "           Forsmalldatasets, ()observedthatusingrectifyingnon- Jarrettetal.2009\n",
      "            linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\n",
      "          Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed\n",
      "              linearnetwork,enablingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent\n",
      "    featurevectorstoclassidentities.\n",
      "           Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\n",
      "           toexceedtheperformanceofrandomlychosenparameters. () Glorotetal.2011a\n",
      "             showedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep\n",
      "          networksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\n",
      "           Rectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\n",
      "        neurosciencehascontinuedtohave aninﬂuence onthedevelopment ofdeep\n",
      "          learningalgorithms. ()motivatedrectiﬁedlinearunitsfrom Glorotetal.2011a\n",
      "        biologicalconsiderations.Thehalf-rectifyingnonlinearitywasintendedtocapture\n",
      "          thesepropertiesofbiologicalneurons:(1)Forsomeinputs,biologicalneurons\n",
      "2 2 2    C HAP T E R 6 . D E E P F E E D F O R W ARD NE T WO R K S\n",
      "          arecompletelyinactive.(2)Forsomeinputs, abiologicalneuron’soutputis\n",
      "             proportionaltoitsinput.(3)Mostofthetime,biologicalneuronsoperateinthe\n",
      "          regimewheretheyareinactive(i.e.,theyshouldhavesparseactivations).\n",
      "          Whenthemodernresurgenceofdeeplearningbeganin2006,feedforward\n",
      "            networkscontinuedtohaveabadreputation. Fromabout2006to2012,itwas\n",
      "           widelybelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywere\n",
      "           assistedbyothermodels,suchasprobabilisticmodels. Today,itisnowknown\n",
      "         thatwiththerightresourcesandengineeringpractices,feedforwardnetworks\n",
      "          performverywell.Today,gradient-basedlearninginfeedforwardnetworksisused\n",
      "           asatooltodevelopprobabilisticmodels,suchasthevariationalautoencoder\n",
      "          andgenerativeadversarialnetworks,describedinchapter.Ratherthanbeing 20\n",
      "           viewedasanunreliabletechnologythatmustbesupportedbyothertechniques,\n",
      "           gradient-basedlearninginfeedforwardnetworkshasbeenviewedsince2012asa\n",
      "            powerfultechnologythatcanbeappliedtomanyothermachinelearningtasks.In\n",
      "         2006,thecommunityusedunsupervisedlearningtosupportsupervisedlearning,\n",
      "            andnow,ironically,itismorecommontousesupervisedlearningtosupport\n",
      " unsupervisedlearning.\n",
      "          Feedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we\n",
      "             expecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\n",
      "         algorithmsandmodeldesignwillimprovetheirperformanceevenfurther. This\n",
      "           chapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\n",
      "            subsequentchapters,weturntohowtousethesemodels—howtoregularizeand\n",
      " trainthem.\n",
      "2 2 3 C h a p t e r 7\n",
      "   Re g u l ar i zat i on for D e e p Learning\n",
      "             Acentralprobleminmachinelearningishowtomakeanalgorithmthatwill\n",
      "              performwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\n",
      "            usedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\n",
      "           attheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\n",
      "            asregularization.Agreatmanyformsofregularizationareavailabletothedeep\n",
      "        learningpractitioner.Infact,developingmoreeﬀectiveregularizationstrategies\n",
      "          hasbeenoneofthemajorresearcheﬀortsintheﬁeld.\n",
      "         Chapterintroducedthebasicconceptsofgeneralization,underﬁtting,overﬁt- 5\n",
      "            ting,bias,varianceandregularization.Ifyouarenotalreadyfamiliarwiththese\n",
      "          notions,pleaserefertothatchapterbeforecontinuingwiththisone.\n",
      "           Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-\n",
      "             izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\n",
      "   toformdeepmodels.\n",
      "           Somesectionsofthischapterdealwithstandardconceptsinmachinelearning.\n",
      "            Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant\n",
      "            sections.However,mostofthischapterisconcernedwiththeextensionofthese\n",
      "        basicconceptstotheparticularcaseofneuralnetworks.\n",
      "            Insection,wedeﬁnedregularizationas“anymodiﬁcationwemaketoa 5.2.2\n",
      "            learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\n",
      "          trainingerror.”Therearemanyregularizationstrategies.Someputextracon-\n",
      "           straintsonamachinelearningmodel,suchasaddingrestrictionsontheparameter\n",
      "              values.Someaddextratermsintheobjectivefunctionthatcanbethoughtofas\n",
      "           correspondingtoasoftconstraintontheparametervalues.Ifchosencarefully,\n",
      "           theseextraconstraintsandpenaltiescanleadtoimprovedperformanceonthe\n",
      "224     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "           testset.Sometimestheseconstraintsandpenaltiesaredesignedtoencodespeciﬁc\n",
      "           kindsofpriorknowledge.Othertimes,theseconstraintsandpenaltiesaredesigned\n",
      "             toexpressagenericpreferenceforasimplermodelclassinordertopromote\n",
      "         generalization.Sometimespenaltiesandconstraintsarenecessarytomakean\n",
      "        underdeterminedproblemdetermined.Otherformsofregularization,knownas\n",
      "         ensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.\n",
      "           Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon\n",
      "         regularizingestimators.Regularizationofanestimatorworksbytradingincreased\n",
      "            biasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable\n",
      "           trade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe\n",
      "           discussedgeneralizationandoverﬁttinginchapter,wefocusedonthreesituations, 5\n",
      "           wherethemodelfamilybeingtrainedeither(1)excludedthetruedata-generating\n",
      "          process—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue\n",
      "          data-generatingprocess,or(3)includedthegeneratingprocessbutalsomany\n",
      "        otherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather\n",
      "             thanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\n",
      "        modelfromthethirdregimeintothesecondregime.\n",
      "           Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\n",
      "           targetfunctionorthetruedata-generatingprocess,orevenacloseapproximation\n",
      "            ofeither.Wealmostneverhaveaccesstothetruedata-generatingprocessso\n",
      "             wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\n",
      "          generatingprocessornot.Mostapplicationsofdeeplearningalgorithms,however,\n",
      "           aretodomainswherethetruedata-generatingprocessisalmostcertainlyoutside\n",
      "          themodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\n",
      "            complicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\n",
      "         generationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\n",
      "             extent,wearealwaystryingtoﬁtasquarepeg(thedata-generatingprocess)into\n",
      "     aroundhole(ourmodelfamily).\n",
      "             Whatthismeansisthatcontrollingthecomplexityofthemodelisnota\n",
      "              simplematterofﬁndingthemodeloftherightsize,withtherightnumberof\n",
      "          parameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,\n",
      "             wealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing\n",
      "          generalizationerror)isalargemodelthathasbeenregularizedappropriately.\n",
      "             Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deepregularized\n",
      "model.\n",
      "2 2 5     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "   7.1ParameterNormPenalties\n",
      "             Regularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\n",
      "          modelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\n",
      "   andeﬀectiveregularizationstrategies.\n",
      "          Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,\n",
      "            suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\n",
      "   rameternormpenaltyΩ(θ    )totheobjectivefunctionJ    .Wedenotetheregularized\n",
      "  objectivefunctionby˜J:\n",
      "˜       J,J,α, (;θXy) = (;θXy)+Ω()θ (7.1)\n",
      "where α ∈[0 , ∞          )isahyperparameterthatweightstherelativecontributionofthe\n",
      "         normpenaltyterm,,relativetothestandardobjectivefunction Ω J .Settingα to0\n",
      "      resultsinnoregularization.Largervaluesofα    correspondtomoreregularization.\n",
      "        Whenourtrainingalgorithmminimizestheregularizedobjectivefunction˜Jit\n",
      "     willdecreaseboththeoriginalobjectiveJ      onthetrainingdataandsomemeasure\n",
      "     ofthesizeoftheparametersθ      (orsomesubsetoftheparameters).Diﬀerent\n",
      "            choicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred. Ω\n",
      "              Inthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\n",
      "   onthemodelparameters.\n",
      "          Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenote\n",
      "            thatforneuralnetworks,wetypicallychoosetouseaparameternormpenalty\n",
      "             Ωthatpenalizes oftheaﬃnetransformationateachlayerand onlytheweights\n",
      "           leavesthebiasesunregularized.Thebiasestypicallyrequirelessdatathanthe\n",
      "           weightstoﬁtaccurately.Eachweightspeciﬁeshowtwovariablesinteract.Fitting\n",
      "            theweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\n",
      "              biascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\n",
      "          variancebyleavingthebiasesunregularized.Also,regularizingthebiasparameters\n",
      "          canintroduceasigniﬁcantamountofunderﬁtting. Wethereforeusethevector\n",
      "w             toindicatealltheweightsthatshouldbeaﬀected byanormpenalty,while\n",
      " thevectorθ      denotesalltheparameters,includingbothw  andtheunregularized\n",
      "parameters.\n",
      "             Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\n",
      "   penaltywithadiﬀerentα          coeﬃcientforeachlayerofthenetwork.Becauseitcan\n",
      "             beexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\n",
      "               reasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\n",
      " searchspace.\n",
      "2 2 6     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      " 7.1.1 L2 ParameterRegularization\n",
      "              Wehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\n",
      "    ofparameternormpenalty:theL2     parameternormpenaltycommonlyknownas\n",
      " weightdecay          .Thisregularizationstrategydrivestheweightsclosertotheorigin1\n",
      "     byaddingaregularizationtermΩ(θ) =1\n",
      "2w2\n",
      "2     totheobjectivefunction.Inother\n",
      " academiccommunities,L2    regularizationisalsoknownas ridgeregressionor\n",
      " Tikhonovregularization.\n",
      "           Wecangainsomeinsightintothebehaviorofweightdecayregularization\n",
      "           bystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\n",
      "      presentation,weassumenobiasparameter,soθ isjustw     .Suchamodelhasthe\n",
      "   followingtotalobjectivefunction:\n",
      "˜  J, (;wXy) =α\n",
      "2w     wwXy +(J;,), (7.2)\n",
      "    withthecorrespondingparametergradient\n",
      "∇ w˜    J,α (;wXy) = w+∇ w    J,. (;wXy) (7.3)\n",
      "             Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\n",
      "      www←−α( +∇ w     J,. (;wXy)) (7.4)\n",
      "     Writtenanotherway,theupdateis\n",
      "     w w←−(1α)−∇ w    J,. (;wXy) (7.5)\n",
      "              Wecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning\n",
      "             ruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\n",
      "           justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\n",
      "           asinglestep.Butwhathappensovertheentirecourseoftraining?\n",
      "          Wewillfurthersimplifytheanalysisbymakingaquadraticapproximation\n",
      "             totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\n",
      "    obtainsminimalunregularizedtrainingcost,w∗=  argminwJ(w   ).Iftheobjective\n",
      "              functionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith\n",
      "1              Moregenerally,wecouldregularizetheparameterstobenearanyspeciﬁcpointinspace\n",
      "               and,surprisingly,stillgetaregularizationeﬀect,butbetterresultswillbeobtainedforavalue\n",
      "                   closertothetrueone,withzerobeingadefaultvaluethatmakessensewhenwedonotknowif\n",
      "                thecorrectvalueshouldbepositiveornegative.Sinceitisfarmorecommontoregularizethe\n",
      "             modelparameterstowardzero,wewillfocusonthisspecialcaseinourexposition.\n",
      "227     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "         meansquarederror,thentheapproximationisperfect.TheapproximationˆJis\n",
      " givenby\n",
      "ˆJJ () = θ (w∗ )+1\n",
      "2 (ww−∗)  Hww (−∗ ), (7.6)\n",
      "whereH    istheHessianmatrixofJ  withrespecttow  evaluatedatw∗  .Thereis\n",
      "       noﬁrst-orderterminthisquadraticapproximation,becausew∗    isdeﬁnedtobea\n",
      "      minimum,wherethegradientvanishes.Likewise,becausew∗    isthelocationofa\n",
      "          minimumof,wecanconcludethatispositivesemideﬁnite. J H\n",
      "  Theminimumofˆ    Joccurswhereitsgradient\n",
      "∇ wˆ  J() = (wHww−∗ ) (7.7)\n",
      "   isequalto.0\n",
      "             Tostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7\n",
      "            weightdecaygradient.Wecannowsolvefortheminimumoftheregularized\n",
      " versionofˆ    J.Weusethevariable˜       wtorepresentthelocationoftheminimum.\n",
      "α˜  wH+(˜ ww−∗ ) = 0 (7.8)\n",
      "  (+ )HαI˜ wHw = ∗(7.9)\n",
      "˜   wHI = (+α)− 1Hw∗(7.10)\n",
      "Asα     approaches0,theregularizedsolution˜wapproachesw∗  .Butwhat\n",
      " happensasα  grows?BecauseH       isrealandsymmetric,wecandecomposeit\n",
      "   intoadiagonalmatrixΛ     andanorthonormalbasisofeigenvectors,Q  ,suchthat\n",
      " HQQ = Λ        .Applyingthedecompositiontoequation,weobtain 7.10\n",
      "˜ wQQ = (Λ + )αI− 1QQΛw∗(7.11)\n",
      "=\n",
      "  QIQ (+Λα)− 1\n",
      "QQΛw∗(7.12)\n",
      "  = (+ )QΛαI− 1ΛQw∗ . (7.13)\n",
      "          Weseethattheeﬀectofweightdecayistorescalew∗    alongtheaxesdeﬁnedby\n",
      "  theeigenvectorsofH    .Speciﬁcally,thecomponentofw∗    thatisalignedwiththe\n",
      "i  -theigenvectorofH     isrescaledbyafactorofλ i\n",
      "λ i + α     .(Youmaywishtoreview\n",
      "          howthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3\n",
      "      AlongthedirectionswheretheeigenvaluesofH    arerelativelylarge,forexample,\n",
      "whereλ i α          ,theeﬀectofregularizationisrelativelysmall.Yetcomponentswith\n",
      "λ i α            willbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustratedin\n",
      " ﬁgure.7.1\n",
      "2 2 8     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "w 1w 2w∗\n",
      "˜ w\n",
      "       Figure7.1:Anillustrationoftheeﬀectof L2      (orweightdecay)regularizationonthevalue\n",
      "  oftheoptimal w           .Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\n",
      "          objective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2 regularizer.At\n",
      " thepoint˜ w           ,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the\n",
      "    eigenvalueoftheHessianof J       issmall. Theobjectivefunctiondoesnotincreasemuch\n",
      "    whenmovinghorizontallyawayfrom w∗       .Becausetheobjectivefunctiondoesnotexpress\n",
      "              astrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.\n",
      "  Theregularizerpulls w 1         closetozero.Intheseconddimension,theobjectivefunction\n",
      "      isverysensitivetomovementsawayfrom w∗     .Thecorrespondingeigenvalueislarge,\n",
      "           indicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w 2relatively\n",
      "little.\n",
      "2 2 9     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "         Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing\n",
      "          theobjectivefunctionarepreservedrelativelyintact. Indirectionsthatdonot\n",
      "           contributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\n",
      "            tellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.\n",
      "         Componentsoftheweightvectorcorrespondingtosuchunimportantdirections\n",
      "          aredecayedawaythroughtheuseoftheregularizationthroughouttraining.\n",
      "              Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization\n",
      "            ofanabstract,generalquadraticcostfunction.Howdotheseeﬀectsrelateto\n",
      "            machinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a\n",
      "             modelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\n",
      "              samekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\n",
      "               beabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\n",
      "             phrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n",
      "    thesumofsquarederrors:\n",
      "  ( )Xwy−   ( )Xwy−. (7.14)\n",
      "   WhenweaddL2     regularization,theobjectivefunctionchangesto\n",
      "  ( )Xwy−   ( )+Xwy−1\n",
      "2αw w. (7.15)\n",
      "        Thischangesthenormalequationsforthesolutionfrom\n",
      " wX= (X)− 1X y (7.16)\n",
      "to\n",
      " wX= (  XI+α)− 1X y. (7.17)\n",
      " ThematrixXX        inequationisproportionaltothecovariancematrix 7.161\n",
      "mXX.\n",
      "UsingL2    regularizationreplacesthismatrixwith\n",
      "X  XI+α− 1  inequation.7.17\n",
      "              Thenewmatrixisthesameastheoriginalone,butwiththeadditionofα tothe\n",
      "            diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\n",
      "     inputfeature.WecanseethatL2    regularizationcausesthelearningalgorithm\n",
      "   to“perceive”theinputX        ashavinghighervariance,whichmakesitshrinkthe\n",
      "            weightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\n",
      "  thisaddedvariance.\n",
      " 7.1.2 L1Regularization\n",
      "WhileL2            weightdecayisthemostcommonformofweightdecay,thereareother\n",
      "            waystopenalizethesizeofthemodelparameters. AnotheroptionistouseL1\n",
      "regularization.\n",
      "2 3 0     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      " Formally,L1        regularizationonthemodelparameterisdeﬁnedasw\n",
      "Ω() = θ||||w 1=\n",
      "i|w i |, (7.18)\n",
      "           thatis,asthesumofabsolutevaluesoftheindividualparameters.2 Wewill\n",
      "    nowdiscusstheeﬀectofL1      regularizationonthesimplelinearregressionmodel,\n",
      "          withnobiasparameter,thatwestudiedinouranalysisofL2 regularization.In\n",
      "        particular,weareinterestedindelineatingthediﬀerencesbetweenL1andL2forms\n",
      "   ofregularization.AswithL2 weightdecay,L1    weightdecaycontrolsthestrength\n",
      "           oftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α.\n",
      "    Thus,theregularizedobjectivefunction˜     J, (;wXy)isgivenby\n",
      "˜  J,α (;wXy) = ||||w 1    +(; )JwXy,, (7.19)\n",
      "      withthecorrespondinggradient(actually,subgradient)\n",
      "∇ w˜    J,α (;wXy) = sign( )+w∇ w   J,, (Xyw;) (7.20)\n",
      "         where issimplythesignofappliedelement-wise. sign( )w w\n",
      "           Byinspectingequation,wecanseeimmediatelythattheeﬀectof 7.20 L1\n",
      "      regularizationisquitediﬀerentfromthatofL2   regularization.Speciﬁcally,wecan\n",
      "           seethattheregularizationcontributiontothegradientnolongerscaleslinearly\n",
      " witheachw i           ;insteaditisaconstantfactorwithasignequaltosign(w i ).One\n",
      "              consequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\n",
      "     algebraicsolutionstoquadraticapproximationsofJ( Xy,;w    )aswedidforL2\n",
      "regularization.\n",
      "            Oursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent\n",
      "             viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\n",
      "           seriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\n",
      "     inthissettingisgivenby\n",
      "∇ wˆ  J() = (wHww−∗ ), (7.21)\n",
      "               where,again,istheHessianmatrixofwithrespecttoevaluatedat H J w w∗.\n",
      " BecausetheL1         penaltydoesnotadmitcleanalgebraicexpressionsinthecase\n",
      "            ofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\n",
      "    thattheHessianisdiagonal,H=diag([H 1 1 ,     ,...,H n , n  ]),whereeachH i , i>0.\n",
      "2 Aswith L2           regularization,wecouldregularizetheparameterstowardavaluethatisnot\n",
      "      zero,butinsteadtowardsomeparametervaluew( ) o    .Inthatcasethe L1 regularizationwould\n",
      "     introducethetermΩ() = θ||−ww( ) o|| 1=\n",
      "i| w i − w( ) o\n",
      "i|.\n",
      "231     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "            Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen\n",
      "           preprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\n",
      "  accomplishedusingPCA.\n",
      "    OurquadraticapproximationoftheL1   regularizedobjectivefunctiondecom-\n",
      "      posesintoasumovertheparameters:\n",
      "ˆ  J,J (;wXy) = (w∗   ; )+Xy,\n",
      "i1\n",
      "2H i , i(w i −w∗\n",
      "i)2 +αw| i|\n",
      " .(7.22)\n",
      "           Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n",
      "       (foreachdimension),withthefollowingform: i\n",
      "w i= sign(w∗\n",
      "i )max\n",
      "|w∗\n",
      "i |−α\n",
      "H i , i ,0\n",
      " . (7.23)\n",
      "   Considerthesituationwherew∗\n",
      "i>  0foralli     .Therearetwopossibleoutcomes:\n",
      "1.  Thecasewherew∗\n",
      "i≤α\n",
      "H i , i     .Heretheoptimalvalueofw i  undertheregularized\n",
      "  objectiveissimplyw i      = 0.ThisoccursbecausethecontributionofJ(w; Xy,)\n",
      "   totheregularizedobjective˜J(w; Xy,   )isoverwhelmed—indirectioni—by\n",
      " theL1      regularization,whichpushesthevalueofw i tozero.\n",
      "2.  Thecasewherew∗\n",
      "i>α\n",
      "H i , i         .Inthiscase,theregularizationdoesnotmovethe\n",
      "  optimalvalueofw i           tozerobutinsteadjustshiftsitinthatdirectionbya\n",
      "  distanceequaltoα\n",
      "H i , i.\n",
      "    Asimilarprocesshappenswhenw∗\n",
      "i<   0,butwiththeL1 penaltymakingw iless\n",
      " negativebyα\n",
      "H i , i  ,or0.\n",
      "  IncomparisontoL2regularization,L1     regularizationresultsinasolutionthat\n",
      " ismoresparse           .Sparsityinthiscontextreferstothefactthatsomeparameters\n",
      "        haveanoptimalvalueofzero.ThesparsityofL1   regularizationisaqualitatively\n",
      "    diﬀerentbehaviorthanariseswithL2    regularization.Equationgavethe 7.13\n",
      "solution˜wforL2        regularization.Ifwerevisitthatequationusingtheassumption\n",
      "      ofadiagonalandpositivedeﬁniteHessianH      thatweintroducedforouranalysisof\n",
      "L1   regularization,weﬁndthat˜w i=H i , i\n",
      "H i , i + αw∗\n",
      "i .Ifw∗\n",
      "i  wasnonzero,then˜w iremains\n",
      "   nonzero.ThisdemonstratesthatL2     regularizationdoesnotcausetheparameters\n",
      "    tobecomesparse,whileL1       regularizationmaydosoforlargeenough.α\n",
      "    ThesparsitypropertyinducedbyL1    regularizationhasbeenusedextensively\n",
      " asa  featureselection       mechanism.Featureselectionsimpliﬁesamachinelearning\n",
      "            problembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\n",
      "2 3 2     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "         particular,thewellknownLASSO( ,)(leastabsoluteshrinkage Tibshirani1995\n",
      "     andselectionoperator)modelintegratesanL1     penaltywithalinearmodeland\n",
      "   aleast-squarescostfunction. TheL1       penaltycausesasubsetoftheweightsto\n",
      "          becomezero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\n",
      "           Insection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\n",
      "       asMAPBayesianinference,andthatinparticular,L2  regularizationisequivalent\n",
      "           toMAPBayesianinferencewithaGaussianpriorontheweights.ForL1regu-\n",
      "  larization,thepenaltyαΩ(w )=α\n",
      "i|w i|      usedtoregularizeacostfunctionis\n",
      "           equivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\n",
      "             whenthepriorisanisotropicLaplacedistribution(equation)over 3.26w∈ Rn:\n",
      " log() =pw\n",
      "i logLaplace(w i ;0,1\n",
      "α) = −||||αw 1        +log log2 nαn−.(7.24)\n",
      "           Fromthepointofviewoflearningviamaximizationwithrespecttow  ,wecan\n",
      "              ignorethe termsbecausetheydonotdependon. log log2α− w\n",
      "     7.2NormPenaltiesasConstrainedOptimization\n",
      "         Considerthecostfunctionregularizedbyaparameternormpenalty:\n",
      "˜       J,J,α. (;θXy) = (;θXy)+Ω()θ (7.25)\n",
      "            Recallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\n",
      "          byconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective\n",
      "             functionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,\n",
      "        calledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting\n",
      "          whethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ    )tobelessthan\n",
      "         someconstant,wecouldconstructageneralizedLagrangefunction k\n",
      "          L − (; ) = (; )+(Ω() θ,αXy,JθXy,αθk.) (7.26)\n",
      "        Thesolutiontotheconstrainedproblemisgivenby\n",
      "θ∗ = argmin\n",
      "θmax\n",
      "α , α ≥ 0  L()θ,α. (7.27)\n",
      "          Asdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ\n",
      "andα           .Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\n",
      "         constraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,\n",
      "2 3 3     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             whileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall\n",
      "proceduresα   mustincreasewheneverΩ(θ) >k   anddecreasewheneverΩ(θ) <k.\n",
      " Allpositiveα  encourageΩ(θ     )toshrink.Theoptimalvalueα∗  willencourageΩ(θ)\n",
      "            toshrink,butnotsostronglytomakebecomelessthan. Ω()θ k\n",
      "            Togainsomeinsightintotheeﬀectoftheconstraint,wecanﬁxα∗ andview\n",
      "       theproblemasjustafunctionof:θ\n",
      "θ∗ = argmin\n",
      "θ L(θ,α∗ ) = argmin\n",
      "θ    J,α (;θXy)+∗ Ω()θ.(7.28)\n",
      "           Thisisexactlythesameastheregularizedtrainingproblemofminimizing˜J.\n",
      "              Wecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\n",
      "    weights.Ifisthe ΩL2         norm,thentheweightsareconstrainedtolieinanL2\n",
      "   ball. Ifisthe ΩL1           norm,thentheweightsareconstrainedtolieinaregionof\n",
      "limitedL1             norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\n",
      "      imposebyusingweightdecaywithcoeﬃcientα∗   becausethevalueofα∗ doesnot\n",
      "     directlytellusthevalueofk      .Inprinciple,onecansolvefork   ,buttherelationship\n",
      "betweenkandα∗    dependsontheformofJ        .Whilewedonotknowtheexactsize\n",
      "            oftheconstraintregion,wecancontrolitroughlybyincreasingordecreasingα\n",
      "         inordertogroworshrinktheconstraintregion.Largerα    willresultinasmaller\n",
      "          constraintregion.Smallerwillresultinalargerconstraintregion. α\n",
      "           Sometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\n",
      "           describedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\n",
      "      descenttotakeastepdownhillonJ(θ   )andthenprojectθ   backtothenearest\n",
      "   pointthatsatisﬁesΩ(θ) <k            .Thiscanbeusefulifwehaveanideaofwhatvalue\n",
      "ofk             isappropriateanddonotwanttospendtimesearchingforthevalueofαthat\n",
      "   correspondstothis.k\n",
      "          Anotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing\n",
      "         constraintswithpenaltiesisthatpenaltiescancausenonconvexoptimization\n",
      "         procedurestogetstuckinlocalminimacorrespondingtosmallθ  .Whentraining\n",
      "           neuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\n",
      "              “deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe\n",
      "              functionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\n",
      "             allverysmall. Whentrainingwithapenaltyonthenormoftheweights,these\n",
      "            conﬁgurationscanbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce\n",
      "J         bymakingtheweightslarger.Explicitconstraintsimplementedbyreprojection\n",
      "             canworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\n",
      "          toapproachtheorigin.Explicitconstraintsimplementedbyreprojectionhavean\n",
      "            eﬀectonlywhentheweightsbecomelargeandattempttoleavetheconstraint\n",
      "region.\n",
      "2 3 4     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\n",
      "           somestabilityontheoptimizationprocedure.Whenusinghighlearningrates,it\n",
      "            ispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce\n",
      "             largegradients,whichtheninducealargeupdatetotheweights.Iftheseupdates\n",
      "       consistentlyincreasethesizeoftheweights,thenθ    rapidlymovesawayfrom\n",
      "         theoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection\n",
      "            preventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\n",
      "           withoutbound. ()recommend usingconstraintscombinedwitha Hintonetal.2012c\n",
      "           highlearningratetoenablerapidexplorationofparameterspacewhilemaintaining\n",
      " somestability.\n",
      "           Inparticular,Hinton 2012cetal.()recommend astrategyintroducedbySrebro\n",
      "            andShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n",
      "             ofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\n",
      "           weightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\n",
      "             hiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\n",
      "         penaltyinaLagrangefunction,itwouldbesimilarto L2    weightdecaybutwitha\n",
      "             separateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\n",
      "          multiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit\n",
      "           obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\n",
      "    anexplicitconstraintwithreprojection.\n",
      "    7.3RegularizationandUnder-ConstrainedProblems\n",
      "           Insomecases,regularizationisnecessaryformachinelearningproblemstobe\n",
      "          properlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearre-\n",
      "       gressionandPCA,dependoninvertingthematrixXX    .Thisisnotpossible\n",
      "whenXX         issingular.Thismatrixcanbesingularwheneverthedata-generating\n",
      "             distributiontrulyhasnovarianceinsomedirection,orwhennovarianceisobserved\n",
      "         insomedirectionbecausetherearefewerexamples(rowsofX   )thaninputfeatures\n",
      " (columnsofX          ).Inthiscase,manyformsofregularizationcorrespondtoinverting\n",
      "X           XI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.\n",
      "          Theselinearproblemshaveclosedformsolutionswhentherelevantmatrix\n",
      "               isinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\n",
      "          underdetermined.Anexampleislogisticregressionappliedtoaproblemwhere\n",
      "        theclassesarelinearlyseparable.Ifaweightvectorw    isabletoachieveperfect\n",
      "  classiﬁcation,then2w       willalsoachieveperfectclassiﬁcationandhigherlikelihood.\n",
      "         Aniterativeoptimizationprocedurelikestochasticgradientdescentwillcontinually\n",
      "   increasethemagnitudeofw         and,intheory,willneverhalt.Inpractice,anumerical\n",
      "2 3 5     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "         implementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights\n",
      "             tocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe\n",
      "          programmerhasdecidedtohandlevaluesthatarenotrealnumbers.\n",
      "           Mostformsofregularizationareabletoguaranteetheconvergenceofiterative\n",
      "        methodsappliedtounderdeterminedproblems. Forexample,weightdecaywill\n",
      "            causegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\n",
      "          slopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.\n",
      "         Theideaofusingregularizationtosolveunderdeterminedproblemsextends\n",
      "            beyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\n",
      "problems.\n",
      "            Aswesawinsection,wecansolveunderdeterminedlinearequationsusing 2.9\n",
      "         theMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse\n",
      "X+    ofamatrixisX\n",
      "X+ =lim\n",
      "α  0(X  XI+α)− 1X . (7.29)\n",
      "           Wecannowrecognizeequationasperforminglinearregressionwithweight 7.29\n",
      "            decay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17\n",
      "           coeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\n",
      "   underdeterminedproblemsusingregularization.\n",
      "  7.4DatasetAugmentation\n",
      "               Thebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\n",
      "               moredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\n",
      "                togetaroundthisproblemistocreatefakedataandaddittothetrainingset.\n",
      "           Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\n",
      " fakedata.\n",
      "            Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacomplicat-\n",
      "  ed,high-dimensionalinputx       andsummarizeitwithasinglecategoryidentityy.\n",
      "                Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety\n",
      "      oftransformations.Wecangeneratenew( x,y     )pairseasilyjustbytransforming\n",
      "      theinputsinourtrainingset. x\n",
      "             Thisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\n",
      "              isdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave\n",
      "     alreadysolvedthedensityestimationproblem.\n",
      "          Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc\n",
      "         classiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude\n",
      "2 3 6     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             anenormousrangeoffactorsofvariation,manyofwhichcanbeeasilysimulated.\n",
      "            Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\n",
      "            oftengreatlyimprovegeneralization,evenifthemodelhasalreadybeendesignedto\n",
      "          bepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\n",
      "            describedinchapter.Manyotheroperations,suchasrotatingtheimageor 9\n",
      "       scalingtheimage,havealsoprovedquiteeﬀective.\n",
      "            Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrect\n",
      "         class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\n",
      "             diﬀerencebetween“b”and“d”andthediﬀerencebetween“6”and“9,”sohorizontal\n",
      "  ﬂipsand 180◦         rotationsarenotappropriatewaysofaugmentingdatasetsforthese\n",
      "tasks.\n",
      "            Therearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant\n",
      "             tobutthatarenoteasytoperform.Forexample,out-of-planerotationcannotbe\n",
      "         implementedasasimplegeometri coperationontheinputpixels.\n",
      "          Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\n",
      "  andHinton2013,).\n",
      "            Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\n",
      "             canalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand\n",
      "              evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\n",
      "              randomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\n",
      "            tonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\n",
      "             ofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\n",
      "          inputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithms,\n",
      "           suchasthedenoisingautoencoder( ,).Noiseinjectionalso Vincentetal.2008\n",
      "               workswhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoing\n",
      "           datasetaugmentationatmultiplelevelsofabstraction. ()recently Pooleetal.2014\n",
      "            showedthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeof\n",
      "           thenoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwill\n",
      "              bedescribedinsection,canbeseenasaprocessofconstructingnewinputs 7.12\n",
      "   by bynoise. multiplying\n",
      "         Whencomparingmachinelearningbenchmarkresults,takingtheeﬀectof\n",
      "        datasetaugmentationintoaccountisimportant.Often,hand-designeddataset\n",
      "          augmentationschemescandramaticallyreducethegeneralizationerrorofama-\n",
      "          chinelearningtechnique.Tocomparetheperformanceofonemachinelearning\n",
      "          algorithmtoanother,itisnecessarytoperformcontrolledexperiments.When\n",
      "          comparingmachinelearningalgorithmAandmachinelearningalgorithmB,make\n",
      "          surethatbothalgorithmsareevaluatedusingthesamehand-designeddataset\n",
      "          augmentationschemes.SupposethatalgorithmAperformspoorlywithnodataset\n",
      "2 3 7     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          augmentation,andalgorithmBperformswellwhencombinedwithnumeroussyn-\n",
      "           thetictransformationsoftheinput.Insuchacasethesynthetictransformations\n",
      "           likelycausedtheimprovedperformance,ratherthantheuseofmachinelearning\n",
      "          algorithmB.Sometimesdecidingwhetheranexperimenthasbeenproperlycon-\n",
      "        trolledrequiressubjectivejudgment.Forexample,machinelearningalgorithms\n",
      "            thatinjectnoiseintotheinputareperformingaformofdatasetaugmentation.\n",
      "           Usually,operationsthataregenerallyapplicable(suchasaddingGaussiannoiseto\n",
      "           theinput)areconsideredpartofthemachinelearningalgorithm,whileoperations\n",
      "            thatarespeciﬁctooneapplicationdomain(suchasrandomlycroppinganimage)\n",
      "      areconsideredtobeseparatepreprocessingsteps.\n",
      "  7.5NoiseRobustness\n",
      "              Sectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\n",
      "          augmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal\n",
      "              varianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\n",
      "             normoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\n",
      "           rememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\n",
      "            theparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise\n",
      "              appliedtothehiddenunitsissuchanimportanttopicthatitmeritsitsown\n",
      "           separatediscussion;thedropoutalgorithmdescribedinsectionisthemain 7.12\n",
      "   developmentofthatapproach.\n",
      "            Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\n",
      "              isbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\n",
      "           contextofrecurrentneuralnetworks( ,; Jimetal.1996Graves2011,). Thiscan\n",
      "       beinterpretedasa stochasticimplementationof Bayesianinference overthe\n",
      "         weights. TheBayesiantreatmentoflearningwouldconsiderthemodelweights\n",
      "           tobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis\n",
      "            uncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect\n",
      " thisuncertainty.\n",
      "            Noiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\n",
      "          assumptions)toamoretraditionalformofregularization,encouragingstabilityof\n",
      "             thefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\n",
      " afunctionˆy( x      )thatmapsasetoffeatures x     toascalarusingtheleast-squares\n",
      "           costfunctionbetweenthemodelpredictionsˆy() xandthetruevalues:y\n",
      " J= E p x, y ( )  (ˆyy () x−)2\n",
      " . (7.30)\n",
      "        Thetrainingsetconsistsoflabeledexamples m {( x(1 ) ,y(1 )     ) (,..., x( ) m ,y( ) m)}.\n",
      "2 3 8     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "            Wenowassumethatwitheachinputpresentationwealsoincludearandom\n",
      "perturbation W ∼N( ; 0,η I        )ofthenetworkweights. Letusimaginethatwe\n",
      "  haveastandardl       -layerMLP.Wedenotetheperturbedmodelasˆy  W( x ).Despite\n",
      "              theinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe\n",
      "        outputofthenetwork.Theobjectivefunctionthusbecomes\n",
      "˜J W= E p , y , ( x  W )\n",
      "(ˆy  W  () ) x−y2\n",
      "(7.31)\n",
      "= E p , y , ( x  W )\n",
      "ˆy2\n",
      " W   ()2ˆ x−yy  W   ()+ xy2\n",
      " . (7.32)\n",
      " Forsmallη   ,theminimizationofJ     withaddedweightnoise(withcovariance\n",
      "η I  ) isequivalent tominimization ofJ   with anadditional regularizationter-\n",
      "m:η E p , y ( x )\n",
      "∇ Wˆy() x2\n",
      "       .Thisformofregularizationencouragestheparameters\n",
      "             togotoregionsofparameterspacewheresmallperturbationsoftheweightshave\n",
      "             arelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel\n",
      "            intoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\n",
      "           weights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby\n",
      "           ﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear\n",
      "   regression(where,forinstance,ˆy( x) = wx+b    ),thisregularizationtermcollapses\n",
      "intoη E p ( ) x x2\n",
      "           ,whichisnotafunctionofparametersandthereforedoesnot\n",
      "    contributetothegradientof˜J W     withrespecttothemodelparameters.\n",
      "      7.5.1InjectingNoiseattheOutputTargets\n",
      "        Mostdatasetshavesomenumberofmistakesinthey     labels.Itcanbeharmfulto\n",
      "maximize logp(  y| x )wheny          isamistake.Onewaytopreventthisistoexplicitly\n",
      "              modelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall\n",
      "constant    ,thetrainingsetlabely    iscorrectwithprobability1 −  ,andotherwise\n",
      "             anyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\n",
      "          incorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\n",
      "   noisesamples.Forexample, labelsmoothing      regularizesamodelbasedona\n",
      " softmaxwithk           outputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1\n",
      "  withtargetsof\n",
      "k − 1 and1 −      ,respectively.Thestandardcross-entropylossmay\n",
      "            thenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax\n",
      "          classiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever\n",
      "               predictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\n",
      "           andlargerweights,makingmoreextremepredictionsforever.Itispossibleto\n",
      "          preventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\n",
      "           smoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\n",
      "          discouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s\n",
      "2 3 9     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          andcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy\n",
      "  etal.,).2015\n",
      "  7.6Semi-SupervisedLearning\n",
      "         Intheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfromP(x)\n",
      "   andlabeledexamplesfromP( xy,    )areusedtoestimateP(  yx|  )orpredicty\n",
      " from.x\n",
      "         Inthecontextofdeeplearning, semi-superv isedlearningusuallyrefersto\n",
      "  learningarepresentation h=f( x).       Thegoalistolearnarepresentationso\n",
      "         thatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\n",
      "           learningcanprovideusefulcluesforhowtogroupexamplesinrepresentation\n",
      "            space.Examplesthatclustertightlyintheinputspaceshouldbemappedto\n",
      "           similarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter\n",
      "            generalizationinmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\n",
      "          long-standingvariantofthisapproachistheapplicationofprincipalcomponents\n",
      "            analysisasapreprocessingstepbeforeapplyingaclassiﬁer(ontheprojecteddata).\n",
      "         Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\n",
      "           model,onecanconstructmodelsinwhichagenerativemodelofeitherP(x )or\n",
      "P( xy,       )sharesparameterswithadiscriminativemodelofP(  yx|  ).Onecan\n",
      "     thentradeoﬀthesupervisedcriterion  −logP(  yx|    )withtheunsupervisedor\n",
      "   generativeone(suchas  −logP(x )or  −logP( xy,    )).Thegenerativecriterionthen\n",
      "            expressesaparticularformofpriorbeliefaboutthesolutiontothesupervised\n",
      "          learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P(x )is\n",
      "    connectedtothestructureofP(  yx|         )inawaythatiscapturedbytheshared\n",
      "          parametrization.Bycontrollinghowmuchofthegenerativecriterionisincluded\n",
      "              inthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative\n",
      "           orapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\n",
      " Bengio2008,).\n",
      "          SalakhutdinovandHinton2008()describeamethodforlearningthekernel\n",
      "             functionofakernelmachineusedforregression,inwhichtheusageofunlabeled\n",
      "           examplesformodelingimproves quitesigniﬁcantly. P()x P( )yx|\n",
      "          See ()formoreinformationaboutsemi-supervisedlearning. Chapelleetal.2006\n",
      "2 4 0     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "  7.7MultitaskLearning\n",
      "           Multitasklearning( ,)isawaytoimprovegeneralizationbypooling Caruana1993\n",
      "            theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\n",
      "           arisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamples\n",
      "            putmorepressureontheparametersofthemodeltowardvaluesthatgeneralize\n",
      "                well,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\n",
      "          constrainedtowardgoodvalues(assumingthesharingisjustiﬁed),oftenyielding\n",
      " bettergeneralization.\n",
      "           Figureillustratesaverycommonformofmultitasklearning,inwhich 7.2\n",
      "   diﬀerentsupervisedtasks(predicting y( ) igiven x    )sharethesameinput x  ,aswell\n",
      "   assomeintermediate-levelrepresentation h(sha r e d )     ,capturingacommonpoolof\n",
      "             factors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\n",
      "parameters:\n",
      "1.           Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask\n",
      "           toachievegoodgeneralization).Thesearetheupperlayersoftheneural\n",
      "   networkinﬁgure.7.2\n",
      "2.           Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe\n",
      "              pooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\n",
      "  inﬁgure.7.2\n",
      "         Improvedgeneralizationandgeneralizationerrorbounds(,)canbe Baxter1995\n",
      "           achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n",
      "           greatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\n",
      "           sharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\n",
      "          willhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\n",
      "            thediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome\n",
      "  ofthetasks.\n",
      "             Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\n",
      "         following:amongthefactors thatexplain thevariationsobservedinthedata\n",
      "            associatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.\n",
      "  7.8EarlyStopping\n",
      "         Whentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt\n",
      "            thetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\n",
      "2 4 1     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "h(1)h(1)h(2)h(2)h(3)h(3)y(1)y(1)y(2)y(2)\n",
      "h(shared)h(shared)\n",
      "xx\n",
      "             Figure7.2:Multitasklearningcanbecastinseveralwaysindeeplearningframeworks,\n",
      "              andthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut\n",
      "             involvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\n",
      "           issupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\n",
      "          canbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively\n",
      "     withtheweightsintoandfrom h(1)and h(2)         )canbelearnedontopofthoseyieldinga\n",
      " sharedrepresentation h(shared)         .Theunderlyingassumptionisthatthereexistsacommon\n",
      "         pooloffactorsthatexplainthevariationsintheinput x     ,whileeachtaskisassociated\n",
      "              withasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\n",
      " hiddenunits h(1)and h(2)      arespecializedtoeachtask(respectivelypredicting y(1)and\n",
      "y(2)    ),whilesomeintermediate-levelrepresentation h(shared)     issharedacrossalltasks.In\n",
      "              theunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe\n",
      "       associatedwithnoneoftheoutputtasks( h(3)        ):thesearethefactorsthatexplainsomeof\n",
      "         theinputvariationsbutarenotrelevantforpredicting y(1) or y(2).\n",
      "2 4 2     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "     0 50 100 150 200 250\n",
      " Time(epochs)000 .005 .010 .015 .020 .  Loss(negativelog-likelihood)\n",
      "  Trainingsetloss\n",
      "  Validationsetloss\n",
      "           Figure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\n",
      "          time(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s  ).Inthis\n",
      "            example,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\n",
      "            decreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto\n",
      "      increaseagain,forminganasymmetricU-shapedcurve.\n",
      "              validationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3\n",
      "   behavior,whichoccursreliably.\n",
      "             Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,\n",
      "              hopefullybettertestseterror)byreturningtotheparametersettingatthepointin\n",
      "              timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\n",
      "            improves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\n",
      "          terminates,wereturntheseparameters,ratherthanthelatestparameters.The\n",
      "          algorithmterminateswhennoparametershaveimprovedoverthebestrecorded\n",
      "          validationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis\n",
      "     speciﬁedmoreformallyinalgorithm.7.1\n",
      "    Thisstrategyisknownas earlystopping      .Itisprobablythemostcommonly\n",
      "             usedformofregularizationindeeplearning.Itspopularityisduetobothits\n",
      "   eﬀectivenessanditssimplicity.\n",
      "             Onewaytothinkofearlystoppingisasaveryeﬃcienthyperparameterselection\n",
      "            algorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\n",
      "             WecanseeinﬁgurethatthishyperparameterhasaU-shapedvalidationset 7.3\n",
      "          performancecurve.Mosthyperparametersthatcontrolmodelcapacityhavesucha\n",
      "             U-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3\n",
      "            earlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining\n",
      "              howmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe\n",
      "2 4 3     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "            chosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\n",
      "               atthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The\n",
      "           “trainingtime” hyperparameterisuniqueinthatbydeﬁnition,asinglerunof\n",
      "           trainingtriesoutmanyvaluesofthehyperparameter. Theonlysigniﬁcantcost\n",
      "          tochoosingthishyperparameterautomaticallyviaearlystoppingisrunningthe\n",
      "         validationsetevaluationperiodicallyduringtraining.Ideally, thisisdonein\n",
      "            paralleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\n",
      "             GPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\n",
      "              costoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis\n",
      "Algorithm 7.1    Theearlystopping meta-algori thmfor determiningthe best\n",
      "            amountoftimetotrain.Thismeta-algorithmisageneralstrategythatworks\n",
      "             wellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\n",
      " validationset.\n",
      "        Letbethenumberofstepsbetweenevaluations. n\n",
      "Letp           bethe“patience,”thenumberoftimestoobserveworseningvalidationset\n",
      "   errorbeforegivingup.\n",
      " Letθ o   betheinitialparameters.\n",
      "  θθ← o\n",
      "  i←0\n",
      "  j←0\n",
      "  v←∞\n",
      "θ∗ ←θ\n",
      "i∗ ←i\n",
      "    whiledoj<p\n",
      "         Updatebyrunningthetrainingalgorithmforsteps. θ n\n",
      "    iin←+\n",
      "v ←ValidationSetError()θ\n",
      " ifv  <vthen\n",
      "  j←0\n",
      "θ∗ ←θ\n",
      "i∗ ←i\n",
      "  vv←\n",
      "else\n",
      "    jj←+1\n",
      " endif\n",
      " endwhile\n",
      "   Bestparametersareθ∗       ,bestnumberoftrainingstepsisi∗.\n",
      "2 4 4     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\n",
      "          frequentlyandobtainingalower-resolutionestimateoftheoptimaltrainingtime.\n",
      "              Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\n",
      "            bestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\n",
      "             theseparametersinaslowerandlargerformofmemory (forexample,trainingin\n",
      "             GPUmemory,butstoringtheoptimalparametersinhostmemory oronadisk\n",
      "            drive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring\n",
      "            training,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.\n",
      "           Earlystoppingisanunobtrusiveformofregularization,inthatitrequires\n",
      "          almostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\n",
      "               orthesetofallowableparametervalues.Thismeansthatitiseasytouseearly\n",
      "           stoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\n",
      "               decay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\n",
      "           networkinabadlocalminimumcorrespondingtoasolutionwithpathologically\n",
      " smallweights.\n",
      "            Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\n",
      "          tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\n",
      "            functiontoencouragebettergeneralization,itisrareforthebestgeneralizationto\n",
      "        occuratalocalminimumofthetrainingobjective.\n",
      "            Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\n",
      "              fedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\n",
      "            aftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra\n",
      "             trainingstep,allthetrainingdataisincluded.Therearetwobasicstrategiesone\n",
      "      canuseforthissecondtrainingprocedure.\n",
      "             Onestrategy(algorithm)istoinitializethemodelagainandretrainonall 7.2\n",
      "               thedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\n",
      "            theearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare\n",
      "            somesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\n",
      "             wayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\n",
      "             thesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\n",
      "           eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\n",
      "   trainingsetisbigger.\n",
      "             Anotherstrategyforusingallthedataistokeeptheparametersobtainedfrom\n",
      "              theﬁrstroundoftrainingandthencontinuetraining,butnowusingallthedata.\n",
      "                  Atthisstage,wenownolongerhaveaguideforwhentostopintermsofanumber\n",
      "             ofsteps.Instead,wecanmonitortheaveragelossfunctiononthevalidationset\n",
      "              andcontinuetraininguntilitfallsbelowthevalueofthetrainingsetobjectiveat\n",
      "            whichtheearlystoppingprocedurehalted.Thisstrategyavoidsthehighcostof\n",
      "2 4 5     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      " Algorithm7.2         Ameta-algorithmforusingearlystoppingtodeterminehowlong\n",
      "       totrain,thenretrainingonallthedata.\n",
      " LetX( ) tr a i n andy( ) tr a i n   bethetrainingset.\n",
      "SplitX( ) tr a i nandy( ) tr a i n into(X( ) s u b tr a i n,X( v a l i d )  ) (andy( ) s u b tr a i n,y( v a l i d ))\n",
      "respectively.\n",
      "       Runearlystopping(algorithm)startingfromrandom 7.1 θusingX( ) s u b tr a i nand\n",
      "y( ) s u b tr a i n   fortrainingdataandX( v a l i d )andy( v a l i d )   forvalidationdata.This\n",
      " returnsi∗     ,theoptimalnumberofsteps.\n",
      "     Settorandomvaluesagain. θ\n",
      "  TrainonX( ) tr a i n andy( ) tr a i n fori∗steps.\n",
      " Algorithm7.3         Meta-algorithmusingearlystoppingtodetermineatwhatobjec-\n",
      "             tivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.\n",
      " LetX( ) tr a i n andy( ) tr a i n   bethetrainingset.\n",
      "SplitX( ) tr a i nandy( ) tr a i n into(X( ) s u b tr a i n,X( v a l i d )  ) (andy( ) s u b tr a i n,y( v a l i d ))\n",
      "respectively.\n",
      "       Runearlystopping(algorithm)startingfromrandom 7.1 θusingX( ) s u b tr a i nand\n",
      "y( ) s u b tr a i n   fortrainingdataandX( v a l i d )andy( v a l i d )   forvalidationdata.This\n",
      " updates.θ\n",
      "   J,←(θX( ) s u b tr a i n ,y( ) s u b tr a i n)\n",
      "  whileJ,(θX( v a l i d ) ,y( v a l i d )  ) >do\n",
      "  TrainonX( ) tr a i n andy( ) tr a i n  forsteps.n\n",
      " endwhile\n",
      "             retrainingthemodelfromscratchbutisnotaswellbehaved.Forexample,the\n",
      "              objectiveonthevalidationsetmaynoteverreachthetargetvalue,sothisstrategy\n",
      "            isnotevenguaranteedtoterminate.Thisprocedureispresentedmoreformallyin\n",
      " algorithm.7.3\n",
      "            Earlystoppingisalsousefulbecauseitreducesthecomputationalcostofthe\n",
      "            trainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber\n",
      "           oftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout\n",
      "             requiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\n",
      "     thegradientsofsuchadditionalterms.\n",
      "      Howearlystoppingactsasaregularizer:      Sofarwehavestatedthatearly\n",
      "            stoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\n",
      "            showinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n",
      "2 4 6     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "w 1w 2w∗\n",
      "˜ w\n",
      "w 1w 2w∗\n",
      "˜ w\n",
      "             Figure7.4:Anillustrationoftheeﬀectofearlystopping. ( L e f t )Thesolidcontourlines\n",
      "            indicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\n",
      "            takenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that\n",
      "             minimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.\n",
      "     ( R i g h t )AnillustrationoftheeﬀectofL2     regularizationforcomparison.Thedashedcircles\n",
      "    indicatethecontoursoftheL2          penalty,whichcausestheminimumofthetotalcosttolie\n",
      "         nearertheoriginthantheminimumoftheunregularizedcost.\n",
      "           istheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n",
      "             ()and ()arguedthatearlystoppinghastheeﬀectof 1995a SjöbergandLjung1995\n",
      "          restrictingtheoptimizationproceduretoarelativelysmallvolumeofparameter\n",
      "        spaceintheneighborhoodoftheinitialparametervalueθ o   ,asillustratedin\n",
      "     ﬁgure.Morespeciﬁcally,imaginetaking 7.4 τ   optimizationsteps(corresponding\n",
      "toτ      trainingiterations)andwithlearningrate     .Wecanviewtheproductτ\n",
      "           asameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting\n",
      "             boththenumberofiterationsandthelearningratelimitsthevolumeofparameter\n",
      "  spacereachablefromθ o   .Inthissense,τ        behavesasifitwerethereciprocalof\n",
      "     thecoeﬃcientusedforweightdecay.\n",
      "              Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\n",
      "         errorfunctionandsimplegradientdescent—earlystoppingisequivalenttoL2\n",
      "regularization.\n",
      "   TocomparewithclassicalL2      regularization,weexamineasimplesettingwhere\n",
      "      theonlyparametersarelinearweights(θ=w      ).Wecanmodelthecostfunction\n",
      "J          withaquadraticapproximationintheneighborhoodoftheempiricallyoptimal\n",
      "    valueoftheweightsw∗:\n",
      "ˆJJ () = θ (w∗ )+1\n",
      "2 (ww−∗)  Hww (−∗ ), (7.33)\n",
      "whereH    istheHessianmatrixofJ  withrespecttow  evaluatedatw∗  .Giventhe\n",
      " assumptionthatw∗   isaminimumofJ(w   ),weknowthatH  ispositivesemideﬁnite.\n",
      "2 4 7     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          UnderalocalTaylorseriesapproximation,thegradientisgivenby\n",
      "∇ wˆ  J() = (wHww−∗ ). (7.34)\n",
      "            Wearegoingtostudythetrajectoryfollowedbytheparametervectorduring\n",
      "            training.Forsimplicity,letussettheinitialparametervectortotheorigin,3\n",
      "w(0 )=0          .LetusstudytheapproximatebehaviorofgradientdescentonJby\n",
      "   analyzinggradientdescentonˆJ:\n",
      "w( ) τ= w( 1 ) τ − −∇ wˆJ(w( 1 ) τ − ) (7.35)\n",
      "= w( 1 ) τ − −Hw(( 1 ) τ − −w∗ ), (7.36)\n",
      "w( ) τ −w∗  = ( )(IH−w( 1 ) τ − −w∗ ). (7.37)\n",
      "            LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH ,exploiting\n",
      "  theeigendecompositionofH:H=QQΛ ,whereΛ    isadiagonalmatrixandQ\n",
      "     isanorthonormalbasisofeigenvectors.\n",
      "w( ) τ −w∗  = (IQQ−Λ)(w( 1 ) τ − −w∗ ) (7.38)\n",
      "Q(w( ) τ −w∗  ) = ( )I−ΛQ(w( 1 ) τ − −w∗ ) (7.39)\n",
      " Assumingthatw(0 )  = 0andthat       ischosentobesmallenoughtoguarantee\n",
      "|1 −λ i |<      1,theparametertrajectoryduringtrainingafterτ  parameterupdates\n",
      "  isasfollows:\n",
      "Qw( ) τ    = [( )I−I−Λτ]Qw∗ . (7.40)\n",
      "   Now,theexpressionforQ˜w   inequationfor7.13L2   regularizationcanberear-\n",
      " rangedas\n",
      "Q˜   wI = (+Λα)− 1ΛQw∗ , (7.41)\n",
      "Q˜     wII = [−(+Λα)− 1α]Qw∗ . (7.42)\n",
      "           Comparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ,\n",
      "      ατ,andarechosensuchthat\n",
      "  ( )I−Λτ  = (+ ) ΛαI− 1 α, (7.43)\n",
      "3            Forneuralnetworks,toobtainsymmetrybreakingbetweenhiddenunits,wecannotinitialize\n",
      "   alltheparametersto 0            ,asdiscussedinsection.However,theargumentholdsforanyother 6.2\n",
      "  initialvalue w ( 0 ).\n",
      "248     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "thenL2           regularizationandearlystoppingcanbeseenasequivalent(atleastunder\n",
      "         thequadraticapproximationoftheobjectivefunction). Goingevenfurther,by\n",
      "       takinglogarithmsandusingtheseriesexpansionforlog (1+x   ),wecanconclude\n",
      "   thatifallλ i    aresmall(thatis,λ i   1andλ i   /α1)then\n",
      " τ≈1\n",
      "α , (7.44)\n",
      " α≈1\n",
      "τ . (7.45)\n",
      "         Thatis,undertheseassumptions,thenumberoftrainingiterationsτ  playsarole\n",
      "   inverselyproportionaltotheL2     regularizationparameter,andtheinverseofτ\n",
      "       playstheroleoftheweightdecaycoeﬃcient.\n",
      "         Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe\n",
      "           objectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\n",
      "            inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\n",
      "           todirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters\n",
      "     correspondingtodirectionsoflesscurvature.\n",
      "           Thederivationsinthissectionhaveshownthatatrajectoryoflengthτends\n",
      "         atapointthatcorrespondstoaminimumoftheL2  -regularizedobjective.Early\n",
      "            stoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\n",
      "          instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\n",
      "             ordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\n",
      "           thereforehastheadvantageoverweightdecayinthatitautomaticallydetermines\n",
      "          thecorrectamountofregularizationwhileweightdecayrequiresmanytraining\n",
      "      experimentswithdiﬀerentvaluesofitshyperparameter.\n",
      "     7.9ParameterTyingandParameterSharing\n",
      "            Thusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\n",
      "               totheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.\n",
      " Forexample,L2       regularization(orweightdecay)penalizesmodelparametersfor\n",
      "            deviatingfromtheﬁxedvalueofzero.Sometimes,however,wemayneedother\n",
      "            waystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\n",
      "           Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake,\n",
      "            butweknow,fromknowledgeofthedomainandmodelarchitecture,thatthere\n",
      "       shouldbesomedependenciesbetweenthemodelparameters.\n",
      "             Acommontypeofdependencythatweoftenwanttoexpressisthatcertain\n",
      "      parameters shouldbe closeto oneanother.Consider thefollowingscenario:\n",
      "2 4 9     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "            wehavetwomodelsperformingthesameclassiﬁcationtask(withthesameset\n",
      "          ofclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehave\n",
      "modelA withparameters w( ) A andmodelB withparameters w( ) B  .Thetwo\n",
      "         modelsmaptheinputtotwodiﬀerentbutrelatedoutputs:ˆy( ) A=f( w( ) A , x )and\n",
      "ˆy( ) B= (g w( ) B , x).\n",
      "            Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\n",
      "           andoutputdistributions)thatwebelievethemodelparametersshouldbeclose\n",
      "  toeachother:∀i,w( ) A\n",
      "i    shouldbeclosetow( ) B\n",
      "i     .Wecanleveragethisinformation\n",
      "           throughregularization.Speciﬁcally,wecanuseaparameternormpenaltyofthe\n",
      " formΩ( w( ) A , w( ) B )= w( ) A − w( ) B2\n",
      "2    .HereweusedanL2  penalty,butother\n",
      "   choicesarealsopossible.\n",
      "            Thiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\n",
      "             theparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to\n",
      "            beclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n",
      "           (tocapturethedistributionoftheobservedinputdata).Thearchitectureswere\n",
      "            constructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe\n",
      "       pairedtocorrespondingparametersintheunsupervisedmodel.\n",
      "            Whileaparameternormpenaltyisonewaytoregularizeparameterstobe\n",
      "              closetooneanother,themorepopularwayistouseconstraints:toforcesets\n",
      "             ofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\n",
      " parametersharing         ,becauseweinterpretthevariousmodelsormodelcomponents\n",
      "            assharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing\n",
      "              overregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\n",
      "              subsetoftheparameters(theuniqueset)needstobestoredinmemory.Incertain\n",
      "         models—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant\n",
      "       reductioninthememory footprintofthemodel.\n",
      "   7.9.1ConvolutionalNeuralNetworks\n",
      "            Byfarthemostpopularandextensiveuseofparametersharingoccursinconvo-\n",
      "       lutionalneuralnetworks(CNNs)appliedtocomputervision.\n",
      "          Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\n",
      "                  Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\n",
      "            totheright.CNNstakethispropertyintoaccountbysharingparametersacross\n",
      "            multipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)\n",
      "              iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda\n",
      "           catwiththesamecatdetectorwhetherthecatappearsatcolumni orcolumn\n",
      "     i+1intheimage.\n",
      "2 5 0     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          ParametersharinghasenabledCNNstodramaticallylowerthenumberof\n",
      "   unique model parameters andto signiﬁcantly increasenetwork sizeswithout\n",
      "            requiringacorrespondingincreaseintrainingdata.Itremainsoneofthebest\n",
      "          examplesofhowtoeﬀectivelyincorporatedomainknowledgeintothenetwork\n",
      "architecture.\n",
      "        CNNsarediscussedinmoredetailinchapter.9\n",
      "  7.10SparseRepresentations\n",
      "            Weightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\n",
      "               strategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\n",
      "          encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\n",
      "    penaltyonthemodelparameters.\n",
      "       Wehavealreadydiscussed(insection)how7.1.2 L1 penalizationinduces\n",
      "         asparseparametrization—meaningthatmanyoftheparametersbecomezero\n",
      "      (orcloseto zero). Representationalsparsity, on theother hand, describesa\n",
      "            representationwheremanyoftheelementsoftherepresentationarezero(orclose\n",
      "              tozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof\n",
      " linearregression:\n",
      "\n",
      "18\n",
      "5\n",
      "15\n",
      "−9\n",
      "−3\n",
      "=\n",
      "     400 20 0−\n",
      "     00 10 3 0−\n",
      "     050 0 0 0\n",
      "     100 10 4−−\n",
      "     100 0 50−\n",
      "\n",
      "2\n",
      "3\n",
      "−2\n",
      "−5\n",
      "1\n",
      "4\n",
      "\n",
      "  y∈ Rm  A∈ Rm n ×  x∈ Rn(7.46)\n",
      "\n",
      "−14\n",
      "1\n",
      "19\n",
      "2\n",
      "23\n",
      "=\n",
      "     3 12 54 1 −−\n",
      "     4 2 3 11 3−−\n",
      "     − −− 15 4 2 3 2\n",
      "     3 1 2 30 3−−\n",
      "     −−−− 54 22 5 1\n",
      "\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "−3\n",
      "0\n",
      "\n",
      "  y∈ Rm  B∈ Rm n ×  h∈ Rn(7.47)\n",
      "            Intheﬁrstexpression,wehaveanexampleofasparselyparametrizedlinear\n",
      "            regressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\n",
      "2 5 1     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "tionh  ofthedatax  .Thatis,h   isafunctionofx    that,insomesense,represents\n",
      "           theinformationpresentin,butdoessowithasparsevector. x\n",
      "         Representationalregularizationisaccomplishedbythesamesortsofmechanisms\n",
      "      thatwehaveusedinparameterregularization.\n",
      "          Normpenaltyregularizationofrepresentationsisperformedbyaddingtothe\n",
      " lossfunctionJ         anormpenaltyontherepresentation.Thispenaltyisdenoted\n",
      "         Ω()h.Asbefore,wedenotetheregularizedlossfunctionby˜J:\n",
      "˜       J,J,α, (;θXy) = (;θXy)+Ω()h (7.48)\n",
      "where α∈[0 ,∞          )weightstherelativecontributionofthenormpenaltyterm,with\n",
      "       largervaluesofcorrespondingtomoreregularization. α\n",
      "  JustasanL1       penaltyontheparametersinducesparametersparsity,anL1\n",
      "         penaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\n",
      "Ω(h) =||||h 1=\n",
      "i|h i|  . Ofcourse,theL1      penaltyisonlyonechoiceofpenalty\n",
      "            thatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\n",
      " aStudentt         priorontherepresentation( ,; ,) OlshausenandField1996Bergstra2011\n",
      "          andKLdivergencepenalties( ,),whichareespecially LarochelleandBengio2008\n",
      "           usefulforrepresentationswithelementsconstrainedtolieontheunitinterval.\n",
      "             Lee2008 Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\n",
      "        basedonregularizingtheaverageactivationacrossseveralexamples,1\n",
      "m\n",
      "ih( ) i ,to\n",
      "             benearsometargetvalue,suchasavectorwith.01foreachentry.\n",
      "         Otherapproachesobtainrepresentationalsparsitywithahardconstrainton\n",
      "    theactivationvalues.Forexample,   orthogonalmatchingpursuit  (Patietal.,\n",
      "   1993)encodesaninputx  withtherepresentationh   thatsolvestheconstrained\n",
      " optimizationproblem\n",
      " argmin\n",
      "h h ,   0 <k   −xWh2 , (7.49)\n",
      "whereh 0      isthenumberofnonzeroentriesofh     .Thisproblemcanbesolved\n",
      " eﬃcientlywhenW         isconstrainedtobeorthogonal.Thismethodisoftencalled\n",
      "OMP-k    ,withthevalueofk        speciﬁedtoindicatethenumberofnonzerofeatures\n",
      "            allowed. ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1\n",
      "    featureextractorfordeeparchitectures.\n",
      "           Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\n",
      "            thisbook,weseemanyexamplesofsparsityregularizationusedinvariouscontexts.\n",
      "2 5 2     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "     7.11BaggingandOtherEnsembleMethods\n",
      "Bagging  (shortfor  bootstrapaggregating      )isatechniqueforreducinggeneral-\n",
      "            izationerrorbycombiningseveralmodels( ,).Theideaistotrain Breiman1994\n",
      "             severaldiﬀerentmodelsseparately,thenhaveallthemodelsvoteontheoutputfor\n",
      "             testexamples.Thisisanexampleofageneralstrategyinmachinelearningcalled\n",
      " modelaveraging       .Techniquesemployingthisstrategyareknownasensemble\n",
      "methods.\n",
      "           Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually\n",
      "         notmakeallthesameerrorsonthetestset.\n",
      "     Considerforexampleasetofk      regressionmodels.Supposethateachmodel\n",
      "  makesanerror i        oneachexample, withtheerrorsdrawnfromazero-mean\n",
      "    multivariatenormaldistributionwithvariances E[2\n",
      "i] =v andcovariances E[ i j] =\n",
      "c             . Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n",
      "1\n",
      "k\n",
      "i i         .Theexpectedsquarederroroftheensemblepredictoris\n",
      "E\n",
      "\n",
      "1\n",
      "k\n",
      "i i2\n",
      "=1\n",
      "k2E\n",
      "\n",
      "i\n",
      "2\n",
      "i+\n",
      "j i = i j\n",
      "\n",
      " , (7.50)\n",
      "=1\n",
      "k v+ k −1\n",
      "k c. (7.51)\n",
      "         Inthecasewheretheerrorsareperfectlycorrelatedandc=v   ,themeansquared\n",
      "  errorreducestov             ,sothemodelaveragingdoesnothelpatall.Inthecasewhere\n",
      "     theerrorsareperfectlyuncorrelatedandc      = 0,theexpectedsquarederrorofthe\n",
      "  ensembleisonly1\n",
      "kv          .Thismeansthattheexpectedsquarederroroftheensemble\n",
      "            isinverselyproportionaltotheensemblesize.Inotherwords,onaverage,the\n",
      "               ensemblewillperformatleastaswellasanyofitsmembers,andifthemembers\n",
      "          makeindependenterrors,theensemblewillperformsigniﬁcantlybetterthanits\n",
      "members.\n",
      "          Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.\n",
      "             Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n",
      "           diﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging\n",
      "             isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\n",
      "     functiontobereusedseveraltimes.\n",
      "   Speciﬁcally,bagginginvolvesconstructingk    diﬀerentdatasets.Eachdataset\n",
      "             hasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\n",
      "          constructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\n",
      "             that,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\n",
      "2 5 3     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "8\n",
      "8First ensemble member\n",
      "Second ensemble memberOriginal dataset\n",
      "First resampled dataset\n",
      "Second re sampled dataset\n",
      "               Figure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\n",
      "                thedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent\n",
      "            resampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets\n",
      "               bysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis\n",
      "                 dataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.Onthe\n",
      "                 seconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearnsthata\n",
      "               looponthebottomofthedigitcorrespondstoan8.Eachoftheseindividualclassiﬁcation\n",
      "              rulesisbrittle,butifweaveragetheiroutput,thenthedetectorisrobust,achieving\n",
      "          maximalconﬁdenceonlywhenbothloopsofthe8arepresent.\n",
      "      originaldatasetandcontainsseveralduplicateexamples4 .Model i  isthentrained\n",
      " ondataset i          .Thediﬀerencesbetweenwhichexamplesareincludedineachdataset\n",
      "            resultindiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample. 7.5\n",
      "            Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\n",
      "              oftenbeneﬁtfrommodelaveragingevenifallthemodelsaretrainedonthesame\n",
      "         dataset.Diﬀerencesinrandominitialization,inrandomselectionofminibatches,\n",
      "         inhyperparameters,orinoutcomesofnondeterministicimplementationsofneural\n",
      "            networksareoftenenoughtocausediﬀerentmembersoftheensembletomake\n",
      "  partiallyindependenterrors.\n",
      "          Modelaveragingisanextremelypowerfulandreliablemethodforreducing\n",
      "         generalizationerror.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\n",
      "          forscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-\n",
      "           tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\n",
      "4        Whenboththeoriginalandtheresampleddatasetcontain m    examples,theexactproportion\n",
      "        ofexamplesmissingfromthenewdatasetis (1 −1\n",
      "m)m       .Thisisthechancethataparticular\n",
      "     exampleisnotchosenamongthe m     possiblesourceexamplesforall m    drawsusedtocreatethe\n",
      " newdataset. As m      approachesinﬁnitythisquantityconvergesto1\n",
      "e     ,whichisslightlylargerthan\n",
      "1\n",
      "3.\n",
      "254     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "           Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\n",
      "          Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-\n",
      "            ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand\n",
      "  Prize(Koren2009,).\n",
      "           Notalltechniquesforconstructingensemblesaredesignedtomaketheensemble\n",
      "          moreregularizedthantheindividualmodels.Forexample,atechniquecalled\n",
      "b o o s t i ng         (FreundandSchapire1996ba,,)constructsanensemblewithhigher\n",
      "           capacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\n",
      "          ofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\n",
      "           networkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual\n",
      "           neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\n",
      "   unitstothenetwork.\n",
      " 7.12Dropout\n",
      "D r o p o ut         (Srivastava 2014etal.,)providesacomputationallyinexpensivebut\n",
      "            powerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,\n",
      "             dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\n",
      "         ofverymanylargeneuralnetworks. Bagginginvolvestrainingmultiplemodels\n",
      "          andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical\n",
      "            wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\n",
      "              networksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\n",
      "              ofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedyetal.2014a\n",
      "          butmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\n",
      "          approximationtotrainingandevaluatingabaggedensembleofexponentiallymany\n",
      " neuralnetworks.\n",
      "         Speciﬁcally,dropouttrainstheensembleconsistingofallsubnetworksthat\n",
      "            canbeformedbyremovingnonoutputunitsfromanunderlyingbasenetwork,as\n",
      "             illustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\n",
      "           aﬃnetransformationsandnonlinearities,wecaneﬀectivelyremoveaunitfroma\n",
      "          networkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressome\n",
      "           slightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake\n",
      "            thediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent\n",
      "             thedropoutalgorithmintermsofmultiplicationbyzeroforsimplicity,butitcan\n",
      "             betriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe\n",
      "network.\n",
      "       Recallthattolearnwithbagging,wedeﬁne k   diﬀerentmodels,construct k\n",
      "2 5 5     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "yy\n",
      "h 1 h 1 h 2 h 2\n",
      "x 1 x 1 x 2 x 2yy\n",
      "h 1 h 1 h 2 h 2\n",
      "x 1 x 1 x 2 x 2yy\n",
      "h 1 h 1 h 2 h 2\n",
      "x 2 x 2yy\n",
      "h 1 h 1 h 2 h 2\n",
      "x 1 x 1yy\n",
      "h 2 h 2\n",
      "x 1 x 1 x 2 x 2\n",
      "yy\n",
      "h 1 h 1\n",
      "x 1 x 1 x 2 x 2yy\n",
      "h 1 h 1 h 2 h 2yy\n",
      "x 1 x 1 x 2 x 2yy\n",
      "h 2 h 2\n",
      "x 2 x 2\n",
      "yy\n",
      "h 1 h 1\n",
      "x 1 x 1yy\n",
      "h 1 h 1\n",
      "x 2 x 2yy\n",
      "h 2 h 2\n",
      "x 1 x 1yy\n",
      "x 1 x 1\n",
      "yy\n",
      "x 2 x 2yy\n",
      "h 2 h 2yy\n",
      "h 1 h 1yyBase network\n",
      "Ensemble of subnetworks\n",
      "            Figure7.6: Dropouttrainsanensembleconsistingofallsubnetworksthatcanbecon-\n",
      "            structedbyremovingnonoutputunitsfromanunderlyingbasenetwork.Here,webegin\n",
      "              withabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\n",
      "              possiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\n",
      "              bydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\n",
      "              alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting\n",
      "            theinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider\n",
      "             layers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\n",
      "smaller.\n",
      "2 5 6     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "           diﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\n",
      " trainmodeli ondataseti         .Dropoutaimstoapproximatethisprocess,butwithan\n",
      "          exponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,\n",
      "           weuseaminibatch-basedlearningalgorithmthatmakessmallsteps,suchas\n",
      "            stochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\n",
      "             randomlysampleadiﬀerentbinarymasktoapplytoalltheinputandhidden\n",
      "             unitsinthenetwork.Themaskforeachunitissampledindependentlyfromall\n",
      "               theothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\n",
      "           included)isahyperparameterﬁxedbeforetrainingbegins. Itisnotafunction\n",
      "             ofthecurrentvalueofthemodelparametersortheinputexample.Typically,an\n",
      "             inputunitisincludedwithprobability0.8,andahiddenunitisincludedwith\n",
      "         probability0.5.Wethenrunforwardpropagation, back-propagation, andthe\n",
      "           learningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\n",
      " withdropout.\n",
      "      Moreformally,supposethatamaskvector µ     speciﬁeswhichunitstoinclude,\n",
      "andJ( θ µ,         )deﬁnesthecostofthemodeldeﬁnedbyparameters θ andmask µ.\n",
      "     Thendropouttrainingconsistsofminimizing E µJ( θ µ,   ).Theexpectationcontains\n",
      "            exponentiallymanyterms,butwecanobtainanunbiasedestimateofitsgradient\n",
      "    bysamplingvaluesof. µ\n",
      "             Dropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\n",
      "            bagging,themodelsareallindependent.Inthecaseofdropout,themodels\n",
      "          shareparameters,witheachmodelinheritingadiﬀerentsubsetofparameters\n",
      "           fromtheparentneuralnetwork.Thisparametersharingmakesitpossibleto\n",
      "           representanexponentialnumberofmodelswithatractableamountofmemory.\n",
      "             Inthecaseofbagging,eachmodelistrainedtoconvergenceonitsrespective\n",
      "            trainingset.Inthecaseofdropout,typicallymostmodelsarenotexplicitly\n",
      "             trainedatall—usually,themodelislargeenoughthatitwouldbeinfeasibleto\n",
      "            sampleallpossiblesubnetworkswithinthelifetimeoftheuniverse.Instead,atiny\n",
      "             fractionofthepossiblesubnetworksareeachtrainedforasinglestep,andthe\n",
      "           parametersharingcausestheremainingsubnetworkstoarriveatgoodsettingsof\n",
      "           theparameters.Thesearetheonlydiﬀerences.Beyondthese,dropoutfollowsthe\n",
      "          baggingalgorithm.Forexample,thetrainingsetencounteredbyeachsubnetwork\n",
      "           isindeedasubsetoftheoriginaltrainingsetsampledwithreplacement.\n",
      "           Tomakeaprediction,abaggedensemblemustaccumulatevotesfromall\n",
      "       itsmembers.Werefertothisprocessas i nf e r e nce    inthiscontext. Sofar,our\n",
      "            descriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\n",
      "            probabilistic.Now,weassumethatthemodel’sroleistooutputaprobability\n",
      "       distribution.Inthecaseofbagging,eachmodeli    producesaprobabilitydistribution\n",
      "2 5 7     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "ˆ x 1ˆ x 1\n",
      "µ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2\n",
      "x 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy\n",
      "h 1 h 1 h 2 h 2\n",
      "x 1 x 1 x 2 x 2\n",
      "           Figure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\n",
      "             dropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\n",
      "            hiddenlayerwithtwohiddenunits,andoneoutputunit. Toperformforward ( Bo t t o m )\n",
      "       propagationwithdropout,werandomlysampleavector µ     withoneentryforeachinput\n",
      "        orhiddenunitinthenetwork.Theentriesof µ     arebinaryandaresampledindependently\n",
      "              fromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\n",
      "     forthehiddenlayersand0 .           8fortheinput.Eachunitinthenetworkismultipliedby\n",
      "            thecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\n",
      "             networkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\n",
      "       ﬁgureandrunningforwardpropagationthroughit. 7.6\n",
      "2 5 8     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "p( ) i(  y| x             ).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\n",
      " thesedistributions,\n",
      "1\n",
      "kk\n",
      "i =1p( ) i   ( )y| x. (7.52)\n",
      "          Inthecaseofdropout,eachsubmodeldeﬁnedbymaskvector µ   deﬁnesaprobability\n",
      "             distribution .Thearithmeticmeanoverallmasksisgivenby py, (| x µ)\n",
      "\n",
      "µ    ppy,, () µ(| x µ) (7.53)\n",
      "wherep( µ         )istheprobabilitydistributionthatwasusedtosample µ attraining\n",
      "time.\n",
      "            Becausethissumincludesanexponentialnumberofterms,itisintractableto\n",
      "            evaluateexceptwhenthestructureofthemodelpermitssomeformofsimpliﬁcation.\n",
      "            Sofar,deepneuralnetsarenotknowntopermitanytractablesimpliﬁcation.\n",
      "          Instead,wecanapproximatetheinferencewithsampling,byaveragingtogether\n",
      "            theoutputfrommanymasks.Even10–20masksareoftensuﬃcienttoobtain\n",
      " goodperformance.\n",
      "            Anevenbetterapproach,however,allowsustoobtainagoodapproximationto\n",
      "             thepredictionsoftheentireensemble,atthecostofonlyoneforwardpropagation.\n",
      "               Todoso,wechangetousingthegeometricmeanratherthanthearithmeticmeanof\n",
      "         theensemblemembers’predicteddistributions.Warde-Farley2014etal.()present\n",
      "         argumentsandempiricalevidencethatthegeometri cmeanperformscomparably\n",
      "      tothearithmeticmeaninthiscontext.\n",
      "           Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\n",
      "           aprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\n",
      "             weimposetherequirementthatnoneofthesubmodelsassignsprobability0toany\n",
      "         event,andwerenormalizetheresultingdistribution.Theunnormalizedprobability\n",
      "         distributiondeﬁneddirectlybythegeometri cmeanisgivenby\n",
      "˜p e n s e m b l e  ( ) =y| x 2d\n",
      "µ    py,, (| x µ) (7.54)\n",
      "whered             isthenumberofunitsthatmaybedropped.Hereweuseauniform\n",
      " distributionover µ       tosimplifythepresentation,butnonuniformdistributionsare\n",
      "         alsopossible.Tomakepredictionswemustrenormalizetheensemble:\n",
      "p e n s e m b l e  ( ) =y| x˜p e n s e m b l e  ( )y| x\n",
      "y˜p e n s e m b l e(y | x) . (7.55)\n",
      "2 5 9     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "              Akeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\n",
      "matep e n s e m b l e byevaluatingp(  y|x         )inonemodel:themodelwithallunits,but\n",
      "      withtheweightsgoingoutofuniti       multipliedbytheprobabilityofincludingunit\n",
      "i              .Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe\n",
      "        outputfromthatunit.Wecallthisapproachthe   weightscalinginferencerule.\n",
      "            Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\n",
      "           inferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\n",
      "       Becauseweusuallyuseaninclusionprobabilityof1\n",
      "2    ,theweightscalingrule\n",
      "               usuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2\n",
      "              themodelasusual.Anotherwaytoachievethesameresultistomultiplythe\n",
      "                statesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2\n",
      "                theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\n",
      "                totalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\n",
      "  missingonaverage.\n",
      "             Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\n",
      "            scalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\n",
      "         classiﬁerwithinputvariablesrepresentedbythevector: n v\n",
      "    Py (= y|v) = softmax\n",
      "W  v+b\n",
      "y . (7.56)\n",
      "            Wecanindexintothefamilyofsubmodelsbyelement-wisemultiplicationofthe\n",
      "     inputwithabinaryvector:d\n",
      "     Py (= y|v;) = dsoftmax\n",
      "W    ( )+dvb\n",
      "y . (7.57)\n",
      "           Theensemblepredictorisdeﬁnedbyrenormalizingthegeometri cmeanoverall\n",
      "  ensemblemembers’predictions:\n",
      "P e n s e m b l e   (= ) = yy|v˜P e n s e m b l e   (= ) yy|v\n",
      "y˜P e n s e m b l e (= yy |v) , (7.58)\n",
      "where\n",
      "˜P e n s e m b l e   (= ) = yy|v2n\n",
      "d∈{} 0 1 ,n      Py. (= y|v;)d (7.59)\n",
      "           Toseethattheweightscalingruleisexact,wecansimplify˜P e n s e m b l e:\n",
      "˜P e n s e m b l e   (= ) = yy|v2n\n",
      "d∈{} 0 1 ,n      Py (= y|v;)d (7.60)\n",
      "2 6 0     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "=2n\n",
      "d∈{} 0 1 ,n softmax (W    ( )+)dvby (7.61)\n",
      "= 2n\n",
      "d∈{} 0 1 ,nexp\n",
      "Wy , :    ( )+dvb y\n",
      "\n",
      "yexp\n",
      "W\n",
      "y , :    ( )+dvb y (7.62)\n",
      "=2n\n",
      "d∈{} 0 1 ,nexp\n",
      "Wy , :    ( )+dvb y\n",
      "2n\n",
      "d∈{} 0 1 ,n\n",
      "yexp\n",
      "W\n",
      "y , :    ( )+dvb y(7.63)\n",
      "Because˜P          willbenormalized,wecansafelyignoremultiplicationbyfactorsthat\n",
      "     areconstantwithrespectto:y\n",
      "˜P e n s e m b l e   (= ) yy|v∝ 2n\n",
      "d∈{} 0 1 ,nexp\n",
      "Wy , :    ( )+dvb y\n",
      "(7.64)\n",
      "= exp\n",
      "1\n",
      "2n\n",
      "d∈{} 0 1 ,nW\n",
      "y , :    ( )+dvb y\n",
      " (7.65)\n",
      "= exp1\n",
      "2W\n",
      "y , :  v+b y\n",
      " . (7.66)\n",
      "           Substitutingthisbackintoequation,weobtainasoftmaxclassiﬁerwith 7.58\n",
      "weights1\n",
      "2 W.\n",
      "           Theweightscalingruleisalsoexactinothersettings,includingregression\n",
      "           networkswithconditionallynormaloutputsaswellasdeepnetworksthathave\n",
      "           hiddenlayerswithoutnonlinearities.However,theweightscalingruleisonlyan\n",
      "         approximationfordeepmodelsthathavenonlinearities.Thoughtheapproximation\n",
      "          hasnotbeentheoreticallycharacterized,itoftenworkswell,empirically.Goodfellow\n",
      "           etal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\n",
      "          better(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations to\n",
      "           theensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximation\n",
      "            wasallowedtosampleupto1,000subnetworks. ()found GalandGhahramani2015\n",
      "          thatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand\n",
      "           theMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\n",
      "   approximationisproblemdependent.\n",
      "           Srivastava 2014etal.()showedthatdropoutismoreeﬀectivethanother\n",
      "        standardcomputationallyinexpensiveregularizers,suchasweightdecay,ﬁlter\n",
      "2 6 1     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          normconstraints,andsparseactivityregularization.Dropoutmayalsobecombined\n",
      "         withotherformsofregularizationtoyieldafurtherimprovement.\n",
      "           Oneadvantageofdropoutisthatitisverycomputationallycheap.Using\n",
      "    dropoutduringtrainingrequiresonly O( n     )computationperexampleperupdate,\n",
      " togenerate n          randombinarynumbersandmultiplythembythestate.Depending\n",
      "      ontheimplementation,itmayalsorequire O( n     )memory tostorethesebinary\n",
      "          numbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\n",
      "               hasthesamecostperexampleasifdropoutwerenotused,thoughwemustpay\n",
      "              thecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\n",
      "examples.\n",
      "           Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit\n",
      "               thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\n",
      "            anymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\n",
      "        gradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\n",
      "          suchasrestrictedBoltzmannmachines(Srivastava 2014etal.,),andrecurrent\n",
      "           neuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\n",
      "         regularizationstrategiesofcomparablepowerimposemoresevererestrictionson\n",
      "    thearchitectureofthemodel.\n",
      "             Thoughthecostperstepofapplyingdropouttoaspeciﬁcmodelisnegligible,\n",
      "             thecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout\n",
      "             isaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset\n",
      "             thiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\n",
      "                seterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\n",
      "            largermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge\n",
      "        datasets,regularizationconferslittlereductioningeneralizationerror. Inthese\n",
      "           cases,thecomputationalcostofusingdropoutandlargermodelsmayoutweigh\n",
      "   thebeneﬁtofregularization.\n",
      "          Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\n",
      "     eﬀective.Bayesianneuralnetworks (,) outperform dropout on the Neal1996\n",
      "           AlternativeSplicingDataset( ,),wherefewerthan5,000examples Xiongetal.2011\n",
      "           areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,\n",
      "        unsupervisedfeaturelearningcangainanadvantageoverdropout.\n",
      "           Wager 2013etal.()showedthat,whenappliedtolinearregression,dropout\n",
      "  isequivalentto L2        weightdecay,withadiﬀerentweightdecaycoeﬃcientfor\n",
      "           eachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis\n",
      "            determinedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\n",
      "       models,dropoutisnotequivalenttoweightdecay.\n",
      "           Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\n",
      "2 6 2     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             approach’ssuccess.Itisjustameansofapproximatingthesumoverallsubmodels.\n",
      "         WangandManning2013()derivedanalyticalapproximationstothismarginaliza-\n",
      "    tion.Theirapproximation,knownas fastdropout    ,resultedinfasterconvergence\n",
      "           timeduetothereducedstochasticityinthecomputationofthegradient. This\n",
      "              methodcanalsobeappliedattesttime,asamoreprincipled(butalsomore\n",
      "        computationallyexpensive)approximationtotheaverageoverallsub-networks\n",
      "           thantheweightscalingapproximation. Fastdropouthasbeenusedtonearly\n",
      "           matchtheperformanceofstandarddropoutonsmallneuralnetworkproblems,but\n",
      "             hasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoalargeproblem.\n",
      "         Justasstochasticityisnotnecessarytoachieve theregularizingeﬀect of\n",
      "            dropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley 2014etal.()\n",
      "      designedcontrolexperimentsusingamethodcalled  dropoutboosting ,which\n",
      "             theydesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\n",
      "          itsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly\n",
      "            maximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\n",
      "          dropoutisanalogoustobagging, thisapproachisanalogoustoboosting.As\n",
      "         intended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect\n",
      "            comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\n",
      "           theinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\n",
      "            dropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis\n",
      "          onlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\n",
      "     performwellindependentlyofeachother.\n",
      "        Dropouthasinspiredotherstochasticapproachestotrainingexponentially\n",
      "           largeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseof\n",
      "            dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\n",
      "              unitstateisconsideredaunitthatcanbedropped(Wan 2013etal.,).Stochastic\n",
      "            poolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\n",
      "         ofconvolutionalnetworks,witheachconvolutionalnetworkattendingtodiﬀerent\n",
      "           spatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidely\n",
      "   usedimplicitensemblemethod.\n",
      "             Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic\n",
      "         behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\n",
      "          implementsaformofbaggingwithparametersharing.Earlier,wedescribed\n",
      "            dropoutasbagginganensembleofmodelsformedbyincludingorexcludingunits.\n",
      "             Yetthismodelaveragingstrategydoesnotneedtobebasedoninclusionand\n",
      "           exclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.Inpractice,\n",
      "            wemustchoosemodiﬁcationfamiliesthatneuralnetworksareabletolearnto\n",
      "           resist. Ideally,weshouldalsousemodelfamiliesthatallowafastapproximate\n",
      "2 6 3     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             inferencerule.Wecanthinkofanyformofmodiﬁcationparametrizedbyavector\n",
      "µ     astraininganensembleconsistingofp(   y,|xµ     )forallpossiblevaluesofµ.\n",
      "    Thereisnorequirementthatµ       haveaﬁnitenumberofvalues.Forexample,µcan\n",
      "           berealvalued. Srivastava 2014etal.()showedthatmultiplyingtheweightsby\n",
      "  µ∼N( 1,I        )canoutperformdropoutbasedonbinarymasks.Because E[µ] = 1,\n",
      "        thestandardnetworkautomaticallyimplementsapproximateinferenceinthe\n",
      "     ensemble,withoutneedinganyweightscaling.\n",
      "            Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,\n",
      "          approximatebagging.Anotherviewofdropoutgoesfurtherthanthis.Dropout\n",
      "              trainsnotjustabaggedensembleofmodels,butanensembleofmodelsthatshare\n",
      "             hiddenunits.Thismeanseachhiddenunitmustbeabletoperformwellregardless\n",
      "               ofwhichotherhiddenunitsareinthemodel.Hiddenunitsmustbepreparedtobe\n",
      "           swappedandinterchangedbetweenmodels.Hinton 2012cetal.()wereinspiredby\n",
      "          anideafrombiology:sexualreproduction,whichinvolvesswappinggenesbetween\n",
      "          twodiﬀerentorganisms,createsevolutionarypressureforgenestobecomenot\n",
      "           justgoodbutreadilyswappedbetweendiﬀerentorganisms.Suchgenesandsuch\n",
      "             featuresarerobusttochangesintheirenvironmentbecausetheyarenotableto\n",
      "           incorrectlyadapttounusualfeaturesofanyoneorganismormodel.Dropout\n",
      "              thusregularizeseachhiddenunittobenotmerelyagoodfeaturebutafeature\n",
      "           thatisgoodinmanycontexts.Warde-Farley 2014etal.()compareddropout\n",
      "           trainingtotrainingoflargeensemblesandconcludedthatdropoutoﬀersadditional\n",
      "       improvementstogeneralization error beyondthose obtainedbyensemblesof\n",
      " independentmodels.\n",
      "             Itisimportanttounderstandthatalargeportionofthepowerofdropout\n",
      "              arisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\n",
      "             canbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\n",
      "              contentoftheinputratherthandestructionoftherawvaluesoftheinput.For\n",
      "       example,ifthemodellearnsahiddenunith i      thatdetectsafacebyﬁndingthe\n",
      "  nose,thendroppingh i          correspondstoerasingtheinformationthatthereisanose\n",
      "       intheimage.Themodelmustlearnanotherh i    ,thateitherredundantlyencodes\n",
      "               thepresenceofanoseordetectsthefacebyanotherfeature,suchasthemouth.\n",
      "           Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\n",
      "               notabletorandomlyerasetheinformationaboutanosefromanimageofaface\n",
      "              unlessthemagnitudeofthenoiseissogreatthatnearlyalltheinformationin\n",
      "          theimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\n",
      "             allowsthedestructionprocesstomakeuseofalltheknowledgeabouttheinput\n",
      "       distributionthatthemodelhasacquiredsofar.\n",
      "            Anotherimportantaspectofdropoutisthatthenoiseismultiplicative.Ifthe\n",
      "2 6 4     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "           noisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunith iwith\n",
      " addednoise    couldsimplylearntohaveh i       becomeverylargeinordertomake\n",
      "  theaddednoise        insigniﬁcantbycomparison.Multiplicativenoisedoesnotallow\n",
      "        suchapathologicalsolutiontothenoiserobustnessproblem.\n",
      "        Anotherdeeplearningalgorithm,batchnormalization,reparametrizesthemodel\n",
      "            inawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\n",
      "            unitsattrainingtime.Theprimarypurposeofbatchnormalizationistoimprove\n",
      "           optimization,butthenoisecanhavearegularizingeﬀect,andsometimesmakes\n",
      "         dropoutunnecessary.Batchnormalizationisdescribedfurtherinsection.8.7.1\n",
      "  7.13AdversarialTraining\n",
      "           Inmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen\n",
      "             evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\n",
      "           modelshaveobtainedatruehuman-levelunderstandingofthesetasks.Toprobe\n",
      "             thelevelofunderstandinganetworkhasoftheunderlyingtask,wecansearch\n",
      "            forexamplesthatthemodelmisclassiﬁes. ()foundthateven Szegedyetal.2014b\n",
      "            neuralnetworksthatperformathumanlevelaccuracyhaveanearly100percent\n",
      "           errorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\n",
      "     proceduretosearchforaninputx   nearadatapointx   suchthatthemodel\n",
      "    outputisverydiﬀerentatx   .Inmanycases,x    canbesosimilartox thata\n",
      "+. 007 × =\n",
      " x s i gn ( ∇ xJ (  θx,,y ) ) x +\n",
      "  s i gn ( ∇ xJ (  θx,,y ) )\n",
      "   y =“panda” “nematode” “gibbon”\n",
      " w/57.7%\n",
      "conﬁdence w/8.2%\n",
      "conﬁdence w/99.3%\n",
      "conﬁdence\n",
      "         Figure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n",
      "            ( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedyetal.2014a\n",
      "                elementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\n",
      "            respecttotheinput,wecanchangeGoogLeNet ’sclassiﬁcationoftheimage.Reproduced\n",
      "      withpermissionfrom (). Goodfellowetal.2014b\n",
      "2 6 5     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "           humanobservercannottellthediﬀerencebetweentheoriginalexampleandthe\n",
      " adversarialexample         ,butthenetworkcanmakehighlydiﬀerentpredictions.See\n",
      "    ﬁgureforanexample. 7.8\n",
      "         Adversarialexampleshavemanyimplications,forexample,incomputersecurity,\n",
      "              thatarebeyondthescopeofthischapter.Theyareinterestinginthecontextof\n",
      "            regularization,however,becauseonecanreducetheerrorrateontheoriginali.i.d.\n",
      "  testsetvia  adversarialtraining     —trainingonadversariallyperturbedexamples\n",
      "           fromthetrainingset( ,; Szegedyetal.2014bGoodfellow 2014betal.,).\n",
      "            Goodfellow 2014betal.()showedthatoneoftheprimarycausesofthese\n",
      "   adversarial examplesis excessive linearity.Neural networks arebuilt out of\n",
      "         primarilylinearbuildingblocks. Insomeexperimentstheoverallfunctionthey\n",
      "             implementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n",
      "            tooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\n",
      "          ifithasnumerousinputs.Ifwechangeeachinputby     ,thenalinearfunction\n",
      " withweights w     canchangebyasmuchas |||| w 1      ,whichcanbeaverylarge\n",
      " amountif w      ishighdimensional.Adversarialtrainingdiscouragesthishighly\n",
      "           sensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\n",
      "               intheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly\n",
      "        introducingalocalconstancypriorintosupervisedneuralnets.\n",
      "           Adversarialtraininghelpstoillustratethepowerofusingalargefunction\n",
      "         familyincombinationwithaggressiveregularization.Purelylinearmodels,like\n",
      "           logisticregression,arenotabletoresistadversarialexamplesbecausetheyare\n",
      "             forcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\n",
      "             fromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture\n",
      "            lineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.\n",
      "        Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\n",
      "   learning.Atapoint x          thatisnotassociatedwithalabelinthedataset,the\n",
      "    modelitselfassignssomelabel ˆ y   .Themodel’slabel ˆ y     maynotbethetruelabel,\n",
      "       butifthemodelishighquality,then ˆ y      hasahighprobabilityofprovidingthe\n",
      "       truelabel.Wecanseekanadversarialexample x    thatcausestheclassiﬁerto\n",
      "  outputalabel ywith y= ˆ y       .Adversarialexamplesgeneratedusingnotthetrue\n",
      "          labelbutalabelprovidedbyatrainedmodelarecalled virtualadversarial\n",
      "examples             (Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe\n",
      "  samelabelto xand x          .Thisencouragestheclassiﬁertolearnafunctionthatis\n",
      "            robusttosmallchangesanywherealongthemanifoldwheretheunlabeleddatalie.\n",
      "           Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylieon\n",
      "            disconnectedmanifolds,andasmallperturbationshouldnotbeabletojumpfrom\n",
      "      oneclassmanifoldtoanotherclassmanifold.\n",
      "2 6 6     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "      7.14TangentDistance,TangentPropandManifold\n",
      " TangentClassiﬁer\n",
      "          Manymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\n",
      "            byassumingthatthedataliesnearalow-dimensionalmanifold,asdescribedin\n",
      " section .5.11.3\n",
      "             Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\n",
      " tangentdistance         algorithm( ,,). Itisanonparametric Simardetal.19931998\n",
      "            nearestneighboralgorithminwhichthemetricusedisnotthegenericEuclidean\n",
      "            distancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\n",
      "            probabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamples,and\n",
      "            thatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer\n",
      "            shouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement\n",
      "             onthemanifold,itwouldmakesensetouseasnearestneighbordistancebetween\n",
      "points x 1and x 2    thedistancebetweenthemanifoldsM 1andM 2  towhichthey\n",
      "         respectivelybelong.Althoughthatmaybecomputationallydiﬃcult(itwould\n",
      "             requiresolvinganoptimizationproblem,toﬁndthenearestpairofpointsonM 1\n",
      "andM 2          ),acheapalternativethatmakessenselocallyistoapproximateM i byits\n",
      "  tangentplaneat x i         andmeasurethedistancebetweenthetwotangents,orbetween\n",
      "             atangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional\n",
      "            linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\n",
      "     onetospecifythetangentvectors.\n",
      "    Inarelatedspirit,the  tangentprop       algorithm( ,)(ﬁgure) Simardetal.1992 7.9\n",
      "            trainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutputf( x )of\n",
      "            theneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\n",
      "           variationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\n",
      "        sameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ xf( x  )tobe\n",
      "      orthogonaltotheknownmanifoldtangentvectors v( ) iat x   ,orequivalentlythat\n",
      "   thedirectionalderivativeoffat x  inthedirections v( ) i    besmallbyaddinga\n",
      "  regularizationpenalty:Ω\n",
      "Ω() =f\n",
      "i\n",
      "( ∇ xf()) xv( ) i2\n",
      " . (7.67)\n",
      "            Thisregularizercanofcoursebescaledbyanappropriatehyperparameter,andfor\n",
      "              mostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\n",
      "outputf( x          )describedhereforsimplicity.Aswiththetangentdistancealgorithm,\n",
      "            thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\n",
      "           theeﬀectoftransformations,suchastranslation,rotation,andscalinginimages.\n",
      "2 6 7     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "x 1x 2N o r m a lT a n g e n t\n",
      "             Figure7.9: Illust rationofthemainideaofthetangentpropalgorithm( , Simardetal.\n",
      "            1992 Rifai 2011c )andmanifoldtangentclassiﬁer(etal.,),whichbothregularizethe\n",
      "  classiﬁeroutputfunction f( x         ).Eachcurverepresentsthemanifoldforadiﬀerentclass,\n",
      "          illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\n",
      "                 Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe\n",
      "               classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\n",
      "            classmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\n",
      "           tangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto\n",
      "                changerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\n",
      "            itmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\n",
      " classiﬁerregularize f( x     ) tonotchangeverymuchas x    movesalongthemanifold.Tangent\n",
      "           propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\n",
      "             directions(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\n",
      "          manifold),whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections\n",
      "              bytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate\n",
      "     manifoldsisdescribedinchapter.14\n",
      "2 6 8     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "             Tangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992\n",
      "         butalsointhecontextofreinforcementlearning(,). Thrun1995\n",
      "      Tangentpropagation i sclosely relatedto datasetaugmentation.Inboth\n",
      "              cases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\n",
      "             byspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\n",
      "             network.Thediﬀerenceisthatinthecaseofdatasetaugmentation,thenetworkis\n",
      "           explicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\n",
      "         morethananinﬁnitesimalamountofthesetransformations.Tangentpropagation\n",
      "           doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\n",
      "          regularizesthemodeltoresistperturbationinthedirectionscorrespondingto\n",
      "   the speciﬁed transformation.While thisanalytical approac h isintellectually\n",
      "             elegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\n",
      "       inﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\n",
      "         largerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels\n",
      "           basedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives\n",
      "              byturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir\n",
      "            derivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\n",
      "          unitscan.Datasetaugmentationworkswellwithrectiﬁedlinearunitsbecause\n",
      "           diﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof\n",
      "  eachoriginalinput.\n",
      "     Tangentpropagationisalsorelatedto doublebackprop   (DruckerandLeCun,\n",
      "           1992)andadversarialtraining( ,; ,). Szegedyetal.2014bGoodfellowetal.2014b\n",
      "          DoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\n",
      "             ﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\n",
      "           outputontheseasontheoriginalinputs.Tangentpropagationanddataset\n",
      "        augmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe\n",
      "            modelbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.Double\n",
      "           backpropandadversarialtrainingbothrequirethatthemodelshouldbeinvariant\n",
      "                 todirectionsofchangeintheinputaslongasthechangeissmall.Justasdataset all\n",
      "        augmentationisthenon-inﬁnitesimalversionoftangentpropagation, adversarial\n",
      "       trainingisthenon-inﬁnitesimalversionofdoublebackprop.\n",
      "           Themanifoldtangentclassiﬁer( ,),eliminatestheneedto Rifaietal.2011c\n",
      "              knowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n",
      "          estimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse\n",
      "         ofthistechniquetoavoidneedinguser-speciﬁedtangentvectors. Asillustrated\n",
      "           inﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\n",
      "             thatariseoutofthegeometry ofimages(suchastranslation,rotation,andscaling)\n",
      "            andincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas\n",
      "2 6 9     C HAP T E R 7 . R E G UL ARI Z A T I O N F O R D E E P L E ARNI N G\n",
      "          movingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer\n",
      "            isthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\n",
      "            unsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer\n",
      "     asintangentprop(equation).7.67\n",
      "       Inthis chapter, wehavedescribed mostofthe generalstrategiesused to\n",
      "          regularizeneuralnetworks.Regularizationisacentralthemeofmachinelearning\n",
      "             andassuchwillberevisitedperiodicallyinmostoftheremainingchapters.Another\n",
      "        centralthemeofmachinelearningisoptimization,describednext.\n",
      "2 7 0 C h a p t e r 8\n",
      "   Opt i miz at i on for T r ai n i n g D e e p\n",
      "Mo dels\n",
      "         Deeplearningalgorithmsinvolveoptimizationinmanycontexts.Forexample,\n",
      "          performinginferenceinmodelssuchasPCAinvolvessolvinganoptimization\n",
      "           problem.Weoftenuseanalyticaloptimizationtowriteproofsordesignalgorithms.\n",
      "            Ofallthemanyoptimizationproblemsinvolvedindeeplearning,themostdiﬃcult\n",
      "               isneuralnetworktraining.Itisquitecommontoinvestdaystomonthsoftimeon\n",
      "             hundredsofmachinestosolveevenasingleinstanceoftheneuralnetworktraining\n",
      "           problem.Becausethisproblemissoimportantandsoexpensive,aspecialized\n",
      "          setofoptimizationtechniqueshavebeendevelopedforsolvingit. Thischapter\n",
      "       presentstheseoptimizationtechniquesforneuralnetworktraining.\n",
      "          Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\n",
      "            wesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\n",
      "  optimizationingeneral.\n",
      "           Thischapterfocusesononeparticularcaseofoptimization:ﬁndingtheparam-\n",
      "eters θ         ofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J ( θ ),which\n",
      "           typicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\n",
      "    wellasadditionalregularizationterms.\n",
      "            Webeginwithadescriptionofhowoptimizationusedasatrainingalgorithm\n",
      "            foramachinelearningtaskdiﬀersfrompureoptimization.Next,wepresentseveral\n",
      "           oftheconcretechallengesthatmakeoptimizationofneuralnetworksdiﬃcult.We\n",
      "        thendeﬁneseveralpracticalalgorithms,includingbothoptimizationalgorithms\n",
      "         themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\n",
      "          adapttheirlearningratesduringtrainingorleverageinformationcontainedin\n",
      "271      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             thesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\n",
      "         severaloptimizationstrategiesthatareformedbycombiningsimpleoptimization\n",
      "   algorithmsintohigher-levelprocedures.\n",
      "      8.1HowLearningDiﬀersfromPureOptimization\n",
      "          Optimizationalgorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\n",
      "         optimizationalgorithmsinseveralways.Machinelearningusuallyactsindirectly.\n",
      "          Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasure\n",
      "P              ,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable. We\n",
      " thereforeoptimizeP       onlyindirectly.WereduceadiﬀerentcostfunctionJ( θ )in\n",
      "      thehopethatdoingsowillimproveP       .Thisisincontrasttopureoptimization,\n",
      " whereminimizingJ          isagoalinandofitself.Optimizationalgorithmsfortraining\n",
      "           deepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof\n",
      "   machinelearningobjectivefunctions.\n",
      "             Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\n",
      " suchas\n",
      "J() = θ E ( ) ˆ x ,y ∼ p d a t a   Lf,y, ((;) x θ) (8.1)\n",
      "whereL    istheper-examplelossfunction,f( x; θ      )isthepredictedoutputwhenthe\n",
      " inputis x ,andˆp d a ta        istheempiricaldistribution.Inthesupervisedlearningcase,\n",
      "y          isthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\n",
      "     supervisedcase,wheretheargumentstoLaref( x; θ )andy     .Itistrivialtoextend\n",
      "     thisdevelopment,forexample,toinclude θor x    asarguments,ortoexcludeyas\n",
      "         arguments,todevelopvariousformsofregularizationorunsupervisedlearning.\n",
      "            Equationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1\n",
      "          wouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\n",
      "       expectationistakenacrossthedata-generatingdistributionp d a ta   ratherthanjust\n",
      "    overtheﬁnitetrainingset:\n",
      "J∗() = θ E ( ) x ,y ∼ p d a t a   Lf,y. ((;) x θ) (8.2)\n",
      "   8.1.1EmpiricalRiskMinimization\n",
      "            Thegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\n",
      "         errorgivenbyequation. Thisquantityisknownasthe 8.2 r i s k  .Weemphasize\n",
      "          herethattheexpectationistakenoverthetrueunderlyingdistributionp d a ta  .Ifwe\n",
      "   knewthetruedistributionp d a ta( x,y      ),riskminimizationwouldbeanoptimization\n",
      "2 7 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "          tasksolvablebyanoptimizationalgorithm.Whenwedonotknowp d a ta( x,y )but\n",
      "             onlyhaveatrainingsetofsamples,however,wehaveamachinelearningproblem.\n",
      "            Thesimplestwaytoconvertamachinelearningproblembackintoanop-\n",
      "            timizationproblemistominimizetheexpectedlossonthetrainingset.This\n",
      "    meansreplacingthetruedistributionp( x,y   ) withtheempiricaldistributionˆp( x,y)\n",
      "          deﬁnedbythetrainingset.Wenowminimizetheempiricalrisk\n",
      "E x ,y ∼ ˆ p d a t a ( ) x , y   [((;))] = Lf x θ,y1\n",
      "mm \n",
      "i =1Lf(( x( ) i  ;) θ,y( ) i ), (8.3)\n",
      "       whereisthenumberoftrainingexamples. m\n",
      "           Thetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\n",
      "as   empiricalriskminimization        .Inthissetting,machinelearningisstillvery\n",
      "         similartostraightforwardoptimization.Ratherthanoptimizingtheriskdirectly,\n",
      "            weoptimizetheempiricalriskandhopethattheriskdecreasessigniﬁcantlyas\n",
      "            well.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\n",
      "       canbeexpectedtodecreasebyvariousamounts.\n",
      "         Nonetheless,empiricalriskminimizationispronetooverﬁtting.Modelswith\n",
      "           highcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\n",
      "          riskminimizationisnotreallyfeasible.Themosteﬀectivemodernoptimization\n",
      "           algorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\n",
      "             as0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned\n",
      "            everywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we\n",
      "           rarelyuseempiricalriskminimization.Instead,wemustuseaslightlydiﬀerent\n",
      "            approach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\n",
      "        fromthequantitythatwetrulywanttooptimize.\n",
      "      8.1.2SurrogateLossFunctionsandEarlyStopping\n",
      "            Sometimes,thelossfunctionweactuallycareabout(say,classiﬁcationerror)isnot\n",
      "           onethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1\n",
      "            lossistypicallyintractable(exponentialintheinputdimension),evenforalinear\n",
      "          classiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\n",
      "a   surrogatelossfunction         instead,whichactsasaproxybuthasadvantages.\n",
      "             Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\n",
      "            surrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\n",
      "             theconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\n",
      "               dothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin\n",
      "expectation.\n",
      "2 7 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             Insomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\n",
      "              more.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\n",
      "             timeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\n",
      "            log-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\n",
      "             onecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart\n",
      "           fromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting\n",
      "            moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\n",
      "        minimizingtheaverage0-1lossonthetrainingset.\n",
      "         Averyimportantdiﬀerencebetweenoptimizationingeneralandoptimization\n",
      "              asweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\n",
      "          atalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\n",
      "            asurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\n",
      "           stopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8\n",
      "              onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\n",
      "             andisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.\n",
      "           Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\n",
      "           whichisverydiﬀerentfromthepureoptimizationsetting,whereanoptimization\n",
      "           algorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\n",
      "    8.1.3BatchandMinibatchAlgorithms\n",
      "          Oneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral\n",
      "           optimizationalgorithmsisthattheobjectivefunctionusuallydecomposesasasum\n",
      "         overthetrainingexamples.Optimizationalgorithmsformachinelearningtypically\n",
      "             computeeachupdatetotheparametersbasedonanexpectedvalueofthecost\n",
      "             functionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\n",
      "         Forexample,maximumlikelihoodestimationproblems,whenviewedinlog\n",
      "       space,decomposeintoasumovereachexample:\n",
      "θ M L  = argmax\n",
      "θm \n",
      "i =1 logp m o d e l( x( ) i ,y( ) i  ;) θ. (8.4)\n",
      "          Maximizingthissumisequivalenttomaximizingtheexpectationoverthe\n",
      "      empiricaldistributiondeﬁnedbythetrainingset:\n",
      "J() = θ E x ,y ∼ ˆ p d a t a logp m o d e l   (;) x,y θ. (8.5)\n",
      "       MostofthepropertiesoftheobjectivefunctionJ     usedbymostofouropti-\n",
      "           mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n",
      "2 7 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "      mostcommonlyusedpropertyisthegradient:\n",
      "∇ θJ() = θ E x ,y ∼ ˆ p d a t a ∇ θ logp m o d e l   (;) x,y θ. (8.6)\n",
      "    Computing thisexpectation exactly isvery expensivebecause itrequires\n",
      "             evaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\n",
      "          computetheseexpectationsbyrandomlysamplingasmallnumberofexamples\n",
      "          fromthedataset,thentakingtheaverageoveronlythoseexamples.\n",
      "           Recallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n\n",
      "   samplesisgivenbyσ/√n,whereσ        isthetruestandarddeviationofthevalueof\n",
      "    thesamples.Thedenominatorof√n       showsthattherearelessthanlinearreturns\n",
      "          tousingmoreexamplestoestimatethegradient.Comparetwohypothetical\n",
      "             estimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\n",
      "           examples.Thelatterrequires100timesmorecomputationthantheformerbut\n",
      "              reducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\n",
      "            algorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof\n",
      "           numberofupdates)iftheyareallowedtorapidlycomputeapproximateestimates\n",
      "         ofthegradientratherthanslowlycomputingtheexactgradient.\n",
      "         Anotherconsiderationmotivatingstatisticalestimationofthegradientfroma\n",
      "              smallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\n",
      "m              samplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\n",
      "            basedestimateofthegradientcouldcomputethecorrectgradientwithasingle\n",
      " sample,usingm         timeslesscomputationthanthenaiveapproach.Inpractice,we\n",
      "            areunlikelytoencounterthisworst-casesituation,butwemayﬁndlargenumbers\n",
      "          ofexamplesthatallmakeverysimilarcontributionstothegradient.\n",
      "         Optimizationalgorithmsthatusetheentiretrainingsetarecalledbatchor\n",
      "         deterministicgradientmethods,becausetheyprocessallthetrainingexamples\n",
      "          simultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\n",
      "             becausetheword“batch”isalsooftenusedtodescribetheminibatchusedby\n",
      "         minibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”\n",
      "                impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe\n",
      "               agroupofexamplesdoesnot.Forexample,itiscommontousetheterm“batch\n",
      "       size”todescribethesizeofaminibatch.\n",
      "            Optimizationalgorithmsthatuseonlyasingleexampleatatimearesometimes\n",
      "calledstochastic andsometimesonline      methods.Theterm“online”isusually\n",
      "            reservedforwhentheexamplesaredrawnfromastreamofcontinuallycreated\n",
      "            examplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveralpassesare\n",
      "made.\n",
      "           Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n",
      "2 7 5      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            thanonebutfewerthanallthetrainingexamples.Theseweretraditionallycalled\n",
      "minibatchor  minibatchstochastic        methods,anditisnowcommontocall\n",
      "   themsimplystochasticmethods.\n",
      "          Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,\n",
      "     presentedindetailinsection.8.3.1\n",
      "        Minibatchsizesaregenerallydrivenbythefollowingfactors:\n",
      "•            Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\n",
      "   lessthanlinearreturns.\n",
      "•         Multicorearchitecturesareusuallyunderutilizedbyextremelysmallbatches.\n",
      "          Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\n",
      "         isnoreductioninthetimetoprocessaminibatch.\n",
      "•              Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\n",
      "             thecase),thentheamountofmemory scaleswiththebatchsize.Formany\n",
      "         hardwaresetupsthisisthelimitingfactorinbatchsize.\n",
      "•           Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.\n",
      "              EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer\n",
      "              betterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\n",
      "     sometimesbeingattemptedforlargemodels.\n",
      "•          Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\n",
      "           perhapsduetothenoisetheyaddtothelearningprocess.Generalization\n",
      "              errorisoftenbestforabatchsizeof1. Trainingwithsuchasmallbatch\n",
      "            sizemightrequireasmalllearningratetomaintainstabilitybecauseofthe\n",
      "             highvarianceintheestimateofthegradient.Thetotalruntimecanbevery\n",
      "               highasaresultoftheneedtomakemoresteps,bothbecauseofthereduced\n",
      "            learningrateandbecauseittakesmorestepstoobservetheentiretraining\n",
      "set.\n",
      "           Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-\n",
      "            batchinvariousways.Somealgorithmsaremoresensitivetosamplingerrorthan\n",
      "           others,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately\n",
      "            withfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\n",
      "          errorsmore.Methodsthatcomputeupdatesbasedonlyonthegradient gare\n",
      "           usuallyrelativelyrobustandcanhandlesmallerbatchsizes,like100.Second-order\n",
      "      methods,whichalsousetheHessianmatrix H    andcomputeupdatessuchas\n",
      "H− 1g          ,typicallyrequiremuchlargerbatchsizes,like10,000. Theselargebatch\n",
      "         sizesarerequiredtominimizeﬂuctuationsintheestimatesof H− 1g .Suppose\n",
      "2 7 6      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "thatH        isestimatedperfectlybuthasapoorconditionnumber. Multiplication\n",
      "byH           oritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsin\n",
      "g       .Verysmallchangesintheestimateofg      canthuscauselargechangesinthe\n",
      "updateH− 1g  ,evenifH    isestimatedperfectly.Ofcourse,H  isestimatedonly\n",
      "   approximately,sotheupdateH− 1g       willcontainevenmoreerrorthanwewould\n",
      "           predictfromapplyingapoorlyconditionedoperationtotheestimateof.g\n",
      "           Itisalsocrucialthattheminibatchesbeselectedrandomly.Computingan\n",
      "             unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\n",
      "            samplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\n",
      "          independentfromeachother,sotwosubsequentminibatchesofexamplesshould\n",
      "           alsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\n",
      "            inawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\n",
      "               haveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\n",
      "              listmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent\n",
      "             timesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe\n",
      "              secondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\n",
      "              weretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\n",
      "            beextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\n",
      "               manypatientsinthedataset.Incasessuchasthese,wheretheorderofthedataset\n",
      "           holdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting\n",
      "          minibatches.Forverylargedatasets,forexample,datasetscontainingbillionsof\n",
      "             examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\n",
      "            atrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\n",
      "                itisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin\n",
      "            shuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive\n",
      "           examplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\n",
      "              willbeforcedtoreusethisorderingeverytimeitpassesthroughthetrainingdata.\n",
      "            Thisdeviationfromtruerandomselectiondoesnotseemtohaveasigniﬁcant\n",
      "            detrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycanseriously\n",
      "     reducetheeﬀectivenessofthealgorithm.\n",
      "        Manyoptimizationproblemsinmachinelearningdecomposeoverexamples\n",
      "           wellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples\n",
      "           inparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X )for\n",
      "   oneminibatchofexamplesX         atthesametimethatwecomputetheupdatefor\n",
      "        severalotherminibatches.Suchasynchronousparalleldistributedapproachesare\n",
      "    discussedfurtherinsection .12.1.3\n",
      "          Aninterestingmotivationforminibatchstochasticgradientdescentisthatit\n",
      "             followsthegradientofthetruegeneralizationerror(equation)aslongasno 8.2\n",
      "2 7 7      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "        examplesarerepeated.Mostimplementationsofminibatchstochasticgradient\n",
      "             descentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe\n",
      "             ﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\n",
      "           generalizationerror.Onthesecondpass,theestimatebecomesbiasedbecauseit\n",
      "            isformedbyresamplingvaluesthathavealreadybeenused,ratherthanobtaining\n",
      "      newfairsamplesfromthedata-generatingdistribution.\n",
      "         Thefactthatstochasticgradientdescentminimizesgeneralizationerroris\n",
      "             easiesttoseeinonlinelearning,whereexamplesorminibatchesaredrawnfroma\n",
      "s t r e a m            ofdata.Inotherwords,insteadofreceivingaﬁxed-sizetrainingset,the\n",
      "               learnerissimilartoalivingbeingwhoseesanewexampleateachinstant,with\n",
      "  everyexample( x,y     )comingfromthedata-generatingdistributionpdata( x,y ).In\n",
      "            thisscenario,examplesareneverrepeated;everyexperienceisafairsamplefrom\n",
      "pdata.\n",
      "       Theequivalenceiseasiesttoderivewhenbothxandy  arediscrete. Inthis\n",
      "           case,thegeneralizationerror(equation)canbewrittenasasum 8.2\n",
      "J∗() =θ\n",
      "x\n",
      "ypdata    ()((;)) x,yLfxθ,y, (8.7)\n",
      "   withtheexactgradient\n",
      " g= ∇ θJ∗() =θ\n",
      "x\n",
      "ypdata ()x,y∇ θ    Lf,y. ((;)xθ) (8.8)\n",
      "            Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\n",
      "             tionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\n",
      "         besidesthelikelihood.Asimilarresultcanbederivedwhenxandy arecontinuous,\n",
      "    undermildassumptionsregardingpdata and.L\n",
      "        Hence, wecanobtainanunbiased estimatorofthe exactgradient ofthe\n",
      "       generalizationerrorbysamplingaminibatchofexamples{x(1)    ,...x() m} withcor-\n",
      " respondingtargetsy() i   fromthedata-generatingdistributionpdata  ,thencomputing\n",
      "            thegradientofthelosswithrespecttotheparametersforthatminibatch:\n",
      "ˆ g=1\n",
      "m∇ θ\n",
      "iLf((x() i  ;)θ,y() i ). (8.9)\n",
      "     Updatinginthedirectionof θ ˆ      gperformsSGDonthegeneralizationerror.\n",
      "         Ofcourse, thisinterpretationappliesonlywhenexamplesarenotreused.\n",
      "            Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,\n",
      "           unlessthetrainingsetisextremelylarge. Whenmultiplesuchepochsareused,\n",
      "2 7 8      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            onlytheﬁrstepochfollowstheunbiasedgradientofthegeneralizationerror,but\n",
      "           ofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased\n",
      "             trainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining\n",
      "   errorandtesterror.\n",
      "           Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\n",
      "           isbecomingmorecommonformachinelearningapplicationstouseeachtraining\n",
      "            exampleonlyonceoreventomakeanincompletepassthroughthetraining\n",
      "             set.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so\n",
      "        underﬁttingandcomputationaleﬃciencybecomethepredominantconcerns.See\n",
      "            also ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008\n",
      "          bottlenecksongeneralizationerror,asthenumberoftrainingexamplesgrows.\n",
      "     8.2ChallengesinNeuralNetworkOptimization\n",
      "         Optimizationingeneralisanextremelydiﬃculttask.Traditionally,machine\n",
      "          learninghasavoidedthediﬃcultyofgeneraloptimizationbycarefullydesigning\n",
      "           theobjectivefunctionandconstraintstoensurethattheoptimizationproblemis\n",
      "          convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnonconvex\n",
      "           case.Evenconvexoptimizationisnotwithoutitscomplications.Inthissection,\n",
      "          wesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\n",
      "   fortrainingdeepmodels.\n",
      " 8.2.1Ill-Conditioning\n",
      "           Somechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\n",
      "      prominentisill-conditioningoftheHessianmatrixH     .Thisisaverygeneral\n",
      "           probleminmostnumericaloptimization,convexorotherwise,andisdescribedin\n",
      "    moredetailinsection.4.3.1\n",
      "         Theill-conditioningproblemisgenerallybelievedtobe presentinneural\n",
      "          networktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\n",
      "            “stuck”inthesensethatevenverysmallstepsincreasethecostfunction.\n",
      "           Recallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\n",
      "           costfunctionpredictsthatagradientdescentstepofwilladd− g\n",
      "1\n",
      "22g  Hgg−  g (8.10)\n",
      "          tothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n",
      "22gHg\n",
      "exceeds gg        . Todeterminewhetherill-conditioningisdetrimentaltoaneural\n",
      "2 7 9      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "      −50050100150200250\n",
      "  Trainingtime(epochs)−20246810121416Gradient norm\n",
      "     0 50100150200250\n",
      "  Trainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .  Classiﬁcationerrorrate\n",
      "                Figure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\n",
      "           example,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\n",
      "           forobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient\n",
      "           evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\n",
      "               isplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\n",
      "             curve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\n",
      "            expectifthetrainingprocessconvergedtoacriticalpoint. Despitetheincreasing ( R i g h t )\n",
      "          gradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation\n",
      "     errordecreasestoalowlevel.\n",
      "    network trainingtask, one canmonitorthesquared gradient norm ggand\n",
      "the gH g          term.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly\n",
      "   throughoutlearning,butthe gH g        termgrowsbymorethananorderofmagnitude.\n",
      "             Theresultisthatlearningbecomesveryslowdespitethepresenceofastrong\n",
      "            gradientbecausethelearningratemustbeshrunktocompensateforevenstronger\n",
      "          curvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1\n",
      "       duringthesuccessfultrainingofaneuralnetwork.\n",
      "         Thoughill-conditioningispresentinothersettingsbesidesneuralnetwork\n",
      "             training,someofthetechniquesusedtocombatitinothercontextsareless\n",
      "           applicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool\n",
      "          forminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butas\n",
      "         weargueinsubsequentsections,Newton’smethodrequiressigniﬁcantmodiﬁcation\n",
      "       beforeitcanbeappliedtoneuralnetworks.\n",
      "2 8 0      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "  8.2.2LocalMinima\n",
      "             Oneofthemostprominentfeaturesofaconvexoptimizationproblemisthatit\n",
      "              canbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis\n",
      "             guaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat\n",
      "             thebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\n",
      "            aﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\n",
      "                knowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.\n",
      "            Withnonconvexfunctions,suchasneuralnets,itispossibletohavemany\n",
      "           localminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave\n",
      "              anextremelylargenumberoflocalminima.Aswewillsee,however,thisisnot\n",
      "   necessarilyamajorproblem.\n",
      "         Neuralnetworksandanymodelswithmultipleequivalentlyparametrizedlatent\n",
      "        variablesallhavemultiplelocalminimabecauseofthe modelidentiﬁability\n",
      "              problem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan\n",
      "             ruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables\n",
      "           areoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging\n",
      "             latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\n",
      "          modifylayer1byswappingtheincomingweightvectorforuniti  withtheincoming\n",
      "   weightvectorforunitj           ,thendothesamefortheoutgoingweightvectors.Ifwe\n",
      "havem  layerswithn    unitseach,thentherearen !m    waysofarrangingthehidden\n",
      "          units.Thiskindofnonidentiﬁabilityisknownasweightspacesymmetry.\n",
      "           Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\n",
      "          additionalcausesofnonidentiﬁability.Forexample,inanyrectiﬁedlinearor\n",
      "              maxoutnetwork,wecanscalealltheincomingweightsandbiasesofaunitby\n",
      "α        ifwealsoscaleallitsoutgoingweightsby1\n",
      "α     .Thismeansthat—ifthecost\n",
      "             functiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\n",
      "           weightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear\n",
      "      ormaxoutnetworkliesonan ( mn ×     )-dimensionalhyperbolaofequivalentlocal\n",
      "minima.\n",
      "          Thesemodelidentiﬁabilityissuesmeanthataneuralnetworkcostfunction\n",
      "            canhaveanextremelylargeorevenuncountablyinﬁniteamountoflocalminima.\n",
      "          However,alltheselocalminimaarisingfromnonidentiﬁabilityareequivalentto\n",
      "             eachotherincostfunctionvalue.Asaresult, theselocalminimaarenota\n",
      "   problematicformofnonconvexity.\n",
      "             Localminimacanbeproblematiciftheyhavehighcostincomparisontothe\n",
      "          globalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\n",
      "            units,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\n",
      "2 8 1      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             andSussman1989Brady 1989GoriandTesi1992 ,;etal.,; ,).Iflocalminima\n",
      "            withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\n",
      " optimizationalgorithms.\n",
      "           Whethernetworksofpracticalinteresthavemanylocalminimaofhighcost\n",
      "         andwhetheroptimizationalgorithmsencounterthemremainopenquestions.For\n",
      "           manyyears,mostpractitionersbelievedthatlocalminimawereacommonproblem\n",
      "            plaguingneuralnetworkoptimization.Today,thatdoesnotappeartobethecase.\n",
      "            Theproblemremainsanactiveareaofresearch,butexpertsnowsuspectthat,\n",
      "            forsuﬃcientlylargeneuralnetworks,mostlocalminimahavealowcostfunction\n",
      "               value,andthatitisnotimportanttoﬁndatrueglobalminimumratherthanto\n",
      "               ﬁndapointinparameterspacethathaslowbutnotminimalcost( , Saxeetal.\n",
      "            2013Dauphin2014Goodfellow 2015Choromanska2014 ; etal.,; etal.,; etal.,).\n",
      "         Manypractitionersattributenearlyalldiﬃcultywithneuralnetworkoptimiza-\n",
      "           tiontolocalminima.Weencouragepractitionerstocarefullytestforspeciﬁc\n",
      "             problems. Atestthatcanruleoutlocalminimaastheproblemisplottingthe\n",
      "               normofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\n",
      "             insigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical\n",
      "         point.Inhigh-dimensionalspaces,positivelyestablishingthatlocalminimaare\n",
      "            theproblemcanbeverydiﬃcult.Manystructuresotherthanlocalminimaalso\n",
      "  havesmallgradients.\n",
      "       8.2.3Plateaus,SaddlePointsandOtherFlatRegions\n",
      "       Formanyhigh-dimensionalnonconvexfunctions, localminima(andmaxima)\n",
      "              areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\n",
      "             point.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,\n",
      "             whileothershavealowercost. Atasaddlepoint,theHessianmatrixhasboth\n",
      "         positiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\n",
      "           positiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\n",
      "            alongnegativeeigenvalueshavelowervalue. Wecanthinkofasaddlepointas\n",
      "             beingalocalminimumalongonecross-sectionofthecostfunctionandalocal\n",
      "         maximumalonganothercross-section.Seeﬁgureforanillustration. 4.5\n",
      "        Manyclassesof randomfunctionsexhibitthefollowing behavior:inlow-\n",
      "         dimensionalspaces,localminimaarecommon.Inhigher-dimensionalspaces,local\n",
      "           minimaarerare,andsaddlepointsaremorecommon.Forafunction f : Rn → Rof\n",
      "              thistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\n",
      " exponentiallywith n        .Tounderstandtheintuitionbehindthisbehavior,observe\n",
      "           thattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues. The\n",
      "2 8 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             Hessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\n",
      "               Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle\n",
      "               dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\n",
      " once.In n        -dimensionalspace,itisexponentiallyunlikelythatall n  cointosseswill\n",
      "              beheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\n",
      "            Anamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\n",
      "             Hessianbecomemorelikelytobepositiveaswereachregionsoflowercost. In\n",
      "               ourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\n",
      "heads n               timesifweareatacriticalpointwithlowcost.Italsomeansthatlocal\n",
      "              minimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith\n",
      "             highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\n",
      "        highcostaremorelikelytobelocalmaxima.\n",
      "            Thishappensformanyclassesofrandomfunctions.Doesithappenforneural\n",
      "         networks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n",
      "           (feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\n",
      "            chapter)withnononlinearitieshaveglobalminimaandsaddlepointsbutno 14\n",
      "           localminimawithhighercostthantheglobalminimum.Theyobservedwithout\n",
      "          proofthattheseresultsextendtodeepernetworkswithoutnonlinearities.The\n",
      "              outputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\n",
      "             tostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\n",
      "          anonconvexfunctionoftheirparameters.Suchnetworksareessentiallyjust\n",
      "          multiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\n",
      "            tothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\n",
      "            thesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof\n",
      "          deepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\n",
      "           experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\n",
      "         manyhigh-costsaddlepoints.Choromanska 2014etal.()providedadditional\n",
      "        theoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\n",
      "        functionsrelatedtoneuralnetworksdoessoaswell.\n",
      "           Whataretheimplicationsoftheproliferationofsaddlepointsfortraining\n",
      "         algorithms?Forﬁrst-orderoptimization,algorithmsthatuseonlygradientinfor-\n",
      "             mation,thesituationisunclear.Thegradientcanoftenbecomeverysmallneara\n",
      "            saddlepoint.Ontheotherhand,gradientdescentempiricallyseemsabletoescape\n",
      "           saddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\n",
      "         severallearningtrajectoriesofstate-of-the-artneuralnetworks,withanexample\n",
      "             giveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2\n",
      "              aprominentsaddlepoint,wheretheweightsareallzero,buttheyalsoshowthe\n",
      "          gradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015\n",
      "2 8 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "   \n",
      "Projection2of θ\n",
      "   Projection1of θJ(\n",
      ")θ\n",
      "             Figure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Thesevisualizations\n",
      "         appearsimilarforfeedforwardneuralnetworks,convolutionalnetworks,andrecurrent\n",
      "           networksappliedtorealobjectrecognitionandnaturallanguageprocessingtasks.Sur-\n",
      "          prisingly,thesevisualizationsusuallydonotshowmanyconspicuousobstacles.Prior\n",
      "             tothesuccessofstochasticgradientdescentfortrainingverylargemodelsbeginningin\n",
      "             roughly2012,neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmore\n",
      "           nonconvexstructurethanisrevealedbytheseprojections.Theprimaryobstaclerevealed\n",
      "               bythisprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,\n",
      "              but,asindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepoint\n",
      "              readily.Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecost\n",
      "             function,perhapsbecauseofhighnoiseinthegradient,poorconditioningoftheHessian\n",
      "              matrixinthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visiblein\n",
      "            theﬁgureviaanindirectarcingpath.ImageadaptedwithpermissionfromGoodfellow\n",
      "  etal.().2015\n",
      "           alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\n",
      "            repelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\n",
      "         maybediﬀerentformorerealisticusesofgradientdescent.\n",
      "         ForNewton’smethod,saddlepointsclearlyconstituteaproblem.Gradient\n",
      "             descentisdesignedtomove“downhill”andisnotexplicitlydesignedtoseeka\n",
      "             criticalpoint.Newton’smethod,however,isdesignedtosolveforapointwherethe\n",
      "            gradientiszero.Withoutappropriatemodiﬁcation,itcanjumptoasaddlepoint.\n",
      "         Theproliferationofsaddlepointsinhigh-dimensionalspacespresumablyexplains\n",
      "          whysecond-ordermethodshavenotsucceededinreplacinggradientdescentfor\n",
      "        neuralnetworktraining. ()introduceda Dauphinetal.2014  saddle-freeNewton\n",
      "method        forsecond-orderoptimizationandshowedthatitimprovessigniﬁcantly\n",
      "           overthetraditionalversion.Second-ordermethodsremaindiﬃculttoscaletolarge\n",
      "            neuralnetworks,butthissaddle-freeapproachholdspromiseifitcanbescaled.\n",
      "2 8 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "\n",
      " \n",
      "\n",
      "            Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\n",
      "          recurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\n",
      "           fromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\n",
      "               highderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a\n",
      "             gradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe\n",
      "          optimizationworkthathasbeendone. FigureadaptedwithpermissionfromPascanu\n",
      "  etal.().2013\n",
      "            Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\n",
      "           points.Maximaaremuchlikesaddlepointsfromtheperspectiveofoptimization—\n",
      "          manyalgorithmsarenotattractedtothem,butunmodiﬁedNewton’smethod\n",
      "           is.Maximaofmanyclassesofrandomfunctionsbecomeexponentiallyrarein\n",
      "     high-dimensionalspace,justasminimado.\n",
      "            Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,\n",
      "            thegradientandtheHessianareallzero.Suchdegeneratelocationsposemajor\n",
      "           problemsforallnumericaloptimizationalgorithms.Inaconvexproblem,awide,\n",
      "            ﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization\n",
      "             problem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\n",
      "    8.2.4CliﬀsandExplodingGradients\n",
      "          Neuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\n",
      "            cliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3\n",
      "           largeweightstogether.Onthefaceofanextremelysteepcliﬀstructure, the\n",
      "           gradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ\n",
      "   thecliﬀstructurealtogether.\n",
      "             Thecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,\n",
      "2 8 5      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "          butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\n",
      "clipping             heuristicdescribedinsection .Thebasicideaistorecallthatthe 10.11.1\n",
      "            gradientspeciﬁesnottheoptimalstepsize,butonlytheoptimaldirectionwithin\n",
      "         aninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithmproposes\n",
      "            makingaverylargestep,thegradientclippingheuristicintervenestoreducethe\n",
      "              stepsize,makingitlesslikelytogooutsidetheregionwherethegradientindicates\n",
      "          thedirectionofapproximatelysteepestdescent.Cliﬀstructuresaremostcommon\n",
      "            inthecostfunctionsforrecurrentneuralnetworks,becausesuchmodelsinvolvea\n",
      "            multiplicationofmanyfactors,withonefactorforeachtimestep.Longtemporal\n",
      "       sequencesthusincuranextremeamountofmultiplication.\n",
      "  8.2.5Long-TermDependencies\n",
      "        Anotherdiﬃcultythatneuralnetworkoptimizationalgorithmsmustovercome\n",
      "  arises when thecomputational graph becom es extremely deep.Feedforward\n",
      "           networkswithmanylayershavesuchdeepcomputationalgraphs.Sodorecurrent\n",
      "          networks,describedinchapter,whichconstructverydeepcomputationalgraphs 10\n",
      "             byrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\n",
      "          sequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\n",
      " pronounceddiﬃculties.\n",
      "           Forexample,supposethatacomputationalgraphcontainsapaththatconsists\n",
      "     ofrepeatedlymultiplyingbyamatrixW .Aftert     steps,thisisequivalenttomul-\n",
      " tiplyingbyWt  .SupposethatW  hasaneigendecompositionW= Vdiag(λ)V− 1.\n",
      "         Inthissimplecase,itisstraightforwardtoseethat\n",
      "Wt=\n",
      " VλVdiag()− 1t = ()VdiagλtV− 1 . (8.11)\n",
      " Anyeigenvaluesλ i             thatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\n",
      "                aregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\n",
      "    vanishingandexplodinggradientproblem      referstothefactthatgradients\n",
      "        throughsuchagrapharealsoscaledaccordingtodiag(λ)t  .Vanishinggradients\n",
      "            makeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove\n",
      "           thecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ\n",
      "           structuresdescribedearlierthatmotivategradientclippingareanexampleofthe\n",
      "  explodinggradientphenomenon.\n",
      "   TherepeatedmultiplicationbyW       ateachtimestepdescribedhereisvery\n",
      "  similartothe powermethod        algorithmusedtoﬁndthelargesteigenvalueof\n",
      " amatrixW          andthecorrespondingeigenvector.Fromthispointofviewitis\n",
      "  notsurprisingthatxWt     willeventuallydiscardallcomponentsofx thatare\n",
      "       orthogonaltotheprincipaleigenvectorof.W\n",
      "2 8 6      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "     Recurrentnetworksusethesamematrix W     ateachtimestep,butfeedforward\n",
      "            networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\n",
      "      vanishingandexplodinggradientproblem(,). Sussillo2014\n",
      "          Wedeferfurtherdiscussionofthechallengesoftrainingrecurrentnetworks\n",
      "           untilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n",
      "  8.2.6InexactGradients\n",
      "          Mostoptimizationalgorithmsaredesignedwiththeassumptionthatwehave\n",
      "             accesstotheexactgradientorHessianmatrix.Inpractice,weusuallyhaveonly\n",
      "            anoisyorevenbiasedestimateofthesequantities.Nearlyeverydeeplearning\n",
      "           algorithmreliesonsampling-basedestimates,atleastinsofarasusingaminibatch\n",
      "      oftrainingexamplestocomputethegradient.\n",
      "            Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\n",
      "           Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\n",
      "            well.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostly\n",
      "             arisewiththemoreadvancedmodelswecoverinpart.Forexample,contrastive III\n",
      "          divergencegivesatechniqueforapproximatingthegradientoftheintractable\n",
      "    log-likelihoodofaBoltzmannmachine.\n",
      "         Variousneuralnetworkoptimizationalgorithmsaredesignedtoaccountfor\n",
      "            imperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\n",
      "            asurrogatelossfunctionthatiseasiertoapproximatethanthetrueloss.\n",
      "       8.2.7PoorCorrespondencebetweenLocalandGlobalStructure\n",
      "             Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\n",
      "              lossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J( θ )is\n",
      "     poorlyconditionedatthecurrentpoint θ  ,orif θ     liesonacliﬀ,orif θ  isasaddle\n",
      "          pointhidingtheopportunitytomakeprogressdownhillfromthegradient.\n",
      "              Itispossibletoovercomealltheseproblemsatasinglepointandstillperform\n",
      "             poorlyifthedirectionthatresultsinthemostimprovementlocallydoesnotpoint\n",
      "      towarddistantregionsofmuchlowercost.\n",
      "              Goodfellow 2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\n",
      "              thelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat 8.2\n",
      "              thelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\n",
      " mountain-shapedstructure.\n",
      "           Muchofresearchintothediﬃcultiesofoptimizationhasfocusedonwhether\n",
      "2 8 7      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "θJ ( ) θ\n",
      "              Figure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\n",
      "               notpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\n",
      "              eveniftherearenosaddlepointsorlocalminima.Thisexamplecostfunctioncontains\n",
      "              onlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyinthiscase\n",
      "                isbeinginitializedonthewrongsideofthe“mountain”andnotbeingabletotraverseit.\n",
      "         Inhigher-dimensionalspace,learningalgorithmscanoftencircumnavigatesuchmountains,\n",
      "              butthetrajectoryassociatedwithdoingsomaybelongandresultinexcessivetraining\n",
      "     time,asillustratedinﬁgure.8.2\n",
      "              trainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\n",
      "              practice,neuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1\n",
      "              showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\n",
      "            suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\n",
      " −logp(  y| x; θ         )canlackaglobalminimumpointandinsteadasymptotically\n",
      "            approachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith\n",
      "discreteyandp(  y| x        )providedbyasoftmax, thenegativelog-likelihoodcan\n",
      "             becomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\n",
      "              exampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof\n",
      "      zero.Likewise,amodelofrealvaluesp(  y| x) =N(y;f( θ) ,β− 1   )canhavenegative\n",
      "     log-likelihoodthatasymptotestonegativeinﬁnity—iff( θ    )isabletocorrectly\n",
      "      predictthevalueofalltrainingsety      targets,thelearningalgorithmwillincrease\n",
      "β              withoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization 8.4\n",
      "               toﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaor\n",
      " saddlepoints.\n",
      "           Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat\n",
      "           inﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n",
      "  oftheprocess.\n",
      "2 8 8      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\n",
      "          problemsthathavediﬃcultglobalstructure,ratherthanatdevelopingalgorithms\n",
      "   thatusenonlocalmoves.\n",
      "          Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor\n",
      "           trainingneuralnetworksarebasedonmakingsmalllocalmoves.Theprevious\n",
      "            sectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves\n",
      "              canbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe\n",
      "           objectivefunction,suchasitsgradient,onlyapproximately,withbiasorvariance\n",
      "              inourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\n",
      "              notdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually\n",
      "            abletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues,\n",
      "          suchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\n",
      "              thegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In\n",
      "       thesecases,localdescentwithstepsofsize     maydeﬁneareasonablyshortpath\n",
      "              tothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\n",
      "  stepsofsize  δ            .Inthesecases,localdescentmaydeﬁneapathtothesolution,\n",
      "             butthepathcontainsmanysteps,sofollowingitincursahighcomputationalcost.\n",
      "            Sometimeslocalinformationprovidesusnoguide,suchaswhenthefunctionhas\n",
      "                awideﬂatregion,orifwemanagetolandexactlyonacriticalpoint(usuallythis\n",
      "           latterscenarioonlyhappenstomethodsthatsolveexplicitlyforcriticalpoints,\n",
      "              suchasNewton’smethod).Inthesecases,localdescentdoesnotdeﬁneapathto\n",
      "                 asolutionatall.Inothercases,localmovescanbetoogreedyandleadusalonga\n",
      "               paththatmovesdownhillbutawayfromanysolution,asinﬁgure,oralongan 8.4\n",
      "             unnecessarilylongtrajectorytothesolution,asinﬁgure.Currently,wedonot 8.2\n",
      "           understandwhichoftheseproblemsaremostrelevanttomakingneuralnetwork\n",
      "         optimizationdiﬃcult,andthisisanactiveareaofresearch.\n",
      "             Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe\n",
      "             avoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\n",
      "               byapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\n",
      "         withinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosing\n",
      "        goodinitialpointsfortraditionaloptimizationalgorithmstouse.\n",
      "    8.2.8TheoreticalLimitsofOptimization\n",
      "            Severaltheoreticalresultsshowthattherearelimitsontheperformanceofany\n",
      "          optimizationalgorithmwemightdesignforneuralnetworks( , BlumandRivest\n",
      "          1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\n",
      "         littlebearingontheuseofneuralnetworksinpractice.\n",
      "2 8 9      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            Sometheoreticalresultsapplyonlywhentheunitsofaneuralnetworkoutput\n",
      "         discretevalues.Mostneuralnetworkunitsoutputsmoothlyincreasingvalues\n",
      "         thatmakeoptimizationvialocalsearchfeasible. Sometheoreticalresultsshow\n",
      "              thatthereexistproblemclassesthatareintractable,butitcanbediﬃculttotell\n",
      "            whetheraparticularproblemfallsintothatclass.Otherresultsshowthatﬁnding\n",
      "                 asolutionforanetworkofagivensizeisintractable,butinpracticewecanﬁnda\n",
      "            solutioneasilybyusingalargernetworkforwhichmanymoreparametersettings\n",
      "           correspondtoanacceptablesolution.Moreover,inthecontextofneuralnetwork\n",
      "             training,weusuallydonotcareaboutﬁndingtheexactminimumofafunction,\n",
      "            butseekonlytoreduceitsvaluesuﬃcientlytoobtaingoodgeneralizationerror.\n",
      "         Theoreticalanalysisofwhetheranoptimizationalgorithmcanaccomplishthis\n",
      "           goalisextremelydiﬃcult.Developingmorerealisticboundsontheperformanceof\n",
      "         optimizationalgorithmsthereforeremainsanimportantgoalformachinelearning\n",
      "research.\n",
      "  8.3BasicAlgorithms\n",
      "          Wehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\n",
      "            followsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\n",
      "           considerablybyusingstochasticgradientdescenttofollowthegradientofrandomly\n",
      "          selectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n",
      "   8.3.1StochasticGradientDescent\n",
      "           Stochasticgradientdescent(SGD)anditsvariantsareprobablythemostused\n",
      "          optimizationalgorithmsformachinelearningingeneralandfordeeplearning\n",
      "            inparticular. Asdiscussedinsection,itispossibletoobtainanunbiased 8.1.3\n",
      "            estimateofthegradientbytakingtheaveragegradientonaminibatchofm\n",
      "      examplesdrawni.i.dfromthedata-generatingdistribution.\n",
      "           Algorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\n",
      "            AcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\n",
      "        havedescribedSGDasusingaﬁxedlearningrate      .Inpractice,itisnecessaryto\n",
      "             graduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\n",
      "    atiterationask k.\n",
      "            ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\n",
      "  randomsamplingofm          trainingexamples)thatdoesnotvanishevenwhenwearrive\n",
      "             ataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\n",
      "  smallandthen 0         whenweapproachandreachaminimumusingbatchgradient\n",
      "2 9 0      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "      Algorithm8.1Stochasticgradientdescent(SGD)update\n",
      "    Require:Learningrateschedule 1 , 2   ,...\n",
      "   Require:Initialparameterθ\n",
      "  k←1\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1 )     ,...,x( ) m}with\n",
      "  correspondingtargetsy( ) i.\n",
      "  Computegradientestimate: ˆ g←1\n",
      "m∇ θ\n",
      "iLf((x( ) i  ;)θ,y( ) i)\n",
      "      Applyupdate:θθ←− kˆg\n",
      "    kk←+1\n",
      " endwhile\n",
      "            descent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient\n",
      "       conditiontoguaranteeconvergenceofSGDisthat\n",
      "∞ \n",
      "k =1 k  = and∞, (8.12)\n",
      "∞ \n",
      "k =12\n",
      "k  <.∞ (8.13)\n",
      "             Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration:τ\n",
      " k   = (1 )−α 0 +α τ (8.14)\n",
      "  withα=k\n",
      "τ           .Afteriteration,itiscommontoleaveconstant. τ \n",
      "              Thelearningratemaybechosenbytrialanderror,butitisusuallybest\n",
      "             tochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\n",
      "                functionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\n",
      "           subjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,\n",
      "    theparameterstochooseare 0, τ ,andτ .Usuallyτ      maybesettothenumberof\n",
      "            iterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\n",
      " τ         shouldbesettoroughly1percentthevalueof 0      .Themainquestionishowto\n",
      "set 0              .Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthe\n",
      "          costfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif\n",
      "             trainingwithastochasticcostfunction,suchasthecostfunctionarisingfromthe\n",
      "               useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\n",
      "              initiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\n",
      "             Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\n",
      "2 9 1      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             ﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\n",
      "               aftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst\n",
      "            severaliterationsandusealearningratethatishigherthanthebest-performing\n",
      "             learningrateatthistime,butnotsohighthatitcausessevereinstability.\n",
      "           ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\n",
      "            basedoptimizationisthatcomputationtimeperupdatedoesnotgrowwiththe\n",
      "          numberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\n",
      "            oftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\n",
      "              convergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas\n",
      "    processedtheentiretrainingset.\n",
      "            Tostudytheconvergencerateofanoptimizationalgorithmitiscommonto\n",
      " measurethe  excesserror J( θ) −min θ J( θ       ),whichistheamountbywhichthe\n",
      "            currentcostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedto\n",
      "      aconvexproblem,theexcesserroris O(1√\n",
      "k) after k     iterations,whileinthestrongly\n",
      "   convexcase,itis O(1\n",
      "k        ).Theseboundscannotbeimprovedunlessextraconditions\n",
      "          areassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\n",
      "          gradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao\n",
      "        1945)statesthatgeneralizationerrorcannotdecreasefasterthan O(1\n",
      "k ).Bottou\n",
      "            andBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\n",
      "      anoptimizationalgorithmthatconvergesfasterthan O(1\n",
      "k   )formachinelearning\n",
      "       tasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the\n",
      "        asymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\n",
      "              hasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDto\n",
      "           makerapidinitialprogresswhileevaluatingthegradientforveryfewexamples\n",
      "          outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\n",
      "             theremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost\n",
      "      intheconstantfactorsobscuredbythe O(1\n",
      "k     )asymptoticanalysis.Onecanalso\n",
      "            tradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually\n",
      "        increasingtheminibatchsizeduringthecourseoflearning.\n",
      "       FormoreinformationonSGD,see (). Bottou1998\n",
      " 8.3.2Momentum\n",
      "         Whilestochasticgradientdescentremainsapopularoptimizationstrategy,learning\n",
      "            withitcansometimesbeslow.Themethodofmomentum(Polyak1964,)is\n",
      "            designedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\n",
      "        consistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\n",
      "           anexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\n",
      "2 9 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "     − − − 3020100 1020−30−20−1001020\n",
      "            Figure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\n",
      "            Hessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\n",
      "             overcomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\n",
      "            functionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\n",
      "             contoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis\n",
      "               function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\n",
      "              descentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\n",
      "             lookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\n",
      "             thecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\n",
      "              narrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6\n",
      "  descentwithoutmomentum.\n",
      "           intheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5\n",
      "      Formally,themomentumalgorithmintroducesavariablev   thatplaystherole\n",
      "            ofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough\n",
      "            parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\n",
      "   negativegradient.Thename m o m e n t um      derivesfromaphysicalanalogy,in\n",
      "            whichthenegativegradientisaforcemovingaparticlethroughparameterspace,\n",
      "            accordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.\n",
      "            Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\n",
      "           mayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0,1)\n",
      "         determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.\n",
      "     Theupdateruleisgivenby\n",
      "    vv←α−∇ θ\n",
      "1\n",
      "mm \n",
      "i =1L((fx( ) i  ;)θ,y( ) i)\n",
      " , (8.15)\n",
      "     θθv← +. (8.16)\n",
      "2 9 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "       Algorithm8.2Stochasticgradientdescent(SGD)withmomentum\n",
      "      Require:Learningrate,momentumparameter  α\n",
      "      Require:Initialparameter,initialvelocity θ v\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1 )     ,...,x( ) m}with\n",
      "  correspondingtargetsy( ) i.\n",
      "    Computegradientestimate:g←1\n",
      "m∇ θ\n",
      "iLf((x( ) i  ;)θ,y( ) i).\n",
      "       Computevelocityupdate: .vvg←α−\n",
      "      Applyupdate: .θθv← +\n",
      " endwhile\n",
      " Thevelocityv    accumulatesthegradientelements∇ θ1\n",
      "mm\n",
      "i =1L((fx( ) i  ;)θ,y( ) i)\n",
      ".\n",
      " Thelargerα  isrelativeto        ,themorepreviousgradientsaﬀectthecurrentdirection.\n",
      "         TheSGDalgorithmwithmomentumisgiveninalgorithm.8.2\n",
      "             Previously,thesizeofthestepwassimplythenormofthegradientmultiplied\n",
      "               bythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\n",
      "             alignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\n",
      "           gradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\n",
      " observesgradientg        ,thenitwillaccelerateinthedirectionof−g   ,untilreachinga\n",
      "        terminalvelocitywherethesizeofeachstepis\n",
      "||||g\n",
      "  1−α . (8.17)\n",
      "            Itisthushelpfultothinkofthemomentumhyperparameterintermsof1\n",
      "1 − α .For\n",
      "example,α= 0.          9correspondstomultiplyingthemaximumspeedbyrelativeto 10\n",
      "   thegradientdescentalgorithm.\n",
      "  Commonvaluesofα    usedinpracticeinclude0. 50,.  9 0,and.  99.Likethe\n",
      " learningrate,α           mayalsobeadaptedovertime.Typicallyitbeginswithasmall\n",
      "     valueandislaterraised.Adaptingα      overtimeislessimportantthanshrinking\n",
      " overtime.\n",
      "           Wecanviewthemomentumalgorithmassimulatingaparticlesubjectto\n",
      "         continuous-timeNewtoniandynamics.Thephysicalanalogycanhelpbuildintuition\n",
      "        forhowthemomentumandgradientdescentalgorithmsbehave.\n",
      "            Thepositionoftheparticleatanypointintimeisgivenbyθ(t  ).Theparticle\n",
      "          experiencesnetforce.Thisforcecausestheparticletoaccelerate: f()t\n",
      "f() =t∂2\n",
      "∂t2 θ()t. (8.18)\n",
      "2 9 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           Ratherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,\n",
      "    wecanintroducethevariable v(t        )representingthevelocityoftheparticleattime\n",
      "          tandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:\n",
      "v() =t∂\n",
      "∂t θ()t, (8.19)\n",
      "f() =t∂\n",
      "∂t v()t. (8.20)\n",
      "          Themomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia\n",
      "         numericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations\n",
      "           isEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby\n",
      "            theequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.\n",
      "            Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare\n",
      "             theforces?Oneforceisproportionaltothenegativegradientofthecostfunction:\n",
      "− ∇ θJ( θ           ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\n",
      "            Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\n",
      "          gradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\n",
      "               usesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\n",
      "              asbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\n",
      "             steeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\n",
      "      untilitbeginstogouphillagain.\n",
      "               Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\n",
      "             thentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\n",
      "               onesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\n",
      "            assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\n",
      "   otherforce,proportionalto − v(t      ).Inphysicsterminology,thisforcecorresponds\n",
      "              toviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas\n",
      "            syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\n",
      "    convergetoalocalminimum.\n",
      "   Whydoweuse − v(t         )andviscousdraginparticular? Partofthereasonto\n",
      "use − v(t          )ismathematicalconvenience—anintegerpowerofthevelocityiseasy\n",
      "              toworkwith.Yetotherphysicalsystemshaveotherkindsofdragbasedonother\n",
      "            integerpowersofthevelocity.Forexample,aparticletravelingthroughtheair\n",
      "           experiencesturbulentdrag,withforceproportionaltothesquareofthevelocity,\n",
      "            whileaparticlemovingalongthegroundexperiencesdryfriction,withaforce\n",
      "           ofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\n",
      "             proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\n",
      "               small.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\n",
      "            withanonzeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\n",
      "2 9 5      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             willmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\n",
      "  pointgrowinglikeO( logt          ).Wemustthereforeusealowerpowerofthevelocity.\n",
      "               Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\n",
      "               Whentheforceduetothegradientofthecostfunctionissmallbutnonzero,the\n",
      "              constantforceduetofrictioncancausetheparticletocometorestbeforereaching\n",
      "            alocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough\n",
      "             thatthegradientcancontinuetocausemotionuntilaminimumisreached,but\n",
      "           strongenoughtopreventmotionifthegradientdoesnotjustifymoving.\n",
      "  8.3.3NesterovMomentum\n",
      "            Sutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\n",
      "         inspiredbyNesterov’sacceleratedgradientmethod( ,,).The Nesterov19832004\n",
      "       updaterulesinthiscasearegivenby\n",
      "    vv←α−∇ θ\n",
      "1\n",
      "mm \n",
      "i =1L\n",
      "fx(( ) i    ;+ )θαv,y( ) i\n",
      " , (8.21)\n",
      "     θθv← +, (8.22)\n",
      "  wheretheparametersαand        playasimilarroleasinthestandardmomentum\n",
      "        method.ThediﬀerencebetweenNesterovmomentumandstandardmomentum\n",
      "           iswherethegradientisevaluated.WithNesterovmomentum,thegradientis\n",
      "           evaluatedafterthecurrentvelocityisapplied.ThusonecaninterpretNesterov\n",
      "          momentumasattemptingtoaddacorrectionfactor tothestandardmethod\n",
      "        ofmomentum.Thecomplete Nesterovmomentumalgorithmispresentedin\n",
      " algorithm.8.3\n",
      "           Intheconvexbatchgradientcase,Nesterovmomentumbringstherateof\n",
      "     convergenceoftheexcesserrorfromO(1/k )(afterk steps)toO(1/k2  )asshown\n",
      "      byNesterov 1983().Unfortunately, inthestochasticgradientcase, Nesterov\n",
      "       momentumdoesnotimprovetherateofconvergence.\n",
      "   8.4ParameterInitializationStrategies\n",
      "            Someoptimizationalgorithmsarenotiterativebynatureandsimplysolvefora\n",
      "          solutionpoint.Otheroptimizationalgorithmsareiterativebynaturebut,when\n",
      "           appliedtotherightclassofoptimizationproblems,convergetoacceptablesolutions\n",
      "           inanacceptableamountoftimeregardlessofinitialization.Deeplearningtraining\n",
      "          algorithmsusuallydonothaveeitheroftheseluxuries. Trainingalgorithmsfor\n",
      "2 9 6      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "        Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\n",
      "      Require:Learningrate,momentumparameter  α\n",
      "      Require:Initialparameter,initialvelocity θ v\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1)     ,...,x() m}with\n",
      "  correspondinglabelsy() i.\n",
      "  Applyinterimupdate:˜    θθv← +α.\n",
      "      Computegradient(atinterimpoint):g←1\n",
      "m∇˜ θ\n",
      "iLf((x() i;˜ θy),() i).\n",
      "       Computevelocityupdate: .vvg←α−\n",
      "      Applyupdate: .θθv← +\n",
      " endwhile\n",
      "             deeplearningmodelsareusuallyiterativeandthusrequiretheusertospecifysome\n",
      "            initialpointfromwhichtobegintheiterations.Moreover,trainingdeepmodelsis\n",
      "            asuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedbythechoice\n",
      "          ofinitialization.Theinitialpointcandeterminewhetherthealgorithmconverges\n",
      "            atall,withsomeinitialpointsbeingsounstablethatthealgorithmencounters\n",
      "          numericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,theinitial\n",
      "           pointcandeterminehowquicklylearningconvergesandwhetheritconvergesto\n",
      "              apointwithhighorlowcost.Also,pointsofcomparablecostcanhavewildly\n",
      "          varyinggeneralizationerror,andtheinitialpointcanaﬀectthegeneralization\n",
      " aswell.\n",
      "        Moderninitializationstrategiesaresimpleandheuristic.Designingimproved\n",
      "          initializationstrategiesisadiﬃculttaskbecauseneuralnetworkoptimizationis\n",
      "           notyetwellunderstood.Mostinitializationstrategiesarebasedonachievingsome\n",
      "             nicepropertieswhenthenetworkisinitialized.However,wedonothaveagood\n",
      "          understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\n",
      "           afterlearningbeginstoproceed. Afurtherdiﬃcultyisthatsomeinitialpoints\n",
      "           maybebeneﬁcialfromtheviewpointofoptimizationbutdetrimentalfromthe\n",
      "          viewpointofgeneralization.Ourunderstandingofhowtheinitialpointaﬀects\n",
      "            generalizationisespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect\n",
      "  theinitialpoint.\n",
      "           Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitial\n",
      "          parametersneedto“breaksymmetry”betweendiﬀerentunits.Iftwohidden\n",
      "            unitswiththesameactivationfunctionareconnectedtothesameinputs,then\n",
      "           theseunitsmusthavediﬀerentinitialparameters. Iftheyhavethesameinitial\n",
      "          parameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\n",
      "               andmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\n",
      "2 9 7      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           modelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent\n",
      "             updatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually\n",
      "              besttoinitializeeachunittocomputeadiﬀerentfunctionfromalltheotherunits.\n",
      "               Thismayhelptomakesurethatnoinputpatternsarelostinthenullspace\n",
      "             offorwardpropagationandthatnogradientpatternsarelostinthenullspace\n",
      "           ofback-propagation. Thegoalofhavingeachunitcomputeadiﬀerentfunction\n",
      "          motivatesrandominitializationoftheparameters.Wecouldexplicitlysearchfor\n",
      "              alargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,but\n",
      "             thisoftenincursanoticeablecomputationalcost.Forexample,ifwehaveatmost\n",
      "           asmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalizationonan\n",
      "            initialweightmatrixandbeguaranteedthateachunitwouldcomputeavery\n",
      "          diﬀerentfunctionfromeachotherunit.Randominitializationfromahigh-entropy\n",
      "         distributionoverahigh-dimensionalspaceiscomputationallycheaperandunlikely\n",
      "           toassignanyunitstocomputethesamefunctionaseachother.\n",
      "            Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\n",
      "        initializeonlytheweightsrandomly.Extraparameters—forexample,parameters\n",
      "          encodingtheconditionalvarianceofaprediction—areusuallysettoheuristically\n",
      "      chosenconstantsmuchlikethebiasesare.\n",
      "           Wealmostalwaysinitializealltheweightsinthe modeltovaluesdrawn\n",
      "           randomlyfromaGaussianoruniformdistribution.ThechoiceofGaussianor\n",
      "            uniformdistributiondoesnotseemtomattermuchbuthasnotbeenexhaustively\n",
      "             studied.Thescaleoftheinitialdistribution,however,doeshavealargeeﬀecton\n",
      "             boththeoutcomeoftheoptimizationprocedureandtheabilityofthenetworkto\n",
      "generalize.\n",
      "         Largerinitialweightswillyieldastrongersymmetry-breakingeﬀect,helping\n",
      "             toavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor\n",
      "          back-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe\n",
      "           matrixresultinlargeroutputsofmatrixmultiplication.Initialweightsthatare\n",
      "           toolargemay,however,resultinexplodingvaluesduringforwardpropagationor\n",
      "         back-propagation.Inrecurrentnetworks,largeweightscanalsoresultin c ha o s\n",
      "           (suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\n",
      "         ofthedeterministicforwardpropagationprocedureappearsrandom).Tosome\n",
      "          extent,theexplodinggradientproblemcanbemitigatedbygradientclipping\n",
      "           (thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\n",
      "            Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\n",
      "          tosaturate,causingcompletelossofgradientthroughsaturatedunits.These\n",
      "         competingfactorsdeterminetheidealinitialscaleoftheweights.\n",
      "         Theperspectivesofregularizationandoptimizationcangiveverydiﬀerent\n",
      "2 9 8      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "          insightsintohowweshouldinitializeanetwork.Theoptimizationperspective\n",
      "           suggeststhattheweightsshouldbelargeenoughtopropagateinformationsuc-\n",
      "         cessfully,butsomeregularizationconcernsencouragemakingthemsmaller.The\n",
      "           useofanoptimizationalgorithm,suchasstochasticgradientdescent,thatmakes\n",
      "              smallincrementalchangestotheweightsandtendstohaltinareasthatarenearer\n",
      "              totheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,\n",
      "            orduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa\n",
      "            priorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall\n",
      "            fromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\n",
      "            decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystopping\n",
      "               isnotthesameasweightdecay,butitdoesprovidealooseanalogyforthinking\n",
      "           abouttheeﬀectofinitialization.Wecanthinkofinitializingtheparameters θto\n",
      "θ 0       asbeingsimilartoimposingaGaussianpriorp( θ  )withmean θ 0  .Fromthis\n",
      "       pointofview,itmakessensetochoose θ 0         tobenear0.Thispriorsaysthatitis\n",
      "              morelikelythatunitsdonotinteractwitheachotherthanthattheydointeract.\n",
      "            Unitsinteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesa\n",
      "            strongpreferenceforthemtointeract.Ontheotherhand,ifweinitialize θ 0to\n",
      "            largevalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,\n",
      "    andhowtheyshouldinteract.\n",
      "            Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\n",
      "           heuristicistoinitializetheweightsofafullyconnectedlayerwithm  inputsand\n",
      "n      outputsbysamplingeachweightfromU(−1√m,1√m    ),whileGlorotandBengio\n",
      "     ()suggestusingthe 2010 normalizedinitialization\n",
      "W i , j ∼U\n",
      "−\n",
      "6\n",
      "  mn+,\n",
      "6\n",
      "  mn+\n",
      " . (8.23)\n",
      "           Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\n",
      "             alllayerstohavethesameactivationvarianceandthegoalofinitializingall\n",
      "            layerstohavethesamegradientvariance.Theformulaisderivedusingthe\n",
      "           assumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\n",
      "        withnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\n",
      "            butmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\n",
      " nonlinearcounterparts.\n",
      "          Saxe 2013etal.()recommend initializingtorandomorthogonalmatrices,with\n",
      "    acarefullychosenscalingorgainfactorg     thataccountsforthenonlinearityapplied\n",
      "              ateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof\n",
      "          nonlinearactivationfunctions.Thisinitializationschemeisalsomotivatedbya\n",
      "            modelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\n",
      "2 9 9      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            Undersuchamodel,thisinitializationschemeguaranteesthatthetotalnumberof\n",
      "         trainingiterationsrequiredtoreachconvergenceisindependentofdepth.\n",
      "   Increasingthescalingfactor g       pushesthenetworktowardtheregimewhere\n",
      "           activationsincreaseinnormastheypropagateforwardthroughthenetworkand\n",
      "           gradientsincreaseinnormastheypropagatebackward. ()showedthat Sussillo2014\n",
      "              settingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas1,000layers,\n",
      "           withoutneedingtouseorthogonalinitializations.Akeyinsightofthisapproach\n",
      "            isthatinfeedforwardnetworks,activationsandgradientscangroworshrinkon\n",
      "           eachstepofforwardorback-propagation,followingarandomwalkbehavior.This\n",
      "             isbecausefeedforwardnetworksuseadiﬀerentweightmatrixateachlayer.Ifthis\n",
      "           randomwalkistunedtopreservenorms,thenfeedforwardnetworkscanmostly\n",
      "           avoidthevanishingandexplodinggradientsproblemthatariseswhenthesame\n",
      "           weightmatrixisusedateachstep,asdescribedinsection.8.2.5\n",
      "           Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\n",
      "           optimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay\n",
      "            beusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe\n",
      "           normofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\n",
      "            atinitializationmaynotpersistafterlearninghasbeguntoproceed.Third,the\n",
      "          criteriamightsucceedatimprovingthespeedofoptimizationbutinadvertently\n",
      "             increasegeneralizationerror.Inpractice,weusuallyneedtotreatthescaleofthe\n",
      "           weightsasahyperparameterwhoseoptimalvalueliessomewhereroughlynearbut\n",
      "      notexactlyequaltothetheoreticalpredictions.\n",
      "              Onedrawbacktoscalingrulesthatsetalltheinitialweightstohavethesame\n",
      "   standarddeviation,suchas1√m       ,isthateveryindividualweightbecomesextremely\n",
      "          smallwhenthelayersbecomelarge. ()introducedanalternative Martens2010\n",
      "  initializationschemecalled sparseinitialization      ,inwhicheachunitisinitialized\n",
      "  tohaveexactly k             nonzeroweights.Theideaistokeepthetotalamountofinputto\n",
      "       theunitindependentfromthenumberofinputs m    withoutmakingthemagnitude\n",
      "     ofindividualweightelementsshrinkwith m     .Sparseinitializationhelpstoachieve\n",
      "           morediversityamongtheunitsatinitializationtime.However,italsoimposes\n",
      "              averystrongpriorontheweightsthatarechosentohavelargeGaussianvalues.\n",
      "             Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”largevalues,\n",
      "            thisinitializationschemecancauseproblemsforunits,suchasmaxoutunits,that\n",
      "          haveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.\n",
      "             Whencomputationalresourcesallowit,itisusuallyagoodideatotreatthe\n",
      "              initialscaleoftheweightsforeachlayerasahyperparameter,andtochoosethese\n",
      "          scalesusingahyperparametersearchalgorithmdescribedinsection ,such 11.4.2\n",
      "            asrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\n",
      "3 0 0      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           canalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor\n",
      "               thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\n",
      "             lookattherangeorstandarddeviationofactivationsorgradientsonasingle\n",
      "              minibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\n",
      "          minibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\n",
      "          Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptablysmallactivationsand\n",
      "            increasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\n",
      "              initialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\n",
      "               usefultolookattherangeorstandarddeviationofthegradientsaswellasthe\n",
      "          activations. Thisprocedurecaninprinciplebeautomatedandisgenerallyless\n",
      "        computationallycostlythanhyperparameteroptimizationbasedonvalidationset\n",
      "               errorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona\n",
      "              singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\n",
      "           set.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore\n",
      "       formallyandstudiedby (). MishkinandMatas2015\n",
      "  So far we have focused on the i nitialization ofthe weights.Fortunately,\n",
      "      initializationofotherparametersistypicallyeasier.\n",
      "           Theapproachforsettingthebiasesmustbecoordinatedwiththeapproach\n",
      "             forsettingtheweights.Settingthebiasestozeroiscompatiblewithmostweight\n",
      "             initializationschemes.Thereareafewsituationswherewemaysetsomebiasesto\n",
      " nonzerovalues:\n",
      "•                 Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto\n",
      "             obtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\n",
      "             theinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\n",
      "              onlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation\n",
      "            functionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\n",
      "            Forexample,iftheoutputisadistributionoverclasses,andthisdistribution\n",
      "          isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\n",
      " byelement c i  ofsomevector c       ,thenwecansetthebiasvector b bysolving\n",
      " theequationsoftmax ( b) = c         .Thisappliesnotonlytoclassiﬁersbutalsoto\n",
      "           modelswewillencounterinPart,suchasautoencodersandBoltzmann III\n",
      "          machines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\n",
      "data x              ,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\n",
      "     matchthemarginaldistributionover. x\n",
      "•           Sometimeswe maywanttochoosethebiastoavoid causingtoomuch\n",
      "            saturationatinitialization.Forexample,wemaysetthebiasofaReLU\n",
      "             hiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\n",
      "          Thisapproachisnotcompatiblewithweightinitializationschemesthatdo\n",
      "3 0 1      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           notexpectstronginputfromthebiasesthough.Forexample, itisnot\n",
      "        recommended forusewithrandomwalkinitialization(,). Sussillo2014\n",
      "•             Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\n",
      "         function.Insuchsituations,wehaveaunitwithoutputu  andanotherunit\n",
      " h∈[0,         1],andtheyaremultipliedtogethertoproduceanoutputuh. We\n",
      " canviewh     asagatethatdetermineswhether  uhu≈or uh≈ 0. Inthese\n",
      "       situations,wewanttosetthebiasforh sothat h≈     1mostofthetimeat\n",
      " initialization.Otherwiseu        doesnothaveachancetolearn.Forexample,\n",
      "              Jozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\n",
      "      theLSTMmodel,describedinsection.10.10\n",
      "           Anothercommontypeofparameterisavarianceorprecisionparameter.For\n",
      "          example,wecanperformlinearregressionwithaconditionalvarianceestimate\n",
      "  usingthemodel\n",
      "    py y (|Nx) = (|wT    x+1)b,/β, (8.24)\n",
      "whereβ          isaprecisionparameter.Wecanusuallyinitializevarianceorprecision\n",
      "             parametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\n",
      "               enoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,\n",
      "              thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\n",
      "             thevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\n",
      "          Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-\n",
      "            ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\n",
      "              strategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\n",
      "           theparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\n",
      "           Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\n",
      "           supervisedtrainingonanunrelatedtaskcansometimesyieldaninitializationthat\n",
      "          oﬀersfasterconvergencethanarandominitialization.Someoftheseinitialization\n",
      "         strategiesmayyieldfasterconvergenceandbettergeneralizationbecausethey\n",
      "           encodeinformationaboutthedistributionintheinitialparametersofthemodel.\n",
      "           Othersapparentlyperformwellprimarilybecausetheysettheparameterstohave\n",
      "             therightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.\n",
      "     8.5AlgorithmswithAdaptiveLearningRates\n",
      "           Neuralnetworkresearchershavelongrealizedthatthelearningrateisreliably\n",
      "           oneofthemostdiﬃculttosethyperparameters becauseitsigniﬁcantlyaﬀects\n",
      "              modelperformance.Aswediscussinsectionsand,thecostisoftenhighly 4.38.2\n",
      "3 0 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           sensitivetosomedirectionsinparameterspaceandinsensitivetoothers.The\n",
      "            momentumalgorithmcanmitigatetheseissuessomewhat,butitdoessoatthe\n",
      "            expenseofintroducinganotherhyperparameter.Inthefaceofthis,itisnatural\n",
      "               toaskifthereisanotherway.Ifwebelievethatthedirectionsofsensitivityare\n",
      "              somewhataxisaligned,itcanmakesensetouseaseparatelearningrateforeach\n",
      "          parameterandautomaticallyadapttheselearningratesthroughoutthecourseof\n",
      "learning.\n",
      "         The algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\n",
      "          toadaptingindividuallearningratesformodelparametersduringtraining.The\n",
      "               approachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\n",
      "             toagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\n",
      "           increase.Ifthatpartialderivativechangessign,thenthelearningrateshould\n",
      "              decrease.Ofcourse,thiskindofrulecanonlybeappliedtofullbatchoptimization.\n",
      "          Morerecently,anumberofincremental(orminibatch-based)methodshave\n",
      "            beenintroducedthatadaptthelearningratesofmodelparameters.Inthissection,\n",
      "       webrieﬂyreviewafewofthesealgorithms.\n",
      " 8.5.1AdaGrad\n",
      "TheAdaGrad         algorithm,showninalgorithm,individuallyadaptsthelearning 8.4\n",
      "            ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\n",
      "               rootofthesumofallthehistoricalsquaredvaluesofthegradient(Duchietal.,\n",
      "        2011).Theparameterswith thelargestpartial derivativeof theloss havea\n",
      "          correspondinglyrapiddecreaseintheirlearningrate,whileparameterswithsmall\n",
      "            partialderivativeshavearelativelysmalldecreaseintheirlearningrate.Thenet\n",
      "            eﬀectisgreaterprogressinthemoregentlyslopeddirectionsofparameterspace.\n",
      "          Inthecontextofconvexoptimization,theAdaGradalgorithmenjoyssome\n",
      "        desirabletheoreticalproperties.Empirically,however,fortrainingdeepneural\n",
      "          networkmodels,theaccumulationofsquaredgradientsfromthebeginningof\n",
      "            trainingcanresultinaprematureandexcessivedecreaseintheeﬀectivelearning\n",
      "           rate.AdaGradperformswellforsomebutnotalldeeplearningmodels.\n",
      " 8.5.2RMSProp\n",
      "TheRMSProp         algorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012\n",
      "          thenonconvexsettingbychangingthegradientaccumulationintoanexponentially\n",
      "           weightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenappliedto\n",
      "             aconvexfunction.Whenappliedtoanonconvexfunctiontotrainaneuralnetwork,\n",
      "3 0 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "          thelearningtrajectorymaypassthroughmanydiﬀerentstructuresandeventually\n",
      "              arriveataregionthatisalocallyconvexbowl.AdaGradshrinksthelearningrate\n",
      "             accordingtotheentirehistoryofthesquaredgradientandmayhavemadethe\n",
      "            learningratetoosmallbeforearrivingatsuchaconvexstructure.RMSPropuses\n",
      "            anexponentiallydecayingaveragetodiscardhistoryfromtheextremepastsothat\n",
      "                itcanconvergerapidlyafterﬁndingaconvexbowl,asifitwereaninstanceofthe\n",
      "     AdaGradalgorithminitializedwithinthatbowl.\n",
      "    Algorithm8.4TheAdaGradalgorithm\n",
      "    Require:Globallearningrate\n",
      "   Require:Initialparameterθ\n",
      "     Require:Smallconstant,perhapsδ 10− 7   ,fornumericalstability\n",
      "     Initializegradientaccumulationvariabler= 0\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1 )     ,...,x( ) m}with\n",
      "  correspondingtargetsy( ) i.\n",
      "   Computegradient:g←1\n",
      "m∇ θ\n",
      "iLf((x( ) i  ;)θ,y( ) i).\n",
      "         Accumulatesquaredgradient: . rrgg←+\n",
      "  Computeupdate:∆  θ←−\n",
      "δ +√r g     .(Divisionandsquarerootapplied\n",
      "element-wise)\n",
      "      Applyupdate: .θθθ← +∆\n",
      " endwhile\n",
      "    Algorithm8.5TheRMSPropalgorithm\n",
      "       Require:Globallearningrate,decayrate  ρ\n",
      "   Require:Initialparameterθ\n",
      "Require: Smallconstantδ  , usually10− 6    , usedtostabilize divisionbysmall\n",
      "numbers\n",
      "    Initializeaccumulationvariablesr= 0\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1 )     ,...,x( ) m}with\n",
      "  correspondingtargetsy( ) i.\n",
      "   Computegradient:g←1\n",
      "m∇ θ\n",
      "iLf((x( ) i  ;)θ,y( ) i).\n",
      "           Accumulatesquaredgradient: . rrgg ←ρ+(1 )−ρ\n",
      "   Computeparameterupdate:∆θ=−√\n",
      "δ + rg .(1√\n",
      "δ + r appliedelement-wise)\n",
      "      Applyupdate: .θθθ← +∆\n",
      " endwhile\n",
      "3 0 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            RMSPropisshowninitsstandardforminalgorithmandcombinedwith 8.5\n",
      "           Nesterovmomentuminalgorithm.ComparedtoAdaGrad,theuseofthe 8.6\n",
      "     movingaverageintroducesanewhyperparameter,ρ     ,thatcontrolsthelengthscale\n",
      "   ofthemovingaverage.\n",
      "           Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\n",
      "            timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\n",
      "        optimizationmethodsbeingemployedroutinelybydeeplearningpractitioners.\n",
      " 8.5.3Adam\n",
      "Adam           ( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\n",
      "          algorithmandispresentedinalgorithm.Thename“Adam” derivesfrom 8.7\n",
      "            thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis\n",
      "            perhapsbestseenasavariantonthecombinationofRMSPropandmomentum\n",
      "          withafewimportantdistinctions.First,inAdam,momentumisincorporated\n",
      "           directlyasanestimateoftheﬁrst-ordermoment(withexponentialweighting)of\n",
      "            thegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\n",
      "           applymomentumtotherescaledgradients.Theuseofmomentumincombination\n",
      "           withrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n",
      "           biascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum\n",
      "          term)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\n",
      "            attheorigin(seealgorithm).RMSPropalsoincorporatesanestimateofthe 8.7\n",
      "      Algorithm8.6RMSPropalgorithmwithNesterovmomentum\n",
      "          Require:Globallearningrate,decayrate,momentumcoeﬃcient  ρ α\n",
      "      Require:Initialparameter,initialvelocity θ v\n",
      "    Initializeaccumulationvariabler= 0\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1)     ,...,x() m}with\n",
      "  correspondingtargetsy() i.\n",
      "  Computeinterimupdate:˜    θθv← +α.\n",
      "   Computegradient:g←1\n",
      "m∇˜ θ\n",
      "iLf((x() i;˜ θy),() i).\n",
      "          Accumulategradient: . rrgg ←ρ+(1 )−ρ\n",
      "      Computevelocityupdate:vv←α−√r  g.(1√r appliedelement-wise)\n",
      "      Applyupdate: .θθv← +\n",
      " endwhile\n",
      "3 0 5      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "         (uncentered)second-ordermoment;however,itlacksthecorrectionfactor.Thus,\n",
      "           unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias\n",
      "             earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\n",
      "           ofhyperparameters, thoughthelearningratesometimesneedstobechangedfrom\n",
      "  thesuggesteddefault.\n",
      "     8.5.4ChoosingtheRightOptimizationAlgorithm\n",
      "             Wehavediscussedaseriesofrelatedalgorithmsthateachseektoaddressthe\n",
      "            challengeofoptimizingdeepmodelsbyadaptingthelearningrateforeachmodel\n",
      "            parameter.Atthispoint,anaturalquestionis:whichalgorithmshouldonechoose?\n",
      "            Unfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014\n",
      "          presentedavaluablecomparisonofalargenumberofoptimizationalgorithms\n",
      "              acrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\n",
      "         algorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\n",
      "    Algorithm8.7TheAdamalgorithm\n",
      "      Require:Stepsize(Suggesteddefault:)  0001.\n",
      "Require:      Exponentialdecayratesformomentestimates,ρ1andρ2 in[0,1).\n",
      "     (Suggesteddefaults:andrespectively) 09. 0999.\n",
      "Require: Smallconstantδ     usedfornumericalstabilization(Suggesteddefault:\n",
      "10−8)\n",
      "   Require:Initialparametersθ\n",
      "         Initialize1stand2ndmomentvariables,s= 0r= 0\n",
      "    Initializetimestept= 0\n",
      "     while do stoppingcriterionnotmet\n",
      "   Sampleaminibatchofm     examplesfromthetrainingset{x(1)     ,...,x() m}with\n",
      "  correspondingtargetsy() i.\n",
      "   Computegradient:g←1\n",
      "m∇ θ\n",
      "iLf((x() i  ;)θ,y() i)\n",
      "    tt←+1\n",
      "       Updatebiasedﬁrstmomentestimate:s←ρ1    s+(1−ρ1)g\n",
      "       Updatebiasedsecondmomentestimate:r←ρ2    r+(1−ρ2  )gg\n",
      "    Correctbiasinﬁrstmoment:ˆ s←s\n",
      "1 − ρt\n",
      "1\n",
      "    Correctbiasinsecondmoment:ˆ r←r\n",
      "1 − ρt\n",
      "2\n",
      "   Computeupdate:∆= θ−ˆs√\n",
      "ˆ r+ δ  (operationsappliedelement-wise)\n",
      "      Applyupdate:θθθ← +∆\n",
      " endwhile\n",
      "3 0 6      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "        performedfairlyrobustly,nosinglebestalgorithmhasemerged.\n",
      "         Currently,themostpopularoptimizationalgorithmsactivelyinuseinclude\n",
      "        SGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta,\n",
      "              andAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n",
      "           largelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter\n",
      "tuning).\n",
      "   8.6ApproximateSecond-OrderMethods\n",
      "            Inthissectionwediscusstheapplicationofsecond-ordermethodstotrainingdeep\n",
      "             networks.See ()foranearliertreatmentofthissubject.For LeCunetal.1998a\n",
      "           simplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\n",
      "risk:\n",
      "J() = θ E x ,y ∼ ˆ p d a t a ( ) x , y   [((;))] = Lfxθ,y1\n",
      "mm \n",
      "i =1Lf((x( ) i  ;)θ,y( ) i ).(8.25)\n",
      "           Themethodswediscusshereextendreadily,however,tomoregeneralobjective\n",
      "          functions,suchasthosethatincludeparameterregularizationterms,asdiscussed\n",
      "  inchapter.7\n",
      "  8.6.1Newton’sMethod\n",
      "           Insection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3\n",
      "          ordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\n",
      "          optimization.Themostwidelyusedsecond-ordermethodisNewton’smethod.We\n",
      "            nowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto\n",
      "  neuralnetworktraining.\n",
      "           Newton’smethodisanoptimizationschemebasedonusingasecond-orderTay-\n",
      "    lorseriesexpansiontoapproximateJ(θ   )nearsomepointθ 0  ,ignoringderivatives\n",
      "  ofhigherorder:\n",
      " JJ () θ≈(θ 0    )+(θθ− 0)∇ θJ(θ 0 )+1\n",
      "2  (θθ− 0)  Hθθ (− 0 ),(8.26)\n",
      "whereH   istheHessianofJ  withrespecttoθ  evaluatedatθ 0     .Ifwethensolvefor\n",
      "            thecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\n",
      "θ∗= θ 0 −H− 1∇ θJ(θ 0 ). (8.27)\n",
      "3 0 7      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "        Thusforalocallyquadraticfunction(withpositivedeﬁniteH  ),byrescaling\n",
      "  thegradientbyH− 1        ,Newton’smethodjumpsdirectlytotheminimum. Ifthe\n",
      "           objectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\n",
      "          updatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s\n",
      "    method,giveninalgorithm.8.8\n",
      "            Forsurfacesthatarenotquadratic,aslongastheHessianremainspositive\n",
      "          deﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step\n",
      "           iterativeprocedure.First,updateorcomputetheinverseHessian(i.e.,byupdat-\n",
      "         ingthequadraticapproximation). Second,updatetheparametersaccordingto\n",
      " equation.8.27\n",
      "           Insection,wediscusshowNewton’smethodisappropriateonlywhen 8.2.3\n",
      "            theHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective\n",
      "           functionistypicallynonconvex,withmanyfeatures,suchassaddlepoints,that\n",
      "           areproblematicforNewton’smethod. IftheeigenvaluesoftheHessianarenot\n",
      "            allpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually\n",
      "            causeupdatestomoveinthewrongdirection.Thissituationcanbeavoided\n",
      "         byregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\n",
      "           constant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α\n",
      "θ∗= θ 0  −[((Hfθ 0  ))+ ]αI− 1∇ θf(θ 0 ). (8.28)\n",
      "          ThisregularizationstrategyisusedinapproximationstoNewton’smethod,such\n",
      "        astheLevenberg-Marquardtalgorithm(Levenberg1944Marquardt1963 ,; ,),and\n",
      "              worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\n",
      "            closetozero.Whentherearemoreextremedirectionsofcurvature,thevalue\n",
      "ofα          wouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues. Asα\n",
      "         increasesinsize,however,theHessianbecomesdominatedbytheαIdiagonal,\n",
      " Algorithm8.8    Newton’smethodwithobjectiveJ(θ) =1\n",
      "mm\n",
      "i =1L(f(x( ) i;θ) ,y( ) i)\n",
      "   Require:Initialparameterθ 0\n",
      "     Require:Trainingsetofexamplesm\n",
      "     while do stoppingcriterionnotmet\n",
      "   Computegradient:g←1\n",
      "m∇ θ\n",
      "iLf((x( ) i  ;)θ,y( ) i)\n",
      "   ComputeHessian:H←1\n",
      "m∇2\n",
      "θ\n",
      "iLf((x( ) i  ;)θ,y( ) i)\n",
      "   ComputeHessianinverse:H− 1\n",
      "   Computeupdate:∆= θ−H− 1g\n",
      "     Applyupdate:θθθ = +∆\n",
      " endwhile\n",
      "3 0 8      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           andthedirectionchosenbyNewton’smethodconvergestothestandardgradient\n",
      " dividedbyα      . Whenstrongnegativecurvatureispresent,α    mayneedtobeso\n",
      "           largethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith\n",
      "    aproperlychosenlearningrate.\n",
      "          Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n",
      "            suchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural\n",
      "          networksislimitedbythesigniﬁcantcomputationalburdenitimposes.The\n",
      "              numberofelementsintheHessianissquaredinthenumberofparameters,sowith\n",
      "k            parameters(andforevenverysmallneuralnetworks,thenumberofparameters\n",
      "k            canbeinthemillions),Newton’smethodwouldrequiretheinversionofa  kk×\n",
      "   matrix—withcomputationalcomplexityofO(k3     ).Also,sincetheparameterswill\n",
      "             changewitheveryupdate,theinverseHessianhastobecomputedateverytraining\n",
      "            iteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\n",
      "            canbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,\n",
      "            wediscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s\n",
      "     methodwhileside-steppingthecomputationalhurdles.\n",
      "  8.6.2ConjugateGradients\n",
      "            Conjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse\n",
      "   Hessianbyiterativelydescending  conjugatedirections    .Theinspirationforthis\n",
      "             approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\n",
      "            descent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\n",
      "            thedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\n",
      "            steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective\n",
      "         back-and-forthzig-zagpattern.Thishappensbecauseeachlinesearchdirection,\n",
      "             whengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\n",
      " searchdirection.\n",
      "     Lettheprevioussearchdirectionbe d t − 1      .Attheminimum,wheretheline\n",
      "        searchterminates,thedirectionalderivativeiszeroindirection d t − 1:∇ θJ( θ)·\n",
      "d t − 1            =0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,\n",
      "d t=∇ θJ( θ      ) willhavenocontributioninthedirection d t − 1 .Thus d t isorthogonal\n",
      "to d t − 1   .Thisrelationshipbetween d t − 1and d t     isillustratedinﬁgurefor8.6\n",
      "            multipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof\n",
      "           orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\n",
      "            searchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\n",
      "           descendingtotheminimuminthecurrentgradientdirection,wemustreminimize\n",
      "            theobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\n",
      "               theendofeachlinesearchweare,inasense,undoingprogresswehavealready\n",
      "3 0 9      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "     󰤓󰤓󰤓󰤓󰤓󰤓\n",
      "             Figure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\n",
      "              methodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\n",
      "                deﬁnedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\n",
      "                seenwithusingaﬁxedlearningrateinﬁgure,butevenwiththeoptimalstepsize 4.6\n",
      "           thealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,at\n",
      "               theminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointis\n",
      "   orthogonaltothatdirection.\n",
      "             madeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\n",
      "    seekstoaddressthisproblem.\n",
      "              Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthatis\n",
      "c o nj uga t e            tothepreviouslinesearchdirection;thatis,itwillnotundoprogress\n",
      "      madeinthatdirection.Attrainingiterationt    ,thenextsearchdirectiond ttakes\n",
      " theform:\n",
      "d t= ∇ θ   Jβ ()+θ td t − 1 , (8.29)\n",
      "whereβ t          isacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,d t − 1,\n",
      "        weshouldaddbacktothecurrentsearchdirection.\n",
      " Twodirections,d tandd t − 1     ,aredeﬁnedasconjugateifd\n",
      "tHd t − 1 = 0,where\n",
      "    HistheHessianmatrix.\n",
      "          Thestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\n",
      " eigenvectorsofH tochooseβ t        ,whichwouldnotsatisfyourgoalofdeveloping\n",
      "           amethodthatismorecomputationallyviablethanNewton’smethodforlarge\n",
      "         problems. Canwecalculatetheconjugatedirectionswithoutresortingtothese\n",
      "       calculations?Fortunately,theanswertothatisyes.\n",
      "      Twopopularmethodsforcomputingtheβ tare\n",
      "3 1 0      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "1. Fletcher-Reeves:\n",
      "β t=∇ θJ( θ t)∇ θJ( θ t)\n",
      "∇ θJ( θ t − 1)∇ θJ( θ t − 1)(8.30)\n",
      "2. Polak-Ribière:\n",
      "β t=(∇ θJ( θ t  )−∇ θJ( θ t − 1))∇ θJ( θ t)\n",
      "∇ θJ( θ t − 1)∇ θJ( θ t − 1)(8.31)\n",
      "           Foraquadraticsurface,theconjugatedirectionsensurethatthegradientalong\n",
      "            thepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\n",
      "         minimumalongthepreviousdirections.Asaconsequence,inak-dimensional\n",
      "        parameterspace,theconjugategradientmethodrequiresatmostk  linesearchesto\n",
      "           achievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm.8.9\n",
      "     Algorithm8.9Theconjugategradientmethod\n",
      "   Require:Initialparameters θ 0\n",
      "     Require:Trainingsetofexamplesm\n",
      " Initialize ρ 0= 0\n",
      " Initializeg 0= 0\n",
      "  Initializet= 1\n",
      "     while do stoppingcriterionnotmet\n",
      "   Initializethegradient g t= 0\n",
      "  Computegradient: g t←1\n",
      "m∇ θ\n",
      "iLf(( x( ) i  ;) θ, y( ) i)\n",
      " Computeβ t=( g t − g t − 1 )g t\n",
      "g\n",
      "t − 1g t − 1(Polak-Ribière)\n",
      "    (Nonlinearconjugategradient:optionallyresetβ t    tozero,forexampleiftis\n",
      "         amultipleofsomeconstant,suchas) kk= 5\n",
      "   Computesearchdirection: ρ t= − g t +β t ρ t − 1\n",
      "     Performlinesearchtoﬁnd:∗= argmin 1\n",
      "mm\n",
      "i =1Lf(( x( ) i ; θ t + ρ t ), y( ) i)\n",
      "       (Onatrulyquadraticcostfunction, analyticallysolvefor∗ ratherthan\n",
      "   explicitlysearchingforit)\n",
      "  Applyupdate: θ t +1= θ t +∗ρ t\n",
      "    tt←+1\n",
      " endwhile\n",
      "  NonlinearConjugateGradients:       Sofarwehavediscussedthemethodof\n",
      "          conjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse,\n",
      "            ourprimaryinterestinthischapteristoexploreoptimizationmethodsfortraining\n",
      "          neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\n",
      "3 1 1      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "         objectivefunctionisfarfromquadratic.Perhapssurprisingly, themethodof\n",
      "           conjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.\n",
      "          Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n",
      "             arenolongerassuredtoremainattheminimumoftheobjectiveforprevious\n",
      "    directions.Asaresult,the   nonlinearconjugategradients  algorithmincludes\n",
      "           occasionalresetswherethemethodofconjugategradientsisrestartedwithline\n",
      "    searchalongtheunalteredgradient.\n",
      "         Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\n",
      "           gradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto\n",
      "           initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\n",
      "        commencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\n",
      "          gradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\n",
      "            versionshavebeenusedsuccessfullyfortrainingneuralnetworks( ,). Leetal.2011\n",
      "         Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshavebeen\n",
      "          proposedearlier,suchasthescaledconjugategradientsalgorithm(,). Moller1993\n",
      " 8.6.3BFGS\n",
      "The   Broyden–Fletcher–Goldfarb–Shanno(BFGS)algorithm  attemptsto\n",
      "          bringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\n",
      "           burden.Inthatrespect,BFGSissimilartotheconjugategradientmethod.\n",
      "           However,BFGStakesamoredirectapproachtotheapproximationofNewton’s\n",
      "       update.RecallthatNewton’supdateisgivenby\n",
      "θ∗= θ 0 − H− 1∇ θJ( θ 0 ), (8.32)\n",
      "where H   istheHessianofJ  withrespectto θ  evaluatedat θ 0  .Theprimary\n",
      "          computationaldiﬃcultyinapplyingNewton’supdateisthecalculationofthe\n",
      " inverseHessian H− 1        .Theapproachadoptedbyquasi-Newtonmethods(ofwhich\n",
      "            theBFGSalgorithmisthemostprominent)istoapproximatetheinversewith\n",
      " amatrix M t          thatisiterativelyreﬁnedbylow-rankupdatestobecomeabetter\n",
      "  approximationof H− 1.\n",
      "           ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\n",
      "      textbooksonoptimization,includingin (). Luenberger1984\n",
      "    OncetheinverseHessianapproximation M t     isupdated,thedirectionofdescent\n",
      "ρ t  isdeterminedby ρ t= M t g t         .Alinesearchisperformedinthisdirectionto\n",
      "     determinethesizeofthestep,∗         ,takeninthisdirection.Theﬁnalupdatetothe\n",
      "   parametersisgivenby\n",
      "θ t +1= θ t +∗ρ t . (8.33)\n",
      "3 1 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseries\n",
      "         oflinesearcheswiththedirectionincorporatingsecond-orderinformation.Unlike\n",
      "           conjugategradients,however,thesuccessoftheapproachisnotheavilydependent\n",
      "               onthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.\n",
      "            Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\n",
      "             lesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\n",
      "    storetheinverseHessianmatrix,M  ,thatrequires O( n2   )memory,makingBFGS\n",
      "           impracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\n",
      "parameters.\n",
      "Limited Memory BFGS (or L-BFGS)   The memorycosts ofthe BFGS\n",
      "          algorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse\n",
      " HessianapproximationM      .TheL-BFGSalgorithmcomputestheapproximationM\n",
      "            usingthesamemethodastheBFGSalgorithmbutbeginningwiththeassumption\n",
      "thatM( 1 ) t −          istheidentitymatrix,ratherthanstoringtheapproximationfromone\n",
      "              steptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS\n",
      "          aremutuallyconjugate.However,unlikethemethodofconjugategradients,this\n",
      "            procedureremainswellbehavedwhentheminimumofthelinesearchisreached\n",
      "           onlyapproximately.TheL-BFGSstrategywithnostoragedescribedherecanbe\n",
      "            generalizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\n",
      "              vectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n",
      "    8.7OptimizationStrategiesandMeta-Algorithms\n",
      "          Manyoptimizationtechniquesarenotexactlyalgorithmsbutrathergeneraltem-\n",
      "            platesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\n",
      "    incorporatedintomanydiﬀerentalgorithms.\n",
      "  8.7.1BatchNormalization\n",
      "            Batchnormalization( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015\n",
      "            innovationsinoptimizingdeepneuralnetworks,anditisactuallynotanopti-\n",
      "           mizationalgorithmatall.Instead,itisamethodofadaptivereparametrization,\n",
      "        motivatedbythediﬃcultyoftrainingverydeepmodels.\n",
      "           Verydeepmodelsinvolvethecompositionofseveralfunctions,orlayers.The\n",
      "            gradienttellshowtoupdateeachparameter,undertheassumptionthattheother\n",
      "             layersdonotchange.Inpractice,weupdateallthelayerssimultaneously.Whenwe\n",
      "          maketheupdate,unexpectedresultscanhappenbecausemanyfunctionscomposed\n",
      "3 1 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "         togetherarechangedsimultaneously,usingupdatesthatwerecomputedunder\n",
      "           theassumptionthattheotherfunctionsremainconstant.Asasimpleexample,\n",
      "               supposewehaveadeepneuralnetworkthathasonlyoneunitperlayeranddoes\n",
      "        notuseanactivationfunctionateachhiddenlayer:ˆy=xw 1w 2w 3   ...w l .Here,w i\n",
      "     providestheweightusedbylayeri    .Theoutputoflayeriish i=h i − 1w i .The\n",
      "outputˆy      isalinearfunctionoftheinputx      butanonlinearfunctionoftheweights\n",
      "w i           .Supposeourcostfunctionhasputagradientofon1 ˆy     ,sowewishtodecrease\n",
      "ˆy         slightly.Theback-propagationalgorithmcanthencomputeagradientg=∇ wˆy.\n",
      "       Considerwhathappenswhenwemakeanupdate    wwg←− . Theﬁrst-order\n",
      "   Taylorseriesapproximationofˆy     predictsthatthevalueofˆy  willdecreasebygg.\n",
      "    Ifwewantedtodecreaseˆy by0.      1,thisﬁrst-orderinformationavailableinthe\n",
      "       gradientsuggestswecouldsetthelearningrateto0 1 .\n",
      "gg    .Yet,theactualupdate\n",
      "           willincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforderl .The\n",
      "      newvalueofˆyisgivenby\n",
      "xw( 1 −g 1)(w 2 −g 2    )(...w l −g l ). (8.34)\n",
      "          Anexampleofonesecond-ordertermarisingfromthisupdateis2g 1g 2l\n",
      "i =3w i.\n",
      "     Thistermmightbenegligibleifl\n",
      "i =3w i      issmall,ormightbeexponentiallylarge\n",
      "      iftheweightsonlayersthrough 3l        aregreaterthan.Thismakesitveryhard 1\n",
      "             tochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe\n",
      "            parametersforonelayerdependsostronglyonalltheotherlayers.Second-order\n",
      "           optimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese\n",
      "            second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\n",
      "        evenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization\n",
      "         algorithmsareexpensiveandusuallyrequirenumerousapproximationsthatprevent\n",
      "         themfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions.Building\n",
      "ann    -thorderoptimizationalgorithmfor n>      2thusseemshopeless.Whatcanwe\n",
      " doinstead?\n",
      "          Batchnormalizationprovidesanelegantwayofreparametrizingalmostanydeep\n",
      "        network.Thereparametrizationsigniﬁcantlyreducestheproblemofcoordinating\n",
      "           updatesacrossmanylayers.Batchnormalizationcanbeappliedtoanyinput\n",
      "      orhiddenlayerinanetwork.LetH       beaminibatchofactivationsofthelayer\n",
      "            tonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\n",
      "             appearinginarowofthematrix.Tonormalize,wereplaceitwith H\n",
      "H=  Hµ−\n",
      "σ , (8.35)\n",
      "whereµ         isavectorcontainingthemeanofeachunitandσ   isavectorcontaining\n",
      "            thestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting\n",
      "3 1 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      " thevectorµ  andthevectorσ        tobeappliedtoeveryrowofthematrixH .Within\n",
      "      eachrow,thearithmeticiselement-wise,soH i , j   isnormalizedbysubtractingµ j\n",
      "  anddividingbyσ j        .TherestofthenetworkthenoperatesonH  inexactlythe\n",
      "        samewaythattheoriginalnetworkoperatedon.H\n",
      "  Attrainingtime,\n",
      " µ=1\n",
      "m\n",
      "iH i , : (8.36)\n",
      "and\n",
      " σ=\n",
      " δ+1\n",
      "m\n",
      "i  ( )Hµ−2\n",
      "i , (8.37)\n",
      "whereδ       isasmallpositivevaluesuchas10− 8    ,imposedtoavoidencountering\n",
      "   theundeﬁnedgradientof√zatz     =0.Crucially,weback-propagatethrough\n",
      "           theseoperationsforcomputingthemeanandthestandarddeviation,andfor\n",
      "   applyingthemtonormalizeH        .Thismeansthatthegradientwillneverpropose\n",
      "           anoperationthatactssimplytoincreasethestandarddeviation ormeanof\n",
      "h i            ;thenormalizationoperationsremovetheeﬀectofsuchanactionandzero\n",
      "             outitscomponentinthegradient.Thiswasamajorinnovationofthebatch\n",
      "       normalizationapproach. Previousapproacheshadinvolvedaddingpenaltiesto\n",
      "           thecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\n",
      "          involvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\n",
      "          Theformerapproachusuallyresultedinimperfectnormalizationandthelatter\n",
      "          usuallyresultedinsigniﬁcantwastedtime,asthelearningalgorithmrepeatedly\n",
      "          proposedchangingthemeanandvariance,andthenormalizationsteprepeatedly\n",
      "          undidthischange.Batchnormalizationreparametrizesthemodeltomakesome\n",
      "         unitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.\n",
      "  Attesttime,µandσ        maybereplacedbyrunningaveragesthatwerecollected\n",
      "             duringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\n",
      "     withoutneedingtousedeﬁnitionsofµandσ     thatdependonanentireminibatch.\n",
      " Revisitingtheˆy=xw 1w 2   ...w l         example,weseethatwecanmostlyresolvethe\n",
      "      diﬃcultiesinlearningthismodelbynormalizingh l − 1  .Supposethatx isdrawn\n",
      "    fromaunitGaussian.Thenh l − 1       willalsocomefromaGaussian,becausethe\n",
      " transformationfromxtoh l  islinear.However,h l − 1     willnolongerhavezeromean\n",
      "          andunitvariance.Afterapplyingbatchnormalization,weobtainthenormalized\n",
      "ˆh l − 1           thatrestoresthezeromeanandunitvarianceproperties.Foralmostany\n",
      "    updatetothelowerlayers,ˆh l − 1      willremainaunitGaussian.Theoutputˆymay\n",
      "       thenbelearnedasasimplelinearfunctionˆy=w lˆh l − 1     .Learninginthismodelis\n",
      "              nowverysimplebecausetheparametersatthelowerlayersdonothaveaneﬀect\n",
      "            inmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. Insome\n",
      "3 1 5      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "              cornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelowerlayer\n",
      "              weightstocanmaketheoutputbecomedegenerate,andchangingthesignofone 0\n",
      "        ofthelowerweightscanﬂiptherelationshipbetweenˆh l − 1and y  .Thesesituations\n",
      "           areveryrare.Withoutnormalization,nearlyeveryupdatewouldhaveanextreme\n",
      "    eﬀectonthestatisticsof h l − 1       .Batchnormalizationhasthusmadethismodel\n",
      "              signiﬁcantlyeasiertolearn.Inthisexample,theeaseoflearningofcoursecameat\n",
      "              thecostofmakingthelowerlayersuseless.Inourlinearexample,thelowerlayers\n",
      "              nolongerhaveanyharmfuleﬀect,buttheyalsonolongerhaveanybeneﬁcialeﬀect.\n",
      "            Thisisbecausewehavenormalizedouttheﬁrst-andsecond-orderstatistics,which\n",
      "              isallthatalinearnetworkcaninﬂuence.Inadeepneuralnetworkwithnonlinear\n",
      "         activationfunctions,thelowerlayerscanperformnonlineartransformationsof\n",
      "            thedata,sotheyremainuseful.Batchnormalizationactstostandardizeonlythe\n",
      "              meanandvarianceofeachunitinordertostabilizelearning,butitallowsthe\n",
      "            relationshipsbetweenunitsandthenonlinearstatisticsofasingleunittochange.\n",
      "             Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,\n",
      "            wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\n",
      "             layer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\n",
      "       theinspirationforbatchnormalization.Unfortunately, eliminatingalllinear\n",
      "          interactionsismuchmoreexpensivethanstandardizingthemeanandstandard\n",
      "            deviationofeachindividualunit,andsofarbatchnormalizationremainsthemost\n",
      " practicalapproach.\n",
      "            Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\n",
      "           poweroftheneuralnetworkcontainingthatunit.Tomaintaintheexpressive\n",
      "              powerofthenetwork,itiscommontoreplacethebatchofhiddenunitactivations\n",
      "HwithγH+β    ratherthansimplythenormalizedH  .Thevariablesγandβ\n",
      "             arelearnedparametersthatallowthenewvariabletohaveanymeanandstandard\n",
      "             deviation.Atﬁrstglance,thismayseemuseless—whydidwesetthemeanto 0,\n",
      "               andthenintroduceaparameterthatallowsittobesetbacktoanyarbitraryvalue\n",
      "             β?Theansweristhatthenewparametrizationcanrepresentthesamefamilyof\n",
      "           functionsoftheinputastheoldparametrization,butthenewparametrization\n",
      "          hasdiﬀerentlearningdynamics.Intheoldparametrization,themeanofHwas\n",
      "          determinedbyacomplicatedinteractionbetweentheparametersinthelayers\n",
      "belowH       .Inthenewparametrization,themeanofγH+β  isdeterminedsolely\n",
      "            by.Thenewparametrizationismucheasiertolearnwithgradientdescent. β\n",
      "       Mostneuralnetworklayerstaketheformof φ(XW+b ),where φ issome\n",
      "          ﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It\n",
      "            isnaturaltowonderwhetherweshouldapplybatchnormalizationtotheinput\n",
      "X     ,ortothetransformedvalueXW+b     . ()recommend IoﬀeandSzegedy2015\n",
      "3 1 6      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "   thelatter.Morespeciﬁcally,XW+b      shouldbereplacedbyanormalizedversion\n",
      "ofXW           .Thebiastermshouldbeomittedbecauseitbecomesredundantwith\n",
      "theβ         parameterappliedbythebatchnormalizationreparametrization.Theinput\n",
      "              toalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\n",
      "             rectiﬁedlinearfunctioninapreviouslayer.Thestatisticsoftheinputarethus\n",
      "         morenon-Gaussianandlessamenabletostandardizationbylinearoperations.\n",
      "            Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\n",
      " samenormalizingµandσ         ateveryspatiallocationwithinafeaturemap,sothat\n",
      "            thestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\n",
      "  8.7.2CoordinateDescent\n",
      "             Insomecases,itmaybepossibletosolveanoptimizationproblemquicklyby\n",
      "       breakingitintoseparatepieces.Ifweminimizef(x     )withrespecttoasingle\n",
      "variablex i      , thenminimizeitwithrespecttoanother variablex j  , andsoon,\n",
      "            repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\n",
      "     minimum.Thispracticeisknownas  coordinatedescent   ,becauseweoptimize\n",
      "     onecoordinateatatime. Moregenerally,  blockcoordinatedescent refersto\n",
      "           minimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\n",
      "             “coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas\n",
      "    thestrictlyindividualcoordinatedescent.\n",
      "           Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe\n",
      "          optimizationproblemcanbeclearlyseparatedintogroupsthatplayrelatively\n",
      "            isolatedroles,orwhenoptimizationwithrespecttoonegroupofvariablesis\n",
      "           signiﬁcantlymoreeﬃcientthanoptimizationwithrespecttoallofthevariables.\n",
      "     Forexample,considerthecostfunction\n",
      "  J,(HW) =\n",
      "i , j|H i , j |+\n",
      "i , j\n",
      "  XW−H2\n",
      "i , j . (8.38)\n",
      "            Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\n",
      "    toﬁndaweightmatrixW        thatcanlinearlydecodeamatrixofactivationvalues\n",
      "H    toreconstructthetrainingsetX      .Mostapplicationsofsparsecodingalso\n",
      "            involveweightdecayoraconstraintonthenormsofthecolumnsofW  ,toprevent\n",
      "          thepathologicalsolutionwithextremelysmallandlarge. H W\n",
      " ThefunctionJ         isnotconvex.However, wecandividetheinputstothe\n",
      "       trainingalgorithmintotwosets:thedictionaryparametersW  andthecode\n",
      "representationsH          .Minimizingtheobjectivefunctionwithrespecttoeitheroneof\n",
      "            thesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n",
      "3 1 7      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "           usanoptimizationstrategythatallowsustouseeﬃcientconvexoptimization\n",
      "    algorithms,byalternatingbetweenoptimizingWwithH   ﬁxed,thenoptimizing\n",
      "   HWwithﬁxed.\n",
      "             Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\n",
      "           stronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunctionf(x) =\n",
      "(x 1−x 2)2+α\n",
      "x2\n",
      "1 +x2\n",
      "2\n",
      "whereα       isapositiveconstant.Theﬁrsttermencourages\n",
      "            thetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem\n",
      "               tobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve\n",
      "             theprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.\n",
      " Forsmallα          ,however,coordinatedescentwillmakeveryslowprogressbecausethe\n",
      "               ﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers\n",
      "        signiﬁcantlyfromthecurrentvalueoftheothervariable.\n",
      "  8.7.3PolyakAveraging\n",
      "          Polyakaveraging(PolyakandJuditsky1992,)consistsofaveragingseveralpoints\n",
      "           inthetrajectorythroughparameterspacevisitedbyanoptimizationalgorithm.If\n",
      "t      iterationsofgradientdescentvisitpointsθ(1 )     ,...,θ( ) t     ,thentheoutputofthe\n",
      "   Polyakaveragingalgorithmisˆθ( ) t=1\n",
      "t\n",
      "iθ( ) i      .Onsomeproblemclasses,suchas\n",
      "          gradientdescentappliedtoconvexproblems,thisapproachhasstrongconvergence\n",
      "          guarantees.Whenappliedtoneuralnetworks,itsjustiﬁcationismoreheuristic,\n",
      "             butitperformswellinpractice.Thebasicideaisthattheoptimizationalgorithm\n",
      "              mayleapbackandforthacrossavalleyseveraltimeswithoutevervisitingapoint\n",
      "               nearthebottomofthevalley.Theaverageofallthelocationsoneithersideshould\n",
      "        beclosetothebottomofthevalleythough.\n",
      "           Innonconvexproblems,thepathtakenbytheoptimizationtrajectorycanbe\n",
      "          verycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter\n",
      "              spacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\n",
      "              barriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\n",
      "            whenapplyingPolyakaveragingtononconvexproblems,itistypicaltousean\n",
      "   exponentiallydecayingrunningaverage:\n",
      "ˆθ( ) t= αˆθ( 1 ) t −   +(1 )−αθ( ) t . (8.39)\n",
      "          Therunningaverageapproachisusedinnumerousapplications.SeeSzegedy\n",
      "      etal.()forarecentexample. 2015\n",
      "3 1 8      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "  8.7.4SupervisedPretraining\n",
      "             Sometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious\n",
      "                 ifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis\n",
      "             sometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake\n",
      "               themodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve\n",
      "             asimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat\n",
      "           involvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof\n",
      "            trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\n",
      "pretraining.\n",
      " Greedyalgorithms        breakaproblemintomanycomponents,thensolvefor\n",
      "          theoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\n",
      "          individuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\n",
      "        solution.Nonetheless,greedyalgorithmscanbecomputationallymuchcheaper\n",
      "              thanalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedy\n",
      "            solutionisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowed\n",
      " byaﬁne-tuning         stageinwhichajointoptimizationalgorithmsearchesforan\n",
      "          optimalsolutiontothefullproblem.Initializingthejointoptimizationalgorithm\n",
      "              withagreedysolutioncangreatlyspeeditupandimprovethequalityofthe\n",
      "  solutionitﬁnds.\n",
      "        Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\n",
      "          deeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms\n",
      "         thatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\n",
      "        problems.Thisapproachisknownas . greedysupervisedpretraining\n",
      "           Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\n",
      "             eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\n",
      "            thelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining\n",
      "              isillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\n",
      "             ofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\n",
      "            hiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n",
      "           ()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015\n",
      "             theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\n",
      "            networks(withuptonineteenlayersofweights). Themiddlelayersofthenew,\n",
      "            verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\n",
      "               Anotheroption,exploredbyYu2010etal.(),istousethe ofthepreviously outputs\n",
      "             trainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\n",
      "  Why would greedy supervised pretraininghelp?The hypothesis initially\n",
      "               discussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n",
      "3 1 9      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "y y\n",
      "h( 1 )h( 1 )\n",
      "x x\n",
      "( a )U( 1 )U( 1 )\n",
      "W( 1 )W( 1 ) y yh( 1 )h( 1 )\n",
      "x x\n",
      "( b )U( 1 )U( 1 )W( 1 )W( 1 )\n",
      "y yh( 1 )h( 1 )\n",
      "x x\n",
      "( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\n",
      "y y U( 2 )U( 2 ) W( 2 )W( 2 )\n",
      "y yh( 1 )h( 1 )\n",
      "x x\n",
      "( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\n",
      "U( 2 )U( 2 )\n",
      "W( 2 )W( 2 )\n",
      "             Figure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengioetal.2007\n",
      "           (a)Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe (b)\n",
      "            samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand (c)\n",
      "              discardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput\n",
      "             toanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\n",
      "                astheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\n",
      "             manylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. (d)\n",
      "              Tofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat\n",
      "        theendorateachstageofthisprocess.\n",
      "3 2 0      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "            intermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\n",
      "    termsofoptimizationandgeneralization.\n",
      "           Anapproachrelatedtosupervisedpretrainingextendstheideatothecontext\n",
      "            oftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith\n",
      "               eightlayersofweightsonasetoftasks(asubsetofthe1,000ImageNet object\n",
      "         categories)andtheninitializeasame-sizenetworkwiththeﬁrst k  layersofthe\n",
      "            ﬁrstnet. Allthelayersofthesecondnetwork(withtheupperlayersinitialized\n",
      "            randomly)arethenjointlytrainedtoperformadiﬀerentsetoftasks(another\n",
      "          subsetofthe1,000ImageNet objectcategories),withfewertrainingexamples\n",
      "             thanfortheﬁrstsetoftasks.Otherapproachestotransferlearningwithneural\n",
      "     networksarediscussedinsection.15.2\n",
      "      Anotherrelatedlineofworkisthe F i t N e t s     ( ,)approach. Romeroetal.2015\n",
      "             Thisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\n",
      "              enoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen\n",
      " becomesa t e a c her     forasecondnetwork,designatedthe s t uden t  .Thestudent\n",
      "            networkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\n",
      "           diﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\n",
      "             studentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\n",
      "                theoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayerof\n",
      "              theteachernetwork.Thisextrataskprovidesasetofhintsabouthowthehidden\n",
      "          layersshouldbeusedandcansimplifytheoptimizationproblem.Additional\n",
      "            parametersareintroducedtoregressthemiddlelayeroftheﬁvelayerteacher\n",
      "            networkfromthemiddlelayerofthedeeperstudentnetwork.Insteadofpredicting\n",
      "            theﬁnalclassiﬁcationtarget,however,theobjectiveistopredictthemiddlehidden\n",
      "             layeroftheteachernetwork.Thelowerlayersofthestudentnetworksthushave\n",
      "            twoobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,\n",
      "              aswellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\n",
      "              anddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow\n",
      "            network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\n",
      "            computationalcostifitisthinenoughtohavefarfewerparameters.Without\n",
      "             thehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\n",
      "            experiments,onboththetrainingandthetestset. Hintsonmiddlelayersmay\n",
      "              thusbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcult\n",
      "            totrain,butotheroptimizationtechniquesorchangesinthearchitecturemayalso\n",
      "  solvetheproblem.\n",
      "3 2 1      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "     8.7.5DesigningModelstoAidOptimization\n",
      "            Toimproveoptimization,thebeststrategyisnotalwaystoimprovetheoptimization\n",
      "          algorithm.Instead,manyimprovementsintheoptimizationofdeepmodelshave\n",
      "         comefromdesigningthemodelstobeeasiertooptimize.\n",
      "          Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecrease\n",
      "         injaggednonmonotonicpatterns,butthiswouldmakeoptimizationextremely\n",
      "               diﬃcult.Inpractice,itismoreimportanttochooseamodelfamilythatiseasyto\n",
      "            optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\n",
      "            neuralnetworklearningoverthepastthirtyyearshavebeenobtainedbychanging\n",
      "         themodelfamilyratherthanchangingtheoptimizationprocedure.Stochastic\n",
      "            gradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n",
      "         1980s,remainsinuseinmodernstate-of-the-artneuralnetworkapplications.\n",
      "           Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-\n",
      "         formationsbetweenlayersandactivationfunctionsthatarediﬀerentiablealmost\n",
      "           everywhere,withsigniﬁcantslopeinlargeportionsoftheirdomain.Inparticular,\n",
      "           modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunitshave\n",
      "           allmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\n",
      "           networksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\n",
      "          optimizationeasier.Thegradientﬂowsthroughmanylayersprovidedthatthe\n",
      "        Jacobianofthelineartransformationhasreasonablesingularvalues. Moreover,\n",
      "            linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s\n",
      "             outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\n",
      "             whichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\n",
      "           modernneuralnetshavebeendesignedsothattheirlocalgradientinformation\n",
      "        correspondsreasonablywelltomovingtowardadistantsolution.\n",
      "       Othermodel designstrategies canhelp tomakeoptimizationeasier.For\n",
      "           example,linearpathsorskipconnectionsbetweenlayersreducethelengthof\n",
      "           theshortestpathfromthelowerlayer’sparameterstotheoutput, andthus\n",
      "           mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\n",
      "              toskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\n",
      "            intermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedyetal.2014a\n",
      "            anddeeplysupervisednets( ,).These“auxiliaryheads”aretrained Leeetal.2014\n",
      "               toperformthesametaskastheprimaryoutputatthetopofthenetworkto\n",
      "            ensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete,\n",
      "            theauxiliaryheadsmaybediscarded.Thisisanalternativetothepretraining\n",
      "            strategies,whichwereintroducedintheprevioussection.Inthisway,onecan\n",
      "              trainjointlyallthelayersinasinglephasebutchangethearchitecture,sothat\n",
      "            intermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n",
      "3 2 2      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "              shoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\n",
      "     8.7.6ContinuationMethodsandCurriculumLearning\n",
      "             Asarguedinsection,manyofthechallengesinoptimizationarisefromthe 8.2.7\n",
      "             globalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\n",
      "         estimatesoflocalupdatedirections.Thepredominantstrategyforovercoming\n",
      "              thisproblemistoattempttoinitializetheparametersinaregionconnectedtothe\n",
      "            solutionbyashortpaththroughparameterspacethatlocaldescentcandiscover.\n",
      " Continuationmethods        areafamilyofstrategiesthatcanmakeoptimization\n",
      "            easierbychoosinginitialpointstoensurethatlocaloptimizationspendsmostof\n",
      "            itstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\n",
      "            toconstructaseriesofobjectivefunctionsoverthesameparameters.Tominimize\n",
      "  acostfunctionJ( θ     ),weconstructnewcostfunctions{J(0 )     ,...,J( ) n}  .Thesecost\n",
      "       functionsaredesignedtobeincreasinglydiﬃcult,withJ(0 )   beingfairlyeasyto\n",
      " minimize,andJ( ) n    ,themostdiﬃcult,beingJ( θ     ),thetruecostfunctionmotivating\n",
      "      theentireprocess.WhenwesaythatJ( ) i  iseasierthanJ( +1 ) i    ,wemeanthatit\n",
      "     iswellbehavedovermoreof θ       space.Arandominitializationismorelikelyto\n",
      "            landintheregionwherelocaldescentcanminimizethecostfunctionsuccessfully\n",
      "              becausethisregionislarger.Theseriesofcostfunctionsaredesignedsothata\n",
      "               solutiontooneisagoodinitialpointofthenext. Wethusbeginbysolvingan\n",
      "           easyproblem,thenreﬁnethesolutiontosolveincrementallyharderproblemsuntil\n",
      "         wearriveatasolutiontothetrueunderlyingproblem.\n",
      "        Traditionalcontinuationmethods(predatingtheuseofcontinuationmethods\n",
      "           forneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\n",
      "               SeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\n",
      "         methods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\n",
      "         whichaddsnoisetothe parameters ( ,).Continuation Kirkpatricketal.1983\n",
      "           methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n",
      "          ()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\n",
      "         Continuationmethodstraditionallyweremostlydesignedwiththegoalof\n",
      "          overcomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto\n",
      "             reachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\n",
      "          thesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the\n",
      "          originalcostfunction.Thisblurringoperationcanbedonebyapproximating\n",
      "J( ) i() = θ Eθ∼N ( θ; θ , σ( ) 2 i)J( θ ) (8.40)\n",
      "          viasampling. Theintuitionforthisapproachisthatsomenonconvexfunctions\n",
      "          becomeapproximatelyconvexwhenblurred.Inmanycases,thisblurringpreserves\n",
      "3 2 3      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "             enoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe\n",
      "          globalminimumbysolvingprogressivelyless-blurredversionsoftheproblem.This\n",
      "            approachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne\n",
      "              aseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom\n",
      "              onefunctiontothenext,arrivingattheglobalminimum,butitmightrequireso\n",
      "            manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\n",
      "        NP-hardoptimizationproblemsremainNP-hard,evenwhencontinuationmethods\n",
      "           areapplicable.Theothertwowayscontinuationmethodsfailbothcorrespondto\n",
      "            themethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,no\n",
      "          matterhowmuchitisblurred.Consider,forexample,thefunction J( θ) = − θθ.\n",
      "             Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\n",
      "               ofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe\n",
      "  originalcostfunction.\n",
      "          Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\n",
      "             problemoflocalminima,localminimaarenolongerbelievedtobetheprimary\n",
      "        problemforneuralnetworkoptimization.Fortunately,continuationmethodscan\n",
      "           stillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\n",
      "         eliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning\n",
      "             oftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates\n",
      "          easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\n",
      "     andprogresstowardaglobalsolution.\n",
      "        Bengio 2009etal.()observedthatanapproachcalled  curriculumlearning,\n",
      "orshaping          ,canbeinterpretedasacontinuationmethod.Curriculumlearningis\n",
      "              basedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\n",
      "           andprogresstolearningmorecomplexconceptsthatdependonthesesimpler\n",
      "           concepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal\n",
      "           training(,; Skinner1958Peterson2004KruegerandDayan2009 ,; ,)andinmachine\n",
      "          learning( ,;,;,). () Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009\n",
      "        justiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) i   aremadeeasierby\n",
      "          increasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions\n",
      "            tothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and\n",
      "          experimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\n",
      "         curriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning\n",
      "             hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\n",
      "             Collobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\n",
      "            vision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\n",
      "            tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin\n",
      "            whichhumansteach( ,):teachersstartbyshowingeasierand Khanetal.2011\n",
      "           moreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface\n",
      "3 2 4      C HAP T E R 8 . O P T I M I Z A T I O N F O R T R AI NI NG D E E P M O D E L S\n",
      "          withthelessobviouscases.Curriculum-basedstrategiesaremoreeﬀectivefor\n",
      "           teachinghumansthanstrategiesbasedonuniformsamplingofexamplesandcan\n",
      "          alsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen\n",
      "2013).\n",
      "          Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\n",
      "         contextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:\n",
      "            ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\n",
      "             stochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways\n",
      "            presentedtothelearner,butwheretheaverageproportionofthemorediﬃcult\n",
      "         examples(here,thosewithlonger-term dependencies)isgraduallyincreased.With\n",
      "         adeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\n",
      "      fromthefulltrainingset)wasobserved.\n",
      "             Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto\n",
      "            regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\n",
      "              theneuralnetworkfamilythatallowneuralnetworkstoscaletoverylargesizesand\n",
      "          processinputdatathathasspecialstructure.Theoptimizationmethodsdiscussed\n",
      "           inthischapterareoftendirectlyapplicabletothesespecializedarchitectureswith\n",
      "   littleornomodiﬁcation.\n",
      "3 2 5 C h a p t e r 9\n",
      " C on v ol utional N e t w ork s\n",
      " Convolutionalnetworks     (,),alsoknownas LeCun1989  convolutionalneural\n",
      "networks            ,orCNNs,areaspecializedkindofneuralnetworkforprocessingdata\n",
      "           thathasaknowngrid-liketopology.Examplesincludetime-seriesdata,whichcan\n",
      "               bethoughtofasa1-Dgridtakingsamplesatregulartimeintervals,andimagedata,\n",
      "              whichcanbethoughtofasa2-Dgridofpixels.Convolutionalnetworkshavebeen\n",
      "        tremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural\n",
      "         network”indicatesthatthenetworkemploysamathematicaloperationcalled\n",
      "convolution         .Convolutionisaspecializedkindoflinearoperation.Convolutional\n",
      "            networksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\n",
      "       multiplicationinatleastoneoftheirlayers.\n",
      "            Inthischapter,weﬁrstdescribewhatconvolutionis.Next,weexplainthe\n",
      "          motivationbehindusingconvolutioninaneuralnetwork. Wethendescribean\n",
      " operationcalledpooling       ,whichalmostallconvolutionalnetworksemploy.Usually,\n",
      "           theoperationusedinaconvolutionalneuralnetworkdoesnotcorrespondprecisely\n",
      "             tothedeﬁnitionofconvolutionasusedinotherﬁelds,suchasengineeringor\n",
      "          puremathematics.Wedescribeseveralvariantsontheconvolutionfunctionthat\n",
      "           arewidelyusedinpracticeforneuralnetworks. Wealsoshowhowconvolution\n",
      "             maybeappliedtomanykindsofdata,withdiﬀerentnumbersofdimensions.We\n",
      "         thendiscussmeansofmakingconvolutionmoreeﬃcient.Convolutionalnetworks\n",
      "          standoutasanexampleofneuroscientiﬁcprinciplesinﬂuencingdeeplearning.\n",
      "         Wediscusstheseneuroscientiﬁcprinciples,thenconcludewithcommentsabout\n",
      "            theroleconvolutionalnetworkshaveplayedinthehistoryofdeeplearning.One\n",
      "             topicthischapterdoesnotaddressishowtochoosethearchitectureofyour\n",
      "             convolutionalnetwork.Thegoalofthischapteristodescribethekindsoftools\n",
      "         thatconvolutionalnetworksprovide,whilechapterdescribesgeneralguidelines 11\n",
      "326   C HAP T E R 9 . C O NV O L UT I O N AL NE T W O R K S\n",
      "           forchoosingwhichtoolstouseinwhichcircumstances.Researchintoconvolutional\n",
      "            networkarchitecturesproceedssorapidlythatanewbestarchitectureforagiven\n",
      "           benchmarkisannouncedeveryfewweekstomonths,renderingitimpracticalto\n",
      "          describeinprintthebestarchitecture.Nonetheless,thebestarchitectureshave\n",
      "        consistentlybeencomposedofthebuildingblocksdescribedhere.\n",
      "   9.1TheConvolutionOperation\n",
      "              Initsmostgeneralform,convolutionisanoperationontwofunctionsofareal-\n",
      "           valuedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples\n",
      "     oftwofunctionswemightuse.\n",
      "             Supposewearetrackingthelocationofaspaceshipwithalasersensor.Our\n",
      "     lasersensorprovidesasingleoutputx(t       ),thepositionofthespaceshipattimet.\n",
      "Bothxandt             arerealvalued,thatis,wecangetadiﬀerentreadingfromthelaser\n",
      "     sensoratanyinstantintime.\n",
      "             Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\n",
      "           estimateofthespaceship’sposition,wewouldliketoaverageseveralmeasurements.\n",
      "             Ofcourse,morerecentmeasurementsaremorerelevant,sowewillwantthisto\n",
      "           beaweightedaveragethatgivesmoreweighttorecentmeasurements. Wecan\n",
      "     dothiswithaweightingfunctionw(a ),wherea      istheageofameasurement.If\n",
      "             weapplysuchaweightedaverageoperationateverymoment,weobtainanew\n",
      "           functionprovidingasmoothedestimateofthepositionofthespaceship: s\n",
      "st() =\n",
      "   xawtada. ()(−) (9.1)\n",
      "   Thisoperationiscalled c o n v o l ut i o n     .Theconvolutionoperationistypically\n",
      "   denotedwithanasterisk:\n",
      "   stxwt. () = (∗)() (9.2)\n",
      "  Inourexample,w          needstobeavalidprobabilitydensityfunction,ortheoutput\n",
      "      willnotbeaweightedaverage.Also,w       needstobeforallnegativearguments, 0\n",
      "             oritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\n",
      "           limitationsareparticulartoourexample,though.Ingeneral,convolutionisdeﬁned\n",
      "               foranyfunctionsforwhichtheaboveintegralisdeﬁnedandmaybeusedforother\n",
      "    purposesbesidestakingweightedaverages.\n",
      "          Inconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the\n",
      "functionx         )totheconvolutionisoftenreferredtoasthe i nput   ,andthesecond\n",
      "3 2 7   C HAP T E R 9 . C O NV O L UT I O N AL NE T W O R K S\n",
      "     argument(inthisexample,thefunctionw  )asthekernel    .Theoutputissometimes\n",
      "     referredtoasthe . featuremap\n",
      "             Inourexample,theideaofalasersensorthatcanprovidemeasurementsat\n",
      "             everyinstantisnotrealistic.Usually,whenweworkwithdataonacomputer,\n",
      "            timewillbediscretized,andoursensorwillprovidedataatregularintervals.\n",
      "              Inourexample,itmightbemorerealistictoassumethatourlaserprovidesa\n",
      "      measurementoncepersecond.Thetimeindext     canthentakeononlyinteger\n",
      "     values.Ifwenowassumethatxandw    aredeﬁnedonlyonintegert   ,wecandeﬁne\n",
      "  thediscreteconvolution:\n",
      "  stxwt () = (∗)() =∞\n",
      "a = −∞   xawta. ()(−) (9.3)\n",
      "          Inmachinelearningapplications,theinputisusuallyamultidimensionalarray\n",
      "             ofdata,andthekernelisusuallyamultidimensionalarrayofparametersthatare\n",
      "           adaptedbythelearningalgorithm.Wewillrefertothesemultidimensionalarrays\n",
      "             astensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored\n",
      "            separately,weusuallyassumethatthesefunctionsarezeroeverywherebutinthe\n",
      "               ﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpractice,we\n",
      "            canimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof\n",
      " arrayelements.\n",
      "             Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\n",
      "      example,ifweuseatwo-dimensionalimageI      asourinput,weprobablyalsowant\n",
      "     touseatwo-dimensionalkernel:K\n",
      "    Si,jIKi,j () = (∗)() =\n",
      "m\n",
      "n       Im,nKim,jn. ( )(−−) (9.4)\n",
      "       Convolutioniscommutative,meaningwecanequivalentlywrite\n",
      "   Si,jKIi,j () = (∗)() =\n",
      "m\n",
      "n      Iim,jnKm,n. (−−)( ) (9.5)\n",
      "           Usuallythelatterformulaismorestraightforwardtoimplementinamachine\n",
      "             learninglibrary,becausethereislessvariationintherangeofvalidvaluesofm\n",
      " and.n\n",
      "        Thecommutativepropertyofconvolutionarisesbecausewehaveﬂippedthe\n",
      "         kernelrelativetotheinput,inthesensethatasm     increases,theindexintothe\n",
      "             inputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip\n",
      "           thekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n",
      "3 2 8   C HAP T E R 9 . C O NV O L UT I O N AL NE T W O R K S\n",
      "              isusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\n",
      "        networkimplementation.Instead,manyneuralnetworklibrariesimplementa\n",
      "   relatedfunctioncalledthecross-correlation      ,whichisthesameasconvolution\n",
      "    butwithoutﬂippingthekernel:\n",
      "   Si,jKIi,j () = (∗)() =\n",
      "m\n",
      "n       Iim,jnKm,n. (+ +)( ) (9.6)\n",
      "         Manymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\n",
      "            Inthistextwefollowthisconventionofcallingbothoperationsconvolutionand\n",
      "              specifywhetherwemeantoﬂipthekernelornotincontextswherekernelﬂipping\n",
      "            isrelevant.Inthecontextofmachinelearning,thelearningalgorithmwilllearn\n",
      "             theappropriatevaluesofthekernelintheappropriateplace,soanalgorithmbased\n",
      "              onconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelativetothe\n",
      "             kernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorareforconvolution\n",
      "           tobeusedaloneinmachinelearning;insteadconvolutionisusedsimultaneously\n",
      "           withotherfunctions,andthecombinationofthesefunctionsdoesnotcommute\n",
      "          regardlessofwhethertheconvolutionoperationﬂipsitskernelornot.\n",
      "           Seeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\n",
      "   toa2-Dtensor.\n",
      "           Discreteconvolutioncanbeviewedasmultiplicationbyamatrix,butthe\n",
      "            matrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\n",
      "            forunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\n",
      "             equaltotherowaboveshiftedbyoneelement.ThisisknownasaToeplitz\n",
      "matrix    .Intwodimensions,a   doublyblockcirculantmatrix  correspondsto\n",
      "           convolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\n",
      "           eachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\n",
      "              whoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\n",
      "           smallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\n",
      "           matrixmultiplicationanddoesnotdependonspeciﬁcpropertiesofthematrix\n",
      "         structureshouldworkwithconvolution,withoutrequiringanyfurtherchanges\n",
      "           totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\n",
      "            furtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare\n",
      "      notstrictlynecessaryfromatheoreticalperspective.\n",
      " 9.2Motivation\n",
      "          Convolutionleveragesthreeimportantideasthatcanhelpimproveamachine\n",
      " learningsystem:  sparseinteractions,  parametersharingandequivariant\n",
      "3 2 9   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "   a b c d\n",
      "   e f g h\n",
      "   i j k l w x\n",
      " y z\n",
      "   a w + b x +\n",
      "  e y + f z   a w + b x +\n",
      "  e y + f z   b w + c x +\n",
      "  f y + g z   b w + c x +\n",
      "  f y + g z   c w + d x +\n",
      "  g y + h z   c w + d x +\n",
      "  g y + h z\n",
      "   e w + f x +\n",
      "  i y + j z   e w + f x +\n",
      "  i y + j z   f w + g x +\n",
      "  j y + k z   f w + g x +\n",
      "  j y + k z   g w + h x +\n",
      "  k y + l z   g w + h x +\n",
      "  k y + l zI n p u t\n",
      "K e r n e l\n",
      "O u t p u t\n",
      "             Figure9.1:Anexampleof2-Dconvolutionwithoutkernelﬂipping.Werestricttheoutput\n",
      "             toonlypositionswherethekernelliesentirelywithintheimage,called“valid”convolution\n",
      "              insomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-leftelementof\n",
      "             theoutputtensorisformedbyapplyingthekerneltothecorrespondingupper-leftregion\n",
      "   oftheinputtensor.\n",
      "representations      .Moreover, convolutionprovidesa meansforworkingwith\n",
      "            inputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\n",
      "          Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\n",
      "         parameterswithaseparateparameterdescribingtheinteractionbetweeneach\n",
      "            inputunitandeachoutputunit.Thismeansthateveryoutputunitinteracts\n",
      "        witheveryinputunit.Convolutionalnetworks,however,typicallyhavesparse\n",
      "interactions   (alsoreferredtoas sparseconnectivityor sparseweights ).This\n",
      "     is accomp lishedbymaking the kernelsmaller than the input.For example,\n",
      "            whenprocessinganimage,theinputimagemighthavethousandsormillionsof\n",
      "            pixels,butwecandetectsmall,meaningfulfeaturessuchasedgeswithkernels\n",
      "              thatoccupyonlytensorhundredsofpixels.Thismeansthatweneedtostore\n",
      "          fewerparameters,whichbothreducesthememory requirementsofthemodel\n",
      "           andimprovesitsstatisticaleﬃciency.Italsomeansthatcomputingtheoutput\n",
      "3 3 0   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "          requiresfeweroperations.Theseimprovementsineﬃciencyareusuallyquitelarge.\n",
      "  Iftherearem  inputsandn     outputs,thenmatrixmultiplicationrequires mn×\n",
      "       parameters,andthealgorithmsusedinpracticehaveO(  mn×   )runtime(per\n",
      "            example).Ifwelimitthenumberofconnectionseachoutputmayhavetok ,then\n",
      "     thesparselyconnectedapproachrequiresonly  kn×  parametersandO(  kn×)\n",
      "           runtime.Formanypracticalapplications,itispossibletoobtaingoodperformance\n",
      "      onthemachinelearningtaskwhilekeepingk     severalordersofmagnitudesmaller\n",
      "thanm          .Forgraphicaldemonstrationsofsparseconnectivity,seeﬁgureand9.2\n",
      "             ﬁgure.Inadeepconvolutionalnetwork,unitsinthedeeperlayersmay 9.3 indirectly\n",
      "               interactwithalargerportionoftheinput,asshowninﬁgure.Thisallowsthe 9.4\n",
      "         networktoeﬃcientlydescribecomplicatedinteractionsbetweenmanyvariablesby\n",
      "          constructingsuchinteractionsfromsimplebuildingblocksthateachdescribeonly\n",
      " sparseinteractions.\n",
      " Parametersharing          referstousingthesameparameterformorethanone\n",
      "              functioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "           Figure9.2:Sparseconnectivity,viewedfrombelow.Wehighlightoneinputunit,x 3 ,and\n",
      "    highlighttheoutputunitsin s      thatareaﬀectedbythisunit. ( T o p )When s  isformedby\n",
      "            convolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x . When ( Bo t t o m )\n",
      "s             isformedbymatrixmultiplication,connectivityisnolongersparse,soalltheoutputs\n",
      "   areaﬀectedbyx 3.\n",
      "3 3 1   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "           Figure9.3:Sparseconnectivity,viewedfromabove.Wehighlightoneoutputunit, s 3 ,and\n",
      "    highlighttheinputunitsinx         thataﬀectthisunit.Theseunitsareknownasthe r e c e p t i v e\n",
      "ﬁ e l dof s 3 .(Top)Whens           isformedbyconvolutionwithakernelofwidth,onlythree 3\n",
      "  inputsaﬀect s 3          . Whenisformedbymatrixmultiplication,connectivityisno (Bottom)s\n",
      "       longersparse,soalltheinputsaﬀect s 3.\n",
      "x 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\n",
      "x 4 x 4h 4 h 4\n",
      "x 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\n",
      "               Figure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\n",
      "                islargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif\n",
      "           thenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12\n",
      "             (section).Thismeansthateventhough 9.3 directconnectionsinaconvolutionalnetare\n",
      "                verysparse,unitsinthedeeperlayerscanbeindirectlyconnectedtoallormostofthe\n",
      " inputimage.\n",
      "3 3 2   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "              isusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\n",
      "             oneelementoftheinputandthenneverrevisited.Asasynonymforparameter\n",
      "       sharing,onecansaythatanetworkhas tiedweights     ,becausethevalueofthe\n",
      "               weightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In\n",
      "             aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\n",
      "           oftheinput(exceptperhapssomeoftheboundarypixels, dependingonthe\n",
      "          designdecisionsregardingtheboundary).Theparametersharingusedbythe\n",
      "           convolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n",
      "              foreverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof\n",
      "   forwardpropagation—i tisstillO (  kn×       )—butitdoesfurtherreducethestorage\n",
      "    requirementsofthemodeltok   parameters.Recallthatk   isusuallyseveralorders\n",
      "   ofmagnitudesmallerthanm .Sincemandn     areusuallyroughlythesamesize,kis\n",
      "   practicallyinsigniﬁcantcomparedto mn×     .Convolutionisthusdramaticallymore\n",
      "          eﬃcientthandensematrixmultiplicationintermsofthememory requirements\n",
      "           andstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,\n",
      "  seeﬁgure.9.5\n",
      "              Asanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6\n",
      "         howsparseconnectivityandparametersharingcandramaticallyimprovethe\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\n",
      "            Figure9.5:Parametersharing.Blackarrowsindicatetheconnectionsthatuseaparticular\n",
      "           parameterintwodiﬀerentmodels. ( T o p )Theblackarrowsindicateusesofthecentral\n",
      "             elementofa3-elementkernelinaconvolutionalmodel.Becauseofparametersharing,this\n",
      "            singleparameterisusedatallinputlocations. Thesingleblackarrowindicates ( Bo t t o m )\n",
      "                theuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\n",
      "          hasnoparametersharing,sotheparameterisusedonlyonce.\n",
      "3 3 3   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "              Figure9.6:Eﬃciencyofedgedetection.Theimageontherightwasformedbytaking\n",
      "              eachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelon\n",
      "               theleft.Thisshowsthestrengthofalltheverticallyorientededgesintheinputimage,\n",
      "              whichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\n",
      "               Theinputimageis320pixelswide,whiletheoutputimageis319pixelswide.This\n",
      "           transformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and\n",
      " requires319 ×280 ×  3=267,     960ﬂoating-pointoperations(twomultiplicationsand\n",
      "            oneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\n",
      "       transformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×  280,orover\n",
      "             eightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor\n",
      "       representingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\n",
      "          performsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000\n",
      "              timesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\n",
      "              zero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\n",
      "           andconvolutionwouldrequirethesamenumberofﬂoating-pointoperationstocompute.\n",
      "       Thematrixwouldstillneedtocontain2 ×319 ×   280=178,  640entries.Convolution\n",
      "            isanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear\n",
      "          transformationofa smalllocalregionacrossthe entireinput.Photocredit:Paula\n",
      "Goodfellow.\n",
      "          eﬃciencyofalinearfunctionfordetectingedgesinanimage.\n",
      "            Inthecaseofconvolution,theparticularformofparametersharingcausesthe\n",
      "     layertohaveapropertycalled e q ui v a r i a nc e      totranslation.Tosayafunctionis\n",
      "             equivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\n",
      "  Speciﬁcally,afunctionf(x     )isequivarianttoafunctiongiff(g(x)) =g(f(x )).In\n",
      "      thecaseofconvolution,ifweletg       beanyfunctionthattranslatestheinput,that\n",
      "         is,shiftsit,thentheconvolutionfunctionisequivarianttog   .Forexample,letI\n",
      "         beafunctiongivingimagebrightnessatintegercoordinates.Letg  beafunction\n",
      "         mappingoneimagefunctiontoanotherimagefunction,suchthatI=g(I  )isthe\n",
      "  imagefunctionwithI( x,y) =I( x −1 ,y     ).ThisshiftseverypixelofI  oneunitto\n",
      "       theright.IfweapplythistransformationtoI     ,thenapplyconvolution,theresult\n",
      "         willbethesameasifweappliedconvolutiontoI    ,thenappliedthetransformation\n",
      "3 3 4   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "g          totheoutput.Whenprocessingtime-seriesdata,thismeansthatconvolution\n",
      "             producesasortoftimelinethatshowswhendiﬀerentfeaturesappearintheinput.\n",
      "                Ifwemoveaneventlaterintimeintheinput,theexactsamerepresentationofit\n",
      "            willappearintheoutput,justlater.Similarlywithimages,convolutioncreatesa\n",
      "                2-Dmapofwherecertainfeaturesappearintheinput.Ifwemovetheobjectinthe\n",
      "             input,itsrepresentationwillmovethesameamountintheoutput.Thisisuseful\n",
      "              forwhenweknowthatsomefunctionofasmallnumberofneighboringpixelsis\n",
      "          usefulwhenappliedtomultipleinputlocations.Forexample,whenprocessing\n",
      "              images,itisusefultodetectedgesintheﬁrstlayerofaconvolutionalnetwork.\n",
      "               Thesameedgesappearmoreorlesseverywhereintheimage,soitispracticalto\n",
      "              shareparametersacrosstheentireimage.Insomecases,wemaynotwishtoshare\n",
      "            parametersacrosstheentireimage.Forexample,ifweareprocessingimagesthat\n",
      "             arecroppedtobecenteredonanindividual’sface,weprobablywanttoextract\n",
      "           diﬀerentfeaturesatdiﬀerentlocations—thepartofthenetworkprocessingthetop\n",
      "              ofthefaceneedstolookforeyebrows,whilethepartofthenetworkprocessing\n",
      "          thebottomofthefaceneedstolookforachin.\n",
      "         Convolutionisnotnaturallyequivarianttosomeothertransformations,such\n",
      "             aschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\n",
      "     forhandlingthesekindsoftransformations.\n",
      "            Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby\n",
      "        matrixmultiplicationwithaﬁxed-shapematrix.Convolutionenablesprocessing\n",
      "             ofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n",
      " 9.3Pooling\n",
      "             Atypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7\n",
      "             Intheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea\n",
      "             setoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\n",
      "          anonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.\n",
      "     Thisstageissometimescalledthe  detectorstage       .Inthethirdstage,weusea\n",
      "         poolingfunctiontomodifytheoutputofthelayerfurther.\n",
      "              Apoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\n",
      "        summarystatisticofthenearbyoutputs.Forexample,the maxpooling(Zhou\n",
      "          andChellappa1988,)operationreportsthemaximumoutputwithinarectangular\n",
      "          neighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\n",
      " neighborhood,the L2        normofarectangularneighborhood,oraweightedaverage\n",
      "       basedonthedistancefromthecentralpixel.\n",
      "3 3 5   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "Convolutional Layer\n",
      "Input to layerConvolution stage :\n",
      "Ane transform ﬃDetector stage:\n",
      "Nonlinearity\n",
      "e.g., rectiﬁed linearPooling stageNext layer\n",
      "Input to layersConvolution layer:\n",
      "Ane transform ﬃDetector layer: Nonlinearity\n",
      "e.g., rectiﬁed linearPooling layerNext layer Complex layer terminology Simple layer terminology\n",
      "             Figure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo\n",
      "           commonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\n",
      "             theconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\n",
      "            eachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping\n",
      "             betweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\n",
      "             ( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\n",
      "                layers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\n",
      "    notevery“layer”hasparameters.\n",
      "         Inallcases,poolinghelpstomaketherepresentationapproximately i n v a r i a n t\n",
      "            tosmalltranslationsoftheinput.Invariancetotranslationmeansthatifwe\n",
      "              translatetheinputbyasmallamount,thevaluesofmostofthepooledoutputs\n",
      "               donotchange.Seeﬁgureforanexampleofhowthisworks. 9.8 Invariancetolocal\n",
      "             translationcanbeausefulpropertyifwecaremoreaboutwhethersomefeature\n",
      "            ispresentthanexactlywhereitis.Forexample,whendeterminingwhetheran\n",
      "              imagecontainsaface,weneednotknowthelocationoftheeyeswithpixel-perfect\n",
      "                  accuracy,wejustneedtoknowthatthereisaneyeontheleftsideofthefaceand\n",
      "                aneyeontherightsideoftheface.Inothercontexts,itismoreimportantto\n",
      "               preservethelocationofafeature.Forexample,ifwewanttoﬁndacornerdeﬁned\n",
      "3 3 6   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "  0.1 1. 0.2 1. 1. 1.\n",
      "0.10.2\n",
      " ... ... ... ...\n",
      "  0.3 0.1 1. 1. 0.3 1.\n",
      "0.21.\n",
      " ... ... ... ...DETECTOR STAGEPOOLING STAGE\n",
      "POOLING STAGE\n",
      "DETECTOR STAGE\n",
      "             Figure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\n",
      "             ofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\n",
      "               rowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions\n",
      "              andapoolingregionwidthofthreepixels. Aviewofthesamenetwork,after ( Bo t t o m )\n",
      "                 theinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\n",
      "                changed,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\n",
      "              unitsaresensitiveonlytothemaximumvalueintheneighborhood,notitsexactlocation.\n",
      "              bytwoedgesmeetingataspeciﬁcorientation,weneedtopreservethelocationof\n",
      "        theedgeswellenoughtotestwhethertheymeet.\n",
      "             Theuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat\n",
      "            thefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\n",
      "            assumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.\n",
      "           Poolingoverspatialregionsproducesinvariancetotranslation,butifwepool\n",
      "          overtheoutputsofseparatelyparametrizedconvolutions,thefeaturescanlearn\n",
      "        whichtransformationstobecomeinvariantto(seeﬁgure).9.9\n",
      "          Becausepoolingsummarizestheresponsesoverawholeneighborhood,itis\n",
      "           possibletousefewerpoolingunitsthandetectorunits,byreportingsummary\n",
      "    statisticsforpoolingregionsspaced k       pixelsapartratherthan1pixelapart.See\n",
      "           ﬁgureforanexample.Thisimprovesthecomputationaleﬃciencyofthe 9.10\n",
      "      networkbecausethenextlayerhasroughly k     timesfewerinputstoprocess.When\n",
      "3 3 7   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "Large response\n",
      "in pooling unitLarge response\n",
      "in pooling unit\n",
      "Large\n",
      "response\n",
      "in detector\n",
      "unit 1Large\n",
      "response\n",
      "in detector\n",
      "unit 3\n",
      "             Figure9.9:Exampleoflearnedinvariances.Apoolingunitthatpoolsovermultiplefeatures\n",
      "             thatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\n",
      "                  theinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn\n",
      "               tobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahandwritten5.\n",
      "                Eachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin\n",
      "               theinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector\n",
      "              unit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\n",
      "            wasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting\n",
      "              intwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly\n",
      "             thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellowetal.,\n",
      "           2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\n",
      "           invarianttotranslation;thismultichannelapproachisonlynecessaryforlearningother\n",
      "transformations.\n",
      "  0. 1 1. 0. 2 1. 0. 2\n",
      "0. 10. 1\n",
      " 0. 0 0. 1\n",
      "              Figure9.10:Poolingwithdownsampling.Hereweusemaxpoolingwithapoolwidthof\n",
      "               threeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor\n",
      "             oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\n",
      "                thattherightmostpoolingregionhasasmallersizebutmustbeincludedifwedonot\n",
      "       wanttoignoresomeofthedetectorunits.\n",
      "                thenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas\n",
      "            whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication),this\n",
      "3 3 8   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "            reductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand\n",
      "      reducedmemory requirementsforstoringtheparameters.\n",
      "           Formanytasks,poolingisessentialforhandlinginputsofvaryingsize. For\n",
      "              example,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation\n",
      "               layermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\n",
      "           oﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe\n",
      "            samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\n",
      "              ﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary\n",
      "            statistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\n",
      "            Sometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\n",
      "             useinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\n",
      "           poolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\n",
      "           locationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\n",
      "             diﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna\n",
      "             singlepoolingstructurethatisthenappliedtoallimages( ,). Jiaetal.2012\n",
      "          Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\n",
      "        top-downinformation,suchasBoltzmannmachinesandautoencoders.These\n",
      "             issuesarediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\n",
      "         PoolinginconvolutionalBoltzmannmachinesispresentedinsection. The 20.6\n",
      "          inverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworksare\n",
      "   coveredinsection .20.10.6\n",
      "        Someexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation\n",
      "        usingconvolutionandpoolingareshowninﬁgure.9.11\n",
      "    9.4Convolutionand Poolingas anInﬁnitely Strong\n",
      "Prior\n",
      "    Recalltheconceptofa  priorprobabilitydistribution    fromsection.Thisis 5.6\n",
      "            aprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\n",
      "          aboutwhatmodelsarereasonable,beforewehaveseenanydata.\n",
      "           Priorscanbeconsideredweakorstrongdependingonhowconcentratedthe\n",
      "              probabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh\n",
      "            entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\n",
      "               thedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\n",
      "             entropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\n",
      "         moreactiveroleindeterminingwheretheparametersendup.\n",
      "           Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\n",
      "3 3 9   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "Input image: \n",
      "256x256x3Output of \n",
      "convolution + \n",
      "ReLU: 256x256x64Output of pooling \n",
      "with stride 4: \n",
      "64x64x64Output of \n",
      "convolution + \n",
      "ReLU: 64x64x64Output of pooling \n",
      "with stride 4: \n",
      "16x16x64Output of reshape to \n",
      "vector:\n",
      "16,384 uni tsOutput of matrix \n",
      "multiply: 1,000 unitsOutput of softmax: \n",
      "1,000 class \n",
      "probabilities\n",
      "Input image: \n",
      "256x256x3Output of \n",
      "convolution + \n",
      "ReLU: 256x256x64Output of pooling \n",
      "with stride 4: \n",
      "64x64x64Output of \n",
      "convolution + \n",
      "ReLU: 64x64x64Output of pooling to \n",
      "3x3 grid: 3x3x64Output of reshape to \n",
      "vector:\n",
      "576 uni tsOutput of matrix \n",
      "multiply: 1,000 unitsOutput of softmax: \n",
      "1,000 class \n",
      "probabilities\n",
      "Input image: \n",
      "256x256x3Output of \n",
      "convolution + \n",
      "ReLU: 256x256x64Output of pooling \n",
      "with stride 4: \n",
      "64x64x64Output of \n",
      "convolution + \n",
      "ReLU: 64x64x64Output of \n",
      "convolution:\n",
      "16x16x1,000Output of average \n",
      "pooling: 1x1x1,000Output of softmax: \n",
      "1,000 class \n",
      "probabilities\n",
      "Output of pooling \n",
      "with stride 4: \n",
      "16x16x64\n",
      "         Figure9.11: Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The\n",
      "               speciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare\n",
      "              designedtobeveryshallowtoﬁtontothepage.Realconvolutionalnetworksalsooften\n",
      "            involvesigniﬁcantamountsofbranching,unlikethechainstructuresusedhereforsimplicity.\n",
      "           ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize.Afteralternatingbetween\n",
      "              convolutionandpoolingforafewlayers,thetensorfortheconvolutionalfeaturemapis\n",
      "              reshapedtoﬂattenoutthespatialdimensions.Therestofthenetworkisanordinary\n",
      "          feedforwardnetworkclassiﬁer,asdescribedinchapter. Aconvolutionalnetwork 6 ( C e n t e r )\n",
      "             thatprocessesavariablysizedimagebutstillmaintainsafullyconnectedsection.This\n",
      "              networkusesapoolingoperationwithvariablysizedpoolsbutaﬁxednumberofpools,\n",
      "                inordertoprovideaﬁxed-sizevectorof576unitstothefullyconnectedportionofthe\n",
      "           network. Aconvolutionalnetworkthatdoesnothaveanyfullyconnectedweight ( R i g h t )\n",
      "             layer.Instead,thelastconvolutionallayeroutputsonefeaturemapperclass.Themodel\n",
      "               presumablylearnsamapofhowlikelyeachclassistooccurateachspatiallocation.\n",
      "              Averagingafeaturemapdowntoasinglevalueprovidestheargumenttothesoftmax\n",
      "   classiﬁeratthetop.\n",
      "3 4 0   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "          thattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\n",
      "      supportthedatagivetothosevalues.\n",
      "             Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,\n",
      "            butwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior\n",
      "               saysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\n",
      "              neighborbutshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\n",
      "            exceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden\n",
      "             unit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely\n",
      "           strongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n",
      "             saysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\n",
      "            equivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior\n",
      "        thateachunitshouldbeinvarianttosmalltranslations.\n",
      "            Ofcourse,implementingaconvolutionalnetasafullyconnectednetwithan\n",
      "         inﬁnitelystrongpriorwouldbeextremelywastefulcomputationally.Butthinking\n",
      "              ofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan\n",
      "        giveussomeinsightsintohowconvolutionalnetswork.\n",
      "           Onekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting.Like\n",
      "           anyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\n",
      "             bythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\n",
      "           information,thenusingpoolingonallfeaturescanincreasethetrainingerror.\n",
      "          Someconvolutionalnetworkarchitectures( ,)aredesignedto Szegedyetal.2014a\n",
      "              usepoolingonsomechannelsbutnotonotherchannels,inordertogetboth\n",
      "           highlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation\n",
      "          invariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\n",
      "             verydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\n",
      "inappropriate.\n",
      "            Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\n",
      "          tionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\n",
      "              performance.Modelsthatdonotuseconvolutionwouldbeabletolearnevenifwe\n",
      "             permutedallthepixelsintheimage.Formanyimagedatasets,thereareseparate\n",
      "    benchmarksformodelsthatare  permutationinvariant   andmustdiscoverthe\n",
      "             conceptoftopologyvialearningandformodelsthathavetheknowledgeofspatial\n",
      "       relationshipshardcodedintothembytheirdesigner.\n",
      "3 4 1   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "      9.5VariantsoftheBasicConvolutionFunction\n",
      "           Whendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\n",
      "            notreferexactlytothestandarddiscreteconvolutionoperationasitisusually\n",
      "          understoodinthemathematicalliterature.Thefunctionsusedinpracticediﬀer\n",
      "           slightly.Herewedescribethesediﬀerencesindetailandhighlightsomeuseful\n",
      "       propertiesofthefunctionsusedinneuralnetworks.\n",
      "             First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\n",
      "           actuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\n",
      "             parallel.Thisisbecauseconvolutionwithasinglekernelcanextractonlyonekind\n",
      "             offeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\n",
      "         networktoextractmanykindsoffeatures,atmanylocations.\n",
      "               Additionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\n",
      "           gridofvector-valuedobservations. Forexample,acolorimagehasared,green\n",
      "            andblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\n",
      "               tothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput\n",
      "           ofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we\n",
      "              usuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with\n",
      "            oneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates\n",
      "           ofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\n",
      "            willactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin\n",
      "              thebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\n",
      "        Becauseconvolutionalnetworksusuallyusemultichannelconvolution,thelinear\n",
      "             operationstheyarebasedonarenotguaranteedtobecommutative,evenifkernel\n",
      "          ﬂippingisused.Thesemultichanneloperationsareonlycommutativeifeach\n",
      "          operationhasthesamenumberofoutputchannelsasinputchannels.\n",
      "      Assumewehavea4-Dkerneltensor K  withelement K i , j , k , l  givingtheconnection\n",
      "     strengthbetweenaunitinchanneli       oftheoutputandaunitinchannelj ofthe\n",
      "    input,withanoﬀsetofk rowsandl       columnsbetweentheoutputunitandthe\n",
      "        inputunit.Assumeourinputconsistsofobserveddata V  withelement V i , j , kgiving\n",
      "       thevalueoftheinputunitwithinchanneli atrowj andcolumnk  .Assumeour\n",
      "  outputconsistsof Z    withthesameformatas V .If Z   isproducedbyconvolving K\n",
      "     acrosswithoutﬂipping,then V K\n",
      "Z i , j , k =\n",
      "l , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , (9.7)\n",
      "   wherethesummationoverl,mandn       isoverallvaluesforwhichthetensor\n",
      "          indexingoperationsinsidethesummationarevalid.Inlinearalgebranotation,\n",
      "3 4 2   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "             weindexintoarraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −  1inthe\n",
      "           aboveformula.Programmi nglanguagessuchasCandPythonindexstartingfrom\n",
      "      0,renderingtheaboveexpressionevensimpler.\n",
      "              Wemaywanttoskipoversomepositionsofthekerneltoreducethecomputa-\n",
      "              tionalcost(attheexpenseofnotextractingourfeaturesasﬁnely).Wecanthink\n",
      "              ofthisasdownsamplingtheoutputofthefullconvolutionfunction.Ifwewantto\n",
      "  sampleonlyeverys           pixelsineachdirectionintheoutput,thenwecandeﬁnea\n",
      "     downsampledconvolutionfunctionsuchthatc\n",
      "Z i , j , k  = ( )c K V,,s i , j , k=\n",
      "l , m , n\n",
      "V l , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n\n",
      " .(9.8)\n",
      "  Werefertos asthe s t r i de       ofthisdownsampledconvolution.Itisalsopossible\n",
      "              todeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan 9.12\n",
      "illustration.\n",
      "          Oneessentialfeatureofanyconvolutionalnetworkimplementationistheability\n",
      "     toimplicitlyzeropadtheinput V       tomakeitwider.Withoutthisfeature,the\n",
      "             widthoftherepresentationshrinksbyonepixellessthanthekernelwidthat\n",
      "             eachlayer.Zeropaddingtheinputallowsustocontrolthekernelwidthand\n",
      "            thesizeoftheoutputindependently.Withoutzeropadding,weareforcedto\n",
      "            choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\n",
      "          kernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.\n",
      "     Seeﬁgureforanexample. 9.13\n",
      "           Threespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\n",
      "             theextremecaseinwhichnozeropaddingisusedwhatsoever,andtheconvolution\n",
      "             kernelisallowedtovisitonlypositionswheretheentirekerneliscontainedentirely\n",
      "        withintheimage.InMATLABterminology,thisiscalled v a l i d  convolution.In\n",
      "                thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\n",
      "             theinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\n",
      "              thesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidthmand\n",
      "   thekernelhaswidthk      ,theoutputwillbeofwidth  mk−     +1. Therateofthis\n",
      "             shrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\n",
      "             greaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\n",
      "             inthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill\n",
      "   eventuallydropto1×        1,atwhichpointadditionallayerscannotmeaningfullybe\n",
      "          consideredconvolutional.Anotherspecialcaseofthezero-paddingsettingiswhen\n",
      "                justenoughzeropaddingisaddedtokeepthesizeoftheoutputequaltothesize\n",
      "     oftheinput.MATLABcallsthis s a m e      convolution.Inthiscase,thenetwork\n",
      "           cancontainasmanyconvolutionallayersastheavailablehardwarecansupport,\n",
      "          sincetheoperationofconvolutiondoesnotmodifythearchitecturalpossibilities\n",
      "3 4 3   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\n",
      "x 4 x 4 x 5 x 5s 3 s 3\n",
      "x 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\n",
      "x 4 x 4z 4 z 4\n",
      "x 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3S t r i d e d\n",
      "c onv ol u t i on\n",
      "D ow n s am p l i n g\n",
      "C onv ol u t i on\n",
      "             Figure 9.12:Convolutionwithastride.Inthisexample,weuseastrideoftwo.\n",
      "            ( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation. ( Bo t -\n",
      "           t o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto\n",
      "          convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\n",
      "         involvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\n",
      "   thatarethendiscarded.\n",
      "            availabletothenextlayer.Theinputpixelsneartheborder,however,inﬂuence\n",
      "             feweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmakethe\n",
      "          borderpixelssomewhatunderrepresentedinthemodel.Thismotivatestheother\n",
      "      extremecase,whichMATLABreferstoas f ul l     convolution,inwhichenoughzeros\n",
      "       areaddedforeverypixeltobevisitedk      timesineachdirection,resultinginan\n",
      "                 outputimageofwidth .Inthiscase,theoutputpixelsneartheborder mk+−1\n",
      "              areafunctionoffewerpixelsthantheoutputpixelsnearthecenter.Thiscan\n",
      "              makeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin\n",
      "           theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\n",
      "3 4 4   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      " . . . . . .. . .\n",
      ". . . . . .. . . . . .. . . . . .\n",
      "             Figure9.13:Theeﬀectofzeropaddingonnetworksize.Consideraconvolutionalnetwork\n",
      "                  withakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\n",
      "           onlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\n",
      "             network,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\n",
      "                shrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly\n",
      "               abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\n",
      "              soarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\n",
      "             bemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressive,andsome\n",
      "            shrinkingisinevitableinthiskindofarchitecture. Byaddingﬁveimplicitzeros ( Bo t t o m )\n",
      "              toeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\n",
      "     makeanarbitrarilydeepconvolutionalnetwork.\n",
      "           termsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”\n",
      "convolution.\n",
      "              Insomecases,wedonotactuallywanttouseconvolution,butwanttouse\n",
      "           locallyconnectedlayersinstead(,,).Inthiscase,theadjacency LeCun19861989\n",
      "               matrixinthegraphofourMLPisthesame,buteveryconnectionhasitsown\n",
      "     weight,speciﬁedbya6-Dtensor W   .Theindicesinto W arerespectively: i ;the\n",
      "3 4 5   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      " outputchannel;j   ,theoutputrow;k   ,theoutputcolumn;l   ,theinputchannel;m,\n",
      "      therowoﬀsetwithintheinput;andn       ,thecolumnoﬀsetwithintheinput.The\n",
      "          linearpartofalocallyconnectedlayeristhengivenby\n",
      "Z i , j , k=\n",
      "l , m , n[ V l , j m , k n + − 1 + − 1w i , j , k , l , m , n  ]. (9.9)\n",
      "    Thisissometimesalsocalled  unsharedconvolution      ,becauseitisasimilaroper-\n",
      "           ationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\n",
      "         acrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\n",
      "connections.\n",
      "            Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\n",
      "                 afunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame\n",
      "                featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\n",
      "                   isapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\n",
      "image.\n",
      "             Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\n",
      "            inwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\n",
      "channeli           tobeafunctionofonlyasubsetoftheinputchannelsl  .Acommon\n",
      "        waytodothisistomaketheﬁrstm       outputchannelsconnecttoonlytheﬁrst\n",
      "n   inputchannels,thesecondm       outputchannelsconnecttoonlythesecondn\n",
      "            inputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15\n",
      "         betweenfewchannelsallowsthenetworktohavefewerparameters, reducing\n",
      "         memory consumption,increasingstatisticaleﬃciency,andreducingtheamountof\n",
      "        computationneededtoperformforwardandback-propagation. Itaccomplishes\n",
      "        thesegoalswithoutreducingthenumberofhiddenunits.\n",
      " Tiledconvolution           ( ,; ,)oﬀersacom- GregorandLeCun2010aLeetal.2010\n",
      "           promisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan\n",
      "               learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\n",
      "            thatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\n",
      "            neighboringlocationswillhavediﬀerentﬁlters,asinalocallyconnectedlayer,but\n",
      "            thememory requirementsforstoringtheparameterswillincreaseonlybyafactor\n",
      "                 ofthesizeofthissetofkernels,ratherthanbythesizeoftheentireoutputfeature\n",
      "            map.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiledconvolution, 9.16\n",
      "  andstandardconvolution.\n",
      "     Todeﬁnetiledconvolutionalgebraically,let K      bea6-Dtensor,wheretwoof\n",
      "           thedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan\n",
      "             havingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\n",
      "   throughasetoft         diﬀerentchoicesofkernelstackineachdirection.Ift  isequalto\n",
      "3 4 6   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "x 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\n",
      "x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "    a        b a        b a        b a        b a            a        b c      d e     f g      h  i  \n",
      "x 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\n",
      "         Figure9.14:Comparisonoflocalconnections,convolution,andfullconnections.\n",
      "               (Top)Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\n",
      "              auniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\n",
      "             (Center)Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\n",
      "              thesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunits\n",
      "              interactwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\n",
      "            hasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly\n",
      "              acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\n",
      "             (Bottom)Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\n",
      "               edgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\n",
      "            diagram).Itdoesnot,however,havetherestrictedconnectivityofthelocallyconnected\n",
      "layer.\n",
      "           theoutputwidth,thisisthesameasalocallyconnectedlayer.\n",
      "Z i , j , k=\n",
      "l , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % +1 % +1 , (9.10)\n",
      "      wherepercentisthemodulooperation,witht%t = 0(,t +1)%t   = 1,andsoon.\n",
      "             Itisstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangefor\n",
      " eachdimension.\n",
      "3 4 7   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "Input TensorOutput Tensor\n",
      "Spatial coordinatesChannel coordinates\n",
      "           Figure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto\n",
      "              onlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\n",
      "    thesecondtwoinputchannels.\n",
      "3 4 8   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "    a        b a        b a        b a        b a           a        b c      d e     f g      h  i  \n",
      "x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\n",
      "x 4 x 4s 4 s 4\n",
      "x 5 x 5s 5 s 5\n",
      "    a        b c        d a        b c        d a        \n",
      "           Figure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\n",
      "             convolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\n",
      "                 sizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\n",
      "             Thediﬀerencesbetweenthemethodsliesinhowtheyshareparameters.(Top)Alocally\n",
      "              connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsown\n",
      "             weightbylabelingeachconnectionwithauniqueletter. Tiledconvolutionhasa (Center)\n",
      " setof t        diﬀerentkernels.Hereweillustratethecaseof t     = 2. Oneofthesekernelshas\n",
      "                edgeslabeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.”Eachtimewe\n",
      "                 moveonepixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.This\n",
      "             meansthat,likethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerent\n",
      "           parameters.Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailable\n",
      "                kernels,wecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultiple\n",
      "of t          steps,thentheyshareparameters. Traditionalconvolutionisequivalentto (Bottom)\n",
      "  tiledconvolutionwith t           = 1.Thereisonlyonekernel,anditisappliedeverywhere,as\n",
      "              indicatedinthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere.\n",
      "          Locallyconnectedlayersandtiledconvolutionallayersbothhaveaninteresting\n",
      "            interactionwithmaxpooling:thedetectorunitsoftheselayersaredrivenby\n",
      "           diﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof\n",
      "3 4 9   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "           thesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\n",
      "           learnedtransformation(seeﬁgure).Convolutionallayersarehardcodedtobe 9.9\n",
      "   invariantspeciﬁcallytotranslation.\n",
      "         Otheroperationsbesidesconvolutionareusuallynecessarytoimplementa\n",
      "           convolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\n",
      "             gradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\n",
      "          Insomesimplecases, thisoperationcanbeperformedusingtheconvolution\n",
      "             operation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\n",
      "    donothavethisproperty.\n",
      "             Recallthatconvolutionisalinearoperationandcanthusbedescribedasa\n",
      "             matrixmultiplication(ifweﬁrstreshapetheinputtensorintoaﬂatvector).The\n",
      "             matrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparse,and\n",
      "              eachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\n",
      "             helpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\n",
      "network.\n",
      "           Multiplicationbythetransposeofthematrixdeﬁnedbyconvolutionisone\n",
      "          suchoperation.Thisistheoperationneededtoback-propagateerrorderivatives\n",
      "           throughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks\n",
      "              thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\n",
      "             wishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simardetal.1992\n",
      "           Reconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\n",
      "             describedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\n",
      "         Transposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\n",
      "          models.Likethekernelgradientoperation,thisinputgradientoperationcan\n",
      "           sometimesbeimplementedusingaconvolutionbutingeneralrequiresathird\n",
      "          operationtobeimplemented. Caremustbetakentocoordinatethistranspose\n",
      "            operationwiththeforwardpropagation. Thesizeoftheoutputthatthetranspose\n",
      "           operationshouldreturndependsonthezero-paddingpolicyandstrideofthe\n",
      "           forwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s\n",
      "            outputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan\n",
      "              resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\n",
      "        toldwhatthesizeoftheoriginalinputwas.\n",
      "        Thesethreeoperations—convolution,backpropfromoutputtoweights,and\n",
      "           backpropfromoutputtoinputs—aresuﬃcienttocomputeallthegradientsneeded\n",
      "     totrainany depthof feedforward convolutional network, as wellas totrain\n",
      "         convolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\n",
      "           convolution. See ()forafullderivationoftheequationsinthe Goodfellow2010\n",
      "           fullygeneralmultidimensional,multiexamplecase.Togiveasenseofhowthese\n",
      "3 5 0   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "         equationswork,wepresentthetwo-dimensional,singleexampleversionhere.\n",
      "          Supposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\n",
      "   convolutionofkernelstack K    appliedtomultichannelimage V withstridesas\n",
      "               deﬁnedby ,asinequation.Supposewewanttominimizesomeloss c,,s ( K V) 9.8\n",
      "functionJ( V K,        ).Duringforwardpropagation, wewillneedtousec itselfto\n",
      "output Z             ,whichisthenpropagatedthroughtherestofthenetworkandusedto\n",
      "   computethecostfunctionJ       .Duringback-propagation,wewillreceiveatensor G\n",
      "  suchthat G i , j , k=∂\n",
      "∂ Z i , j , k J,. ( V K)\n",
      "             Totrainthenetwork,weneedtocomputethederivativeswithrespecttothe\n",
      "           weightsinthekernel.Todoso,wecanuseafunction\n",
      "  g,,s ( G V) i , j , k , l=∂\n",
      "∂ K i , j , k , l J,( V K) =\n",
      "m , nG i , m , n Vj , m s k , n s l ( − × 1 ) + ( − × 1 ) + .(9.11)\n",
      "               Ifthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\n",
      "    thegradientwithrespectto V       toback-propagatetheerrorfurtherdown.Todo\n",
      "     so,wecanuseafunction\n",
      "  h,,s ( K G) i , j , k=∂\n",
      "∂ V i , j , k  J,( V K) (9.12)\n",
      "=\n",
      "l , m\n",
      "s . t.\n",
      "( 1 ) + = l − × s m j\n",
      "n , p\n",
      "s . t.\n",
      "( 1 ) + = n − × s p k\n",
      "qK q , i , m , p G q , l , n .(9.13)\n",
      "     Autoencodernetworks, described inchapter, arefeedforwardnetworks 14\n",
      "              trainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n",
      "   whichcopiesitsinputx   toanapproximatereconstructionr  usingthefunction\n",
      "W Wx       .Itis common formore gen eralautoencoderstouse multiplication\n",
      "              bythetransposeoftheweightmatrixjustasPCAdoes.Tomakesuchmodels\n",
      "     convolutional,wecanusethefunctionh      toperformthetransposeoftheconvolution\n",
      "     operation.Supposewehavehiddenunits H    inthesameformatas Z  andwedeﬁne\n",
      " areconstruction\n",
      "    R K H = (h,,s.) (9.14)\n",
      "           Totraintheautoencoder,wewillreceivethegradientwithrespectto Ras\n",
      " atensor E            .Totrainthedecoder,weneedtoobtainthegradientwithrespect\n",
      "to K    .Thisisgivenbyg(  H E,,s         ).Totraintheencoder,weneedtoobtainthe\n",
      "   gradientwithrespectto H    .Thisisgivenbyc(  K E,,s     ).Itisalsopossibleto\n",
      " diﬀerentiatethroughgusingcandh        ,buttheseoperationsarenotneededforthe\n",
      "      back-propagationalgorithmonanystandardnetworkarchitectures.\n",
      "3 5 1   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "             Generally,wedonotuseonlyalinearoperationtotransformfromtheinputs\n",
      "              totheoutputsinaconvolutionallayer.Wegenerallyalsoaddsomebiastermto\n",
      "            eachoutputbeforeapplyingthenonlinearity.Thisraisesthequestionofhowto\n",
      "            shareparametersamongthebiases.Forlocallyconnectedlayers,itisnaturalto\n",
      "               giveeachunititsownbias,andfortiledconvolution,itisnaturaltosharethe\n",
      "             biaseswiththesametilingpatternasthekernels.Forconvolutionallayers,itis\n",
      "               typicaltohaveonebiasperchanneloftheoutputandshareitacrossalllocations\n",
      "               withineachconvolutionmap.Iftheinputisofknown,ﬁxedsize,however,itisalso\n",
      "             possibletolearnaseparatebiasateachlocationoftheoutputmap.Separating\n",
      "             thebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,butitallows\n",
      "            themodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerentlocations.\n",
      "             Forexample,whenusingimplicitzeropadding,detectorunitsattheedgeofthe\n",
      "         imagereceivelesstotalinputandmayneedlargerbiases.\n",
      "  9.6StructuredOutputs\n",
      "          Convolutionalnetworkscanbeusedtooutputahigh-dimensionalstructuredobject,\n",
      "               ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorarealvaluefor\n",
      "             aregressiontask.Typicallythisobjectisjustatensor,emittedbyastandard\n",
      "         convolutionallayer.Forexample,themodelmightemitatensor S  ,where S i , j , k\n",
      "     istheprobabilitythatpixel( j,k         )oftheinputtothenetworkbelongstoclassi.\n",
      "              Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\n",
      "      thatfollowtheoutlinesofindividualobjects.\n",
      "              Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthan\n",
      "            theinputplane,asshowninﬁgure. Inthekindsofarchitecturestypically 9.13\n",
      "              usedforclassiﬁcationofasingleobjectinanimage,thegreatestreductioninthe\n",
      "            spatialdimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.\n",
      "              Toproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\n",
      "            altogether( ,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\n",
      "            gridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\n",
      "      useapoolingoperatorwithunitstride.\n",
      "            Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\n",
      "            oftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween\n",
      "         neighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto\n",
      "             usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\n",
      "             thedeepnet( ,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\n",
      "           bythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\n",
      "3 5 2   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "ˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )\n",
      "H( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\n",
      "XXU U UV V V W W\n",
      "            Figure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\n",
      "              inputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\n",
      "            channels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y   ,withaprobability\n",
      "             distributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,\n",
      "        imagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Y    inasingleshot,the\n",
      "     recurrentnetworkiterativelyreﬁnesitsestimateˆ Y     byusingapreviousestimateofˆ Y\n",
      "             asinputforcreatinganewestimate. Thesameparametersareusedforeachupdated\n",
      "               estimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof\n",
      " convolutionkernels U           isusedoneachsteptocomputethehiddenrepresentationgiventhe\n",
      "    inputimage.Thekerneltensor V          isusedtoproduceanestimateofthelabelsgiventhe\n",
      "         hiddenvalues.Onallbuttheﬁrststep,thekernels W  areconvolvedoverˆ Y toprovide\n",
      "                inputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because\n",
      "                thesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\n",
      "   describedinchapter.10\n",
      "           kindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\n",
      "       thearchitectureofsucharecurrentconvolutionalnetwork.\n",
      "             Onceapredictionforeachpixelismade,variousmethodscanbeusedto\n",
      "            furtherprocessthesepredictionstoobtainasegmentationoftheimageintoregions\n",
      "              ( ,; Briggmanetal.2009Turaga 2010Farabet2013 etal.,; etal.,).Thegeneralidea\n",
      "              istoassumethatlargegroupsofcontiguouspixelstendtobeassociatedwiththe\n",
      "         samelabel.Graphicalmodelscandescribetheprobabilisticrelationshipsbetween\n",
      "         neighboringpixels.Alternatively,theconvolutionalnetworkcanbetrainedto\n",
      "           maximizeanapproximationofthegraphicalmodeltrainingobjective( , Ningetal.\n",
      "    2005Thompson2014 ; etal.,).\n",
      "3 5 3   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "  9.7DataTypes\n",
      "           Thedatausedwithaconvolutionalnetworkusuallyconsistofseveralchannels,\n",
      "             eachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace\n",
      "            ortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1\n",
      "   andnumberofchannels.\n",
      "            Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\n",
      "().2010\n",
      "               Sofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest\n",
      "          datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\n",
      "             isthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\n",
      "        inputsimplycannotberepresentedbytraditional,matrixmultiplication-based\n",
      "          neuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\n",
      "         evenwhencomputationalcostandoverﬁttingarenotsigniﬁcantissues.\n",
      "             Forexample,consideracollectionofimagesinwhicheachimagehasadiﬀerent\n",
      "               widthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof\n",
      "            ﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\n",
      "               diﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\n",
      "         convolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\n",
      "           multiplication;thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock\n",
      "             circulantmatrixforeachsizeofinput.Sometimestheoutputofthenetworkas\n",
      "                wellastheinputisallowedtohavevariablesize,forexample,ifwewanttoassign\n",
      "                aclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis\n",
      "           necessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for\n",
      "                example,ifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase,\n",
      "            wemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\n",
      "              poolingregionsscaleinsizeproportionaltothesizeoftheinput,tomaintaina\n",
      "             ﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategyareshown\n",
      "  inﬁgure.9.11\n",
      "           Notethattheuseofconvolutionforprocessingvariablysizedinputsmakes\n",
      "            senseonlyforinputsthathavevariablesizebecausetheycontainvaryingamounts\n",
      "            ofobservationofthesamekindofthing—diﬀerentlengthsofrecordingsovertime,\n",
      "           diﬀerentwidthsofobservationsoverspace,andsoforth.Convolutiondoesnot\n",
      "             makesenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent\n",
      "           kindsofobservations.Forexample,ifweareprocessingcollegeapplications,and\n",
      "            ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\n",
      "             applicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\n",
      "            sameweightsoverfeaturescorrespondingtothegradesaswellasthefeatures\n",
      "3 5 4   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "  Singlechannel Multichannel\n",
      "1-D  Audio waveform:The axis we\n",
      "   convolveovercorrespondsto\n",
      "    time.Wediscretizetimeand\n",
      "    measuretheamplitudeofthe\n",
      "    waveformoncepertimestep.   Skeletonanimationdata:Anima-\n",
      "   tionsof3-Dcomputer-rendered\n",
      "    charactersaregeneratedbyalter-\n",
      "      ingtheposeofa“skeleton”over\n",
      "      time.Ateachpointintime,the\n",
      "     poseofthecharacterisdescribed\n",
      "      byaspeciﬁcationoftheanglesof\n",
      "      eachofthejointsinthecharac-\n",
      "    ter’sskeleton.Eachchannelin\n",
      "      thedatawefeedtotheconvolu-\n",
      "    tionalmodelrepresentstheangle\n",
      "     aboutoneaxisofonejoint.\n",
      "2-D      Audiodatathathasbeenprepro-\n",
      "    cessedwithaFouriertransform:\n",
      "     Wecantransformtheaudiowave-\n",
      "      formintoa2-Dtensorwithdif-\n",
      "    ferentrowscorrespondingtodif-\n",
      "  ferentfrequenciesand diﬀerent\n",
      "   columnscorrespondingtodiﬀer-\n",
      "     entpointsintime.Usingconvolu-\n",
      "      tioninthetimemakesthemodel\n",
      "     equivarianttoshiftsintime.Us-\n",
      "    ingconvolutionacrossthefre-\n",
      "    quencyaxismakesthemodel\n",
      "    equivarianttofrequency,sothat\n",
      "      thesamemelodyplayedinadif-\n",
      "    ferentoctaveproducesthesame\n",
      "    representationbutatadiﬀerent\n",
      "    heightinthenetwork’soutput.    Colorimagedata:Onechannel\n",
      "     containstheredpixels,onethe\n",
      " green pixels, and one the blue\n",
      "   pixels.Theconvolutionkernel\n",
      "    movesoverboththehorizontal\n",
      "      andtheverticalaxesoftheim-\n",
      "   age,conferringtranslationequiv-\n",
      "   arianceinbothdirections.\n",
      "3-D    Volumetricdata:Acommon\n",
      "       sourceofthiskindofdataismed-\n",
      "    icalimagingtechnology,suchas\n",
      " CTscans.     Colorvideodata:Oneaxiscorre-\n",
      "      spondstotime,onetotheheight\n",
      "      ofthevideoframe,andoneto\n",
      "     thewidthofthevideoframe.\n",
      "             Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\n",
      "networks.\n",
      "3 5 5   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "    correspondingtothetestscores.\n",
      "   9.8EﬃcientConvolutionAlgorithms\n",
      "        Modernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\n",
      "        thanonemillionunits.Powerfulimplementationsexploitingparallelcomputation\n",
      "           resources,asdiscussedinsection,areessential. Inmanycases,however,it 12.1\n",
      "           isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\n",
      "algorithm.\n",
      "            Convolutionisequivalenttoconvertingboththeinputandthekerneltothe\n",
      "        frequencydomainusingaFouriertransform,performingpoint-wisemultiplication\n",
      "            ofthetwosignals, andconvertingbacktothetimedomainusinganinverse\n",
      "            Fouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\n",
      "   implementationofdiscreteconvolution.\n",
      " Whenad         -dimensionalkernelcanbeexpressedastheouter productofd\n",
      "        vectors,onevectorperdimension,thekerneliscalled s e pa r a bl e  .Whenthe\n",
      "           kernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocomposed\n",
      "         one-dimensionalconvolutionswitheachofthesevectors.Thecomposedapproach\n",
      "     issigniﬁcantlyfasterthanperformingoned    -dimensionalconvolutionwiththeir\n",
      "           outerproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\n",
      "   Ifthekernelisw        elementswideineachdimension,thennaivemultidimensional\n",
      " convolutionrequiresO(wd       )runtimeandparameterstoragespace,whileseparable\n",
      " convolutionrequiresO(  wd×       )runtimeandparameterstoragespace.Ofcourse,\n",
      "        noteveryconvolutioncanberepresentedinthisway.\n",
      "        Devisingfasterwaysofperformingconvolutionorapproximateconvolution\n",
      "              withoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\n",
      "           niquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause\n",
      "             inthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\n",
      "     anetworkthantoitstraining.\n",
      "    9.9RandomorUnsupervisedFeatures\n",
      "          Typically,themostexpensivepartofconvolutionalnetworktrainingislearning\n",
      "           thefeatures.Theoutputlayerisusuallyrelativelyinexpensivebecauseofthe\n",
      "            smallnumberoffeaturesprovidedasinputtothislayerafterpassingthrough\n",
      "         severallayersofpooling.Whenperformingsupervisedtrainingwithgradient\n",
      "           descent,everygradientsteprequiresacompleterunofforwardpropagationand\n",
      "3 5 6   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "            backwardpropagationthroughtheentirenetwork.Onewaytoreducethecostof\n",
      "             convolutionalnetworktrainingistousefeaturesthatarenottrainedinasupervised\n",
      "fashion.\n",
      "        Therearethreebasicstrategiesfor obtainingconvolutionkernelswithout\n",
      "           supervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\n",
      "              designthembyhand,forexample,bysettingeachkerneltodetectedgesata\n",
      "            certainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\n",
      "       criterion.Forexample, ()apply Coatesetal.2011 k    -meansclusteringtosmall\n",
      "             imagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.InPartIII\n",
      "         wedescribemanymoreunsupervisedlearningapproaches.Learningthefeatures\n",
      "           withanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\n",
      "              classiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\n",
      "             theentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\n",
      "            lastlayer.Learningthelastlayeristhentypicallyaconvexoptimizationproblem,\n",
      "           assumingthelastlayerissomethinglikelogisticregressionoranSVM.\n",
      "         Randomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett\n",
      "                 etal. etal. ,;2009Saxe ,;2011Pinto2011CoxandPinto2011Saxe etal.,; ,).etal.\n",
      "          ()showedthatlayersconsistingofconvolutionfollowedbypoolingnaturally 2011\n",
      "         becomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\n",
      "            Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\n",
      "         aconvolutionalnetwork:ﬁrst,evaluatetheperformanceofseveralconvolutional\n",
      "             networkarchitecturesbytrainingonlythelastlayer;thentakethebestofthese\n",
      "          architecturesandtraintheentirearchitectureusingamoreexpensiveapproach.\n",
      "            Anintermediateapproachistolearnthefeatures,butusingmethodsthatdo\n",
      "           notrequirefullforwardandback-propagationateverygradientstep.Aswith\n",
      "           multilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer\n",
      "              inisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe\n",
      "              secondlayerinisolationgiventhosefeatures,andsoon.Inchapterwedescribed 8\n",
      "            howtoperformsupervisedgreedylayer-wisepretraining,andinpartextendthis III\n",
      "           togreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The\n",
      "           canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\n",
      "          convolutionaldeepbeliefnetwork( ,).Convolutionalnetworksoﬀer Leeetal.2009\n",
      "             ustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\n",
      "           withmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\n",
      "                time,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\n",
      "             Wecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels\n",
      "             ofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\n",
      "           totrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining\n",
      "3 5 7   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "             process.Usingthisapproach,wecantrainverylargemodelsandincurahigh\n",
      "            computationalcostonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\n",
      "            2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\n",
      "           fromroughly2007to2013,whenlabeleddatasetsweresmallandcomputational\n",
      "           powerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\n",
      "         purelysupervisedfashion,usingfullforwardandback-propagationthroughthe\n",
      "     entirenetworkoneachtrainingiteration.\n",
      "          Aswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto\n",
      "             teaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised\n",
      "          pretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,orit\n",
      "            maysimplyallowustotrainmuchlargerarchitecturesbecauseofthereduced\n",
      "     computationalcostofthelearningrule.\n",
      "     9.10TheNeuroscientiﬁcBasisforConvolutional\n",
      "Networks\n",
      "    Convolutional networksare perhaps thegreatestsuccess storyof biologically\n",
      "        inspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided\n",
      "             bymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere\n",
      "  drawnfromneuroscience.\n",
      "        Thehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments\n",
      "        longbeforetherelevantcomputationalmodelsweredeveloped.Neurophysiologists\n",
      "           DavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\n",
      "             ofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland\n",
      "         Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\n",
      "            aNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary\n",
      "            deeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\n",
      "            cats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected\n",
      "              inpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\n",
      "            thatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc\n",
      "             patternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto\n",
      " otherpatterns.\n",
      "           Theirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\n",
      "               beyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\n",
      "        focusonasimpliﬁed,cartoonviewofbrainfunction.\n",
      "               Inthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown\n",
      " asthe   primaryvisualcortex          . V1istheﬁrstareaofthebrainthatbeginsto\n",
      "3 5 8   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "         performsigniﬁcantlyadvancedprocessingofvisualinput. Inthiscartoonview,\n",
      "             imagesareformedbylightarrivingintheeyeandstimulatingtheretina,the\n",
      "             light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\n",
      "              somesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\n",
      "            represented.Theimagethenpassesthroughtheopticnerveandabrainregion\n",
      "            calledthelateralgeniculatenucleus. Themainrole,asfarasweareconcerned\n",
      "              here,ofbothanatomicalregionsisprimarilyjusttocarrythesignalfromtheeye\n",
      "          toV1,whichislocatedatthebackofthehead.\n",
      "           AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:\n",
      "1.            V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure,\n",
      "    mirroring thestructure ofthe image in the retina.For example, light\n",
      "             arrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof\n",
      "         V1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\n",
      "     deﬁnedintermsoftwo-dimensionalmaps.\n",
      "2.  V1containsmany simplecells        .Asimplecell’sactivitycantosomeextent\n",
      "           becharacterizedbyalinearfunctionoftheimageina small,spatially\n",
      "          localizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare\n",
      "       designedtoemulatethesepropertiesofsimplecells.\n",
      "3.   V1alsocontainsmany  complexcells      .Thesecellsrespondtofeaturesthat\n",
      "            aresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\n",
      "             tosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\n",
      "          ofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\n",
      "           inlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.\n",
      "         Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\n",
      "          inconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellowetal.2013a\n",
      "             ThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\n",
      "              basicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\n",
      "            thevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\n",
      "             appliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\n",
      "             layersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept\n",
      "            andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n",
      "            nicknamed“grandmothercells”—theideaisthatapersoncouldhaveaneuronthat\n",
      "           activateswhenseeinganimageoftheirgrandmother,regardlessofwhethershe\n",
      "                appearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\n",
      "               herfaceorzoomed-outshotofherentirebody,whethersheisbrightlylitorin\n",
      "   shadow,andsoon.\n",
      "3 5 9   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "            Thesegrandmothercellshavebeenshowntoactuallyexistinthehumanbrain,\n",
      "            inaregioncalledthemedialtemporallobe( ,).Researchers Quirogaetal.2005\n",
      "          testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\n",
      "             Theyfoundwhathascometobecalledthe“HalleBerryneuron,”anindividual\n",
      "              neuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena\n",
      "               personseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\n",
      "              thewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;\n",
      "            otherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,andso\n",
      "forth.\n",
      "          Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern\n",
      "        convolutionalnetworks,whichwouldnotautomaticallygeneralizetoidentifying\n",
      "             apersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\n",
      "             network’slastlayeroffeaturesisabrainareacalledtheinferotemporalcortex(IT).\n",
      "            Whenviewinganobject,informationﬂowsfromtheretina,throughtheLGN,to\n",
      "              V1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst100ms\n",
      "              ofglimpsinganobject.Ifapersonisallowedtocontinuelookingattheobject\n",
      "             formoretime,theninformationwillbegintoﬂowbackwardasthebrainuses\n",
      "             top-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.Ifwe\n",
      "            interrupttheperson’sgaze,however,andobserveonlytheﬁringratesthatresult\n",
      "             fromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobesimilar\n",
      "           toaconvolutionalnetwork.ConvolutionalnetworkscanpredictITﬁringratesand\n",
      "         performsimilarlyto(time-limited)humansonobjectrecognitiontasks( ,DiCarlo\n",
      "2013).\n",
      "         Thatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks\n",
      "           andthemammal ianvisionsystem.Someofthesediﬀerencesarewellknown\n",
      "           tocomputationalneuroscientistsbutoutsidethescopeofthisbook.Someof\n",
      "            thesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe\n",
      "         mammal ianvisionsystemworksremainunanswered.Asabrieflist:\n",
      "•              Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\n",
      "f o v e a              .Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\n",
      "               armslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,\n",
      "              thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\n",
      "         togetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\n",
      "         receivelargefull-resolutionphotographsasinput.Thehumanbrainmakes\n",
      "   severaleyemovementscalled s a c c a des     toglimpsethemostvisuallysalient\n",
      "         ortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\n",
      "            intodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\n",
      "         deeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\n",
      "3 6 0   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "         languageprocessing,asdescribedinsection .Severalvisualmodels 12.4.5.1\n",
      "           withfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\n",
      "          thedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).\n",
      "•           Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\n",
      "         hearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\n",
      "    sofararepurelyvisual.\n",
      "•            Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\n",
      "         abletounderstandentirescenes,includingmanyobjectsandrelationships\n",
      "          betweenobjects,anditprocessesrich3-Dgeometri cinformationneededfor\n",
      "          ourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\n",
      "            appliedtosomeoftheseproblems,buttheseapplicationsareintheirinfancy.\n",
      "•            EvensimplebrainareaslikeV1areheavilyaﬀectedbyfeedbackfromhigher\n",
      "          levels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\n",
      "         hasnotyetbeenshowntooﬀeracompellingimprovement.\n",
      "•           WhilefeedforwardITﬁringratescapturemuchofthesameinformationas\n",
      "          convolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\n",
      "         computationsare.Thebrainprobablyusesverydiﬀerentactivationand\n",
      "         poolingfunctions.Anindividualneuron’sactivationprobablyisnotwell\n",
      "            characterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves\n",
      "           multiplequadraticﬁltersforeachneuron( ,).Indeedour Rustetal.2005\n",
      "           cartoonpictureof“simplecells”and“complexcells”mightcreateanonexistent\n",
      "            distinction;simplecellsandcomplexcellsmightbothbethesamekindof\n",
      "          cellbutwiththeir“parameters”enablingacontinuumofbehaviorsranging\n",
      "         fromwhatwecall“simple”towhatwecall“complex.”\n",
      "       Itisalsoworth mentioning thatneuroscience hastold usrelativelylittle\n",
      "         abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\n",
      "          sharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\n",
      "             ofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\n",
      "        back-propagationalgorithmandgradientdescent.Forexample,theneocognitron\n",
      "          (Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\n",
      "          themodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\n",
      "algorithm.\n",
      "     Lang andHinton1988() introducedthe use ofback-propagationto train\n",
      "  time-delayneuralnetworks     (TDNNs).Tousecontemporaryterminology,\n",
      "         TDNNsareone-dimensionalconvolutionalnetworksappliedtotimeseries.Back-\n",
      "           propagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-\n",
      "3 6 1   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "            tionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\n",
      "          ofback-propagation-based trainingofTDNNs, ()developedthe LeCunetal.1989\n",
      "          modernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\n",
      "   convolutionappliedtoimages.\n",
      "            Sofarwehavedescribedhowsimplecellsareroughlylinearandselective\n",
      "           forcertainfeatures,complexcellsaremorenonlinearandbecomeinvariantto\n",
      "     some tran sformations of thesesimple cellfeatures,and stacksof layers that\n",
      "          alternatebetweenselectivityandinvariancecanyieldgrandmothercellsforspeciﬁc\n",
      "           phenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.\n",
      "             Inadeepnonlinearnetwork,itcanbediﬃculttounderstandthefunctionof\n",
      "             individualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir\n",
      "             responsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan\n",
      "            justdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\n",
      "            channelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\n",
      "             donothaveaccesstotheweightsthemselves.Instead,weputanelectrodein\n",
      "             theneuron,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s\n",
      "             retina,andrecordhoweachofthesesamplescausestheneurontoactivate.We\n",
      "              canthenﬁtalinearmodeltotheseresponsestoobtainanapproximationofthe\n",
      "      neuron’sweights.Thisapproachisknownas  reversecorrelation  (Ringachand\n",
      " Shapley2004,).\n",
      "            ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\n",
      "by Gaborfunctions         . TheGaborfunctiondescribestheweightata2-Dpoint\n",
      "              intheimage. Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,\n",
      "I( x,y                ).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\n",
      "     locations,deﬁnedbyasetofxcoordinates X   andasetofycoordinates Y ,then\n",
      "         applyingweightsthatarealsoafunctionofthelocation,w( x,y   ).Fromthispoint\n",
      "             ofview,theresponseofasimplecelltoanimageisgivenby\n",
      "sI() =\n",
      "x ∈ X\n",
      "y ∈ Y   wx,yIx,y. ()() (9.15)\n",
      "         Speciﬁcally, takestheformofaGaborfunction: wx,y()\n",
      "   wx,yα,β (; x ,β y   ,f,φ,x 0 ,y 0   ,τα) = exp\n",
      "−β xx 2 −β yy 2\n",
      "cos(fx  +)φ,(9.16)\n",
      "where\n",
      "x  = (xx− 0      )cos()+(τyy− 0   )sin()τ (9.17)\n",
      "and\n",
      "y  = (−xx− 0      )sin()+(τyy− 0   )cos()τ. (9.18)\n",
      "3 6 2   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "           Figure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\n",
      "           largepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\n",
      "          correspondstozeroweight. ( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\n",
      "    thatcontrolthecoordinatesystem:x 0,y 0 ,andτ      . EachGaborfunctioninthisgridis\n",
      "   assignedavalueofx 0andy 0        proportionaltoitspositioninitsgrid,andτ  ischosenso\n",
      "               thateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthe\n",
      "     grid.Fortheothertwoplots,x 0,y 0 ,andτ     areﬁxedtozero. Gaborfunctions ( C e n t e r )\n",
      "    withdiﬀerentGaussianscaleparametersβ xandβ y     .Gaborfunctionsarearrangedin\n",
      "  increasingwidth(decreasingβ x           )aswemovelefttorightthroughthegrid,andincreasing\n",
      " height(decreasingβ y            )aswemovetoptobottom.Fortheothertwoplots,theβvalues\n",
      "           areﬁxedto1.5timestheimagewidth. ( R i g h t )Gaborfunctionswithdiﬀerentsinusoid\n",
      "parametersfandφ      .Aswemovetoptobottom,f        increases,andaswemovelefttoright,\n",
      "φ      increases.Fortheothertwoplots,φ    isﬁxedto0andf      isﬁxedto5timestheimage\n",
      "width.\n",
      "Here,α,β x,β y,f,φ,x 0,y 0 ,andτ     areparametersthatcontroltheproperties\n",
      "            oftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\n",
      "    diﬀerentsettingsoftheseparameters.\n",
      " Theparametersx 0,y 0 ,andτ     deﬁneacoordinatesystem. Wetranslateand\n",
      "rotatexandy toformxandy        .Speciﬁcally,thesimplecellwillrespondtoimage\n",
      "     featurescenteredatthepoint(x 0,y 0        ),anditwillrespondtochangesinbrightness\n",
      "           aswemovealongalinerotatedradiansfromthehorizontal. τ\n",
      "    Viewedasafunctionofxandy  ,thefunctionw    thenrespondstochangesin\n",
      "     brightnessaswemovealongthex       axis. Ithastwoimportantfactors:oneisa\n",
      "        Gaussianfunction,andtheotherisacosinefunction.\n",
      "  TheGaussianfactor α e x p\n",
      "−β xx 2 −β yy 2\n",
      "       canbeseenasagatingtermthat\n",
      "           ensuresthatthesimplecellwillrespondonlytovaluesnearwherexandyare\n",
      "              bothzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescaling\n",
      "factorα          adjuststhetotalmagnitudeofthesimplecell’sresponse,whileβ xandβ y\n",
      "       controlhowquicklyitsreceptiveﬁeldfallsoﬀ.\n",
      "3 6 3   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "  Thecosinefactor cos(fx+φ        ) controlshowthesimplecellrespondstochanging\n",
      "  brightnessalongthex  axis.Theparameterf      controlsthefrequencyofthecosine,\n",
      "     andcontrolsitsphaseoﬀset. φ\n",
      "            Altogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\n",
      "             toaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc\n",
      "             location.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage\n",
      "               hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\n",
      "             weightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\n",
      "             inhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when\n",
      "              theimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\n",
      "negative.\n",
      "           ThecartoonviewofacomplexcellisthatitcomputestheL2  normofthe\n",
      "      2-Dvectorcontainingtwosimplecells’responses:c(I )=\n",
      "s 0()I2 +s 1()I2. An\n",
      "    importantspecialcaseoccurswhens 1     hasallthesameparametersass 0except\n",
      "forφ ,andφ   issetsuchthats 1       isonequartercycleoutofphasewiths 0  .Inthis\n",
      "case,s 0ands 1 forma  quadraturepair       .Acomplexcelldeﬁnedinthisway\n",
      "     respondswhentheGaussianreweightedimageI( x,y)exp( −β xx 2−β yy 2) contains\n",
      "     ahigh-amplitudesinusoidalwavewithfrequencyf indirectionτ near(x 0 ,y 0),\n",
      "              regardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis\n",
      "        invarianttosmalltranslationsoftheimageindirectionτ     ,ortonegatingtheimage\n",
      "      (replacingblackwithwhiteandviceversa).\n",
      "         Someofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\n",
      "          learningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\n",
      "           modelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\n",
      "         asimpleunsupervisedlearningalgorithm,sparsecoding,learnsfeatureswith\n",
      "             receptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat\n",
      "          anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\n",
      "          Gabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\n",
      "           learningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19\n",
      "          showssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge\n",
      "             detectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe“right”\n",
      "                modelofthebrainjustbasedonthefeaturesitlearns(thoughitcancertainlybea\n",
      "               badsignifanalgorithmdoeslearnsomesortofedgedetectorwhenappliedto not\n",
      "           naturalimages).Thesefeaturesareanimportantpartofthestatisticalstructure\n",
      "            ofnaturalimagesandcanberecoveredbymanydiﬀerentapproachestostatistical\n",
      "              modeling.SeeHyvärinen 2009etal.()forareviewoftheﬁeldofnaturalimage\n",
      "statistics.\n",
      "3 6 4   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "            Figure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc\n",
      "            colorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscent\n",
      "             oftheGaborfunctionsknowntobepresentintheprimaryvisualcortex. ( L e f t )Weights\n",
      "           learnedbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)applied\n",
      "             tosmallimagepatches. ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafully\n",
      "          supervisedconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesame\n",
      " maxoutunit.\n",
      "       9.11ConvolutionalNetworksandtheHistoryofDeep\n",
      "Learning\n",
      "           Convolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\n",
      "            learning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\n",
      "            bystudyingthebraintomachinelearningapplications.Theywerealsosomeof\n",
      "            theﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\n",
      "         consideredviable.Convolutionalnetworks werealsosomeoftheﬁrstneural\n",
      "          networkstosolveimportantcommercialapplicationsandremainattheforefront\n",
      "            ofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\n",
      "          neuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\n",
      "              readingchecks( ,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\n",
      "               byNCRwasreadingover10percentofallthechecksintheUnitedStates.Later,\n",
      "         severalOCRandhandwritingrecognitionsystemsbasedonconvolutionalnets\n",
      "             weredeployedbyMicrosoft( ,).Seechapterformoredetails Simardetal.2003 12\n",
      "          onsuchapplicationsandmoremodernapplicationsofconvolutionalnetworks.See\n",
      "             LeCun 2010etal.()foramorein-depthhistoryofconvolutionalnetworksupto\n",
      "2010.\n",
      "          Convolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\n",
      "           intensityofcommerci alinterestindeeplearningbeganwhenKrizhevskyetal.\n",
      "3 6 5   C HAP T E R 9 . C O NVO L U T I O NAL NE T W O R K S\n",
      "         ()wontheImageNet objectrecognitionchallenge,butconvolutionalnetworks 2012\n",
      "            hadbeenusedtowinothermachinelearningandcomputervisioncontestswith\n",
      "    lessimpactforyearsearlier.\n",
      "           Convolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith\n",
      "         back-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\n",
      "          whengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\n",
      "         simplybethatconvolutionalnetworksweremorecomputationallyeﬃcientthan\n",
      "            fullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem\n",
      "         andtunetheirimplementationandhyperparameters. Largernetworksalsoseem\n",
      "           tobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\n",
      "            appeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\n",
      "           availableandactivationfunctionsthatwerepopularduringthetimeswhenfully\n",
      "             connectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\n",
      "          barrierstothesuccessofneuralnetworkswerepsychological(practitionersdid\n",
      "               notexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse\n",
      "          neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\n",
      "              performedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\n",
      "             deeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\n",
      "          Convolutionalnetworksprovideawaytospecializeneuralnetworkstowork\n",
      "             withdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\n",
      "            verylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional\n",
      "          imagetopology.Toprocessone-dimensionalsequentialdata,weturnnextto\n",
      "         anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\n",
      "networks.\n",
      "3 6 6 C h a p t e r 1 0\n",
      "  Seq ue n ce Mo deling: Re curre n t\n",
      "  and Re curs i v e N e t s\n",
      "  Recurrentneuralnetworks         ,orRNNs( ,),areafamily Rumelhartetal.1986a\n",
      "           ofneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\n",
      "            isaneuralnetworkthatisspecializedforprocessingagridofvalues X suchas\n",
      "             animage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\n",
      "    processingasequenceofvalues x(1 )     ,..., x( ) τ    .Justasconvolutionalnetworks\n",
      "            canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\n",
      "            networkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\n",
      "         longersequencesthanwouldbepracticalfornetworkswithoutsequence-based\n",
      "         specialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\n",
      "length.\n",
      "            Togofrommultilayernetworkstorecurrentnetworks,weneedtotakeadvantage\n",
      "              ofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsofthe\n",
      "         1980s:sharingparametersacrossdiﬀerentpartsofamodel. Parametersharing\n",
      "             makesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms\n",
      "           (diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\n",
      "              foreachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\n",
      "          seenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths\n",
      "           andacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen\n",
      "            aspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.\n",
      "              Forexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,\n",
      "               IwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand\n",
      "               extracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\n",
      "              theyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n",
      "367       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "              wordorinthesecondwordofthesentence.Supposethatwetrainedafeedforward\n",
      "          networkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected\n",
      "          feedforwardnetworkwouldhaveseparateparametersforeachinputfeature,so\n",
      "               itwouldneedtolearnalltherulesofthelanguageseparatelyateachpositionin\n",
      "           thesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\n",
      "   acrossseveraltimesteps.\n",
      "             Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This\n",
      "          convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\n",
      "            Hinton1988Waibel1989Lang1990 ,; etal.,;etal.,).Theconvolutionoperation\n",
      "             allowsanetworktoshareparametersacrosstimebutisshallow.Theoutputof\n",
      "              convolutionisasequencewhereeachmemberoftheoutputisafunctionofa\n",
      "            smallnumberofneighboringmembersoftheinput.Theideaofparametersharing\n",
      "            manifestsintheapplicationofthesameconvolutionkernelateachtimestep.\n",
      "           Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe\n",
      "              outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\n",
      "            outputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\n",
      "           Thisrecurrentformulationresultsinthesharingofparametersthroughavery\n",
      "  deepcomputationalgraph.\n",
      "             Forthesimplicityofexposition,werefertoRNNsasoperatingonasequence\n",
      "  thatcontainsvectors x( ) t    withthetimestepindex t    rangingfromto1 τ .In\n",
      "         practice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\n",
      "    withadiﬀerentsequencelength τ       foreachmemberoftheminibatch.Wehave\n",
      "           omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\n",
      "               neednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\n",
      "              onlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\n",
      "             acrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\n",
      "            thenetworkmayhaveconnectionsthatgobackwardintime,providedthatthe\n",
      "          entiresequenceisobservedbeforeitisprovidedtothenetwork.\n",
      "            Thischapterextendstheideaofacomputationalgraphtoincludecycles.These\n",
      "              cyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue\n",
      "            atafuturetimestep.Suchcomputationalgraphsallowustodeﬁnerecurrent\n",
      "           neuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and\n",
      "   userecurrentneuralnetworks.\n",
      "           Formoreinformationonrecurrentneuralnetworksthanisavailableinthis\n",
      "          chapter,wereferthereadertothetextbookofGraves2012().\n",
      "3 6 8       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "s( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\n",
      "f fs ( ) . . .s ( ) . . .s ( ) . . .s ( ) . . .\n",
      "f f f f f f\n",
      "            Figure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\n",
      "          unfoldedcomputationalgraph.Eachnoderepresentsthestateatsometimet  ,andthe\n",
      "functionf   mapsthestateatt   tothestateatt       +1.Thesameparameters(thesamevalue\n",
      "           ofusedtoparametrize)areusedforalltimesteps. θ f\n",
      "   10.1UnfoldingComputationalGraphs\n",
      "              Acomputationalgraphisawaytoformalizethestructureofasetofcomputations,\n",
      "            suchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\n",
      "             Pleaserefertosection forageneralintroduction.Inthissectionweexplain 6.5.1\n",
      "  theideaof unf o l di n g       arecursiveorrecurrentcomputationintoacomputational\n",
      "            graphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.\n",
      "            Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\n",
      "structure.\n",
      "         Forexample,considertheclassicalformofadynamicalsystem:\n",
      "s( ) t= (fs( 1 ) t −  ;)θ, (10.1)\n",
      " wheres( ) t      iscalledthestateofthesystem.\n",
      "       Equationisrecurrentbecausethedeﬁnitionof 10.1 s attimet  refersbackto\n",
      "       thesamedeﬁnitionattime.t−1\n",
      "      Foraﬁnitenumberoftimestepsτ       ,thegraphcanbeunfoldedbyapplying\n",
      " thedeﬁnition τ−         1times.Forexample,ifweunfoldequationfor10.1τ  = 3time\n",
      "  steps,weobtain\n",
      "s(3 )=(fs(2 )  ;)θ (10.2)\n",
      "=((ffs(1 )   ;);)θθ. (10.3)\n",
      "           Unfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas\n",
      "          yieldedanexpressionthatdoesnotinvolverecurrence. Suchanexpressioncan\n",
      "         nowberepresentedbyatraditionaldirectedacycliccomputationalgraph. The\n",
      "           unfoldedcomputationalgraphofequationandequationisillustratedin 10.1 10.3\n",
      " ﬁgure.10.1\n",
      "            Asanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\n",
      "3 6 9       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "f fhh\n",
      "x xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th ( ) . . .h ( ) . . .h ( ) . . .h ( ) . . .\n",
      "f f\n",
      "U n f ol df f f f f\n",
      "            Figure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\n",
      "   informationfromtheinputx     byincorporatingitintothestateh   thatispassedforward\n",
      "              throughtime.(Left)Circuitdiagram.Theblacksquareindicatesadelayofasingletime\n",
      "            step. Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach (Right)\n",
      "        nodeisnowassociatedwithoneparticulartimeinstance.\n",
      " signalx( ) t,\n",
      "s( ) t= (fs( 1 ) t − ,x( ) t  ;)θ, (10.4)\n",
      "             whereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\n",
      "           Recurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas\n",
      "          almostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\n",
      "          anyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\n",
      "           Manyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\n",
      "             deﬁnethevaluesoftheirhiddenunits. Toindicatethatthestateisthehidden\n",
      "           unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 h torepresent\n",
      " thestate,\n",
      "h( ) t= (fh( 1 ) t − ,x( ) t  ;)θ, (10.5)\n",
      "           illustratedinﬁgure;typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\n",
      "             asoutputlayersthatreadinformationoutofthestatetomakepredictions. h\n",
      "            Whentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\n",
      "          thefuturefromthepast,thenetworktypicallylearnstouseh( ) t    asakindoflossy\n",
      "            summaryofthetask-relevantaspectsofthepastsequenceofinputsuptot .This\n",
      "            summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n",
      "(x( ) t ,x( 1 ) t − ,x( 2 ) t −     ,...,x(2 ) ,x(1 )     )toaﬁxedlengthvectorh( ) t   .Dependingonthe\n",
      "           trainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\n",
      "            sequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNis\n",
      "           usedinstatisticallanguagemodeling,typicallytopredictthenextwordgiven\n",
      "            previouswords,storingalltheinformationintheinputsequenceuptotimet\n",
      "             maynotbenecessary;storingonlyenoughinformationtopredicttherestofthe\n",
      "          sentenceissuﬃcient.Themostdemandingsituationiswhenweaskh( ) t  toberich\n",
      "            enoughtoallowonetoapproximatelyrecovertheinputsequence,asinautoencoder\n",
      "3 7 0       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "  frameworks(chapter).14\n",
      "              Equationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5\n",
      "              iswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n",
      "            physicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\n",
      "             view,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts\n",
      "              whosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2\n",
      "             Throughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\n",
      "               thataninteractiontakesplacewithadelayofasingletimestep,fromthestate\n",
      " attimet    tothestateattimet           +1.TheotherwaytodrawtheRNNisasan\n",
      "          unfoldedcomputationalgraph,inwhicheachcomponentisrepresentedbymany\n",
      "            diﬀerentvariables,withonevariablepertimestep,representingthestateofthe\n",
      "               componentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\n",
      "              separatenodeofthecomputationalgraph,asintherightofﬁgure.Whatwe 10.2\n",
      "                callunfoldingistheoperationthatmapsacircuit,asintheleftsideoftheﬁgure,\n",
      "             toacomputationalgraphwithrepeatedpieces,asintherightside.Theunfolded\n",
      "          graphnowhasasizethatdependsonthesequencelength.\n",
      "            Wecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\n",
      "h( ) t=g( ) t( x( ) t , x( 1 ) t − , x( 2 ) t −     ,..., x(2 ) , x(1 ) ) (10.6)\n",
      "=(f h( 1 ) t − , x( ) t  ;) θ. (10.7)\n",
      " Thefunctiong( ) t     takesthewholepastsequence( x( ) t , x( 1 ) t − , x( 2 ) t −     ,..., x(2 ) , x(1 ))\n",
      "           asinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\n",
      "   allowsustofactorizeg( ) t     intorepeatedapplicationofafunctionf  .Theunfolding\n",
      "     processthusintroducestwomajoradvantages:\n",
      "1.            Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\n",
      "             inputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto\n",
      "           anotherstate,ratherthanspeciﬁedintermsofavariable-lengthhistoryof\n",
      "states.\n",
      "2.        Itispossibletousethetransitionfunction same f   withthesameparameters\n",
      "   ateverytimestep.\n",
      "          Thesetwofactorsmakeitpossibletolearnasinglemodelf   thatoperatesonall\n",
      "             timestepsandallsequencelengths,ratherthanneedingtolearnaseparatemodel\n",
      "g( ) t           forallpossibletimesteps.Learningasinglesharedmodelallowsgeneralization\n",
      "              tosequencelengthsthatdidnotappearinthetrainingset,andenablesthemodel\n",
      "            tobeestimatedwithfarfewertrainingexamplesthanwouldberequiredwithout\n",
      " parametersharing.\n",
      "3 7 1       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "            Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\n",
      "          graphissuccinct. Theunfoldedgraphprovidesanexplicitdescriptionofwhich\n",
      "           computationstoperform.Theunfoldedgraphalsohelpsillustratetheideaof\n",
      "          informationﬂowforwardintime(computingoutputsandlosses)andbackward\n",
      "           intime(computinggradients)byexplicitlyshowingthepathalongwhichthis\n",
      " informationﬂows.\n",
      "   10.2RecurrentNeuralNetworks\n",
      "          Armedwiththegraph-unrollingandparameter-sharingideasofsection,we10.1\n",
      "        candesignawidevarietyofrecurrentneuralnetworks.\n",
      "         Someexamplesofimportantdesignpatternsforrecurrentneuralnetworks\n",
      "  includethefollowing:\n",
      "•            Recurrentnetworksthatproduceanoutputateachtimestepandhave\n",
      "        recurrentconnectionsbetweenhiddenunits,illustratedinﬁgure10.3\n",
      "•            Recurrentnetworksthatproduceanoutputateachtimestepandhave\n",
      "            recurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\n",
      "         unitsatthenexttimestep,illustratedinﬁgure10.4\n",
      "•         Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\n",
      "           readanentiresequenceandthenproduceasingleoutput,illustratedin\n",
      " ﬁgure10.5\n",
      "           Figureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\n",
      "   mostofthechapter.\n",
      "             Therecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8\n",
      "             sensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\n",
      "               arecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter\n",
      "              anumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\n",
      "             usedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput\n",
      "         (SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,; ,; ,;\n",
      "          Hyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\n",
      "          sotheseresultsregardexactimplementationofthefunction,notapproximations.\n",
      "             TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinput,\n",
      "             anditsoutputsmustbediscretizedtoprovideabinaryoutput.Itispossible\n",
      "              tocomputeallfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize\n",
      "            (SiegelmannandSontag1995[]use886units).The“input”oftheTuringmachineis\n",
      "3 7 2       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "UUV V\n",
      "WWo( t − 1 )o( t − 1 )\n",
      "hhooy y\n",
      "LL\n",
      "x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\n",
      "h ( ) . . .h ( ) . . .h ( ) . . .h ( ) . . .V V V V V V\n",
      "UU UU UUU n f ol d\n",
      "             Figure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\n",
      "     thatmapsaninputsequenceof x      valuestoacorrespondingsequenceofoutput ovalues.\n",
      " Aloss L    measureshowfareach o     isfromthecorrespondingtrainingtarget y  .Whenusing\n",
      "   softmaxoutputs,weassume o      istheunnormalizedlogprobabilities.Theloss Linternally\n",
      "computesˆ y=softmax( o     ) andcomparesthistothetarget y      .TheRNNhasinputtohidden\n",
      "     connectionsparametrizedbyaweightmatrix U   ,hidden-to-hiddenrecurrentconnections\n",
      "    parametrizedbyaweightmatrix W    ,andhidden-to-outputconnectionsparametrized\n",
      "   byaweightmatrix V         .Equation deﬁnesforwardpropagationinthismodel. 10.8 (Left)\n",
      "              TheRNNanditslossdrawnwithrecurrentconnections. Thesameseenasa (Right)\n",
      "           time-unfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\n",
      " timeinstance.\n",
      "3 7 3       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "             aspeciﬁcationofthefunctiontobecomputed,sothesamenetworkthatsimulates\n",
      "            thisTuringmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfor\n",
      "           theproofcansimulateanunboundedstackbyrepresentingitsactivationsand\n",
      "      weightswithrationalnumbersofunboundedprecision.\n",
      "           WenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\n",
      "             ﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\n",
      "          hiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,\n",
      "             theﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\n",
      "                Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\n",
      "             orcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\n",
      "o            asgivingtheunnormalizedlogprobabilitiesofeachpossiblevalueofthediscrete\n",
      "            variable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\n",
      "  obtainavectorˆy       ofnormalizedprobabilitiesovertheoutput.Forwardpropagation\n",
      "       beginswithaspeciﬁcationoftheinitialstateh(0 )      .Then,foreachtimestepfrom\n",
      "          ttτ = 1to= ,weapplythefollowingupdateequations:\n",
      "a( ) t    = +bWh( 1 ) t − +Ux( ) t , (10.8)\n",
      "h( ) t =tanh(a( ) t ), (10.9)\n",
      "o( ) t    = +cVh( ) t , (10.10)\n",
      "ˆy( ) t =softmax(o( ) t ), (10.11)\n",
      "      wheretheparametersarethebiasvectorsbandc    alongwiththeweightmatrices\n",
      "U,VandW      ,respectively,forinput-to-hidden,hidden-to-outputandhidden-\n",
      "            to-hiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\n",
      "              inputsequencetoanoutputsequenceofthesamelength.Thetotallossfora\n",
      "               givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\n",
      "            thesumofthelossesoverallthetimesteps.Forexample,ifL( ) t  isthenegative\n",
      "  log-likelihoodofy( ) t givenx(1 )     ,...,x( ) t ,then\n",
      "L\n",
      "{x(1 )     ,...,x( ) τ }{,y(1 )     ,...,y( ) τ}\n",
      "(10.12)\n",
      "=\n",
      "tL( ) t(10.13)\n",
      " =−\n",
      "t logp m o d e l\n",
      "y( ) t |{x(1 )     ,...,x( ) t}\n",
      " , (10.14)\n",
      "wherep m o d e l\n",
      "y( ) t |{x(1 )     ,...,x( ) t}\n",
      "      isgivenbyreadingtheentryfory( ) t fromthe\n",
      "  model’soutputvectorˆy( ) t         .Computingthegradientofthislossfunctionwithrespect\n",
      "          totheparametersisanexpensiveoperation.Thegradientcomputationinvolves\n",
      "           performingaforwardpropagationpassmovinglefttorightthroughourillustration\n",
      "3 7 4       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "UV\n",
      "Wo( t − 1 )o( t − 1 )\n",
      "hhooy y\n",
      "LL\n",
      "x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) t  W W W Wo ( ) . . .o ( ) . . .\n",
      "h ( ) . . .h ( ) . . .  V V V\n",
      "  U U UU n f ol d\n",
      "             Figure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput\n",
      "       tothehiddenlayer.Ateachtimestep t   ,theinputis xt    ,thehiddenlayeractivations\n",
      "are h( ) t   ,theoutputsare o( ) t   ,thetargetsare y( ) t    ,andthelossis L( ) t  .(Left)Circuit\n",
      "           diagram.(Right)Unfoldedcomputationalgraph.SuchanRNNislesspowerful(can\n",
      "              expressasmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.10.3\n",
      "                 TheRNNinﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoits 10.3\n",
      " hiddenrepresentation h andtransmit h         tothefuture.TheRNNinthisﬁgureistrained\n",
      "      toputaspeciﬁcoutputvalueinto o ,and o        istheonlyinformationitisallowedtosend\n",
      "        tothefuture.Therearenodirectconnectionsfrom h   goingforward.Theprevious h\n",
      "              isconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\n",
      "Unless o          isveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\n",
      "                 fromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto\n",
      "              trainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowing greater\n",
      "       parallelizationduringtraining,asdescribedinsection .10.2.1\n",
      "3 7 5       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "h( t − 1 )h( t − 1 )\n",
      "Wh( ) th( ) t . . . . . .\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tx( ) . . .x( ) . . . W W\n",
      "U  U Uh( ) τh( ) τ\n",
      "x( ) τx( ) τW\n",
      "Uo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ\n",
      "V\n",
      ". . . . . .\n",
      "            Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\n",
      "               ofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\n",
      "           ﬁxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatarget\n",
      "            rightattheend(asdepictedhere),orthegradientontheoutput o( ) t   canbeobtainedby\n",
      "    back-propagatingfromfurtherdownstreammodules.\n",
      "            oftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3\n",
      "         movingrighttoleftthroughthegraph.Theruntimeis O( τ   ) andcannotbereduced\n",
      "         byparallelizationbecausetheforwardpropagationgraphisinherentlysequential;\n",
      "           eachtimestepmaybecomputedonlyafterthepreviousone. Statescomputed\n",
      "             intheforwardpassmustbestoreduntiltheyarereusedduringthebackward\n",
      "      pass,sothememory costisalso O( τ    ).Theback-propagationalgorithmapplied\n",
      "    totheunrolledgraphwith O( τ   )costiscalled   back-propagationthroughtime\n",
      "           (BPTT)andisdiscussedfurtherinsection .Thenetworkwithrecurrence 10.2.2\n",
      "              betweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\n",
      "alternative?\n",
      "       10.2.1TeacherForcingandNetworkswithOutputRecurrence\n",
      "             Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\n",
      "               thehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4\n",
      "         becauseitlackshidden-to-hiddenrecurrentconnections.Forexample,itcannot\n",
      "         simulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\n",
      "            recurrence,itrequiresthattheoutputunitscapturealltheinformationaboutthe\n",
      "              pastthatthenetworkwillusetopredictthefuture.Becausetheoutputunitsare\n",
      "3 7 6       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "o( t − 1 )o( t − 1 )o( ) to( ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tW\n",
      " V V\n",
      " U Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tW\n",
      " V V\n",
      " U U\n",
      " Train time Test time\n",
      "             Figure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis\n",
      "              applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\n",
      "           nexttimestep.(Left) correctoutput Attraintime,wefeedthe y( ) t   drawnfromthetrain\n",
      "   setasinputto h( + 1 ) t           .(Right)Whenthemodelisdeployed,thetrueoutputisgenerally\n",
      "         notknown.Inthiscase,weapproximatethecorrectoutput y( ) t   withthemodel’soutput\n",
      "o( ) t        ,andfeedtheoutputbackintothemodel.\n",
      "            explicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\n",
      "            thenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser\n",
      "                knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\n",
      "        trainingsettargets.Theadvantageofeliminatinghidden-to-hiddenrecurrence\n",
      "            isthat,foranylossfunctionbasedoncomparingthepredictionattime t tothe\n",
      "   trainingtargetattime t          ,allthetimestepsaredecoupled.Trainingcanthusbe\n",
      "      parallelized,withthegradientforeachstep t      computedinisolation.Thereisno\n",
      "             needtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining\n",
      "       setprovidestheidealvalueofthatoutput.\n",
      "          Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\n",
      "     themodelmaybetrainedwith  teacherforcing     .Teacherforcingisaprocedure\n",
      "           thatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\n",
      "     modelreceivesthegroundtruthoutput y( ) t   asinputattime t   +1. Wecansee\n",
      "           thisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n",
      "3 7 7       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "  likelihoodcriterionis\n",
      " logp\n",
      "y(1 ) , y(2 ) | x(1 ) , x(2 )\n",
      "(10.15)\n",
      "  =logp\n",
      "y(2 ) | y(1 ) , x(1 ) , x(2 )\n",
      "  +logp\n",
      "y(1 ) | x(1 ) , x(2 )\n",
      " . (10.16)\n",
      "       Inthisexample,weseethatattimet       = 2,themodelistrainedtomaximizethe\n",
      "  conditionalprobabilityof y(2 )  givenboththe x      sequencesofarandtheprevious y\n",
      "           valuefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,\n",
      "           ratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections\n",
      "             shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\n",
      "     Thisisillustratedinﬁgure.10.6\n",
      "          Weoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\n",
      "         throughtimeinmodelsthatlackhidden-to-hiddenconnections.Teacherforcing\n",
      "           maystillbeappliedtomodelsthathavehidden-to-hiddenconnectionsaslong\n",
      "             astheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputed\n",
      "               inthenexttimestep.Assoonasthehiddenunitsbecomeafunctionofearlier\n",
      "            timesteps,however,theBPTTalgorithmisnecessary.Somemodelsmaythusbe\n",
      "      trainedwithbothteacherforcingandBPTT.\n",
      "             Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\n",
      "   laterusedinan c l o s e d-l o o p       mode,withthenetworkoutputs(orsamplesfrom\n",
      "             theoutputdistribution)fedbackasinput.Inthiscase,thefed-backinputsthat\n",
      "             thenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\n",
      "                thatitwillseeattesttime.Onewaytomitigatethisproblemistotrainwith\n",
      "         bothteacher-forcedinputsandfree-runninginputs,forexamplebypredicting\n",
      "             thecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\n",
      "            output-to-inputpaths.Inthisway,thenetworkcanlearntotakeintoaccount\n",
      "            inputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\n",
      "              seenduringtrainingandhowtomapthestatebacktowardonethatwillmake\n",
      "           thenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\n",
      "              etal.,)tomitigatethegapbetweentheinputsseenattrainingtimeand 2015b\n",
      "             theinputsseenattesttimerandomlychoosestousegeneratedvaluesoractual\n",
      "           datavaluesasinput.Thisapproachexploitsacurriculumlearningstrategyto\n",
      "        graduallyusemoreofthegeneratedvaluesasinput.\n",
      "        10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\n",
      "         Computingthegradientthrougharecurrentneuralnetworkisstraightforward.\n",
      "         Onesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n",
      "         totheunrolledcomputationalgraph.Nospecializedalgorithmsarenecessary.\n",
      "3 7 8       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "          Gradientsobtainedbyback-propagationmaythenbeusedwithanygeneral-purpose\n",
      "     gradient-basedtechniquestotrainanRNN.\n",
      "            TogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\n",
      "            exampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n",
      "           (equationandequation).Thenodesofourcomputationalgraphinclude 10.8 10.12\n",
      " theparameters U, V, W, band c        aswellasthesequenceofnodesindexedby\n",
      "tfor x( ) t, h( ) t, o( ) tandL( ) t  . Foreachnode N     weneedtocomputethegradient\n",
      "∇ NL             recursively,basedonthegradientcomputedatnodesthatfollowitinthe\n",
      "            graph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss:\n",
      "∂L\n",
      "∂L( ) t = 1. (10.17)\n",
      "       Inthisderivationweassumethattheoutputs o( ) t      areusedastheargumenttothe\n",
      "     softmaxfunctiontoobtainthevectorˆ y      ofprobabilitiesovertheoutput.Wealso\n",
      "           assumethatthelossisthenegativelog-likelihoodofthetruetargety( ) t giventhe\n",
      "    inputsofar.Thegradient∇o( ) tL     ontheoutputsattimestept  ,forall i,t  ,isas\n",
      "follows:\n",
      "(∇o( ) tL)i=∂L\n",
      "∂o( ) t\n",
      "i=∂L\n",
      "∂L( ) t∂L( ) t\n",
      "∂o( ) t\n",
      "i =ˆy( ) t\n",
      "i − 1i y =( ) t . (10.18)\n",
      "              Weworkourwaybackward,startingfromtheendofthesequence.Attheﬁnal\n",
      "   timestep,τ h( ) τ  onlyhas o( ) τ       asadescendent,soitsgradientissimple:\n",
      "∇h( ) τ L= V∇o( ) τ L. (10.19)\n",
      "           Wecantheniteratebackwardintimetoback-propagategradientsthroughtime,\n",
      "fromt= τ−  1downtot  = 1,notingthat h( ) t(for  t<τ    )hasasdescendentsboth\n",
      "o( ) t and h( +1 ) t      .Itsgradientisthusgivenby\n",
      "∇h( ) t L=\n",
      "∂ h( +1 ) t\n",
      "∂ h( ) t\n",
      "(∇h( +1) t  L)+\n",
      "∂ o( ) t\n",
      "∂ h( ) t\n",
      "(∇o( ) t L) (10.20)\n",
      "= Wdiag\n",
      " 1−\n",
      "h( +1 ) t2\n",
      "(∇h( +1) t   L)+ V(∇o( ) t  L,)(10.21)\n",
      "wherediag\n",
      " 1−\n",
      "h( +1 ) t 2\n",
      "      indicatesthediagonalmatrixcontainingtheelements\n",
      "1−(h( +1 ) t\n",
      "i)2           .ThisistheJacobianofthehyperbolictangentassociatedwiththe\n",
      "       hiddenunitattime. it+1\n",
      "          Oncethegradientson theinternalnodesofthecomputationalgraphare\n",
      "          obtained, wecanobtainthegradientsontheparameternodes.Becausethe\n",
      "3 7 9       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "            parametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\n",
      "          denotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\n",
      "  implementusethebprop        methodofsection,whichcomputesthecontribution 6.5.6\n",
      "           ofasingleedgeinthecomputationalgraphtothegradient.The ∇ Wfoperator\n",
      "         usedincalculus,however,takesintoaccountthecontributionof W  tothevalue\n",
      "off            duetoedgesinthecomputationalgraph.Toresolvethisambiguity,we all\n",
      "  introducedummyvariables W( ) t      thataredeﬁnedtobecopiesof W  butwitheach\n",
      "W( ) t    usedonlyattimestept    .Wemaythenuse ∇W( ) t   todenotethecontribution\n",
      "         oftheweightsattimesteptothegradient. t\n",
      "           Usingthisnotation,thegradientontheremainingparametersisgivenby\n",
      "∇ c L=\n",
      "t\n",
      "∂ o( ) t\n",
      "∂ c\n",
      "∇o( ) t L=\n",
      "t∇o( ) t L, (10.22)\n",
      "∇ b L=\n",
      "t\n",
      "∂ h( ) t\n",
      "∂ b( ) t\n",
      "∇h( ) t L=\n",
      "tdiag\n",
      " 1 −\n",
      "h( ) t2\n",
      "∇h( ) tL,(10.23)\n",
      "∇ V L=\n",
      "t\n",
      "i\n",
      "∂L\n",
      "∂o( ) t\n",
      "i\n",
      "∇V( ) to( ) t\n",
      "i=\n",
      "t( ∇o( ) t L) h( ) t , (10.24)\n",
      "∇ W L=\n",
      "t\n",
      "i\n",
      "∂L\n",
      "∂h( ) t\n",
      "i\n",
      "∇W( ) th( ) t\n",
      "i(10.25)\n",
      "=\n",
      "tdiag\n",
      " 1 −\n",
      "h( ) t2\n",
      "( ∇h( ) t L) h( 1 ) t − , (10.26)\n",
      "∇ U L=\n",
      "t\n",
      "i\n",
      "∂L\n",
      "∂h( ) t\n",
      "i\n",
      "∇U( ) th( ) t\n",
      "i(10.27)\n",
      "=\n",
      "tdiag\n",
      " 1 −\n",
      "h( ) t2\n",
      "( ∇h( ) t L) x( ) t , (10.28)\n",
      "          Wedonotneedtocomputethegradientwithrespectto x( ) t  fortrainingbecause\n",
      "            itdoesnothaveanyparametersasancestorsinthecomputationalgraphdeﬁning\n",
      " theloss.\n",
      "      10.2.3RecurrentNetworksasDirectedGraphicalModels\n",
      "           Intheexamplerecurrentnetworkwehavedevelopedsofar,thelossesL( ) twere\n",
      "   cross-entropiesbetweentrainingtargets y( ) t andoutputs o( ) t    .Aswithafeedforward\n",
      "              network,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\n",
      "3 8 0       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "              Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\n",
      "             usuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and\n",
      "            weusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.\n",
      "           Meansquarederroristhecross-entropylossassociatedwithanoutputdistribution\n",
      "            thatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\n",
      "   When we use apredictivelog-likelihood training objective, suchas equa-\n",
      "             tion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\n",
      " sequenceelement y( ) t        giventhepastinputs. Thismaymeanthatwemaximize\n",
      " thelog-likelihood\n",
      " log(p y( ) t | x(1 )     ,..., x( ) t ), (10.29)\n",
      "               or,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\n",
      " timestep,\n",
      " log(p y( ) t | x(1 )     ,..., x( ) t , y(1 )     ,..., y( 1 ) t − ). (10.30)\n",
      "       Decomposingthejointprobabilityoverthesequenceof y    valuesasaseriesof\n",
      "           one-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\n",
      "         acrossthewholesequence.Whenwedonotfeedpast y   valuesasinputsthat\n",
      "           conditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges\n",
      " fromany y( ) i     inthepasttothecurrent y( ) t     .Inthiscase,theoutputs yare\n",
      "     conditionallyindependentgiventhesequenceof x      values.Whenwedofeedthe\n",
      "actual y          values(nottheirprediction,buttheactualobservedorgeneratedvalues)\n",
      "           backintothenetwork,thedirectedgraphicalmodelcontainsedgesfromall y( ) i\n",
      "       valuesinthepasttothecurrent y( ) tvalue.\n",
      "              Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\n",
      "    sequenceofscalarrandomvariables Y={y(1 )     ,...,y( ) τ}    ,withnoadditionalinputs\n",
      "x     .Theinputattimestept      issimplytheoutputattimestep t−   1.TheRNNthen\n",
      "            deﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\n",
      "           distributionoftheseobservationsusingthechainrule(equation)forconditional 3.6\n",
      "probabilities:\n",
      "  PP () = Y (y(1 )     ,...,y( ) τ) =τ \n",
      "t =1 P(y( ) t |y( 1 ) t − ,y( 2 ) t −     ,...,y(1 ) ),(10.31)\n",
      "         wheretherighthandsideofthebarisemptyfort     =1,ofcourse.Hencethe\n",
      "      negativelog-likelihoodofasetofvalues{y(1 )     ,...,y( ) τ}     accordingtosuchamodel\n",
      "is\n",
      " L=\n",
      "tL( ) t , (10.32)\n",
      "where\n",
      "L( ) t  = log(−Py( ) t= y( ) t |y( 1 ) t − ,y( 2 ) t −     ,...,y(1 ) ). (10.33)\n",
      "3 8 1       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\n",
      "        Figure10.7:Fullyconnectedgraphicalmodelforasequencey( 1 ) ,y( 2 )     ,...,y( ) t   ,... .Every\n",
      " pastobservationy( ) i      mayinﬂuencetheconditionaldistributionofsomey( ) t(for  t>i),\n",
      "           giventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\n",
      "              graph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6\n",
      "             inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\n",
      "        connectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8\n",
      "            Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\n",
      "         variables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\n",
      "          eﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions. For\n",
      "            example,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\n",
      "    shouldcontainonlyedgesfrom{y( ) t k −     ,...,y( 1 ) t −}toy( ) t   ,ratherthancontaining\n",
      "             edgesfromtheentirehistory.Insomecases,however,webelievethatallpast\n",
      "             inputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare\n",
      "       usefulwhenwebelievethatthedistributionovery( ) t     maydependonavalueofy( ) i\n",
      "               fromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) i ony( 1 ) t −.\n",
      "               OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\n",
      "            deﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\n",
      "     directdependenciesbetweenanypairofy     values.Thegraphicalmodeloverthey\n",
      "            valueswiththecompletegraphstructureisshowninﬁgure.Thecomplete 10.7\n",
      "           graphinterpretationoftheRNNisbasedonignoringthehiddenunits h( ) tby\n",
      "     marginalizingthemoutofthemodel.\n",
      "            ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\n",
      "     resultsfromregardingthehiddenunits h( ) t  asrandomvariables.1 Includingthe\n",
      "1           Theconditionaldistributionoverthesevariablesgiventheirparentsisdeterministic. Thisis\n",
      "              perfectlylegitimate,thoughitissomewhatraretodesignagraphicalmodelwithsuchdeterministic\n",
      " hiddenunits.\n",
      "382       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h ( ) . . .h ( ) . . .\n",
      "             Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even\n",
      "                 thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\n",
      "           eﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\n",
      "and y( ) t              )involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\n",
      "       sharethesameparameterswiththeotherstages.\n",
      "            hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaneﬃcient\n",
      "          parametrizationofthejointdistributionovertheobservations.Supposethatwe\n",
      "          representedanarbitraryjointdistributionoverdiscretevalueswithatabular\n",
      "         representation—anarraycontainingaseparateentryforeachpossibleassignment\n",
      "             ofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\n",
      "occurring. If y  cantakeon k      diﬀerentvalues,thetabularrepresentationwould\n",
      "have O( kτ         )parameters.Bycomparison,becauseofparametersharing,thenumber\n",
      "     ofparametersintheRNNis O         (1)asafunctionofsequencelength.Thenumberof\n",
      "              parametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\n",
      "           toscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\n",
      "       long-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications\n",
      "   ofthesamefunction f   andthesameparameters θ     ateachtimestep.Figure10.8\n",
      "      illustratesthegraphicalmodelinterpretation.Incorporatingthe h( ) t nodesin\n",
      "            thegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate\n",
      "    quantitybetweenthem.Avariable y( ) i       inthedistantpastmayinﬂuenceavariable\n",
      "y( ) t   viaitseﬀecton h           .Thestructureofthisgraphshowsthatthemodelcanbe\n",
      "         eﬃcientlyparametrizedbyusingthesameconditionalprobabilitydistributionsat\n",
      "              eachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\n",
      "        jointassignmentofallvariablescanbeevaluatedeﬃciently.\n",
      "          Evenwiththeeﬃcientparametrizationofthegraphicalmodel,someoperations\n",
      "          remaincomputationallychallenging.Forexample,itisdiﬃculttopredictmissing\n",
      "      valuesinthemiddleofthesequence.\n",
      "           Thepricerecurrentnetworkspayfortheirreducednumberofparametersis\n",
      "      that theparametersmaybediﬃcult. optimizing\n",
      "          Theparametersharingusedinrecurrentnetworksreliesontheassumption\n",
      "            thatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the\n",
      "3 8 3       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "          assumptionisthattheconditionalprobabilitydistributionoverthevariablesat\n",
      "timet     +1 giventhevariablesattimetis s t a t i o nar y    ,meaningthattherelationship\n",
      "             betweentheprevioustimestepandthenexttimestepdoesnotdependont .In\n",
      "      principle,itwouldbepossibletouset         asanextrainputateachtimestepandlet\n",
      "            thelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\n",
      "            diﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent\n",
      "    conditionalprobabilitydistributionforeacht       ,butthenetworkwouldthenhaveto\n",
      "       extrapolatewhenfacedwithnewvaluesof.t\n",
      "              TocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow\n",
      "              todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\n",
      "          simplytosamplefromtheconditionaldistributionateachtimestep. However,\n",
      "          thereisoneadditionalcomplication. TheRNNmusthavesomemechanismfor\n",
      "            determiningthelengthofthesequence.Thiscanbeachievedinvariousways.\n",
      "              Whentheoutputisasymboltakenfromavocabulary,onecanaddaspecial\n",
      "           symbolcorrespondingtotheendofasequence(Schmidhuber2012,).Whenthat\n",
      "            symbolisgenerated,thesamplingprocessstops.Inthetrainingset,weinsert\n",
      "          thissymbolasanextramemberofthesequence,immediatelyafter x( ) τ ineach\n",
      " trainingexample.\n",
      "            AnotheroptionistointroduceanextraBernoullioutputtothemodelthat\n",
      "           representsthedecisiontoeithercontinuegenerationorhaltgenerationateach\n",
      "             timestep.Thisapproachismoregeneralthantheapproachofaddinganextra\n",
      "             symboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\n",
      "              onlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\n",
      "               anRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya\n",
      "            sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\n",
      "            trainedtomaximizethelog-probabilityofthecorrectpredictionastowhetherthe\n",
      "       sequenceendsorcontinuesateachtimestep.\n",
      "      Anotherwaytodeterminethesequencelengthτ      istoaddanextraoutputto\n",
      "     themodelthatpredictstheintegerτ       itself.Themodelcansampleavalueofτ\n",
      "  andthensampleτ         stepsworthofdata.Thisapproachrequiresaddinganextra\n",
      "              inputtotherecurrentupdateateachtimestepsothattherecurrentupdateis\n",
      "              awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\n",
      "      caneitherconsistofthevalueofτ   orcanconsistof τt−    ,thenumberofremaining\n",
      "           timesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\n",
      "              endabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\n",
      "   basedonthedecomposition\n",
      " P( x(1 )     ,..., x( ) τ   ) = ()(PτP x(1 )     ,..., x( ) τ   |τ.) (10.34)\n",
      "3 8 4       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "   Thestrategyofpredictingτ         directlyisused,forexample,byGoodfellowetal.\n",
      "().2014d\n",
      "       10.2.4ModelingSequencesConditionedonContextwithRNNs\n",
      "             IntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected\n",
      "       graphicalmodeloverasequenceofrandomvariablesy( ) t  withnoinputsx .Of\n",
      "            course,ourdevelopmentofRNNsasinequation includedasequenceof 10.8\n",
      "inputsx(1 ) ,x(2 )     ,...,x( ) τ         .Ingeneral,RNNsallowtheextensionofthegraphical\n",
      "          modelviewtorepresentnotonlyajointdistributionoverthey  variablesbut\n",
      "    alsoaconditionaldistributionoverygivenx      .Asdiscussedinthecontextof\n",
      "         feedforwardnetworksinsection ,anymodelrepresentingavariable 6.2.1.1 P(y;θ)\n",
      "         canbereinterpretedasamodelrepresentingaconditionaldistributionP(yω|)\n",
      "withω=θ          .WecanextendsuchamodeltorepresentadistributionP(  yx| )by\n",
      "  usingthesameP(  yω|    )asbefore,butmakingω  afunctionofx    .Inthecaseof\n",
      "              anRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommon\n",
      "  andobviouschoices.\n",
      "          Previously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\n",
      "t =1     ,...,τ          asinput.Anotheroptionistotakeonlyasinglevectorx asinput.\n",
      "Whenx              isaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\n",
      "  thatgeneratesthe y          sequence.Somecommonwaysofprovidinganextrainputto\n",
      "  anRNNare\n",
      "        1. asanextrainputateachtimestep,or\n",
      "    2. astheinitialstateh(0 ) ,or\n",
      "3. both.\n",
      "            Theﬁrstandmostcommonapproachisillustratedinﬁgure.Theinteraction 10.9\n",
      "  betweentheinputx    andeachhiddenunitvectorh( ) t    isparametrizedbyanewly\n",
      "  introducedweightmatrixR         thatwasabsentfromthemodelofonlythesequence\n",
      "ofy   values. ThesameproductxR       isaddedasadditionalinputtothehidden\n",
      "           unitsateverytimestep.Wecanthinkofthechoiceofx   asdeterminingthevalue\n",
      "ofxR             thatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.\n",
      "              Theweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\n",
      " theparametersθ       ofthenonconditionalmodelandturningthemintoω  ,wherethe\n",
      "          biasparameterswithinarenowafunctionoftheinput. ω\n",
      "      Ratherthanreceivingonlyasinglevectorx     asinput,theRNNmayreceive\n",
      "   asequenceofvectorsx( ) t       asinput. TheRNNdescribedinequationcorre- 10.8\n",
      "3 8 5       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t  W W W W\n",
      "h ( ) . . .h ( ) . . .h ( ) . . .h ( ) . . .  V V V  U U U\n",
      "x xy( ) . . .y( ) . . .\n",
      "    R R R R R\n",
      "        Figure10.9:AnRNNthatmapsaﬁxed-lengthvector x    intoadistributionoversequences\n",
      "Y               .ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\n",
      "               usedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\n",
      " Eachelement y( ) t           oftheobservedoutputsequenceservesbothasinput(forthecurrent\n",
      "           timestep)and,duringtraining,astarget(fortheprevioustimestep).\n",
      "3 8 6       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t  W W W W\n",
      "h ( ) . . .h ( ) . . .h ( ) . . .h ( ) . . .  V V V\n",
      "  U U U\n",
      "x( t − 1 )x( t − 1 )R\n",
      "x( ) tx( ) tx( + 1 ) tx( + 1 ) t R R\n",
      "          Figure10.10:Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence\n",
      "ofx      valuesintoadistributionoversequencesofy      valuesofthesamelength.Comparedto\n",
      "             ﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\n",
      "            TheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\n",
      "  givensequencesofx             ofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3\n",
      "   distributionsinwhichthey       valuesareconditionallyindependentfromeachothergiven\n",
      "  thevalues.x\n",
      "3 8 7       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "    spondstoaconditionaldistributionP( y(1 )     ,..., y( ) τ | x(1 )     ,..., x( ) τ   )thatmakesa\n",
      "       conditionalindependenceassumptionthatthisdistributionfactorizesas\n",
      "\n",
      "t P( y( ) t | x(1 )     ,..., x( ) t ). (10.35)\n",
      "          Toremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\n",
      "   theoutputattimet     tothehiddenunitattimet       +1,asshowninﬁgure.The 10.10\n",
      "        modelcanthenrepresentarbitraryprobabilitydistributionsoverthe ysequence.\n",
      "           Thiskindofmodelrepresentingadistributionoverasequencegivenanother\n",
      "             sequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\n",
      "            bethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\n",
      "  10.3BidirectionalRNNs\n",
      "             Alltherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-\n",
      "      ture,meaningthatthestateattimet      capturesonlyinformationfromthepast,\n",
      "x(1 )     ,..., x( 1 ) t −    ,andthepresentinput x( ) t       .Someofthemodelswehavediscussed\n",
      "    alsoallowinformationfrompast y       valuestoaﬀectthecurrentstatewhenthe y\n",
      "  valuesareavailable.\n",
      "          Inmanyapplications,however,wewanttooutputapredictionof y( ) tthat\n",
      "            maydependonthewholeinputsequence.Forexample,inspeechrecognition,the\n",
      "             correctinterpretationofthecurrentsoundasaphonememaydependonthenext\n",
      "            fewphonemesbecauseofco-articulationandmayevendependonthenextfew\n",
      "           wordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthereare\n",
      "           twointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\n",
      "               mayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis\n",
      "         alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\n",
      "     tasks,describedinthenextsection.\n",
      "        Bidirectionalrecurrentneuralnetworks(orbidirectionalRNNs)wereinvented\n",
      "            toaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\n",
      "           cessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\n",
      "          recognition(Graves2008GravesandSchmidhuber2009 etal.,; ,),speechrecogni-\n",
      "           tion(GravesandSchmidhuber2005Graves2013 Baldi ,;etal.,),andbioinformatics(\n",
      "  etal.,).1999\n",
      "           Asthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\n",
      "            throughtime,beginningfromthestartofthesequence,withanotherRNNthat\n",
      "            movesbackwardthroughtime,beginningfromtheendofthesequence.Figure10.11\n",
      "     illustratesthetypicalbidirectionalRNN,with h( ) t     standingforthestateofthe\n",
      "3 8 8       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\n",
      "x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t\n",
      "          Figure10.11:Computationofatypicalbidirectionalrecurrentneuralnetwork,meant\n",
      "     tolearntomapinputsequences x  totargetsequences y  ,withloss L( ) t  ateachstep t.\n",
      "The h           recurrencepropagatesinformationforwardintime(towardtheright),whilethe g\n",
      "            recurrencepropagatesinformationbackwardintime(towardtheleft).Thusateachpoint\n",
      "t   ,theoutputunits o( ) t          canbeneﬁtfromarelevantsummaryofthepastinits h( ) tinput\n",
      "          andfromarelevantsummaryofthefutureinits g( ) tinput.\n",
      "3 8 9       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "      sub-RNNthatmovesforwardthroughtimeandg( ) t     standingforthestateofthe\n",
      "         sub-RNNthatmovesbackwardthroughtime. Thisallowstheoutputunitso( ) t\n",
      "             tocomputearepresentationthatdependsonboththepastandthefuturebut\n",
      "        ismostsensitivetotheinputvaluesaroundtimet     ,withouthavingtospecifya\n",
      "  ﬁxed-sizewindowaroundt         (asonewouldhavetodowithafeedforwardnetwork,\n",
      "           aconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).\n",
      "           Thisideacanbenaturallyextendedtotwo-dimensionalinput,suchasimages,\n",
      "               byhavingRNNs,eachonegoinginoneofthefourdirections:up,down,left, four\n",
      "    right.Ateachpoint( i,j      )ofa2-Dgrid,anoutputO i , j   couldthencomputea\n",
      "          representationthatwouldcapturemostlylocalinformationbutcouldalsodepend\n",
      "      on long-rangeinputs,if theRNN isable tolearn to carrythat information.\n",
      "           Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\n",
      "          expensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\n",
      "            samefeaturemap( ,; Visinetal.2015Kalchbrenner 2015etal.,).Indeed,the\n",
      "             forwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\n",
      "            theyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior\n",
      "           totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\n",
      "interactions.\n",
      "  10.4Encoder-DecoderSequence-to-Sequence\n",
      "Architectures\n",
      "                WehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5\n",
      "                vector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9\n",
      "             sequence. Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11\n",
      "           mapaninputsequencetoanoutputsequenceofthesamelength.\n",
      "               HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\n",
      "             outputsequencewhichisnotnecessarilyofthesamelength.Thiscomesupin\n",
      "         manyapplications,suchasspeechrecognition,machinetranslationandquestion\n",
      "            answering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\n",
      "          notofthesamelength(althoughtheirlengthsmightberelated).\n",
      "              WeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea\n",
      "   representationofthiscontext,C  .ThecontextC      mightbeavectororsequenceof\n",
      "       vectorsthatsummarizetheinputsequenceXx= ((1 )     ,...,x( n x )).\n",
      "         ThesimplestRNNarchitectureformappingavariable-lengthsequenceto\n",
      "           anothervariable-lengthsequencewasﬁrstproposedby ()and Choetal.2014a\n",
      "           shortlyafterbySutskever2014etal.(),whoindependentlydevelopedthatarchi-\n",
      "3 9 0       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "E n c ode r\n",
      "…\n",
      "x( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\n",
      "D e c ode r\n",
      "…\n",
      "y( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\n",
      "         Figure10.12:Exampleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\n",
      "       forlearningtogenerateanoutputsequence( y( 1 )     ,..., y( n y )    )givenaninputsequence\n",
      "( x( 1 ) , x( 2 )     ,..., x( n x )             ).ItiscomposedofanencoderRNNthatreadstheinputsequenceas\n",
      "              wellasadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityof\n",
      "               agivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocompute\n",
      "    agenerallyﬁxed-sizecontextvariableC       ,whichrepresentsasemanticsummaryofthe\n",
      "          inputsequenceandisgivenasinputtothedecoderRNN.\n",
      "3 9 1       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "           tectureandweretheﬁrsttoobtainstate-of-the-arttranslationusingthisapproach.\n",
      "           Theformersystemisbasedonscoringproposalsgeneratedbyanothermachine\n",
      "           translationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\n",
      "        thetranslations.Theseauthorsrespectivelycalledthisarchitecture,illustrated\n",
      "        inﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\n",
      "     ideaisverysimple:(1)Anencoderorreaderorinput    RNNprocessestheinput\n",
      "     sequence.TheencoderemitsthecontextC       ,usuallyasasimplefunctionofits\n",
      "   ﬁnalhiddenstate. (2)Adecoderorwriteroroutput    RNNisconditionedon\n",
      "            thatﬁxed-lengthvector(justasinﬁgure)togeneratetheoutputsequence 10.9\n",
      "Y  =( y(1 )     ,..., y( n y )         ).Theinnovationofthiskindofarchitectureoverthose\n",
      "          presentedinearliersectionsofthischapteristhatthelengthsn xandn ycan\n",
      "       varyfromeachother,whilepreviousarchitecturesconstrainedn x=n y=τ  .Ina\n",
      "         sequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\n",
      "  theaverageof  logP( y(1 )     ,..., y( n y ) | x(1 )     ,..., x( n x )     )overallthepairsof xand y\n",
      "       sequencesinthetrainingset.Thelaststate h n x     oftheencoderRNNistypically\n",
      "   usedasarepresentationC          oftheinputsequencethatisprovidedasinputtothe\n",
      " decoderRNN.\n",
      "  IfthecontextC          isavector,thenthedecoderRNNissimplyavector-to-\n",
      "              sequenceRNN,asdescribedinsection .Aswehaveseen,thereareatleast 10.2.4\n",
      "             twowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\n",
      "                astheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\n",
      "          ateachtimestep.Thesetwowayscanalsobecombined.\n",
      "              Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\n",
      "  asthedecoder.\n",
      "         OneclearlimitationofthisarchitectureiswhenthecontextC   outputbythe\n",
      "             encoderRNNhasadimensionthatistoosmalltoproperlysummarizealong\n",
      "            sequence.Thisphenomenonwasobservedby ()inthecontext Bahdanauetal.2015\n",
      "      ofmachinetranslation.TheyproposedtomakeC   avariable-lengthsequencerather\n",
      "       thanaﬁxed-sizevector.Additionally,theyintroducedan  attentionmechanism\n",
      "       thatlearnstoassociateelementsofthesequenceC    toelementsoftheoutput\n",
      "      sequence.Seesection formoredetails. 12.4.5.1\n",
      "   10.5DeepRecurrentNetworks\n",
      "            ThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\n",
      "  andassociatedtransformations:\n",
      "      1. fromtheinputtothehiddenstate,\n",
      "3 9 2       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "hy\n",
      "xz\n",
      "  ( a) ( b ) ( c )xhy\n",
      "xhy\n",
      "             Figure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\n",
      "              etal.,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a(a)\n",
      "            hierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- (b)\n",
      "         hidden,hidden-to-hidden,andhidden-to-outputparts.Thismaylengthentheshortest\n",
      "            pathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby (c)\n",
      "  introducingskipconnections.\n",
      "          2. fromtheprevioushiddenstatetothenexthiddenstate,and\n",
      "      3. fromthehiddenstatetotheoutput.\n",
      "             WiththeRNNarchitectureofﬁgure,eachofthesethreeblocksisassociated 10.3\n",
      "              withasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,eachof\n",
      "          theseblockscorrespondstoashallowtransformation.Byashallowtransformation,\n",
      "            wemeanatransformationthatwouldberepresentedbyasinglelayerwithin\n",
      "            adeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃne\n",
      "     transformationfollowedbyaﬁxednonlinearity.\n",
      "           Woulditbeadvantageoustointroducedepthineachoftheseoperations?\n",
      "           Experimentalevidence(Graves2013Pascanu2014a etal.,; etal.,)stronglysuggests\n",
      "             so.Theexperimentalevidenceisinagreemen twiththeideathatweneedenough\n",
      "3 9 3       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "           depthtoperformtherequiredmappings.SeealsoSchmidhuber1992ElHihi (),\n",
      "           andBengio1996 Jaeger2007a (),or ()forearlierworkondeepRNNs.\n",
      "             Graves2013etal.()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\n",
      "               thestateofanRNNintomultiplelayers,asinﬁgure(left).Wecanthink 10.13\n",
      "              ofthelowerlayersinthehierarchydepictedinﬁgureaasplayingarole 10.13\n",
      "            intransformingtherawinputintoarepresentationthatismoreappropriate,at\n",
      "              thehigherlevelsofthehiddenstate.Pascanu 2014aetal.()goastepfurther\n",
      "              andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\n",
      "         enumeratedabove,asillustratedinﬁgureb.Considerationsofrepresentational 10.13\n",
      "            capacitysuggestallocatingenoughcapacityineachofthesethreesteps,butdoing\n",
      "            sobyaddingdepthmayhurtlearningbymakingoptimizationdiﬃcult.Ingeneral,\n",
      "            itiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\n",
      "           ﬁgurebmakestheshortestpathfromavariableintimestep 10.13 t  toavariable\n",
      "  intimestep t           +1becomelonger. Forexample,ifanMLPwithasinglehidden\n",
      "             layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\n",
      "            shortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe\n",
      "             ordinaryRNNofﬁgure.However,asarguedby 10.3 Pascanu 2014aetal.(),this\n",
      "           canbemitigatedbyintroducingskipconnectionsinthehidden-to-hiddenpath,as\n",
      "   illustratedinﬁgurec.10.13\n",
      "   10.6RecursiveNeuralNetworks\n",
      "  Recursiveneuralnetworks2      representyetanothergeneralizationofrecurrentnet-\n",
      "             works,withadiﬀerentkindofcomputationalgraph,whichisstructuredasadeep\n",
      "          tree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\n",
      "            graphforarecursivenetworkisillustratedinﬁgure.Recursiveneuralnet- 10.14\n",
      "            workswereintroducedbyPollack1990(),andtheirpotentialuseforlearningto\n",
      "          reasonwasdescribedby ().Recursivenetworkshavebeensuccessfully Bottou2011\n",
      "             appliedtoprocessingdatastructuresasinputtoneuralnets(Frasconi1997etal.,,\n",
      "             1998 Socher2011ac2013a ),innaturallanguageprocessing(etal.,,,),aswellasin\n",
      "     computervision(Socher2011betal.,).\n",
      "             Oneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence\n",
      "   ofthesamelength τ         ,thedepth(measuredasthenumberofcompositionsof\n",
      "      nonlinearoperations)canbedrasticallyreducedfrom τto O( log τ  ),whichmight\n",
      "            helpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\n",
      "                thetree.Oneoptionistohaveatreestructurethatdoesnotdependonthedata,\n",
      "2            Wesuggestnotabbreviating“recursiveneuralnetwork”as“RNN”toavoidconfusionwith\n",
      "  “recurrentneuralnetwork.”\n",
      "394       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "x( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )  V V Vy yL L\n",
      "x( 4 )x( 4 )Voo\n",
      "   U W U WUW\n",
      "             Figure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\n",
      "          recurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ) ,x( 2 )     ,...,x( ) tcan\n",
      "       bemappedtoaﬁxed-sizerepresentation(theoutputo      ),withaﬁxedsetofparameters\n",
      "  (theweightmatricesU,V,W         ).Theﬁgureillustratesasupervisedlearningcaseinwhich\n",
      "           sometargetisprovidedthatisassociatedwiththewholesequence. y\n",
      "3 9 5       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "           suchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\n",
      "          cansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\n",
      "            languagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto\n",
      "             thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\n",
      "            parser( ,,). Ideally,onewouldlikethelearneritselfto Socheretal.2011a2013a\n",
      "             discoverandinferthetreestructurethatisappropriateforanygiveninput,as\n",
      "   suggestedby (). Bottou2011\n",
      "           Manyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\n",
      "              etal.()and 1997 Frasconi1998etal.()associatethedatawithatreestructure,\n",
      "          andassociatetheinputsandtargets withindividual nodesofthetree.The\n",
      "            computationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial\n",
      "          neuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone\n",
      "          nonlinearity).Forexample,Socher2013aetal.()proposeusingtensoroperations\n",
      "           andbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\n",
      "             betweenconcepts(Weston 2010Bordes2012 etal.,; etal.,)whentheconceptsare\n",
      "    representedbycontinuousvectors(embeddings).\n",
      "     10.7TheChallengeofLong-TermDependencies\n",
      "         Themathematicalchallengeoflearninglong-termdependenciesinrecurrentnet-\n",
      "            worksisintroducedinsection.Thebasicproblemisthatgradientspropagated 8.2.5\n",
      "              overmanystagestendtoeithervanish(mostofthetime)orexplode(rarely,but\n",
      "             withmuchdamagetotheoptimization).Evenifweassumethattheparametersare\n",
      "            suchthattherecurrentnetworkisstable(canstorememories,withgradientsnot\n",
      "         exploding),thediﬃcultywithlong-termdependenciesarisesfromtheexponentially\n",
      "         smallerweightsgiventolong-terminteractions(involvingthemultiplicationof\n",
      "          manyJacobians)comparedtoshort-termones.Manyothersourcesprovidea\n",
      "            deepertreatment(Hochreiter1991Doya1993Bengio1994Pascanu ,;,; etal.,; etal.,\n",
      "            2013).Inthissection,wedescribetheprobleminmoredetail.Theremaining\n",
      "      sectionsdescribeapproachestoovercomingtheproblem.\n",
      "         Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple\n",
      "           times,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\n",
      "     behavior,asillustratedinﬁgure.10.15\n",
      "         Inparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\n",
      "          somewhatresemblesmatrixmultiplication.Wecanthinkoftherecurrencerelation\n",
      "h( ) t= Wh( 1 ) t −(10.36)\n",
      "           asaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\n",
      "3 9 6       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "      − − − 60 40 20 0 20 40 60\n",
      " Inputcoordinate−4−3−2−101234  Projectionofoutput0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "         Figure10.15:Repeatedfunctioncomposition.Whencomposingmanynonlinearfunctions\n",
      "  (likethelinear-tanh           layershownhere),theresultishighlynonlinear,typicallywithmost\n",
      "              ofthevaluesassociatedwithatinyderivative,somevalueswithalargederivative,and\n",
      "            manyalternationsbetweenincreasinganddecreasing.Here,weplotalinearprojectionof\n",
      "           a100-dimensionalhiddenstatedowntoasingledimension,plottedonthe y -axis.The\n",
      "x              -axisisthecoordinateoftheinitialstatealongarandomdirectioninthe100-dimensional\n",
      "              space.Wecanthusviewthisplotasalinearcross-sectionofahigh-dimensionalfunction.\n",
      "              Theplotsshowthefunctionaftereachtimestep,orequivalently,aftereachnumberof\n",
      "      timesthetransitionfunctionhasbeencomposed.\n",
      "3 9 7       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "  andlackinginputsx      .Asdescribedin section, thisrecurrencerelation 8.2.5\n",
      "         essentiallydescribesthepowermethod.Itmaybesimpliﬁedto\n",
      "h( ) t=\n",
      "Wth(0 ) , (10.37)\n",
      "        andifadmitsaneigendecompositionoftheform W\n",
      " WQQ = Λ , (10.38)\n",
      "         withorthogonal,therecurrencemaybesimpliﬁedfurtherto Q\n",
      "h( ) t= QΛtQh(0 ) . (10.39)\n",
      "       Theeigenvaluesareraisedtothepoweroft    ,causingeigenvalueswithmagnitude\n",
      "              lessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\n",
      "   explode.Anycomponentofh(0 )       thatisnotalignedwiththelargesteigenvector\n",
      "   willeventuallybediscarded.\n",
      "           Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine\n",
      "  multiplyingaweightw    byitselfmanytimes. Theproductwt  willeithervanish\n",
      "      orexplodedependingonthemagnitudeofw      .Ifwemakeanonrecurrentnetwork\n",
      "    thathasadiﬀerentweightw( ) t         ateachtimestep,thesituationisdiﬀerent.Ifthe\n",
      "          initialstateisgivenby,thenthestateattime 1 t  isgivenby\n",
      "tw( ) t. Suppose\n",
      " thatthew( ) t        valuesaregeneratedrandomly,independentlyfromoneanother,with\n",
      "   zeromeanandvariancev      .ThevarianceoftheproductisO(vn   ).Toobtainsome\n",
      " desiredvariancev∗       wemaychoosetheindividualweightswithvariancev=n√\n",
      "v∗.\n",
      "           Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\n",
      "         vanishingandexplodinggradientproblem,asarguedby (). Sussillo2014\n",
      "         ThevanishingandexplodinggradientproblemforRNNswasindependently\n",
      "          discoveredbyseparateresearchers(Hochreiter1991Bengio19931994 ,; etal.,,).\n",
      "               Onemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\n",
      "           parameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\n",
      "              ordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN\n",
      "             mustenteraregionofparameterspacewheregradientsvanish( ,, Bengioetal.1993\n",
      "          1994).Speciﬁcally,wheneverthemodelisabletorepresentlong-termdependencies,\n",
      "          thegradientofalong-terminteractionhasexponentiallysmallermagnitudethan\n",
      "             thegradientofashort-terminteraction.Thismeansnotthatitisimpossibleto\n",
      "             learn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\n",
      "             becausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\n",
      "        ﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments\n",
      "               in ()showthatasweincreasethespanofthedependenciesthat Bengioetal.1994\n",
      "3 9 8       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "        needtobecaptured,gradient-basedoptimizationbecomesincreasinglydiﬃcult,\n",
      "            withtheprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\n",
      "         reaching0forsequencesofonlylength10or20.\n",
      "           Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n",
      "            (), (),and 1993Bengioetal.1994 SiegelmannandSontag1995(),withareview\n",
      "           inPascanu 2013etal.(). Theremainingsectionsofthischapterdiscussvarious\n",
      "           approachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-\n",
      "           termdependencies(insomecasesallowinganRNNtolearndependenciesacross\n",
      "          hundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\n",
      "       oneofthemainchallengesindeeplearning.\n",
      "   10.8EchoStateNetworks\n",
      "    Therecurrentweightsmappingfrom h( 1 ) t −to h( ) t    andtheinputweightsmapping\n",
      " from x( ) t to h( ) t           aresomeofthemostdiﬃcultparameterstolearninarecurrent\n",
      "            network.Oneproposed(,; ,; ,; Jaeger2003Maassetal.2002JaegerandHaas2004\n",
      "            Jaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights\n",
      "               suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\n",
      "             inputs,andonlylearntheoutputweights.Thisistheideathatwasindependently\n",
      " proposedfor  echostatenetworks       ,orESNs( ,;, JaegerandHaas2004Jaeger\n",
      " 2007b),and  liquidstatemachines        ( ,).Thelatterissimilar, Maassetal.2002\n",
      "            exceptthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-\n",
      "            valuedhiddenunitsusedforESNs.BothESNsandliquidstatemachinesare\n",
      "termed  reservoircomputing        (LukoševičiusandJaeger2009,)todenotethefact\n",
      "            thatthehiddenunitsformareservoiroftemporalfeaturesthatmaycapture\n",
      "      diﬀerentaspectsofthehistoryofinputs.\n",
      "           Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\n",
      "            theyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\n",
      "     historyofinputsuptotime t       )intoaﬁxed-lengthvector(therecurrentstate h( ) t),\n",
      "             onwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve\n",
      "             theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\n",
      "             convexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\n",
      "             oflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\n",
      "              criterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\n",
      "    simplelearningalgorithms(,). Jaeger2003\n",
      "3 9 9       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "           Theimportantquestionistherefore: howdowesettheinputandrecurrent\n",
      "              weightssothatarichsetofhistoriescanberepresentedintherecurrentneural\n",
      "          networkstate? Theanswerproposedinthereservoircomputingliteratureisto\n",
      "             viewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\n",
      "           weightssuchthatthedynamicalsystemisneartheedgeofstability.\n",
      "             TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\n",
      "             statetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\n",
      "           characteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\n",
      "J( ) t=∂ s( ) t\n",
      "∂ s( 1 ) t −     .Ofparticularimportanceisthe  spectralradiusofJ( ) t  ,deﬁnedto\n",
      "         bethemaximumoftheabsolutevaluesofitseigenvalues.\n",
      "            Tounderstandtheeﬀectofthespectralradius,considerthesimplecaseof\n",
      "    back-propagationwithaJacobianmatrixJ    thatdoesnotchangewith t .This\n",
      "          casehappens,forexample,whenthenetworkispurelylinear. SupposethatJhas\n",
      " aneigenvectorv  withcorrespondingeigenvalue λ     .Considerwhathappensaswe\n",
      "            propagateagradientvectorbackwardthroughtime.Ifwebeginwithagradient\n",
      "vectorg         ,thenafteronestepofback-propagation,wewillhaveJg  ,andafter n\n",
      "   stepswewillhaveJng        .Nowconsiderwhathappensifweinsteadback-propagate\n",
      "   aperturbedversionofg    .Ifwebeginwithg+ δv      ,thenafteronestep,wewill\n",
      "haveJ(g+ δv ).After n   steps,wewillhaveJn(g+ δv     ).Fromthiswecansee\n",
      "   thatback-propagationstartingfromg   andback-propagationstartingfromg+ δv\n",
      " divergeby δJnvafter n   stepsofback-propagation. Ifv     ischosentobeaunit\n",
      " eigenvectorofJ witheigenvalue λ      ,thenmultiplicationbytheJacobiansimply\n",
      "           scalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare\n",
      "    separatedbyadistanceof δ λ||n.Whenv      correspondstothelargestvalueof|| λ,\n",
      "          thisperturbationachievesthewidestpossibleseparationofaninitialperturbation\n",
      "  ofsize. δ\n",
      "When || λ >   1,thedeviationsize δ λ||n   growsexponentiallylarge.When || λ <1,\n",
      "     thedeviationsizebecomesexponentiallysmall.\n",
      "            Ofcourse,thisexampleassumedthattheJacobianwasthesameatevery\n",
      "           timestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\n",
      "           nonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon\n",
      "            manytimestepsandhelppreventtheexplosionresultingfromalargespectral\n",
      "           radius. Indeed,themostrecentworkonechostatenetworksadvocatesusinga\n",
      "           spectralradiusmuchlargerthanunity( ,;,). Yildizetal.2012Jaeger2012\n",
      "         Everythingwehavesaidaboutback-propagationviarepeatedmatrixmultipli-\n",
      "           cationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\n",
      "   wherethestateh( +1 ) t= h( ) t  W.\n",
      "   WhenalinearmapW alwaysshrinksh   asmeasuredbythe L2 norm,then\n",
      "4 0 0       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "     wesaythatthemapis c o n t r a c t i v e        .Whenthespectralradiusislessthanone,\n",
      "  themappingfrom h( ) tto h( +1 ) t       iscontractive,soasmallchangebecomessmaller\n",
      "           aftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\n",
      "               thepastwhenweuseaﬁnitelevelofprecision(suchas32-bitintegers)tostore\n",
      "  thestatevector.\n",
      "         TheJacobianmatrixtellsushowasmallchangeof h( ) t  propagatesonestep\n",
      "      forward,orequivalently,howthegradienton h( +1 ) t   propagatesonestepbackward,\n",
      "    duringback-propagation.Notethatneither Wnor J    needtobesymmetric(al-\n",
      "            thoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\n",
      "       eigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n",
      "         behavior(ifthesameJacobianwasappliediteratively).Eventhough h( ) t ora\n",
      "  smallvariationof h( ) t        ofinterestinback-propagationarerealvalued,theycan\n",
      "           beexpressedinsuchacomplex-valuedbasis.Whatmatters iswhathappens\n",
      "          tothemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedba-\n",
      "           siscoeﬃcientswhenwemultiplythematrixbythevector. Aneigenvaluewith\n",
      "         magnitudegreaterthanonecorrespondstomagniﬁcation(exponentialgrowth,if\n",
      "          appliediteratively)whileamagnitudesmallerthanonecorrespondstoshrinking\n",
      "    (exponentialdecay,ifappliediteratively).\n",
      "            Withanonlinearmap, theJacobianisfreetochangeateachstep.The\n",
      "         dynamicsthereforebecomemorecomplicated. Itremainstrue,however,thata\n",
      "            smallinitialvariationcanturnintoalargevariationafterseveralsteps.One\n",
      "              diﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\n",
      "    asquashingnonlinearitysuchastanh      cancausetherecurrentdynamicstobecome\n",
      "       bounded.Notethatit ispossiblefor back-propagation toretainunbounded\n",
      "         dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\n",
      "   whenasequenceoftanh            unitsareallinthemiddleoftheirlinearregimeandare\n",
      "           connectedbyweightmatriceswithspectralradiusgreaterthan.Nonetheless,it 1\n",
      "              israreforalltheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\n",
      "              Thestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome\n",
      "            spectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\n",
      "           doesnotexplodebecauseofthestabilizingeﬀectofsaturatingnonlinearitieslike\n",
      "tanh.\n",
      "             Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights\n",
      "              inESNscouldbeusedto theweightsinafullytrainablerecurrentnet- initialize\n",
      "        work(withthehidden-to-hiddenrecurrentweightstrainedusingback-propagation\n",
      "         throughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\n",
      "              etal.,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\n",
      "        withthesparseinitializationschemedescribedinsection.8.4\n",
      "4 0 1       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "       10.9LeakyUnitsandOtherStrategiesforMultiple\n",
      " TimeScales\n",
      "             Onewaytodealwithlong-termdependenciesistodesignamodelthatoperates\n",
      "             atmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained\n",
      "             timescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\n",
      "            scalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.\n",
      "            Variousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These\n",
      "           includetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate\n",
      "            signalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections\n",
      "     usedtomodelﬁne-grainedtimescales.\n",
      "     10.9.1AddingSkipConnectionsthroughTime\n",
      "              Onewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\n",
      "              thedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\n",
      "               datesbackto ()andfollowsfromtheideaofincorporatingdelaysin Linetal.1996\n",
      "          feedforwardneuralnetworks(LangandHinton1988,).Inanordinaryrecurrent\n",
      "         network,arecurrentconnectiongoesfromaunitattime t    toaunitattime t +1.\n",
      "           Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\n",
      "            Aswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\n",
      "             withrespecttothenumberoftimesteps. ()introducedrecurrent Linetal.1996\n",
      "     connectionswithatimedelayof d     tomitigatethisproblem.Gradientsnow\n",
      "     diminishexponentiallyasafunctionofτ\n",
      "d ratherthan τ    .Sincethereareboth\n",
      "          delayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ.\n",
      "          Thisallowsthelearningalgorithmtocapturelongerdependencies,althoughnot\n",
      "         alllong-termdependenciesmayberepresentedwellinthisway.\n",
      "         10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\n",
      "                Anotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\n",
      "            haveunitswithlinearself-connectionsandaweightnearoneontheseconnections.\n",
      "     Whenweaccumulatearunningaverage µ( ) t  ofsomevalue v( ) t  byapplyingthe\n",
      "update µ( ) t ← α µ( 1 ) t − +(1− α) v( ) t ,the α        parameterisanexampleofalinearself-\n",
      " connectionfrom µ( 1 ) t −to µ( ) t .When α      isnearone,therunningaverageremembers\n",
      "         informationaboutthepastforalongtime,andwhen α   isnearzero,information\n",
      "           aboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\n",
      "          behavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalled l e a k y\n",
      "4 0 2       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "uni t s.\n",
      "  Skipconnectionsthroughd          timestepsareawayofensuringthataunitcan\n",
      "        alwayslearntobeinﬂuencedbyavaluefromd     timestepsearlier. Theuseofa\n",
      "              linearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe\n",
      "           unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\n",
      "             thiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingtherealvalued\n",
      "        αratherthanbyadjustingtheinteger-valuedskiplength.\n",
      "             Theseideaswereproposedby ()andby (). Mozer1992 ElHihiandBengio1996\n",
      "              Leakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n",
      "   ( ,). Jaegeretal.2007\n",
      "            Therearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\n",
      "            units. Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for\n",
      "           example,bysamplingtheirvaluesfromsomedistributiononceatinitializationtime.\n",
      "            Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\n",
      "            Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\n",
      "      dependencies(,; Mozer1992Pascanu2013etal.,).\n",
      "  10.9.3RemovingConnections\n",
      "          Anotherapproachtohandlinglong-termdependenciesistheideaoforganizing\n",
      "              thestateoftheRNNatmultipletimescales( ,),with ElHihiandBengio1996\n",
      "           informationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.\n",
      "          Thisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier\n",
      "         becauseitinvolvesactivelyremovinglength-oneconnectionsandreplacingthem\n",
      "              withlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona\n",
      "          longtimescale. Skipconnectionsthroughtimeedges.Unitsreceivingsuch add\n",
      "               newconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\n",
      "     focusontheirother,short-termconnections.\n",
      "              Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\n",
      "             operateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\n",
      "            buttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.\n",
      "             Thiswastheproposalin ()andhasbeensuccessfullyusedin Mozer1992 Pascanu\n",
      "             etal.().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\n",
      "             atdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis\n",
      "              theapproachof ()and ().Itworked ElHihiandBengio1996 Koutniketal.2014\n",
      "      wellonanumberofbenchmarkdatasets.\n",
      "4 0 3       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "       10.10TheLongShort-TermMemoryandOtherGated\n",
      "RNNs\n",
      "            Asofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\n",
      " arecalled gatedRNNs   .Theseincludethe  longshort-termmemoryand\n",
      "      networksbasedonthegatedrecurrentunit.\n",
      "             Likeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\n",
      "          timethat havederivativesthatneithervanishnorexplode.Leakyunitsdid\n",
      "           thiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\n",
      "          parameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n",
      "   ateachtimestep.\n",
      "           Leakyunitsallowthenetworktoaccumulateinformation(suchasevidencefor\n",
      "            aparticularfeatureorcategory)overalongduration.Oncethatinformationhas\n",
      "              beenused,however,itmightbeusefulfortheneuralnetworktoforgettheold\n",
      "             state. Forexample,ifasequenceismadeofsubsequencesandwewantaleaky\n",
      "           unittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\n",
      "             forgettheoldstatebysettingittozero. Insteadofmanuallydecidingwhento\n",
      "                clearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n",
      "    iswhatgatedRNNsdo.\n",
      " 10.10.1LSTM\n",
      "           Thecleverideaofintroducingself-loopstoproducepathswherethegradient\n",
      "           canﬂowforlongdurationsisacorecontributionoftheinitial longshort-term\n",
      "memory         (LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\n",
      "              hasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\n",
      "             ﬁxed( ,).Bymakingtheweightofthisself-loopgated(controlled Gersetal.2000\n",
      "            byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\n",
      "               Inthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale\n",
      "            ofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\n",
      "            areoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\n",
      "       inmanyapplications, suchasunconstrainedhandwritingrecognition(Graves\n",
      "            etal.,),speechrecognition( 2009 Graves2013GravesandJaitly2014 etal.,; ,),\n",
      "         handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014etal.,),\n",
      "              imagecaptioning( ,; Kirosetal.2014bVinyals2014bXu 2015 etal.,;etal.,),and\n",
      "    parsing(Vinyals2014aetal.,).\n",
      "          TheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16\n",
      "          forwardpropagationequationsaregivenbelow,forashallowrecurrentnetwork\n",
      "4 0 4       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "×\n",
      "   input input gate forget gate output gateoutput\n",
      "stateself-loop×\n",
      " + ×\n",
      "            Figure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected\n",
      "            recurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\n",
      "              Aninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe\n",
      "               accumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa\n",
      "               linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\n",
      "                beshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\n",
      "              inputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasan\n",
      "                extrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\n",
      "4 0 5       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "          architecture.Deeperarchitectureshavealsobeensuccessfullyused(Gravesetal.,\n",
      "             2013Pascanu2014a ; etal.,).Insteadofaunitthatsimplyappliesanelement-wise\n",
      "          nonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM\n",
      "           recurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),\n",
      "               inadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputsand\n",
      "            outputsasanordinaryrecurrentnetwork,butalsohasmoreparametersanda\n",
      "            systemofgatingunitsthatcontrolstheﬂowofinformation.Themostimportant\n",
      "    componentisthestateunits( ) t\n",
      "i         ,whichhasalinearself-loopsimilartotheleaky\n",
      "            unitsdescribedintheprevioussection.Here,however,theself-loopweight(orthe\n",
      "      associatedtimeconstant)iscontrolledbya forgetgateunitf( ) t\n",
      "i  (fortimestept\n",
      "                 andcell),whichsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\n",
      "f( ) t\n",
      "i= σ\n",
      "bf\n",
      "i+\n",
      "jUf\n",
      "i , jx( ) t\n",
      "j+\n",
      "jWf\n",
      "i , jh( 1 ) t −\n",
      "j\n",
      " , (10.40)\n",
      "where x( ) t     isthecurrentinputvectorand h( ) t     isthecurrenthiddenlayervector,\n",
      "        containingtheoutputsofalltheLSTMcells,and bf, Uf, Wf arerespectively\n",
      "            biases,inputweights,andrecurrentweightsfortheforgetgates.TheLSTMcell\n",
      "            internalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\n",
      "f( ) t\n",
      "i:\n",
      "s( ) t\n",
      "i= f( ) t\n",
      "is( 1 ) t −\n",
      "i  +g( ) t\n",
      "iσ\n",
      "b i+\n",
      "jU i , jx( ) t\n",
      "j+\n",
      "jW i , jh( 1 ) t −\n",
      "j\n",
      " ,(10.41)\n",
      "where b, Uand W        respectivelydenotethebiases,inputweights,andrecurrent\n",
      "     weightsintotheLSTMcell.The   externalinputgateunitg( ) t\n",
      "i iscomputed\n",
      "              similarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n",
      "       0and1),butwithitsownparameters:\n",
      "g( ) t\n",
      "i= σ\n",
      "bg\n",
      "i+\n",
      "jUg\n",
      "i , jx( ) t\n",
      "j+\n",
      "jWg\n",
      "i , jh( 1 ) t −\n",
      "j\n",
      " . (10.42)\n",
      " Theoutputh( ) t\n",
      "i          oftheLSTMcellcanalsobeshutoﬀ,viathe  outputgateq( ) t\n",
      "i,\n",
      "       whichalsousesasigmoidunitforgating:\n",
      "h( ) t\n",
      "i= tanh\n",
      "s( ) t\n",
      "i\n",
      "q( ) t\n",
      "i , (10.43)\n",
      "q( ) t\n",
      "i= σ\n",
      "bo\n",
      "i+\n",
      "jUo\n",
      "i , jx( ) t\n",
      "j+\n",
      "jWo\n",
      "i , jh( 1 ) t −\n",
      "j\n",
      " , (10.44)\n",
      "4 0 6       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "  whichhasparameters bo, Uo, Wo      foritsbiases,inputweightsandrecurrent\n",
      "            weights,respectively.Amongthevariants,onecanchoosetousethecellstates( ) t\n",
      "i\n",
      "            asanextrainput(withitsweight)intothethreegatesofthei   -thunit,asshown\n",
      "        inﬁgure.Thiswouldrequirethreeadditionalparameters. 10.16\n",
      "          LSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\n",
      "          thanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor\n",
      "           testingtheabilitytolearnlong-termdependencies( ,; Bengioetal.1994Hochreiter\n",
      "          andSchmidhuber1997Hochreiter2001 ,; etal.,),thenonchallengingsequence\n",
      "        processingtaskswherestate-of-the-artperformancewasobtained(Graves2012,;\n",
      "             Graves2013Sutskever2014 etal.,; etal.,).VariantsandalternativestotheLSTM\n",
      "        thathavebeenstudiedandusedarediscussednext.\n",
      "   10.10.2OtherGatedRNNs\n",
      "          WhichpiecesoftheLSTMarchitectureareactuallynecessary?Whatother\n",
      "          successfularchitecturescouldbedesignedthatallowthenetworktodynamically\n",
      "         controlthetimescaleandforgettingbehaviorofdiﬀerentunits?\n",
      "             SomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\n",
      "              whoseunitsarealsoknownasgatedrecurrentunits,orGRUs( ,; Choetal.2014b\n",
      "              Chung20142015aJozefowicz2015Chrupala2015 etal.,,; etal.,; etal.,).Themain\n",
      "            diﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\n",
      "            forgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\n",
      "  arethefollowing:\n",
      "h( ) t\n",
      "i= u( 1 ) t −\n",
      "ih( 1 ) t −\n",
      "i    +(1−u( 1 ) t −\n",
      "i)σ\n",
      "b i+\n",
      "jU i , jx( ) t\n",
      "j+\n",
      "jW i , jr( 1 ) t −\n",
      "jh( 1 ) t −\n",
      "j\n",
      ",\n",
      "(10.45)\n",
      "where u     standsfor“update”gateand r       for“reset”gate.Theirvalueisdeﬁnedas\n",
      "usual:\n",
      "u( ) t\n",
      "i= σ\n",
      "bu\n",
      "i+\n",
      "jUu\n",
      "i , jx( ) t\n",
      "j+\n",
      "jWu\n",
      "i , jh( ) t\n",
      "j\n",
      " (10.46)\n",
      "and\n",
      "r( ) t\n",
      "i= σ\n",
      "br\n",
      "i+\n",
      "jUr\n",
      "i , jx( ) t\n",
      "j+\n",
      "jWr\n",
      "i , jh( ) t\n",
      "j\n",
      " . (10.47)\n",
      "           Theresetandupdategatescanindividually“ignore” partsofthestatevector.\n",
      "            Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany\n",
      "4 0 7       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "             dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\n",
      "              ignoreit(attheotherextreme)byreplacingitwiththenew“targetstate”value\n",
      "          (towardwhichtheleakyintegratorwantstoconverge). Theresetgatescontrol\n",
      "              whichpartsofthestategetusedtocomputethenexttargetstate,introducingan\n",
      "           additionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.\n",
      "           Manymorevariantsaroundthisthemecanbedesigned.Forexamplethe\n",
      "            resetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.\n",
      "             Alternately,theproductofaglobalgate(coveringawholegroupofunits,such\n",
      "               asanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobal\n",
      "        controlandlocalcontrol. Severalinvestigationsoverarchitecturalvariationsof\n",
      "             theLSTMandGRU,however,foundnovariantthatwouldclearlybeatbothof\n",
      "               theseacrossawiderangeoftasks( ,; ,). Greﬀetal.2015Jozefowiczetal.2015Greﬀ\n",
      "             etal.()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\n",
      "                etal.()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015\n",
      "               advocatedby (),makestheLSTMasstrongasthebestofthe Gersetal.2000\n",
      "  exploredarchitecturalvariants.\n",
      "    10.11OptimizationforLong-TermDependencies\n",
      "           Sectionandsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\n",
      "         problemsthatoccurwhenoptimizingRNNsovermanytimesteps.\n",
      "           AninterestingideaproposedbyMartensandSutskever2011()isthatsecond\n",
      "           derivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order\n",
      "          optimizationalgorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative\n",
      "           bythesecondderivative(inhigherdimension,multiplyingthegradientbythe\n",
      "             inverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst\n",
      "           derivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively\n",
      "       constant.Unfortunately,second-ordermethodshavemanydrawbacks,including\n",
      "             highcomputationalcost,theneedforalargeminibatch,andatendencytobe\n",
      "          attractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\n",
      "          usingsecond-ordermethods.Later,Sutskever 2013etal.()foundthatsimpler\n",
      "         methodssuchasNesterovmomentumwithcarefulinitializationcouldachieve\n",
      "          similarresults. SeeSutskever2012()formoredetail.Bothoftheseapproaches\n",
      "           havelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\n",
      "               toLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\n",
      "                 mucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\n",
      "  powerfuloptimizationalgorithm.\n",
      "4 0 8       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "             Figure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith\n",
      " twoparameters wand b        .Gradientclippingcanmakegradientdescentperformmore\n",
      "            reasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur\n",
      "          inrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\n",
      "              Thecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\n",
      "             ismultipliedbyitselfonceforeachtimestep.(Left)Gradientdescentwithoutgradient\n",
      "             clippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient\n",
      "            fromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\n",
      "             axesoftheplot.(Right)Gradientdescentwithgradientclippinghasamoremoderate\n",
      "                 reactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat\n",
      "              itcannotbepropelledawayfromthesteepregionnearthesolution.Figureadaptedwith\n",
      "     permissionfromPascanu 2013etal.().\n",
      "  10.11.1ClippingGradients\n",
      "           Asdiscussedinsection,stronglynonlinearfunctions,suchasthosecomputed 8.2.4\n",
      "              byarecurrentnetovermanytimesteps,tendtohavederivativesthatcanbe\n",
      "              eitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3\n",
      "               inﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\n",
      "           parameters)hasa“landscape” inwhichoneﬁnds“cliﬀs”:wideandratherﬂat\n",
      "          regionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\n",
      "    formingakindofcliﬀ.\n",
      "            Thediﬃcultythatarisesisthatwhentheparametergradientisverylarge,\n",
      "           agradientdescentparameterupdatecouldthrowtheparametersveryfarinto\n",
      "             aregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthat\n",
      "             hadbeendonetoreachthecurrentsolution.Thegradienttellsusthedirection\n",
      "          thatcorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurrounding\n",
      "          thecurrentparameters.Outsidethisinﬁnitesimalregion,thecostfunctionmay\n",
      "4 0 9       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "              begintocurvebackupward.Theupdatemustbechosentobesmallenoughto\n",
      "           avoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n",
      "          decayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\n",
      "               rate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\n",
      "              ofteninappropriateandcausesuphillmotionifweenteramorecurvedpartofthe\n",
      "    landscapeonthenextstep.\n",
      "             Asimpletypeofsolutionhasbeeninusebypractitionersformanyyears:\n",
      "  clippingthegradient         .Therearediﬀerentinstancesofthisidea(Mikolov2012,;\n",
      "              Pascanu2013etal.,).Oneoptionistocliptheparametergradientfromaminibatch\n",
      "           element-wise(Mikolov2012,),justbeforetheparameterupdate.Anotheristoclip\n",
      " thenorm||||g  ofthegradientg        (Pascanu2013etal.,)justbeforetheparameter\n",
      "update:\n",
      "    if||||g>v (10.48)\n",
      " g←gv\n",
      "||||g , (10.49)\n",
      "wherev    isthenormthresholdandg      isusedtoupdateparameters.Becausethe\n",
      "           gradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas\n",
      "            weightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\n",
      "             methodhastheadvantageofguaranteeingthateachstepisstillinthegradient\n",
      "         direction,butexperimentssuggestthatbothformsworksimilarly.Although\n",
      "            theparameterupdatehasthesamedirectionasthetruegradient,withgradient\n",
      "           normclipping,theparameterupdatevectornormisnowbounded.Thisbounded\n",
      "          gradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\n",
      "            fact,evensimplytakingarandomstepwhenthegradientmagnitudeisabove\n",
      "               athresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\n",
      "  gradientisnumericallyInforNan     (consideredinﬁniteornot-a-number),then\n",
      "    arandomstepofsizev         canbetakenandwilltypicallymoveawayfromthe\n",
      "        numericallyunstableconﬁguration.Clippingthegradientnormperminibatch\n",
      "            willnotchangethedirectionofthegradientforanindividualminibatch.Taking\n",
      "           theaverageofthenorm-clippedgradientfrommanyminibatches,however,isnot\n",
      "            equivalenttoclippingthenormofthetruegradient(thegradientformedfrom\n",
      "            usingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\n",
      "            thatappearinthesameminibatchassuchexamples,willhavetheircontribution\n",
      "           totheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\n",
      "             gradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall\n",
      "         minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\n",
      "           anunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\n",
      "            introducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\n",
      "4 1 0       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "             wiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\n",
      "              ortheminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\n",
      "          proposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\n",
      "           hiddenunits),butnocomparisonhasbeenpublishedbetweenthesevariants;we\n",
      "      conjecturethatallthesemethodsbehavesimilarly.\n",
      "     10.11.2RegularizingtoEncourageInformationFlow\n",
      "             Gradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\n",
      "         vanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\n",
      "            dependencies,wediscussedtheideaofcreatingpathsinthecomputationalgraphof\n",
      "          theunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\n",
      "               witharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-loops\n",
      "           andgatingmechanisms,describedinsection.Anotherideaistoregularize 10.10\n",
      "          orconstraintheparameterssoastoencourage“informationﬂow.” Inparticular,\n",
      "     wewouldlikethegradientvector∇h( ) tL    beingback-propagatedtomaintainits\n",
      "              magnitude,evenifthelossfunctiononlypenalizestheoutputattheendofthe\n",
      "   sequence.Formally,wewant\n",
      "(∇h( ) tL)∂ h( ) t\n",
      "∂ h( 1 ) t −(10.50)\n",
      "    tobeaslargeas\n",
      "∇h( ) t L. (10.51)\n",
      "          Withthisobjective,Pascanu 2013etal.()proposethefollowingregularizer:\n",
      "Ω =\n",
      "t\n",
      "(∇h( ) tL)∂ h( ) t\n",
      "∂ h( 1 ) t −\n",
      "||∇h( ) tL|| −1\n",
      "2\n",
      " . (10.52)\n",
      "          Computingthegradientofthisregularizermayappeardiﬃcult,butPascanu\n",
      "           etal.()proposeanapproximationinwhichweconsidertheback-propagated 2013\n",
      "vectors∇h( ) tL           asiftheywereconstants(forthepurposeofthisregularizer,so\n",
      "           thatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\n",
      "           thisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\n",
      "          handlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\n",
      "           thedependenciesthatanRNNcanlearn. BecauseitkeepstheRNNdynamics\n",
      "           ontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.\n",
      "        Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\n",
      "                AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\n",
      "        taskswheredataisabundant,suchaslanguagemodeling.\n",
      "4 1 1       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "  10.12ExplicitMemory\n",
      "          Intelligencerequiresknowledge,andacquiringknowledgecanbedonevialearning,\n",
      "         whichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\n",
      "           therearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-\n",
      "             conscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks\n",
      "           diﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\n",
      "          straightforwardtoputintowords—everydaycommonsenseknowledge,like“acatis\n",
      "               akindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplishyour\n",
      "               currentgoals,like“themeetingwiththesalesteamisat3:00PMinroom141.”\n",
      "         Neuralnetworksexcelatstoringimplicitknowledge, buttheystruggleto\n",
      "        memorizefacts. Stochasticgradientdescentrequiresmanypresentationsofthe\n",
      "             sameinputbeforeitcanbestoredinneuralnetworkparameters,andeventhen,\n",
      "            thatinputwillnotbestoredespeciallyprecisely.Graves2014etal.()hypothesized\n",
      "          thatthisisbecauseneuralnetworkslacktheequivalentofthe  workingmemory\n",
      "           systemthatenableshumanbeingstoexplicitlyholdandmanipulatepiecesof\n",
      "        informationthatarerelevantto achieving somegoal.Suchexplicitmemory\n",
      "           componentswouldallowoursystemsnotonlytorapidlyand“intentionally”store\n",
      "            andretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed\n",
      "            forneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\n",
      "               thewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\n",
      "            asimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\n",
      "     responsestotheinput(,). Hinton1990\n",
      "        Toresolvethisdiﬃculty,Weston 2014etal.()introduced  memorynetworks\n",
      "              thatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmechanism.\n",
      "          Memorynetworksoriginallyrequiredasupervisionsignalinstructingthemhowto\n",
      "         usetheirmemory cells.Graves2014etal.()introducedthe  neuralTuringma-\n",
      "chine              ,whichisabletolearntoreadfromandwritearbitrarycontenttomemory\n",
      "          cellswithoutexplicitsupervisionaboutwhichactionstoundertake,andallowed\n",
      "           end-to-endtrainingwithoutthissupervisionsignal,viatheuseofacontent-based\n",
      "          softattentionmechanism(see []andsection ).This Bahdanauetal.2015 12.4.5.1\n",
      "         softaddressingmechanismhasbecomestandardwithotherrelatedarchitectures,\n",
      "          emulatingalgorithmicmechanismsinawaythatstillallowsgradient-basedopti-\n",
      "            mization(Sukhbaatar 2015JoulinandMikolov2015Kumar 2015 etal.,; ,; etal.,;\n",
      "       Vinyals2015aGrefenstette2015 etal.,; etal.,).\n",
      "              Eachmemory cellcanbethoughtofasanextensionofthememory cellsin\n",
      "            LSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate\n",
      "               thatchooseswhichcelltoreadfromorwriteto,justasmemory accessesina\n",
      "4 1 2       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "         digitalcomputerreadfromorwritetoaspeciﬁcaddress.\n",
      "           Itisdiﬃculttooptimizefunctionsthatproduceexactintegeraddresses.To\n",
      "            alleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemory cells\n",
      "             simultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\n",
      "          modifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations\n",
      "              arechosentobefocusedonasmallnumberofcells,forexample,byproducing\n",
      "           themviaasoftmaxfunction.Usingtheseweightswithnonzeroderivativesenables\n",
      "           thefunctionscontrollingaccesstothememory tobeoptimizedusinggradient\n",
      "           descent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould\n",
      "             beincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\n",
      "     memory addressesreceivingalargecoeﬃcient.\n",
      "           Thesememory cellsaretypicallyaugmentedtocontainavector,ratherthan\n",
      "              thesinglescalarstoredbyanLSTMorGRUmemory cell.Therearetworeasons\n",
      "               toincreasethesizeofthememory cell.Onereasonisthatwehaveincreasedthe\n",
      "            costofaccessingamemory cell.Wepaythecomputationalcostofproducing\n",
      "             acoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusterarounda\n",
      "              smallnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,we\n",
      "              canoﬀsetsomeofthiscost.Anotherreasontousevectorvaluedmemory cellsis\n",
      "   thattheyallowfor  content-basedaddressing      ,wheretheweightusedtoread\n",
      "                toorwritefromacellisafunctionofthatcell. Vectorvaluedcellsallowusto\n",
      "              retrieveacompletevectorvaluedmemory ifweareabletoproduceapatternthat\n",
      "              matchessomebutnotallitselements.Thisisanalogoustohowpeoplecanrecall\n",
      "                thelyricsofasongbasedonafewwords.Wecanthinkofacontent-basedread\n",
      "              instructionassaying,“Retrievethelyricsofthesongthathasthechorus‘Weall\n",
      "            liveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe\n",
      "               maketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina\n",
      "              separatememory cell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,\n",
      " location-basedaddressing          isnotallowedtorefertothecontentofthememory.\n",
      "             Wecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof\n",
      "            thesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible\n",
      "       mechanismevenwhenthememory cellsaresmall.\n",
      "               Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then\n",
      "            theinformationitcontainscanbepropagatedforwardintimeandthegradients\n",
      "        propagatedbackwardintimewithouteithervanishingorexploding.\n",
      "            Theexplicitmemory approachisillustratedinﬁgure,whereweseethat 10.18\n",
      "           a“taskneuralnetwork” iscoupledwithamemory.Althoughthattaskneural\n",
      "            networkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\n",
      "             Thetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemory addresses.\n",
      "4 1 3       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "Task network,\n",
      "control ling the memoryMemory cells\n",
      "Writing\n",
      "mechanismReading\n",
      "mechanism\n",
      "              Figure10.18:Aschematicofanetworkwithanexplicitmemory,capturingsomeofthe\n",
      "            keydesignelementsoftheneuralTuringmachine. Inthisdiagramwedistinguishthe\n",
      "              “representation”partofthemodel(the“tasknetwork,”herearecurrentnetinthebottom)\n",
      "                fromthe“memory”partofthemodel(thesetofcells),whichcanstorefacts.Thetask\n",
      "              networklearnsto“control”thememory,decidingwheretoreadfromandwheretowrite\n",
      "            towithinthememory(throughthereadingandwritingmechanisms,indicatedbybold\n",
      "       arrowspointingatthereadingandwritingaddresses).\n",
      "4 1 4       C HAP T E R 1 0 . S E Q UE NC E M O D E L I N G : R E C URRE NT AND R E C URS I V E NE T S\n",
      "             ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM\n",
      "            RNNscannotlearn.Onereasonforthisadvantagemaybethatinformationand\n",
      "           gradientscanbepropagated(forwardintimeorbackwardintime,respectively)\n",
      "   forverylongdurations.\n",
      "         Asanalternativetoback-propagationthroughweightedaveragesofmemory\n",
      "          cells,wecaninterpretthememory addressingcoeﬃcientsasprobabilitiesand\n",
      "          stochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\n",
      "        thatmakediscretedecisionsrequiresspecializedoptimizationalgorithms,described\n",
      "           insection .Sofar,trainingthesestochasticarchitecturesthatmakediscrete 20.9.1\n",
      "         decisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\n",
      "decisions.\n",
      "          Whetheritissoft(allowingback-propagation)orstochasticandhard,the\n",
      "      mechanism forchoosing anaddress isin itsform identicaltothe a t t e n t i o n\n",
      "m e c ha ni s m          ,whichhadbeenpreviouslyintroducedinthecontextofmachine\n",
      "            translation( ,)andisdiscussedinsection .Theidea Bahdanauetal.2015 12.4.5.1\n",
      "           ofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\n",
      "         contextofhandwritinggeneration(Graves2013,),withanattentionmechanism\n",
      "            thatwasconstrainedtomoveonlyforwardintimethroughthesequence.In\n",
      "             thecaseofmachinetranslationandmemory networks,ateachstep,thefocusof\n",
      "            attentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.\n",
      "           Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\n",
      "              data.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\n",
      "               movestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\n",
      "tasks.\n",
      "4 1 5 C h a p t e r 1 1\n",
      " P r ac t i ca l Metho dol o gy\n",
      "          Successfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\n",
      "           knowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\n",
      "             work.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\n",
      "            algorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\n",
      "           obtainedfromexperimentsinordertoimproveamachinelearningsystem.During\n",
      "         day-to-daydevelopmentofmachinelearningsystems,practitionersneedtodecide\n",
      "            whethertogathermoredata,increaseordecreasemodelcapacity,addorremove\n",
      "         regularizingfeatures,improvetheoptimizationofamodel,improveapproximate\n",
      "             inferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allthese\n",
      "               operationsareattheveryleasttimeconsumingtotryout,soitisimportantto\n",
      "            beabletodeterminetherightcourseofactionratherthanblindlyguessing.\n",
      "           Mostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-\n",
      "           rithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost\n",
      "            importantingredienttobeingamachinelearningexpertisknowingawidevariety\n",
      "             ofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-\n",
      "             tice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\n",
      "          algorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\n",
      "           analgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\n",
      "        recommendati onsinthischapterareadaptedfrom(). Ng2015\n",
      "      Werecommend thefollowingpracticaldesignprocess:\n",
      "•            Determineyourgoals—whaterrormetrictouse,andyourtargetvaluefor\n",
      "            thiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\n",
      "       problemthattheapplicationisintendedtosolve.\n",
      "•           Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n",
      "416   C HAP T E R 1 1 . P R A C T I C AL M E T HO D O L O G Y\n",
      "     estimationoftheappropriateperformancemetrics.\n",
      "•          Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\n",
      "         nosewhichcomponentsareperformingworsethanexpectedandwhether\n",
      "            poorperformanceisduetooverﬁtting,underﬁtting,oradefectinthedata\n",
      " orsoftware.\n",
      "•          Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\n",
      "        hyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfrom\n",
      " yourinstrumentation.\n",
      "            Asarunningexample,wewillusetheStreetViewaddressnumbertranscription\n",
      "            system( ,).Thepurposeofthisapplicationistoadd Goodfellowetal.2014d\n",
      "           buildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\n",
      "         theGPScoordinatesassociatedwitheachphotograph.Aconvolutionalnetwork\n",
      "          recognizestheaddressnumberineachphotograph,allowingtheGoogleMaps\n",
      "             databasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\n",
      "            commercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\n",
      "  methodologyweadvocate.\n",
      "         Wenowdescribeeachofthestepsinthisprocess.\n",
      "  11.1PerformanceMetrics\n",
      "              Determiningyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrst\n",
      "             stepbecauseyourerrormetricwillguideallyourfutureactions.Youshouldalso\n",
      "         haveanideaofwhatlevelofperformanceyoudesire.\n",
      "            Keepinmindthatformostapplications,itisimpossibletoachieveabsolute\n",
      "              zeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopeto\n",
      "             achieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobability\n",
      "    distribution.This isbecause your inputfeatures maynot contain complete\n",
      "           informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\n",
      "             stochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.\n",
      "              Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\n",
      "             goalistobuildthebestpossiblereal-worldproductorservice,youcantypically\n",
      "             collectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\n",
      "            thisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\n",
      "           money,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolves\n",
      "            performinginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestion\n",
      "          aboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark\n",
      "4 1 7   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "            speciﬁcationusuallydeterminesthetrainingset,andyouarenotallowedtocollect\n",
      " moredata.\n",
      "           Howcanonedetermineareasonablelevelofperformancetoexpect?Typically,\n",
      "              intheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\n",
      "          basedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\n",
      "               havesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\n",
      "          cost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic\n",
      "             desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\n",
      "         Anotherimportantconsiderationbesidesthetargetvalueoftheperformance\n",
      "            metricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetrics\n",
      "            maybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludes\n",
      "        machinelearningcomponents.Theseperformancemetricsareusuallydiﬀerent\n",
      "               fromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis 5.1.2\n",
      "            commontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.\n",
      "      However,manyapplicationsrequiremoreadvancedmetrics.\n",
      "              Sometimesitismuchmorecostlytomakeonekindofamistakethananother.\n",
      "            Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\n",
      "          incorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\n",
      "              spammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\n",
      "           messagethantoallowaquestionablemessagetopassthrough.Ratherthan\n",
      "              measuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeform\n",
      "              oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\n",
      "   ofallowingspammessages.\n",
      "             Sometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsome\n",
      "              rareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\n",
      "             thatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n",
      "            99.9999percentaccuracyonthedetectiontask,bysimplyhardcodingtheclassiﬁer\n",
      "              toalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\n",
      "             characterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis\n",
      "  toinsteadmeasureprecisionandrecall      .Precisionisthefractionofdetections\n",
      "              reportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\n",
      "             thatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\n",
      "            perfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\n",
      "            wouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\n",
      "               havethedisease(0.0001percentinourexampleofadiseasethatonlyonepeoplein\n",
      "             amillionhave).Whenusingprecisionandrecall,itiscommontoplota PRcurve,\n",
      "   withprecisiononthe y    -axisandrecallonthe x     -axis.Theclassiﬁergeneratesascore\n",
      "            thatishigheriftheeventtobedetectedoccurred. Forexample,afeedforward\n",
      "4 1 8   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "      networkdesignedtodetectadiseaseoutputsˆy=P(y =1 | x  ),estimatingthe\n",
      "          probabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\n",
      "            thedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\n",
      "          threshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmany\n",
      "             cases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumber\n",
      "          ratherthanacurve.Todoso,wecanconvertprecisionp andrecallr intoan\n",
      "  F-scoregivenby\n",
      " F=2pr\n",
      "  pr+ . (11.1)\n",
      "            AnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\n",
      "             Insomeapplications,itispossibleforthemachinelearningsystemtorefuseto\n",
      "            makeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\n",
      "             howconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncan\n",
      "              beharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\n",
      "            Viewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto\n",
      "           transcribetheaddressnumberfromaphotographtoassociatethelocationwhere\n",
      "               thephotowastakenwiththecorrectaddressinamap.Becausethevalueofthe\n",
      "             mapdegradesconsiderablyifthemapisinaccurate,itisimportanttoaddan\n",
      "            addressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystemthinks\n",
      "               thatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,thenthe\n",
      "               bestcourseofactionistoallowahumantotranscribethephotoinstead.Ofcourse,\n",
      "              themachinelearningsystemisonlyusefulifitisabletodramaticallyreducethe\n",
      "           amountofphotosthatthehumanoperatorsmustprocess.Anaturalperformance\n",
      "      metrictouseinthissituationiscoverage      .Coverageisthefractionofexamples\n",
      "              forwhichthemachinelearningsystemisabletoproducearesponse.Itispossible\n",
      "           totradecoverageforaccuracy. Onecanalwaysobtain100percentaccuracyby\n",
      "              refusingtoprocessanyexample,butthisreducesthecoverageto0percent.Forthe\n",
      "            StreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\n",
      "         accuracywhilemaintaining95percentcoverage.Human-levelperformanceonthis\n",
      "    taskis98percentaccuracy.\n",
      "          Manyothermetricsarepossible.Wecan,forexample,measureclick-through\n",
      "         rates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplication\n",
      "     areashaveapplication-speciﬁccriteriaaswell.\n",
      "           Whatisimportantistodeterminewhichperformancemetrictoimproveahead\n",
      "           oftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,\n",
      "             itcanbediﬃculttotellwhetherchangestoamachinelearningsystemmake\n",
      "  progressornot.\n",
      "4 1 9   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "   11.2DefaultBaselineModels\n",
      "          Afterchoosingperformancemetricsandgoals, thenextstepinanypractical\n",
      "            applicationistoestablishareasonableend-to-endsystemassoonaspossible.In\n",
      "            thissection,weproviderecommendati onsforwhichalgorithmstouseastheﬁrst\n",
      "           baselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\n",
      "           progressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\n",
      "  afterthiswriting.\n",
      "            Dependingonthecomplexityofyourproblem,youmayevenwanttobegin\n",
      "             withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\n",
      "              justchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\n",
      "    statisticalmodellikelogisticregression.\n",
      "            Ifyouknowthatyourproblemfallsintoan“AI-comp lete”categorylikeobject\n",
      "           recognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\n",
      "          todowellbybeginningwithanappropriatedeeplearningmodel.\n",
      "            First,choosethegeneralcategoryofmodelbasedonthestructureofyour\n",
      "            data.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,\n",
      "            useafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\n",
      "            topologicalstructure(forexample,iftheinputisanimage),useaconvolutional\n",
      "             network.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\n",
      "            unit(ReLUsortheirgeneralizations,suchasLeakyReLUs,PreLus,ormaxout).If\n",
      "              yourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\n",
      "          AreasonablechoiceofoptimizationalgorithmisSGDwithmomentumwith\n",
      "           adecayinglearningrate(populardecayschemesthatperformbetterorworse\n",
      "          ondiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimum\n",
      "            learningrate,decayingexponentially,ordecreasingthelearningratebyafactorof\n",
      "          2–10eachtimevalidationerrorplateaus).AnotherreasonablealternativeisAdam.\n",
      "         Batchnormalizationcanhaveadramaticeﬀectonoptimizationperformance,\n",
      "        especiallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\n",
      "             Whileitisreasonabletoomitbatchnormalizationfromtheveryﬁrstbaseline,it\n",
      "         shouldbeintroducedquicklyifoptimizationappearstobeproblematic.\n",
      "            Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\n",
      "           shouldincludesomemildformsofregularizationfromthestart.Earlystopping\n",
      "            shouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\n",
      "          toimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\n",
      "         normalizationalsosometimesreducesgeneralizationerrorandallowsdropoutto\n",
      "              beomitted,becauseofthenoiseintheestimateofthestatisticsusedtonormalize\n",
      " eachvariable.\n",
      "4 2 0   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "             Ifyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\n",
      "             willprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalready\n",
      "              knowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\n",
      "              atrainedmodelfromthattask.Forexample,itiscommontousethefeatures\n",
      "           fromaconvolutionalnetworktrainedonImageNet tosolveothercomputervision\n",
      "    tasks( ,). Girshicketal.2015\n",
      "           Acommonquestioniswhethertobeginbyusingunsupervisedlearning,de-\n",
      "            scribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,such III\n",
      "          asnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-\n",
      "         visedlearningtechniques,suchaslearningunsupervisedwordembeddings.In\n",
      "         otherdomains,suchascomputervision,currentunsupervisedlearningtechniques\n",
      "             donotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberof\n",
      "              labeledexamplesisverysmall( ,; Kingmaetal.2014Rasmus2015etal.,).Ifyour\n",
      "            applicationisinacontextwhereunsupervisedlearningisknowntobeimportant,\n",
      "           thenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervised\n",
      "              learninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.You\n",
      "            canalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\n",
      " baselineoverﬁts.\n",
      "      11.3DeterminingWhethertoGatherMoreData\n",
      "             Aftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-\n",
      "            manceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\n",
      "           novicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms.\n",
      "              Yet,itisoftenmuchbettertogathermoredatathantoimprovethelearning\n",
      "algorithm.\n",
      "           Howdoesonedecidewhethertogathermoredata?First,determinewhether\n",
      "            theperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\n",
      "              setispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\n",
      "             available,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\n",
      "               sizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\n",
      "           Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearning\n",
      "         ratehyperparameter.Iflargemodelsandcarefullytunedoptimizationalgorithms\n",
      "               donotworkwell,thentheproblemmightbethe ofthetrainingdata.The quality\n",
      "               datamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\n",
      "          desiredoutputs.Thissuggestsstartingover,collectingcleanerdata,orcollecting\n",
      "    arichersetoffeatures.\n",
      "            Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n",
      "4 2 1   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "              formanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\n",
      "               thenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\n",
      "            trainingsetperformance,thengatheringmoredataisoneofthemosteﬀective\n",
      "          solutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\n",
      "              data,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\n",
      "             amountofdatathatisexpectedtobenecessarytoimprovetestsetperformance\n",
      "           signiﬁcantly. Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\n",
      "             feasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\n",
      "              lessthanthatofthealternatives,sotheanswerisalmostalwaystogathermore\n",
      "            trainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\n",
      "            themostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\n",
      "             medicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\n",
      "              alternativetogatheringmoredataistoreducethesizeofthemodelorimprove\n",
      "        regularization,byadjustinghyperparameters suchasweightdecaycoeﬃcients,\n",
      "             orbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegap\n",
      "           betweentrainandtestperformanceisstillunacceptableevenaftertuningthe\n",
      "       regularizationhyperparameters, thengatheringmoredataisadvisable.\n",
      "            Whendecidingwhethertogathermoredata,itisalsonecessarytodecide\n",
      "             howmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\n",
      "            trainingsetsizeandgeneralizationerror,asinﬁgure.Byextrapolatingsuch 5.4\n",
      "            curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\n",
      "             achieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\n",
      "             numberofexampleswillnothaveanoticeableeﬀectongeneralizationerror.Itis\n",
      "           thereforerecommended toexperimentwithtrainingsetsizesonalogarithmicscale,\n",
      "         forexample,doublingthenumberofexamplesbetweenconsecutiveexperiments.\n",
      "             Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\n",
      "           generalizationerroristoimprovethelearningalgorithmitself.Thisbecomesthe\n",
      "           domainofresearchandnotthedomainofadviceforappliedpractitioners.\n",
      "  11.4SelectingHyperparameters\n",
      "         Mostdeeplearningalgorithmscomewithseveralhyperparameters thatcontrol\n",
      "           manyaspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthe\n",
      "           timeandmemory costofrunningthealgorithm.Someofthesehyperparameters\n",
      "              aﬀectthequalityofthemodelrecoveredbythetrainingprocessanditsabilityto\n",
      "       infercorrectresultswhendeployedonnewinputs.\n",
      "         Therearetwobasicapproachestochoosingthesehyperparameters: choosing\n",
      "        themmanuallyandchoosingthemautomatically.Choosingthehyperparameters\n",
      "4 2 2   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "         manuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\n",
      "       learningmodelsachievegoodgeneralization.Automatichyperparameterselection\n",
      "            algorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\n",
      "   muchmorecomputationallycostly.\n",
      "   11.4.1ManualHyperparameterTuning\n",
      "         Tosethyperparametersmanually,onemustunderstandtherelationshipbetween\n",
      "       hyperparameters,trainingerror,generalizationerrorandcomputationalresources\n",
      "           (memory andruntime).Thismeansestablishingasolidfoundationonthefunda-\n",
      "           mentalideasconcerningtheeﬀectivecapacityofalearningalgorithm,asdescribed\n",
      "  inchapter.5\n",
      "            Thegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-\n",
      "             izationerrorsubjecttosomeruntimeandmemory budget.Wedonotdiscusshow\n",
      "          todeterminetheruntimeandmemory impactofvarioushyperparametershere\n",
      "     becausethisishighlyplatformdependent.\n",
      "           Theprimarygoalofmanualhyperparametersearchistoadjusttheeﬀective\n",
      "            capacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacity\n",
      "          isconstrainedbythreefactors: therepresentationalcapacityofthemodel,the\n",
      "            abilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\n",
      "             trainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\n",
      "              regularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\n",
      "        higherrepresentationalcapacity—itiscapableofrepresentingmorecomplicated\n",
      "          functions. Itcannotnecessarilylearnallthesefunctionsthough,ifthetraining\n",
      "            algorithmcannotdiscoverthatcertainfunctionsdoagoodjobofminimizingthe\n",
      "             trainingcost,orifregularizationtermssuchasweightdecayforbidsomeofthese\n",
      "functions.\n",
      "          ThegeneralizationerrortypicallyfollowsaU-shapedcurvewhenplottedas\n",
      "             afunctionofoneofthehyperparameters, asinﬁgure. Atoneextreme,the 5.3\n",
      "          hyperparametervaluecorrespondstolowcapacity,andgeneralizationerrorishigh\n",
      "             becausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,\n",
      "         thehyperparametervaluecorrespondstohighcapacity,andthegeneralization\n",
      "             errorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\n",
      "            inthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\n",
      "           generalizationerror,byaddingamediumgeneralizationgaptoamediumamount\n",
      "  oftrainingerror.\n",
      "          Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-\n",
      "             parameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample,\n",
      "4 2 3   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "            becauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\n",
      "          Forsomehyperparameters, overﬁttingoccurswhenthevalueofthehyperparame-\n",
      "            terissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzero\n",
      "         correspondstothegreatesteﬀectivecapacityofthelearningalgorithm.\n",
      "           NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\n",
      "              Manyhyperparametersarediscrete,suchasthenumberofunitsinalayerorthe\n",
      "                 numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\n",
      "         alongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\n",
      "         areswitchesthat specify whetherornotto usesomeoptionalcomponentof\n",
      "           thelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\n",
      "           featuresbysubtractingtheirmeananddividingbytheirstandarddeviation.These\n",
      "          hyperparameterscanexploreonlytwopointsonthecurve.Otherhyperparameters\n",
      "           havesomeminimumormaximumvaluethatpreventsthemfromexploringsome\n",
      "             partofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.This\n",
      "              meansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotenter\n",
      "           theoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,\n",
      "     somehyperparameterscanonlysubtractcapacity.\n",
      "          Thelearningrateisperhapsthemostimportanthyperparameter. Ifyou\n",
      "      have timeto tuneonly onehyperparameter, tune thelearning rate. It con-\n",
      "             trolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanother\n",
      "          hyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearning\n",
      "             rateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\n",
      "             largeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\n",
      "            illustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent 11.1\n",
      "           caninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\n",
      "               quadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits\n",
      "             optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\n",
      "             isnotonlyslowerbutmaybecomepermanentlystuckwithahightrainingerror.\n",
      "             Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\n",
      "          Tuningtheparametersotherthanthelearningraterequiresmonitoringboth\n",
      "            trainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,\n",
      "    thenadjustingitscapacityappropriately.\n",
      "               Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave\n",
      "              nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\n",
      "          conﬁdentthatyouroptimizationalgorithmisperformingcorrectly,thenyoumust\n",
      "            addmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\n",
      "       increasesthecomputationalcostsassociatedwiththemodel.\n",
      "                Ifyourerroronthetestsetishigherthanyourtargeterrorrate,youcannow\n",
      "4 2 4   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "10− 210− 1100\n",
      "   Learningrate(logarithmicscale)012345678 Trainingerror\n",
      "            Figure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\n",
      "                 thesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxed\n",
      "               trainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbyafactor\n",
      "          proportionaltothelearningratereduction. Generalizationerrorcanfollowthiscurve\n",
      "              orbecomplicatedbyregularizationeﬀectsarisingoutofhavingtoolargeortoosmall\n",
      "            learningrates,sincepooroptimizationcan,tosomedegree,reduceorpreventoverﬁtting,\n",
      "           andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralizationerror.\n",
      "                taketwokindsofactions.Thetesterroristhesumofthetrainingerrorandthe\n",
      "             gapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\n",
      "          oﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining\n",
      "               errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\n",
      "               drivenbythegapbetweentrainingandtesterror.Yourgoalistoreducethisgap\n",
      "            withoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\n",
      "         changeregularizationhyperparameterstoreduceeﬀectivemodelcapacity,suchas\n",
      "            byaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\n",
      "          largemodelthatisregularizedwell,forexample,byusingdropout.\n",
      "           Mosthyperparameterscanbesetbyreasoningaboutwhethertheyincreaseor\n",
      "         decreasemodelcapacity.Someexamplesareincludedintable.11.1\n",
      "           Whilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\n",
      "             goodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\n",
      "              thisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\n",
      "            izationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\n",
      "           guaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\n",
      "            untilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational\n",
      "             costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n",
      "           principle,thisapproachcouldfailduetooptimizationdiﬃculties,butformany\n",
      "4 2 5   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "HyperparameterIncreases\n",
      "capacity\n",
      "  when... Reason Caveats\n",
      "  Numberofhid-\n",
      " denunitsincreased    Increasingthenumberof\n",
      "   hiddenunitsincreasesthe\n",
      " representationalcapacity\n",
      "  ofthemodel.  Increasingthenumber\n",
      "  ofhiddenunits increases\n",
      "    boththetimeandmemory\n",
      "    costofessentiallyeveryop-\n",
      "   erationonthemodel.\n",
      " Learningrate  tunedop-\n",
      "timally   Animproperlearningrate,\n",
      "  whether toohigh ortoo\n",
      "    low,resultsinamodel\n",
      "   withloweﬀectivecapac-\n",
      "    ityduetooptimizationfail-\n",
      "ure.\n",
      " Convolutionker-\n",
      " nelwidthincreased    Increasingthekernelwidth\n",
      "    increasesthenumberofpa-\n",
      "   rametersinthemodel.   Awiderkernelresults\n",
      "in a narrower output di-\n",
      " mension, reducingmodel\n",
      "   capacityunlessyouuse\n",
      "   implicitzeropaddingto\n",
      "   reducethiseﬀect.Wider\n",
      "   kernelsrequiremoremem-\n",
      "   oryforparameterstorage\n",
      "   andincreaseruntime,but\n",
      "   anarroweroutputreduces\n",
      " memorycost.\n",
      " Implicitzero\n",
      "paddingincreased    Addingimplicitzerosbe-\n",
      "   foreconvolutionkeepsthe\n",
      "  representationsizelarge.  Increasestimeand mem-\n",
      "    orycostofmostopera-\n",
      "tions.\n",
      "  Weightdecayco-\n",
      "eﬃcientdecreased    Decreasingtheweightde-\n",
      "   caycoeﬃcientfreesthe\n",
      "   modelparameterstobe-\n",
      " comelarger.\n",
      "  Dropoutratedecreased    Droppingunitslessoften\n",
      "    givestheunitsmoreoppor-\n",
      "   tunitiesto“conspire”with\n",
      "     eachothertoﬁtthetrain-\n",
      " ingset.\n",
      "         Table11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.\n",
      "            problemsoptimizationdoesnotseemtobeasigniﬁcantbarrier,providedthatthe\n",
      "   modelischosenappropriately.\n",
      "4 2 6   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "    11.4.2AutomaticHyperparameterOptimizationAlgorithms\n",
      "            Theideallearningalgorithmjusttakesadatasetandoutputsafunction,without\n",
      "         requiringhandtuningofhyperparameters. Thepopularityofseverallearning\n",
      "             algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\n",
      "           performwellwithonlyoneortwotunedhyperparameters.Neuralnetworkscan\n",
      "           sometimesperformwellwithonlyasmallnumberoftunedhyperparameters, but\n",
      "          oftenbeneﬁtsigniﬁcantlyfromtuningoffortyormore.Manualhyperparameter\n",
      "            tuningcan workverywellwhentheuserhasagoodstarting point,suchas\n",
      "            onedeterminedbyothershavingworkedonthesametypeofapplicationand\n",
      "            architecture,orwhentheuserhasmonthsoryearsofexperienceinexploring\n",
      "          hyperparametervaluesforneuralnetworksappliedtosimilartasks.Formany\n",
      "          applications,however,thesestartingpointsarenotavailable.Inthesecases,\n",
      "        automatedalgorithmscanﬁndusefulvaluesofthehyperparameters.\n",
      "               Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\n",
      "            goodvaluesofthehyperparameters, werealizethatanoptimizationistakingplace:\n",
      "             wearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjective\n",
      "           function,suchasvalidationerror,sometimesunderconstraints(suchasabudget\n",
      "            fortrainingtime,memory orrecognitiontime).Itisthereforepossible,inprinciple,\n",
      "to develophyperparameter optimization  algorithms thatwrap a learning\n",
      "          algorithmandchooseitshyperparameters, thushidingthehyperparametersofthe\n",
      "       learningalgorithmfromtheuser.Unfortunately,hyperparameteroptimization\n",
      "            algorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\n",
      "          shouldbeexploredforeachofthelearningalgorithm’shyperparameters. These\n",
      "           secondaryhyperparametersareusuallyeasiertochoose,however,inthesensethat\n",
      "             acceptableperformancemaybeachievedonawiderangeoftasksusingthesame\n",
      "    secondaryhyperparametersforalltasks.\n",
      "  11.4.3GridSearch\n",
      "            Whentherearethreeorfewerhyperparameters, thecommonpracticeistoperform\n",
      " gridsearch          .Foreachhyperparameter, theuserselectsasmallﬁnitesetof\n",
      "             valuestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\n",
      "            speciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvalues\n",
      "          foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n",
      "              seterroristhenchosenashavingfoundthebesthyperparameters. Seetheleftof\n",
      "          ﬁgureforanillustrationofagridofhyperparametervalues. 11.2\n",
      "               Howshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n",
      "           (ordered)hyperparameters, thesmallestandlargestelementofeachlistischosen\n",
      "4 2 7   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      " Grid Random\n",
      "            Figure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposes,we\n",
      "            displaytwohyperparameters,butwearetypicallyinterestedinhavingmanymore.(Left)\n",
      "              Toperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\n",
      "             algorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\n",
      "            sets. Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint (Right)\n",
      "        hyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependent\n",
      "            fromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\n",
      "          uniformandlog-uniform(tosamplefromalog-uniformdistribution,takethe e xp ofa\n",
      "           samplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\n",
      "           hyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearch\n",
      "            andrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.\n",
      "            Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcant\n",
      "             inﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\n",
      "             hasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponential\n",
      "           inthenumberofnoninﬂuentialhyperparameters,whilerandomsearchtestsaunique\n",
      "           valueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwith\n",
      "     permissionfrom (). BergstraandBengio2012\n",
      "          conservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\n",
      "               thattheoptimalvalueislikelytobeintheselectedrange.Typically,agridsearch\n",
      "           involvespickingvaluesapproximatelyonalogarithmicscale,e.g.,alearningrate\n",
      "   takenwithintheset{ 0. 1, 0. 01, 10− 3, 10− 4, 10− 5}      ,oranumberofhiddenunits\n",
      "         takenwiththeset . { } 50 100 200 500 1000 2000 ,,,,,\n",
      "           Gridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\n",
      "         supposethatweranagridsearchoverahyperparameterα  usingvaluesof{− 1, 0, 1}.\n",
      "               Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1\n",
      "α          liesandshouldshiftthegridandrunanothersearchwithα  in,forexample,\n",
      "{ 1, 2, 3}        .Ifweﬁndthatthebestvalueofα        is,thenwemaywishtoreﬁneour 0\n",
      "4 2 8   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "            estimatebyzoominginandrunningagridsearchover .{−} 01001.,,.\n",
      "           Theobviousproblemwithgridsearchisthatitscomputationalcostgrows\n",
      "        exponentiallywiththenumberofhyperparameters.Iftherearemhyperparameters,\n",
      "   eachtakingatmostn         values,thenthenumberoftrainingandevaluationtrials\n",
      "  requiredgrowsasO(nm          ).Thetrialsmayberuninparallelandexploitloose\n",
      "         parallelism(withalmostnoneedforcommunicationbetweendiﬀerentmachines\n",
      "           carryingoutthesearch).Unfortunately,becauseoftheexponentialcostofgrid\n",
      "          search,evenparallelizationmaynotprovideasatisfactorysizeofsearch.\n",
      "  11.4.4RandomSearch\n",
      "              Fortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\n",
      "            convenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters:\n",
      "     randomsearch( ,). BergstraandBengio2012\n",
      "           Arandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistribution\n",
      "           foreachhyperparameter,forexample,aBernoulliormultinoulliforbinaryor\n",
      "          discretehyperparameters, orauniformdistributiononalog-scaleforpositive\n",
      "   real-valuedhyperparameters.Forexample,\n",
      "    l o g l e a r n i n g r a t e _ _∼−−u(1,5), (11.2)\n",
      " l e a r n i n g r a t e_ = 10l o g l e a r n i n g r a te _ _ , (11.3)\n",
      "whereu( a,b           )indicatesasampleoftheuniformdistributionintheinterval( a,b).\n",
      " Similarlythe l o g n u m b e r o f h i d d e n u n i t s _ __ _    maybesampledfromu(log(50),\n",
      "log(2000) ).\n",
      "               Unlikeinagridsearch,weshouldnotdiscretizeorbinthevaluesofthehy-\n",
      "             perparameters,sothatwecanexplorealargersetofvaluesandavoidadditional\n",
      "             computationalcost.Infact,asillustratedinﬁgure,arandomsearchcanbe 11.2\n",
      "           exponentiallymoreeﬃcientthanagridsearch,whenthereareseveralhyperpa-\n",
      "            rametersthatdonotstronglyaﬀecttheperformancemeasure.Thisisstudiedat\n",
      "            lengthin (),whofoundthatrandomsearchreducesthe BergstraandBengio2012\n",
      "              validationseterrormuchfasterthangridsearch,intermsofthenumberoftrials\n",
      "   runbyeachmethod.\n",
      "             Aswithgridsearch,wemayoftenwanttorunrepeatedversionsofrandom\n",
      "            search,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.\n",
      "           Themainreasonthatrandomsearchﬁndsgoodsolutionsfasterthangrid\n",
      "               searchisthatithasnowastedexperimentalruns,unlikeinthecaseofgridsearch,\n",
      "           whentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters)\n",
      "4 2 9   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "             wouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters\n",
      "             wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\n",
      "            wouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovalues\n",
      "            doesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,grid\n",
      "         searchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\n",
      "         willstillgivetwoindependentexplorationsoftheotherhyperparameters.\n",
      "   11.4.5Model-BasedHyperparameterOptimization\n",
      "           Thesearchforgoodhyperparameters canbecastasanoptimizationproblem.\n",
      "            Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\n",
      "          validationseterrorthatresultsfromtrainingusingthesehyperparameters. In\n",
      "            simpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiable\n",
      "            errormeasureonthevalidationsetwithrespecttothehyperparameters, wecan\n",
      "            simplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurinetal.\n",
      "          2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\n",
      "           becauseofitshighcomputationandmemory cost,orbecauseofhyperparameters\n",
      "         thathaveintrinsicallynondiﬀerentiableinteractionswiththevalidationseterror,\n",
      "      asinthecaseofdiscrete-valuedhyperparameters.\n",
      "               Tocompensateforthislackofagradient,wecanbuildamodelofthevalidation\n",
      "         seterror,thenproposenewhyperparameterguessesbyperformingoptimization\n",
      "          withinthismodel.Mostmodel-basedalgorithmsforhyperparametersearchusea\n",
      "            Bayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\n",
      "          errorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-\n",
      "        mizationthusinvolvesatrade-oﬀbetweenexploration(proposinghyperparameters\n",
      "              forthatthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\n",
      "         alsoperformpoorly)andexploitation(proposinghyperparametersthatthemodel\n",
      "             isconﬁdentwillperformaswellasanyhyperparametersithasseensofar—usually\n",
      "           hyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\n",
      "         approachestohyperparameteroptimizationincludeSpearmint( ,), Snoeketal.2012\n",
      "          TPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011\n",
      "      Currently,wecannotunambiguouslyrecommend Bayesianhyperparameter\n",
      "           optimizationasanestablishedtoolforachievingbetterdeeplearningresultsor\n",
      "         forobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimization\n",
      "         sometimesperformscomparablytohumanexperts,sometimesbetter,butfails\n",
      "               catastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworksona\n",
      "            particularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeingsaid,\n",
      "          hyperparameteroptimizationisanimportantﬁeldofresearchthat,whileoften\n",
      "             drivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁtnot\n",
      "4 3 0   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "             onlytheentireﬁeldofmachinelearningbutalsothedisciplineofengineeringin\n",
      "general.\n",
      "        Onedrawbackcommontomosthyperparameteroptimizationalgorithmswith\n",
      "            moresophisticationthanrandomsearchisthattheyrequireforatrainingex-\n",
      "            perimenttoruntocompletionbeforetheyareabletoextractanyinformation\n",
      "              fromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-\n",
      "             mationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman\n",
      "             practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparametersis\n",
      "          completelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\n",
      "            ofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\n",
      "          points,thehyperparameteroptimizationalgorithmcanchoosetobeginanew\n",
      "            experiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”\n",
      "            andresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\n",
      " moreinformation.\n",
      "  11.5DebuggingStrategies\n",
      "            Whenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotell\n",
      "            whetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\n",
      "           isabugintheimplementationofthealgorithm. Machinelearningsystemsare\n",
      "     diﬃculttodebugforvariousreasons.\n",
      "              Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\n",
      "              algorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\n",
      "              discoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n",
      "             neuralnetworkonaclassiﬁcationtaskanditachieves5percenttesterror, new\n",
      "             wehavenostraightforwardwayofknowingifthisistheexpectedbehavioror\n",
      " suboptimalbehavior.\n",
      "           Afurtherdiﬃcultyisthatmostmachinelearningmodelshavemultipleparts\n",
      "               thatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\n",
      "          achieveroughlyacceptableperformance.Forexample,supposethatwearetraining\n",
      "        aneuralnetwithseverallayersparametrizedbyweightsW andbiasesb .Suppose\n",
      "           furtherthatwehavemanuallyimplementedthegradientdescentruleforeach\n",
      "            parameterseparately,andwemadeanerrorintheupdateforthebiases:\n",
      "     bb←−α, (11.4)\n",
      "whereα            isthelearningrate.Thiserroneousupdatedoesnotusethegradientat\n",
      "           all.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\n",
      "4 3 1   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "           isclearlynotacorrectimplementationofanyreasonablelearningalgorithm.The\n",
      "             bugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\n",
      "              Dependingonthedistributionoftheinput,theweightsmaybeabletoadaptto\n",
      "    compensateforthenegativebiases.\n",
      "            Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\n",
      "               bothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthe\n",
      "             correctbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\n",
      "       partoftheneuralnetimplementationinisolation.\n",
      "      Someimportantdebuggingtestsincludethefollowing.\n",
      "            Visualizethemodelinaction:Whentrainingamodeltodetectobjectsin\n",
      "           images,viewsomeimageswiththedetectionsproposedbythemodeldisplayed\n",
      "            superimposedontheimage.Whentrainingagenerativemodelofspeech,listento\n",
      "               someofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\n",
      "          fallintothepracticeoflookingonlyatquantitativeperformancemeasurements\n",
      "         likeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\n",
      "          performingitstaskwillhelptodeterminewhetherthequantitativeperformance\n",
      "            numbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\n",
      "           devastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis\n",
      "     performingwellwhenitisnot.\n",
      "            Visualizetheworstmistakes:Mostmodelsareabletooutputsomesortof\n",
      "            conﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedona\n",
      "           softmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\n",
      "               tothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasin\n",
      "         itsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthese\n",
      "         valuesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n",
      "              buttheyaresomewhatusefulinthesensethatexamplesthatareactuallylesslikely\n",
      "           tobecorrectlylabeledreceivesmallerprobabilitiesunderthemodel.Byviewing\n",
      "             thetrainingsetexamplesthatarethehardesttomodelcorrectly,onecanoften\n",
      "            discoverproblemswiththewaythedatahavebeenpreprocessedorlabeled.For\n",
      "           example,theStreetViewtranscriptionsystemoriginallyhadaproblemwherethe\n",
      "            addressnumberdetectionsystemwouldcroptheimagetootightlyandomitsome\n",
      "           digits.Thetranscriptionnetworkthenassignedverylowprobabilitytothecorrect\n",
      "            answerontheseimages.Sortingtheimagestoidentifythemostconﬁdentmistakes\n",
      "           showedthattherewasasystematicproblemwiththecropping.Modifyingthe\n",
      "           detectionsystemtocropmuchwiderimagesresultedinmuchbetterperformance\n",
      "             oftheoverallsystem,eventhoughthetranscriptionnetworkneededtobeableto\n",
      "           processgreatervariationinthepositionandscaleoftheaddressnumbers.\n",
      "            Reasonaboutsoftwareusingtrainingandtesterror:Itisoftendiﬃcultto\n",
      "4 3 2   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "         determinewhethertheunderlyingsoftwareiscorrectlyimplemented.Someclues\n",
      "               canbeobtainedfromthetrainingandtesterrors.Iftrainingerrorislowbuttest\n",
      "             errorishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,\n",
      "          andthemodelisoverﬁttingforfundamentalalgorithmicreasons.Analternative\n",
      "             possibilityisthatthetesterrorismeasuredincorrectlybecauseofaproblemwith\n",
      "             savingthemodelaftertrainingthenreloadingitfortestsetevaluation,orbecause\n",
      "             thetestdatawasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainingand\n",
      "              testerrorsarehigh,thenitisdiﬃculttodeterminewhetherthereisasoftware\n",
      "           defectorwhetherthemodelisunderﬁttingduetofundamentalalgorithmicreasons.\n",
      "      Thisscenariorequiresfurthertests,describednext.\n",
      "              Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\n",
      "              itisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmall\n",
      "             modelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,\n",
      "              aclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiases\n",
      "             oftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectly\n",
      "           labelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\n",
      "            withhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\n",
      "           singleexample,thereisasoftwaredefectpreventingsuccessfuloptimizationonthe\n",
      "             trainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\n",
      "         Compareback-propagatedderivativestonumericalderivatives:Ifyouareusing\n",
      "           asoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-\n",
      "             putations,orifyouareaddinganewoperationtoadiﬀerentiationlibraryand\n",
      "  mustdeﬁneitsbprop          method,thenacommonsourceoferrorisimplementingthis\n",
      "           gradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n",
      "          istocomparethederivativescomputedbyyourimplementationofautomatic\n",
      "        diﬀerentiationtothederivativescomputedby .Because ﬁnitediﬀerences\n",
      "f () =limx\n",
      " → 0    fxfx (+)−()\n",
      " , (11.5)\n",
      "          wecanapproximatethederivativebyusingasmall,ﬁnite:\n",
      "f() x≈    fxfx (+)−()\n",
      " . (11.6)\n",
      "          Wecanimprovetheaccuracyoftheapproximationbyusingthe  centereddiﬀer-\n",
      "ence:\n",
      "f() x≈ fx(+1\n",
      "2   fx )−(−1\n",
      "2)\n",
      " . (11.7)\n",
      "  Theperturbationsize         mustbelargeenoughtoensurethattheperturbationis\n",
      "        notroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.\n",
      "4 3 3   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "             Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\n",
      "g: Rm → Rn          .Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasingle\n",
      "         derivativeatatime.Wecaneitherrunﬁnitediﬀerencingmn   timestoevaluateall\n",
      "   thepartialderivativesofg           ,orapplythetesttoanewfunctionthatusesrandom\n",
      "        projectionsatboththeinputandtheoutputofg     .Forexample,wecanapply\n",
      "        ourtestoftheimplementationofthederivativestof(x ),wheref(x) = uTg( vx),\n",
      "and uand v    arerandomlychosenvectors.Computingf(x  )correctlyrequires\n",
      "    beingabletoback-propagatethroughg        correctlyyetiseﬃcienttodowithﬁnite\n",
      " diﬀerencesbecausef           hasonlyasingleinputandasingleoutput.Itisusually\n",
      "            agoodideatorepeatthistestformorethanonevalueof uand v toreduce\n",
      "            thechanceofthetestoverlookingmistakesthatareorthogonaltotherandom\n",
      "projection.\n",
      "            Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\n",
      "            averyeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbers\n",
      "              asinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\n",
      " observationthat\n",
      "    fxifxif (+) = ()+  ()+(xO2 ), (11.8)\n",
      "    real((+)) = ()+( fxifxO2 )imag(,  fxi (+)\n",
      ") = f  ()+(xO2 ),(11.9)\n",
      "wherei=√−1          .Unlikeinthereal-valuedcaseabove,thereisnocancellation\n",
      "         eﬀectbecausewetakethediﬀerencebetweenthevalueoff  atdiﬀerentpoints.\n",
      "       Thisallowstheuseoftinyvaluesof ,like= 10− 1 5 0   ,whichmaketheO(2 )error\n",
      "    insigniﬁcantforallpracticalpurposes.\n",
      "           Monitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\n",
      "           statisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\n",
      "           oftrainingiterations(maybeoneepoch).Thepreactivationvalueofhiddenunits\n",
      "               cantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,\n",
      "              howoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,\n",
      "            theaverageoftheabsolutevalueofthepreactivationstellsushowsaturated\n",
      "             theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\n",
      "            quicklyvanish,optimizationmaybehampered.Finally,itisusefultocomparethe\n",
      "          magnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\n",
      "            Assuggestedby (),wewouldlikethemagnitudeofparameterupdates Bottou2015\n",
      "             overaminibatchtorepresentsomethinglike1percentofthemagnitudeofthe\n",
      "           parameter,not50percentor0.001percent(whichwouldmaketheparameters\n",
      "               movetooslowly).Itmaybethatsomegroupsofparametersaremovingatagood\n",
      "             pacewhileothersarestalled.Whenthedataissparse(likeinnaturallanguage),\n",
      "4 3 4   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "             someparametersmaybeveryrarelyupdated,andthisshouldbekeptinmind\n",
      "   whenmonitoringtheirevolution.\n",
      "          Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\n",
      "               theresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\n",
      "          imateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\n",
      "          problems. Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\n",
      "          Someguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjective\n",
      "             functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\n",
      "              respecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\n",
      "             andthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\n",
      "             Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\n",
      "         computer,sothedebuggingtestshouldincludesometoleranceparameter.\n",
      "    11.6Example: Multi-DigitNumberRecognition\n",
      "           Toprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology\n",
      "            inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\n",
      "           fromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\n",
      "             manyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\n",
      "        databaseinfrastructure,andsoon,wereofparamountimportance.\n",
      "             Fromthepointofviewofthemachinelearningtask,theprocessbeganwith\n",
      "           datacollection.Thecarscollectedtherawdata,andhumanoperatorsprovided\n",
      "           labels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset\n",
      "          curation,includingusingothermachinelearningtechniquestodetectthehouse\n",
      "    numberspriortotranscribingthem.\n",
      "          Thetranscriptionprojectbeganwithachoiceofperformancemetricsand\n",
      "           desiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthe\n",
      "              choiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\n",
      "              iftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirementfor\n",
      "           thisproject.Speciﬁcally,thegoalwastoobtainhuman-level,98percentaccuracy.\n",
      "               Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Toreachthislevelof\n",
      "         accuracy,theStreetViewtranscriptionsystemsacriﬁcedcoverage.Coveragethus\n",
      "          becamethemainperformancemetricoptimizedduringtheproject,withaccuracy\n",
      "            heldat98percent.Astheconvolutionalnetworkimproved,itbecamepossibleto\n",
      "           reducetheconﬁdencethresholdbelowwhichthenetworkrefusedtotranscribethe\n",
      "        input,eventuallyexceedingthegoalof95percentcoverage.\n",
      "          Afterchoosingquantitativegoals,thenextstepinourrecommended methodol-\n",
      "4 3 5   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "              ogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa\n",
      "         convolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbegan\n",
      "              withsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\n",
      "            tooutputasequenceofpredictions.Tobeginwiththesimplestpossiblebaseline,\n",
      "           theﬁrstimplementationoftheoutputlayerofthemodelconsistedofndiﬀerent\n",
      "      softmaxunitstopredictasequenceofn     characters.Thesesoftmaxunitswere\n",
      "             trainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmaxunit\n",
      " trainedindependently.\n",
      "          Ourrecommended methodologyistoiterativelyreﬁnethebaselineandtest\n",
      "            whethereachchangemakesanimprovement.TheﬁrstchangetotheStreetView\n",
      "          transcriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\n",
      "            metricandthestructureofthedata.Speciﬁcally,thenetworkrefusedtoclassify\n",
      " aninputx       whenevertheprobabilityoftheoutputsequencep(  yx|) <tfor\n",
      " somethresholdt    .Initially,thedeﬁnitionofp(  yx|     )wasad-hoc,basedonsimply\n",
      "          multiplyingallthesoftmaxoutputstogether.Thismotivatedthedevelopmentof\n",
      "           aspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\n",
      "         log-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\n",
      "  muchmoreeﬀectively.\n",
      "             Atthispoint,coveragewasstillbelow90percent,yettherewerenoobvious\n",
      "        theoreticalproblemswiththeapproach.Ourmethodologythereforesuggested\n",
      "          instrumentingthetrainingandtestsetperformancetodeterminewhetherthe\n",
      "           problemwasunderﬁttingoroverﬁtting. Inthiscase,trainingandtestseterror\n",
      "           werenearlyidentical.Indeed,themainreasonthisprojectproceededsosmoothly\n",
      "             wastheavailabilityofadatasetwithtensofmillionsoflabeledexamples.Because\n",
      "              trainingandtestseterrorweresosimilar,thissuggestedthattheproblemwasdue\n",
      "             toeitherunderﬁttingoraproblemwiththetrainingdata.Oneofthedebugging\n",
      "             strategieswerecommend istovisualizethemodel’sworsterrors.Inthiscase,that\n",
      "           meantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\n",
      "           highestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinput\n",
      "              imagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\n",
      "            removedbythecroppingoperation.Forexample,aphotoofanaddress“1849”\n",
      "            mightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblem\n",
      "            couldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\n",
      "         numberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\n",
      "              theteammadeamuchmorepracticaldecision,tosimplyexpandthewidthofthe\n",
      "           cropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\n",
      "          predicted.Thissinglechangeaddedtenpercentagepointstothetranscription\n",
      " system’scoverage.\n",
      "4 3 6   C HAP T E R 1 1 . P R AC T I C AL M E T HO D O L O G Y\n",
      "          Finally,thelastfewpercentagepointsofperformancecamefromadjusting\n",
      "          hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\n",
      "           tainingsomerestrictionsonitscomputationalcost.Becausetrainandtesterror\n",
      "           remainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitswere\n",
      "              duetounderﬁtting,aswellastoafewremainingproblemswiththedatasetitself.\n",
      "           Overall,thetranscriptionprojectwasagreatsuccessandallowedhundredsof\n",
      "             millionsofaddressestobetranscribedbothfasterandatlowercostthanwould\n",
      "     havebeenpossibleviahumaneﬀort.\n",
      "             Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\n",
      "  othersimilarsuccesses.\n",
      "4 3 7 C h a p t e r 1 2\n",
      "Applications\n",
      "             Inthischapter,wedescribehowtousedeeplearningtosolveapplicationsin\n",
      "         computervision,speechrecognition,naturallanguageprocessing,andotherareas\n",
      "          ofcommerci alinterest.Webeginbydiscussingthelarge-scaleneuralnetwork\n",
      "          implementationsrequiredformostseriousAIapplications.Next,wereviewseveral\n",
      "           speciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve. Whileone\n",
      "              goalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad\n",
      "             varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\n",
      "           tasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\n",
      "            Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\n",
      "   vocabulary)perinputfeature.\n",
      "   12.1Large-ScaleDeepLearning\n",
      "           Deeplearningisbasedonthephilosophyofconnectionism:whileanindividual\n",
      "            biologicalneuronoranindividualfeatureinamachinelearningmodelisnot\n",
      "           intelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan\n",
      "            exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\n",
      "             numberofneuronsmustbelarge.Oneofthekeyfactorsresponsibleforthe\n",
      "          improvementinneuralnetwork’saccuracyandtheimprovementofthecomplexity\n",
      "              oftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\n",
      "              thesizeofthenetworksweuse. Aswesawinsection,networksizeshave 1.2.3\n",
      "           grownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksare\n",
      "        onlyaslargeasthenervoussystemsofinsects.\n",
      "           Becausethesizeofneuralnetworksiscritical,deeplearningrequireshigh\n",
      "438  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "    performancehardwareandsoftwareinfrastructure.\n",
      "   12.1.1FastCPUImplementations\n",
      "           Traditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\n",
      "           Today,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPU\n",
      "           computingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\n",
      "          theseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\n",
      "         notmanagethehighcomputationalworkloadrequiredbyneuralnetworks.\n",
      "           AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyond\n",
      "            thescopeofthisbook,butweemphasizeherethatcarefulimplementationfor\n",
      "            speciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\n",
      "          CPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-point\n",
      "          arithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-\n",
      "          pointimplementation,Vanhoucke2011etal.()obtainedathreefoldspeedupover\n",
      "           astrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformance\n",
      "        characteristics,sosometimesﬂoating-pointimplementationscanbefastertoo.\n",
      "         Theimportantprincipleisthatcarefulspecializationofnumericalcomputation\n",
      "            routinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouse\n",
      "           ﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemisses\n",
      "         andusingvectorinstructions.Manymachinelearningresearchersneglectthese\n",
      "         implementationdetails,butwhentheperformanceofanimplementationrestricts\n",
      "          thesizeofthemodel,theaccuracyofthemodelsuﬀers.\n",
      "  12.1.2GPUImplementations\n",
      "         Mostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\n",
      "        units.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\n",
      "          thatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\n",
      "         videogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\n",
      "           performancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\n",
      "     beneﬁcialforneuralnetworksaswell.\n",
      "         Videogamerenderingrequiresperformingmanyoperationsinparallelquickly.\n",
      "            Modelsofcharactersandenvironmentsarespeciﬁedvialistsof3-Dcoordinatesof\n",
      "          vertices.Graphicscardsmustperformmatrixmultiplicationanddivisiononmany\n",
      "           verticesinparalleltoconvertthese3-Dcoordinatesinto2-Don-screencoordinates.\n",
      "             Thegraphicscardmustthenperformmanycomputationsateachpixelinparallelto\n",
      "             determinethecolorofeachpixel.Inbothcases,thecomputationsarefairlysimple\n",
      "4 3 9  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           anddonotinvolvemuchbranchingcomparedtothecomputationalworkloadthat\n",
      "             aCPUusuallyencounters.Forexample,eachvertexinthesamerigidobjectwill\n",
      "               bemultipliedbythesamematrix;thereisnoneedtoevaluateanifstatementper\n",
      "            vertextodeterminewhichmatrixtomultiplyby.Thecomputationsarealsoentirely\n",
      "           independentofeachother,andthusmaybeparallelizedeasily.Thecomputations\n",
      "         alsoinvolveprocessingmassivebuﬀersofmemory,containingbitmapsdescribing\n",
      "             thetexture(colorpattern)ofeachobjecttoberendered.Together,thisresultsin\n",
      "             graphicscardshavingbeendesignedtohaveahighdegreeofparallelismandhigh\n",
      "             memory bandwidth,atthecostofhavingalowerclockspeedandlessbranching\n",
      "    capabilityrelativetotraditionalCPUs.\n",
      "         Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\n",
      "        real-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\n",
      "          largeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,\n",
      "            eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\n",
      "             buﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer,\n",
      "           sothememory bandwidthofthesystemoftenbecomestherate-limitingfactor.\n",
      "           GPUsoﬀeracompellingadvantageoverCPUsbecauseoftheirhighmemory\n",
      "         bandwidth.Neuralnetworktrainingalgorithmstypicallydonotinvolvemuch\n",
      "          branchingorsophisticatedcontrol,sotheyareappropriateforGPUhardware.\n",
      "            Sinceneuralnetworkscanbedividedintomultipleindividual“neurons”thatcanbe\n",
      "           processedindependentlyfromtheotherneuronsinthesamelayer,neuralnetworks\n",
      "       easilybeneﬁtfromtheparallelismofGPUcomputing.\n",
      "            GPUhardwarewasoriginallysospecializedthatitcouldbeusedonlyfor\n",
      "          graphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustom\n",
      "             subroutinestobeusedtotransformthecoordinatesofverticesortoassigncolors\n",
      "            topixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactually\n",
      "             bebasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputing\n",
      "               bywritingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrauetal.\n",
      "           ()implementedatwo-layerfullyconnectedneuralnetworkonaGPUand 2005\n",
      "         reportedathree-timesspeedupovertheirCPU-basedbaseline.Shortlythereafter,\n",
      "            Chellapilla 2006etal.()demonstratedthatthesametechniquecouldbeusedto\n",
      "   acceleratesupervisedconvolutionalnetworks.\n",
      "          Thepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\n",
      "  theadventof   generalpurposeGPUs     .TheseGP-GPUscouldexecutearbitrary\n",
      "       code,notjustrenderingsubroutines. NVIDIA’sCUDAprogramminglanguage\n",
      "             providedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\n",
      "        relativelyconvenientprogrammingmodel,massiveparallelism,andhighmemory\n",
      "          bandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming.\n",
      "4 4 0  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\n",
      "        available( ,; ,). Rainaetal.2009Ciresanetal.2010\n",
      "            WritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospecial-\n",
      "            ists.ThetechniquesrequiredtoobtaingoodperformanceonGPUareverydiﬀerent\n",
      "            fromthoseusedonCPU.Forexample,goodCPU-basedcodeisusuallydesigned\n",
      "             toreadinformationfromthecacheasmuchaspossible.OnGPU,mostwritable\n",
      "              memory locationsarenotcached,soitcanactuallybefastertocomputethesame\n",
      "              valuetwice,ratherthancomputeitonceandreaditbackfrommemory.GPUcode\n",
      "          isalsoinherentlymultithreadedandthediﬀerentthreadsmustbecoordinated\n",
      "            witheachothercarefully.Forexample,memory operationsarefasteriftheycan\n",
      "be c o a l e s c e d          .Coalescedreadsorwritesoccurwhenseveralthreadscaneach\n",
      "              readorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\n",
      "            transaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofread\n",
      "          patternsanddiﬀerentkindsofwritepatterns.Typically,memory operationsare\n",
      "    easiertocoalesceifamong n  threads,thread i  accessesbyte i + j  ofmemory,and j\n",
      "              isamultipleofsomepowerof2.Theexactspeciﬁcationsdiﬀerbetweenmodelsof\n",
      "            GPU.AnothercommonconsiderationforGPUsismakingsurethateachthreadin\n",
      "          agroupexecutesthesameinstructionsimultaneously.Thismeansthatbranching\n",
      "           canbediﬃcultonGPU.Threadsaredividedintosmallgroupscalled w a r ps .Each\n",
      "             threadinawarpexecutesthesameinstructionduringeachcycle,soifdiﬀerent\n",
      "            threadswithinthesamewarpneedtoexecutediﬀerentcodepaths,thesediﬀerent\n",
      "         codepathsmustbetraversedsequentiallyratherthaninparallel.\n",
      "         Becauseofthediﬃcultyofwritinghigh-performanceGPUcode,researchers\n",
      "             shouldstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodetotest\n",
      "             newmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\n",
      "        ofhigh-performanceoperationslikeconvolutionandmatrixmultiplication,then\n",
      "            specifyingmodelsintermsofcallstothislibraryofoperations.Forexample,\n",
      "           themachinelearninglibraryPylearn2( ,)speciﬁesallits Goodfellowetal.2013c\n",
      "            machinelearningalgorithmsintermsofcallstoTheano( ,; Bergstraetal.2010\n",
      "          Bastien2012 Krizhevsky2010 etal.,)andcuda-convnet( ,),whichprovidethese\n",
      "         high-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\n",
      "            multiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\n",
      "              eitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.\n",
      "            OtherlibrarieslikeTensorFlow( ,)andTorch( , Abadietal.2015 Collobertetal.\n",
      "   2011b)providesimilarfeatures.\n",
      "4 4 1  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "   12.1.3Large-ScaleDistributedImplementations\n",
      "           Inmanycases,thecomputationalresourcesavailableonasinglemachineare\n",
      "           insuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinference\n",
      "  acrossmanymachines.\n",
      "           Distributinginferenceissimple,becauseeachinputexamplewewanttoprocess\n",
      "            canberunbyaseparatemachine.Thisisknownas . dataparallelism\n",
      "     Itisalsopossibletoget  modelparallelism    ,wheremultiplemachineswork\n",
      "              togetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthe\n",
      "        model.Thisisfeasibleforbothinferenceandtraining.\n",
      "           Dataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\n",
      "               oftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\n",
      "            returnsintermsofoptimizationperformance.Itwouldbebettertoallowmultiple\n",
      "         machinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\n",
      "           thestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:\n",
      "               thegradientatstepisafunctionoftheparametersproducedbystep. t t − 1\n",
      "    Thiscanbesolvedusing    asynchronousstochasticgradientdescent(Ben-\n",
      "              gio2001Recht2011 etal.,;etal.,).Inthisapproach,severalprocessorcoresshare\n",
      "          thememory representingtheparameters.Eachcorereadsparameterswithouta\n",
      "           lock,thencomputesagradient,thenincrementstheparameterswithoutalock.\n",
      "           Thisreducestheaverageamountofimprovementthateachgradientdescentstep\n",
      "            yields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreased\n",
      "             rateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean\n",
      "          etal.()pioneeredthemultimachineimplementationofthislock-freeapproach 2012\n",
      "         togradientdescent,wheretheparametersaremanagedbya  parameterserver\n",
      "         ratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\n",
      "            remainstheprimarystrategyfortraininglargedeepnetworksandisusedby\n",
      "             mostmajordeeplearninggroupsinindustry( ,; Chilimbietal.2014Wuetal.,\n",
      "          2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescale\n",
      "            ofdistributedlearningsystems,butsomeresearchhasfocusedonhowtobuild\n",
      "         distributednetworkswithrelativelylow-costhardwareavailableintheuniversity\n",
      "    setting( ,). Coatesetal.2013\n",
      "  12.1.4ModelCompression\n",
      "            Inmanycommerci alapplications,itismuchmoreimportantthatthetimeand\n",
      "             memory costofrunninginferenceinamachinelearningmodelbelowthanthat\n",
      "              thetimeandmemory costoftrainingbelow.Forapplicationsthatdonotrequire\n",
      "4 4 2  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "               personalization,itispossibletotrainamodelonce,thendeployittobeusedby\n",
      "             billionsofusers.Inmanycases,theenduserismoreresourceconstrainedthan\n",
      "            thedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\n",
      "        powerfulcomputercluster,thendeployitonmobilephones.\n",
      "         Akeystrategyforreducingthecostofinferenceis modelcompression(Bu-\n",
      "              ciluˇa2006etal.,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\n",
      "            expensivemodelwithasmallermodelthatrequireslessmemory andruntimeto\n",
      "  storeandevaluate.\n",
      "            Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\n",
      "             primarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththe\n",
      "          lowestgeneralizationerrorisanensembleofseveralindependentlytrainedmodels.\n",
      " Evaluatingall n         ensemblemembersisexpensive.Sometimes,evenasinglemodel\n",
      "             generalizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\n",
      "     Theselargemodelslearnsomefunction f( x      ),butdosousingmanymore\n",
      "             parametersthanarenecessaryforthetask.Theirsizeisnecessaryonlybecauseof\n",
      "             thelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunction f( x),\n",
      "           wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simplyby\n",
      "applying f   torandomlysampledpoints x       .Wethentrainthenew,smallermodel\n",
      " tomatch f( x           )onthesepoints. Tomosteﬃcientlyusethecapacityofthenew,\n",
      "        smallmodel,itisbesttosamplethenew x     pointsfromadistributionresembling\n",
      "               theactualtestinputsthatwillbesuppliedtothemodellater.Thiscanbedone\n",
      "           bycorruptingtrainingexamplesorbydrawingpointsfromagenerativemodel\n",
      "     trainedontheoriginaltrainingset.\n",
      "           Alternatively,onecantrainthesmallermodelonlyontheoriginaltraining\n",
      "              points,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\n",
      "         distributionovertheincorrectclasses( ,,). Hintonetal.20142015\n",
      "  12.1.5DynamicStructure\n",
      "           Onestrategyforacceleratingdata-processingsystemsingeneralistobuildsystems\n",
      " thathave  dynamicstructure      inthegraphdescribingthecomputationneeded\n",
      "         toprocessaninput.Data-processingsystemscandynamicallydeterminewhich\n",
      "             subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\n",
      "          networkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\n",
      "           offeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\n",
      "         formofdynamicstructureinsideneuralnetworksissometimescalledconditional\n",
      "computation          (,; ,).Sincemanycomponentsof Bengio2013Bengioetal.2013b\n",
      "             thearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n",
      "4 4 3  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            systemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\n",
      "          Dynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\n",
      "       generallythroughoutthesoftwareengineeringdiscipline. Thesimplestversions\n",
      "           ofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\n",
      "            subsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\n",
      "     beappliedtoaparticularinput.\n",
      "            Avenerablestrategyforacceleratinginferenceinaclassiﬁeristousea c a s c a de\n",
      "              ofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\n",
      "               presenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\n",
      "             wemustuseasophisticatedclassiﬁerwithhighcapacity,whichisexpensivetorun.\n",
      "            Becausetheobjectisrare,however,wecanusuallyusemuchlesscomputation\n",
      "              torejectinputsasnotcontainingtheobject.Inthesesituations,wecantraina\n",
      "            sequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacityand\n",
      "              aretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure\n",
      "              wedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁer\n",
      "             istrainedtohavehighprecision. Attesttime,weruninferencebyrunningthe\n",
      "             classiﬁersinasequence,abandoninganyexampleassoonasanyoneelementin\n",
      "              thecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\n",
      "               highconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecost\n",
      "             offullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascade\n",
      "              canachievehighcapacity.Onewayistomakethelatermembersofthecascade\n",
      "             individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\n",
      "            highcapacity,becausesomeofitsindividualmembersdo. Itisalsopossibleto\n",
      "             makeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\n",
      "               asawholehashighcapacityasaresultofthecombinationofmanysmallmodels.\n",
      "             ViolaandJones2001()usedacascadeofboosteddecisiontreestoimplementa\n",
      "            fastandrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Their\n",
      "            classiﬁerlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmany\n",
      "             windowsareexaminedandrejectediftheydonotcontainfaces.Anotherversionof\n",
      "            cascadesusestheearliermodelstoimplementasortofhardattentionmechanism:\n",
      "             theearlymembersofthecascadelocalizeanobject,andlatermembersofthe\n",
      "           cascadeperformfurtherprocessinggiventhelocationoftheobject.Forexample,\n",
      "          GoogletranscribesaddressnumbersfromStreetViewimageryusingatwo-step\n",
      "           cascadethatﬁrstlocatestheaddressnumberwithonemachinelearningmodel\n",
      "         andthentranscribesitwithanother(Goodfellow 2014detal.,).\n",
      "          Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\n",
      "              nodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\n",
      "            Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n",
      "4 4 4  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "                istotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\n",
      "           splittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\n",
      "        donewiththeprimarygoalofacceleratinginferencecomputations.\n",
      "           Inthesamespirit,onecanuseaneuralnetworkcalledthegater toselect\n",
      "    whichoneoutofseveral  expertnetworks      willbeusedtocomputetheoutput,\n",
      "            giventhecurrentinput.Theﬁrstversionofthisideaiscalledthe  mixtureof\n",
      "experts             (Nowlan1990Jacobs1991 ,; etal.,),inwhichthegateroutputsaset\n",
      "           ofprobabilitiesorweights(obtainedviaasoftmaxnonlinearity),oneperexpert,\n",
      "             andtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputof\n",
      "              theexperts.Inthatcase, theuseofthegaterdoesnotoﬀerareductionin\n",
      "              computationalcost,butifasingleexpertischosenbythegaterforeachexample,\n",
      "  weobtainthe   hardmixtureofexperts      ( ,,),which Collobertetal.20012002\n",
      "          canconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\n",
      "             whenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial.But\n",
      "              whenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossible\n",
      "            tousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)\n",
      "           allthegaterconﬁgurations.Todealwiththisproblem,severalapproacheshave\n",
      "           beenexploredtotraincombinatorialgaters. ()experimentwith Bengioetal.2013b\n",
      "            severalestimatorsofthegradientonthegatingprobabilities,whileBaconetal.\n",
      "          ()and ()usereinforcementlearningtechniques(policy 2015 Bengioetal.2015a\n",
      "              gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\n",
      "          anactualreductionincomputationalcostwithoutnegativelyaﬀectingthequality\n",
      "  oftheapproximation.\n",
      "     Another kindof dynamicstructure isa switch, where ahiddenunit can\n",
      "           receiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicrouting\n",
      "           approachcanbeinterpretedasanattentionmechanism( ,). Olshausenetal.1993\n",
      "              Sofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.\n",
      "         Contemporaryapproachesinsteaduseaweightedaverageovermanypossible\n",
      "            inputs,andthusdonotachieveallthepossiblecomputationalbeneﬁtsofdynamic\n",
      "        structure.Contemporaryattentionmechanismsaredescribedinsection .12.4.5.1\n",
      "          Onemajorobstacletousingdynamicallystructuredsystemsisthedecreased\n",
      "           degreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranches\n",
      "             fordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\n",
      "          asmatrixmultiplicationorbatchconvolutiononaminibatchofexamples. We\n",
      "          canwritemorespecializedsubroutinesthatconvolveeachexamplewithdiﬀerent\n",
      "              kernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumns\n",
      "        ofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃcultto\n",
      "           implementeﬃciently. CPUimplementationswillbeslowasaresultofthelack\n",
      "4 4 5  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "             ofcachecoherence,andGPUimplementationswillbeslowbecauseofthelackof\n",
      "           coalescedmemory transactionsandtheneedtoserializewarpswhenmembersof\n",
      "            awarptakediﬀerentbranches. Insomecases,theseissuescanbemitigatedby\n",
      "            partitioningtheexamplesintogroupsthatalltakethesamebranch,thenprocessing\n",
      "           thesegroupsofexamplessimultaneously.Thiscanbeanacceptablestrategyfor\n",
      "             minimizingthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄine\n",
      "          setting.Inareal-timesettingwhereexamplesmustbeprocessedcontinuously,\n",
      "           partitioningtheworkloadcanresultinload-balancingissues.Forexample,ifwe\n",
      "              assignonemachinetoprocesstheﬁrststepinacascadeandanothermachineto\n",
      "                processthelaststepinacascade,thentheﬁrstwilltendtobeoverloaded,andthe\n",
      "              lastwilltendtobeunderloaded.Similarissuesariseifeachmachineisassignedto\n",
      "       implementdiﬀerentnodesofaneuraldecisiontree.\n",
      "      12.1.6SpecializedHardwareImplementationsofDeepNetworks\n",
      "           Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\n",
      "         onspecializedhardwareimplementationsthatcouldspeeduptrainingand/or\n",
      "           inferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\n",
      "           specializedhardwarefordeepnetworks( ,; , LindseyandLindblad1994Beiuetal.\n",
      "    2003MisraandSaha2010 ; ,).\n",
      "           Diﬀerentformsofspecializedhardware(GrafandJackel1989MeadandIsmail ,; ,\n",
      "               2012Kim 2009Pham 2012Chen2014ab ;etal.,;etal.,;etal.,,)havebeendeveloped\n",
      "         overthelastdecadeswithASICs(application-speciﬁcintegratedcircuits),either\n",
      "           withdigital(basedonbinaryrepresentationsofnumbers),analog(GrafandJackel,\n",
      "          1989MeadandIsmail2012 ; ,)(basedonphysicalimplementationsofcontinuous\n",
      "          valuesasvoltagesorcurrents),orhybridimplementations(combiningdigitaland\n",
      "         analogcomponents).InrecentyearsmoreﬂexibleFPGA(ﬁeldprogrammable\n",
      "           gatedarray)implementations(wheretheparticularsofthecircuitcanbewritten\n",
      "          onthechipafterithasbeenbuilt)havebeendeveloped.\n",
      "       Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\n",
      "            andGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoating-point\n",
      "              numbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\n",
      "            leastatinferencetime( ,; ,; HoltandBaker1991HoliandHwang1993Presley\n",
      "             andHaggard 1994SimardandGraf1994Wawrzynek1996Savich ,; ,; etal.,;etal.,\n",
      "             2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\n",
      "             hasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster\n",
      "         hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\n",
      "             researchonspecializedhardwarefordeepnetworksisthattherateofprogressof\n",
      "             asingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n",
      "4 4 6  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           computingspeedhavecomefromparallelizationacrosscores(eitherinCPUsor\n",
      "             GPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneural\n",
      "          networkera),whenthehardwareimplementationsofneuralnetworks(whichmight\n",
      "              taketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\n",
      "          therapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\n",
      "               hardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware\n",
      "           designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\n",
      "          general-publicapplicationsofdeeplearning(e.g.,withspeech,computervisionor\n",
      " naturallanguage).\n",
      "        Recentworkonlow-precisionimplementationsofbackprop-basedneuralnets\n",
      "            (Vanhoucke 2011Courbariaux 2015Gupta 2015 etal.,; etal.,; etal.,)suggests\n",
      "              thatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeep\n",
      "          neuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionis\n",
      "            requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\n",
      "            ﬁxed-pointrepresentationofnumberscanbeusedtoreducehowmanybitsare\n",
      "          requiredpernumber.Traditionalﬁxed-pointnumbersarerestrictedtoaﬁxed\n",
      "          range(whichcorrespondstoagivenexponentinaﬂoating-pointrepresentation).\n",
      "          Dynamicﬁxed-pointrepresentationssharethatrangeamongasetofnumbers\n",
      "            (suchasalltheweightsinonelayer).Usingﬁxed-pointratherthanﬂoating-point\n",
      "           representationsandusingfewerbitspernumberreducesthehardwaresurfacearea,\n",
      "        powerrequirements,andcomputingtimeneededforperformingmultiplications,\n",
      "            andmultiplicationsarethemostdemandingoftheoperationsneededtouseor\n",
      "      trainamoderndeepnetworkwithbackprop.\n",
      "  12.2ComputerVision\n",
      "            Computervisionhastraditionallybeenoneofthemostactiveresearchareasfor\n",
      "            deeplearningapplications,becausevisionisataskthatiseﬀortlessforhumans\n",
      "            andmanyanimalsbutchallengingforcomputers( ,).Manyof Ballardetal.1983\n",
      "           themostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms\n",
      "      ofobjectrecognitionoropticalcharacterrecognition.\n",
      "             Computervisionisaverybroadﬁeldencompassingawidevarietyofwaysto\n",
      "          processimagesandanamazingdiversityofapplications.Applicationsofcomputer\n",
      "          visionrangefromreproducinghumanvisualabilities,suchasrecognizingfaces,\n",
      "             tocreatingentirelynewcategoriesofvisualabilities.Asanexampleofthelatter\n",
      "         category, onerecentcomputervisionapplicationistorecognizesoundwaves\n",
      "              fromthevibrationstheyinduceinobjectsvisibleinavideo( ,). Davisetal.2014\n",
      "            Mostdeeplearningresearchoncomputervisionhasfocusednotonsuchexotic\n",
      "4 4 7  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            applicationsthatexpandtherealmofwhatispossiblewithimagerybutrather\n",
      "             onasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\n",
      "            learningforcomputervisionisusedforobjectrecognitionordetectionofsome\n",
      "            form,whetherthismeansreportingwhichobjectispresentinanimage,annotating\n",
      "           animagewithboundingboxesaroundeachobject,transcribingasequenceof\n",
      "               symbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\n",
      "          objectitbelongsto. Becausegenerativemodelinghasbeenaguidingprinciple\n",
      "              ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\n",
      "            usingdeepmodels.Whileimagesynthesis isusuallynotconsidereda exnihilo\n",
      "           computervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\n",
      "           imagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\n",
      "   removingobjectsfromimages.\n",
      " 12.2.1Preprocessing\n",
      "        Manyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\n",
      "             inputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturesto\n",
      "           represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\n",
      "             processing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\n",
      "              samereasonablerange,like[0,1]or[-1,1].Mixingimagesthatliein[0,1]with\n",
      "              imagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\n",
      "             thesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\n",
      "            computervisionarchitecturesrequireimagesofastandardsize,soimagesmustbe\n",
      "              croppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.\n",
      "         Someconvolutionalmodelsacceptvariablysizedinputsanddynamicallyadjust\n",
      "              thesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibeletal.,\n",
      "         1989).Otherconvolutionalmodelshavevariablysizedoutputthatautomatically\n",
      "                scalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\n",
      "    image( ,). Hadselletal.2007\n",
      "            Datasetaugmentationmaybeseenasawayofpreprocessingthetrainingset\n",
      "           only.Datasetaugmentationisanexcellentwaytoreducethegeneralizationerror\n",
      "              ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\n",
      "             themodelmanydiﬀerentversionsofthesameinput(forexample,thesameimage\n",
      "           croppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthe\n",
      "             modelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\n",
      "        ensembleapproach,andithelpstoreducegeneralizationerror.\n",
      "             Otherkindsofpreprocessingareappliedtoboththetrainingandthetestset\n",
      "              withthegoalofputtingeachexampleintoamorecanonicalformtoreducethe\n",
      "             amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n",
      "4 4 8  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "              variationinthedatacanreducebothgeneralizationerrorandthesizeofthemodel\n",
      "              neededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmallermodels,and\n",
      "            simplersolutionsaremorelikelytogeneralizewell.Preprocessingofthiskindis\n",
      "              usuallydesignedtoremovesomekindofvariabilityintheinputdatathatiseasy\n",
      "              forahumandesignertodescribeandthatthehumandesignerisconﬁdenthasno\n",
      "            relevancetothetask.Whentrainingwithlargedatasetsandlargemodels,this\n",
      "               kindofpreprocessingisoftenunnecessary,anditisbesttojustletthemodellearn\n",
      "            whichkindsofvariabilityitshouldbecomeinvariantto.Forexample,theAlexNet\n",
      "          systemforclassifyingImageNet hasonlyonepreprocessingstep:subtractingthe\n",
      "          meanacrosstrainingexamplesofeachpixel( ,). Krizhevskyetal.2012\n",
      "  12.2.1.1ContrastNormalization\n",
      "             Oneofthemostobvioussourcesofvariationthatcanbesafelyremovedfor\n",
      "              manytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe\n",
      "             magnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.\n",
      "              Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\n",
      "             deeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\n",
      "              imageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\n",
      "  X∈ Rr c × × 3  ,with X i , j , 1     beingtheredintensityatrowi  ,andcolumnj , X i , j , 2giving\n",
      "    thegreenintensity,and X i , j , 3        givingtheblueintensity.Thenthecontrastofthe\n",
      "    entireimageisgivenby\n",
      "1\n",
      "3rcr\n",
      "i =1c\n",
      "j =13\n",
      "k =1\n",
      "X i , j , k− ¯X2 , (12.1)\n",
      "where ¯         Xisthemeanintensityoftheentireimage:\n",
      "¯  X =1\n",
      "3rcr \n",
      "i =1c \n",
      "j =13 \n",
      "k =1X i , j , k . (12.2)\n",
      "  Globalcontrastnormalization       (GCN)aimstopreventimagesfromhaving\n",
      "          varyingamountsofcontrastbysubtractingthemeanfromeachimage, then\n",
      "        rescalingit sothat thestandarddeviation acrossits pixelsisequal tosome\n",
      "constants            .Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\n",
      "            changethecontrastofazero-contrastimage(onewhosepixelsallhaveequal\n",
      "           intensity).Imageswithverylowbutnonzerocontrastoftenhavelittleinformation\n",
      "         content.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n",
      "           morethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\n",
      "4 4 9  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "  −15 00 15 . . .\n",
      "x 0−15 .00 .15 .x 1 Rawinput\n",
      "  −15 00 15 . . .\n",
      "x 0  GCN,= 10λ− 2\n",
      "  −15 00 15 . . .\n",
      "x 0  GCN,= 0λ\n",
      "              Figure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\n",
      " ( C e n t e r )GCNwithλ            = 0mapsallnonzeroexamplesperfectlyontoasphere.Hereweuse\n",
      "s  = 1and= 10− 8          .BecauseweuseGCNbasedonnormalizingthestandarddeviation\n",
      "  ratherthantheL2         norm,theresultingsphereisnottheunitsphere. Regularized ( R i g h t )\n",
      " GCN,with λ>           0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\n",
      "            variationintheirnorm.Weleaveandthesameasbefore. s\n",
      "      motivatesintroducingasmallpositiveregularizationparameterλ  tobiasthe\n",
      "          estimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\n",
      "   tobeatleast    .Givenaninputimage X     ,GCNproducesanoutputimage X,\n",
      "  deﬁnedsuchthat\n",
      "X\n",
      "i , j , k= sX i , j , k −¯ X\n",
      "max\n",
      ",\n",
      " λ+1\n",
      "3 r cr\n",
      "i =1c\n",
      "j =13\n",
      "k =1\n",
      "X i , j , k −¯ X2 .(12.3)\n",
      "          Datasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely\n",
      "             tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\n",
      "        topracticallyignorethesmalldenominatorproblembysettingλ   = 0andavoid\n",
      "        divisionby0inextremelyrarecasesbysetting     toanextremelylowvaluelike\n",
      "10− 8            . ThisistheapproachusedbyGoodfellow 2013aetal.()ontheCIFAR-10\n",
      "           dataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\n",
      "          intensity,makingaggressiveregularizationmoreuseful. ()used Coatesetal.2011\n",
      "            λ = 0and= 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.\n",
      "  Thescaleparameters            canusuallybesetto,asdoneby (), 1 Coatesetal.2011\n",
      "           orchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\n",
      "         closeto1,asdoneby (). Goodfellowetal.2013a\n",
      "           Thestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\n",
      "4 5 0  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "              oftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\n",
      "          preferabletodeﬁneGCNintermsofstandarddeviationratherthan L2norm\n",
      "            becausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN\n",
      "      basedonstandarddeviationallowsthesame s     tobeusedregardlessofimage\n",
      "     size.However,theobservationthatthe L2     normisproportionaltothestandard\n",
      "            deviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\n",
      "            examplestoasphericalshell. Seeﬁgureforanillustration. Thiscanbea 12.1\n",
      "           usefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\n",
      "            inspaceratherthantoexactlocations.Respondingtomultipledistancesinthe\n",
      "          samedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerent\n",
      "           biases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.\n",
      "        Additionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\n",
      "           multipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\n",
      "            reducingeachexampletoadirectionratherthanadirectionandadistance.\n",
      "       Counterintuitively,thereisapreprocessingoperationknownasspheringthat\n",
      "              isnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\n",
      "             lieonasphericalshell,butratherreferstorescalingtheprincipalcomponentsto\n",
      "            haveequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\n",
      "        sphericalcontours.Spheringismorecommonlyknownas . whitening\n",
      "          Globalcontrastnormalizationwilloftenfailtohighlightimagefeatureswe\n",
      "                 wouldliketohavestandout,suchasedgesandcorners.Ifwehaveascenewitha\n",
      "                 largedarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein\n",
      "            theshadowofabuilding),thenglobalcontrastnormalizationwillensurethatthere\n",
      "             isalargediﬀerencebetweenthebrightnessofthedarkareaandthebrightness\n",
      "             ofthelightarea. Itwillnot,however,ensurethatedgeswithinthedarkregion\n",
      " standout.\n",
      " Thismotivates  localcontrastnormalization   .Localcontrastnormalization\n",
      "            ensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\n",
      "               theimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast 12.2\n",
      "normalization.\n",
      "          Variousdeﬁnitionsoflocalcontrastnormalizationarepossible.Inallcases,\n",
      "             onemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingby\n",
      "             astandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\n",
      "            andstandarddeviationofallpixelsinarectangularwindowcenteredonthe\n",
      "               pixeltobemodiﬁed( ,).Inothercases,thisisaweightedmean Pintoetal.2008\n",
      "           andweightedstandarddeviationusingGaussianweightscenteredonthepixelto\n",
      "           bemodiﬁed. Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor\n",
      "         channelsseparatelywhileotherscombineinformationfromdiﬀerentchannelsto\n",
      "4 5 1  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "   Inputimage GCN LCN\n",
      "            Figure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀects\n",
      "             ofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\n",
      "             scale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\n",
      "           contrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstant\n",
      "              intensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,\n",
      "                 suchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe\n",
      "    normalizationkernelbeingtoohigh.\n",
      "      normalizeeachpixel( ,). Sermanetetal.2012\n",
      "         Localcontrastnormalizationcanusuallybeimplementedeﬃcientlybyusing\n",
      "            separableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\n",
      "        localstandarddeviations,thenusingelement-wisesubtractionandelement-wise\n",
      "    divisionondiﬀerentfeaturemaps.\n",
      "            Localcontrastnormalizationisadiﬀerentiableoperationandcanalsobeusedas\n",
      "              anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\n",
      "    operationappliedtotheinput.\n",
      "          Aswithglobalcontrastnormalization,wetypicallyneedtoregularizelocal\n",
      "           contrastnormalizationtoavoiddivisionbyzero.Infact,becauselocalcontrast\n",
      "           normalizationtypicallyactsonsmallerwindows,itisevenmoreimportantto\n",
      "            regularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\n",
      "             thesameaseachother,andthusmorelikelytohavezerostandarddeviation.\n",
      "  12.2.1.2DatasetAugmentation\n",
      "              Asdescribedinsection,itiseasytoimprovethegeneralizationofaclassiﬁer 7.4\n",
      "              byincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\n",
      "4 5 2  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           examplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe\n",
      "           class.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenableto\n",
      "        thisformof datasetaugmentationbecause theclass isinvariantto somany\n",
      "          transformationsandtheinputcanbeeasilytransformedwithmanygeometri c\n",
      "         operations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,\n",
      "              rotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecialized\n",
      "        computervisionapplications,moreadvancedtransformationsarecommonlyused\n",
      "         fordatasetaugmentation.Theseschemesincluderandomperturbationofthe\n",
      "            colorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevskyetal.2012\n",
      "     theinput( ,). LeCunetal.1998b\n",
      "  12.3SpeechRecognition\n",
      "             Thetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken\n",
      "          naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\n",
      "  thespeaker.LetX= (x(1) ,x(2)     ,...,x ()T      )denotethesequenceofacousticinput\n",
      "          vectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\n",
      "        speechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\n",
      "           features,butsome(JaitlyandHinton2011,)deeplearningsystemslearnfeatures\n",
      "   fromrawinput.Lety= (y1 ,y2     ,...,yN      )denotethetargetoutputsequence(usually\n",
      "      asequenceofwordsorcharacters).The   automaticspeechrecognition(ASR)\n",
      "     taskconsistsofcreatingafunctionf∗\n",
      "ASR     thatcomputesthemostprobablelinguistic\n",
      "      sequencegiventheacousticsequence: y X\n",
      "f∗\n",
      "ASR  () = argmaxX\n",
      "yP∗   ( = ) yX |X, (12.4)\n",
      "whereP∗       isthetrueconditionaldistributionrelatingtheinputsX  tothetargetsy.\n",
      "         Sincethe1980sanduntilabout2009–2012, state-of-the-artspeechrecognition\n",
      "         systemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\n",
      "         models(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand\n",
      "           phonemes( ,),whileHMMsmodeledthesequenceofphonemes. Bahletal.1987\n",
      "        TheGMM-HMMmodel familytreatsacousticwaveformsasbeinggenerated\n",
      "            bythefollowingprocess:ﬁrstanHMMgeneratesasequenceofphonemesand\n",
      "          discretesubphonemicstates(suchasthebeginning, middl e,andendofeach\n",
      "            phoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\n",
      "        audiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\n",
      "            speechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswere\n",
      "            applied,andnumerousASRsystemsfromthelate1980sandearly1990sused\n",
      "4 5 3  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           neuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; etal.,;\n",
      "              Fallside1991Bengio 19911992Konig 1996 ,; etal.,,;etal.,).Atthetime,the\n",
      "          performanceofASRbasedonneuralnetsapproximatelymatchedtheperformance\n",
      "          ofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved26\n",
      "            percentphonemeerrorrateontheTIMIT( ,)corpus(with Garofoloetal.1993\n",
      "           39phonemestodiscriminateamong),whichwasbetterthanorcomparableto\n",
      "          HMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\n",
      "            recognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\n",
      "          Nonetheless,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor\n",
      "            speechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystems\n",
      "            onthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\n",
      "            forswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\n",
      "           academicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\n",
      "           focusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\n",
      "           Later,withmuchlargeranddeepermodelsandmuchlargerdatasets,recognition\n",
      "          accuracywasdramaticallyimprovedbyusingneuralnetworkstoreplaceGMMs\n",
      "           forthetaskofassociatingacousticfeaturestophonemes(orsubphonemicstates).\n",
      "            Startingin2009,speechresearchersappliedaformofdeeplearningbasedon\n",
      "          unsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\n",
      "        basedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\n",
      "            machines(RBMs)tomodeltheinputdata.RBMsaredescribedinpart.III\n",
      "          Tosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\n",
      "           deepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.\n",
      "          Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow\n",
      "           (aroundacenterframe)andpredicttheconditionalprobabilitiesofHMMstates\n",
      "           forthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniﬁcantlyimprove\n",
      "            therecognitionrateonTIMIT( ,,),bringingdownthe Mohamedetal.20092012a\n",
      "             phonemeerrorratefromabout26percentto20.7percent.SeeMohamedetal.\n",
      "              ()forananalysisofreasonsforthesuccessofthesemodels.Extensionstothe 2012b\n",
      "         basicphonerecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures\n",
      "            ( ,)thatfurtherreducedtheerrorrate.Thiswasquickly Mohamedetal.2011\n",
      "           followedupbyworktoexpandthearchitecturefromphonemerecognition(which\n",
      "            iswhatTIMITisfocusedon)tolarge-vocab ularyspeechrecognition( , Dahletal.\n",
      "          2012),whichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequences\n",
      "           ofwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n",
      "           shiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\n",
      "             ontechniquessuchasrectiﬁedlinearunitsanddropout( ,; Zeileretal.2013Dahl\n",
      "              etal.,).Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\n",
      "         startedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\n",
      "4 5 4  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "          etal.()describethebreakthroughsachievedbythesecollaborators,which 2012a\n",
      "        arenowdeployedinproductssuchasmobilephones.\n",
      "           Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\n",
      "            ratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\n",
      "           ofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\n",
      "       unnecessaryordidnotbringanysigniﬁcantimprovement.\n",
      "          Thesebreakthroughsinrecognitionperformanceforworderrorrateinspeech\n",
      "         recognitionwereunprecedented(around30percentimprovement)andwerefollow-\n",
      "              ingalongperiod,ofabouttenyears,duringwhicherrorratesdidnotimprove\n",
      "          muchwiththetraditionalGMM-HMMtechnology,inspiteofthecontinuously\n",
      "              growingsizeoftrainingsets(seeﬁgure2.4ofDengandYu2014[]).Thiscreated\n",
      "            arapidshiftinthespeechrecognitioncommunitytowarddeeplearning.Ina\n",
      "            matterofroughlytwoyears,mostoftheindustrialproductsforspeechrecognition\n",
      "            incorporateddeepneuralnetworks,andthissuccessspurredanewwaveofresearch\n",
      "           intodeeplearningalgorithmsandarchitecturesforASR,whichisongoingtoday.\n",
      "            Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainathetal.\n",
      "           2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\n",
      "          time-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\n",
      "         two-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\n",
      "               longvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\n",
      "   frequencyofspectralcomponents.\n",
      "          Anotherimportantpush,stillongoing,hasbeentowardend-to-enddeeplearning\n",
      "          speechrecognitionsystemsthatcompletelyremovetheHMM.Theﬁrstmajor\n",
      "             breakthroughinthisdirectioncamefromGraves2013etal.(),whotrainedadeep\n",
      "          LSTMRNN(seesection),usingMAPinferenceovertheframe-to-phoneme 10.10\n",
      "              alignment,asin ()andintheCTCframework( LeCunetal.1998b Gravesetal.,\n",
      "             2006Graves2012 Graves2013 ;,).AdeepRNN(etal.,)hasstatevariablesfrom\n",
      "             severallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\n",
      "             ordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. This\n",
      "              workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7percent.\n",
      "               SeePascanu2014a Chung2014 etal.()and etal.()forothervariantsofdeepRNNs,\n",
      "   appliedinothersettings.\n",
      "           Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\n",
      "          systemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level\n",
      "        information(Chorowski2014Lu2015 etal.,;etal.,).\n",
      "4 5 5  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "   12.4NaturalLanguageProcessing\n",
      "  Naturallanguageprocessing         (NLP)istheuseofhumanlanguages,suchas\n",
      "           EnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\n",
      "          specializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimple\n",
      "          programs.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\n",
      "       description. Naturallanguageprocessingincludesapplicationssuchasmachine\n",
      "             translation,inwhichthelearnermustreadasentenceinonehumanlanguageand\n",
      "          emitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\n",
      "           arebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequences\n",
      "        ofwords,characters,orbytesinanaturallanguage.\n",
      "           Aswiththeotherapplicationsdiscussedinthischapter,verygenericneural\n",
      "         networktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\n",
      "           Toachieveexcellentperformanceandtoscalewelltolargeapplications,however,\n",
      "          somedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelof\n",
      "           naturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\n",
      "             sequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\n",
      "             ofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\n",
      "            numberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\n",
      "         anextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\n",
      "             beendevelopedtomakemodelsofsuchaspaceeﬃcient,inbothacomputational\n",
      "   andastatisticalsense.\n",
      " 12.4.1-grams n\n",
      "A  languagemodel        deﬁnesaprobabilitydistributionoversequencesoftokens\n",
      "             inanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\n",
      "              beaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\n",
      "          earliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequences\n",
      "           oftokenscalled-grams.An-gramisasequenceoftokens. n n n\n",
      "  Modelsbasedonn      -gramsdeﬁnetheconditionalprobabilityofthen -thtoken\n",
      "  giventhepreceding n−        1tokens.Themodelusesproductsoftheseconditional\n",
      "        distributionstodeﬁnetheprobabilitydistributionoverlongersequences:\n",
      " Px( 1     ,...,x τ ) = (Px 1     ,...,x n − 1)τ\n",
      "t n = Px( t |x t n − +1     ,...,x t − 1 ).(12.5)\n",
      "           Thisdecompositionisjustiﬁedbythechainruleofprobability.Theprobability\n",
      "    distributionovertheinitialsequenceP(x 1     ,...,x n − 1      )maybemodeledbyadiﬀerent\n",
      "      modelwithasmallervalueof.n\n",
      "4 5 6  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "Trainingn       -grammodelsisstraightforwardbecausethemaximumlikelihood\n",
      "           estimatecanbecomputedsimplybycountinghowmanytimeseachpossible\n",
      "n        -gramoccursinthetrainingset.Modelsbasedonn    -gramshavebeenthecore\n",
      "          buildingblockofstatisticallanguagemodelingformanydecades(Jelinekand\n",
      "       Mercer1980Katz1987ChenandGoodman1999 ,;,; ,).\n",
      "   Forsmallvaluesofn    ,modelshaveparticularnames: uni g r a mforn =1,\n",
      "bi g r a mforn  =2,and t r i g r a mforn      =3. ThesenamesderivefromtheLatin\n",
      "          preﬁxesforthecorrespondingnumbersandtheGreeksuﬃx“-gram,”denoting\n",
      "   somethingthatiswritten.\n",
      "    Usuallywetrainbothann   -grammodelandann−   1 grammodelsimultaneously.\n",
      "     Thismakesiteasytocompute\n",
      " Px( t |x t n − +1     ,...,x t − 1) =P n(x t n − +1     ,...,x t)\n",
      "P n − 1(x t n − +1     ,...,x t − 1)(12.6)\n",
      "           simplybylookinguptwostoredprobabilities.Forthistoexactlyreproduce\n",
      " inferenceinP n           ,wemustomittheﬁnalcharacterfromeachsequencewhenwe\n",
      " trainP n − 1.\n",
      "           Asanexample,wedemonstratehowatrigrammodelcomputestheprobability\n",
      "   ofthesentence“    T H E D O G R A N A W A Y        .”Theﬁrstwordsofthesentencecannotbe\n",
      "            handledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\n",
      "             contextatthebeginningofthesentence.Instead,wemustusethemarginalprob-\n",
      "           abilityoverwordsatthestartofthesentence.WethusevaluateP 3(   T H E D O G R A N).\n",
      "              Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\n",
      " tionaldistributionP(    A W A Y D O G R A N|       ).Puttingthistogetherwithequation,12.6\n",
      " weobtain:\n",
      "    P P ( ) = T H E D O G R A N A W A Y 3   ( ) T H E D O G R A NP 3   ( ) D O G R A N A W A Y/P 2  ( ) D O G R A N.\n",
      "(12.7)\n",
      "      Afundamentallimitationofmaximumlikelihoodforn   -grammodelsisthatP n\n",
      "               asestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even\n",
      "   thoughthetuple(x t n − +1     ,...,x t          )mayappearinthetestset.Thiscancausetwo\n",
      "     diﬀerentkindsofcatastrophicoutcomes.WhenP n − 1     iszero,theratioisundeﬁned,\n",
      "          sothemodeldoesnotevenproduceasensibleoutput.WhenP n − 1  isnonzerobut\n",
      "P n     iszero,thetestlog-likelihoodis−∞    . Toavoidsuchcatastrophicoutcomes,\n",
      "mostn     -grammodelsemploysomeformof s m o o t hi ng  .Smoothingtechniques\n",
      "            shiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\n",
      "            See ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\n",
      "           techniqueconsistsofaddingnonzeroprobabilitymasstoallthepossiblenext\n",
      "            symbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform\n",
      "4 5 7  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "             orDirichletprioroverthecountparameters.Anotherverypopularideaistoform\n",
      "      amixturemodelcontaininghigher-orderandlower-ordern   -grammodels,withthe\n",
      "         higher-ordermodelsprovidingmorecapacityandthelower-ordermodelsbeing\n",
      "      morelikelytoavoidcountsofzero.  Back-oﬀmethods   lookupthelower-order\n",
      "n      -gramsifthefrequencyofthecontextx t − 1     ,...,x t n − +1     istoosmalltousethe\n",
      "        higher-ordermodel.Moreformally,theyestimatethedistributionoverx t byusing\n",
      "contextsx t n k − +     ,...,x t − 1  ,forincreasingk      ,untilasuﬃcientlyreliableestimateis\n",
      "found.\n",
      "Classicaln         -grammodelsareparticularlyvulnerabletothecurseofdimension-\n",
      "  ality.Thereare|| Vnpossiblen -gramsand|| V      isoftenverylarge.Evenwitha\n",
      "    massivetrainingsetandmodestn ,mostn       -gramswillnotoccurinthetrainingset.\n",
      "     Onewaytoviewaclassicaln        -grammodelisthatitisperformingnearestneighbor\n",
      "             lookup.Inotherwords,itcanbeviewedasalocalnonparametricpredictor,similar\n",
      "tok          -nearestneighbors.Thestatisticalproblemsfacingtheseextremelylocalpre-\n",
      "             dictorsaredescribedinsection .Theproblemforalanguagemodeliseven 5.11.2\n",
      "            moreseverethanusual,becauseanytwodiﬀerentwordshavethesamedistance\n",
      "             fromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuch\n",
      "         informationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythe\n",
      "           samecontextareusefulforlocalgeneralization.Toovercometheseproblems,a\n",
      "            languagemodelmustbeabletoshareknowledgebetweenonewordandother\n",
      "  semanticallysimilarwords.\n",
      "     Toimprovethestatisticaleﬃciencyofn -grammodels,  class-basedlanguage\n",
      "models             (Brown1992NeyandKneser1993Niesler1998 etal.,; ,; etal.,)introduce\n",
      "            thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\n",
      "               areinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\n",
      "            setofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\n",
      "             otherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\n",
      "             IDstorepresentthecontextontherightsideoftheconditioningbar.Composite\n",
      "          modelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀare\n",
      "           alsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences\n",
      "              inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\n",
      "   lostinthisrepresentation.\n",
      "   12.4.2NeuralLanguageModels\n",
      "  Neurallanguagemodels         ,orNLMs,areaclassoflanguagemodeldesigned\n",
      "          toovercomethecurseofdimensionalityproblemformodelingnaturallanguage\n",
      "           sequencesbyusingadistributedrepresentationofwords( ,). Bengioetal.2001\n",
      " Unlikeclass-basedn        -grammodels,neurallanguagemodelsareabletorecognize\n",
      "4 5 8  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "    − − − − − 34 32 30 28 26−14−13−12−11−10−9−8−7−6\n",
      "CanadaEuropeOnta rio\n",
      "NorthEnglish\n",
      "CanadianUnionAfrican Africa\n",
      "BritishFrance\n",
      "Russi anChina\n",
      "GermanyFrench\n",
      "AssemblyEU JapanIraq\n",
      "SouthEuropean\n",
      "      350355360365370375380 . . . . . . .171819202122\n",
      "19951996 19971998199920002001\n",
      "200220032004\n",
      "20052006200720082009\n",
      "          Figure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural\n",
      "            machinetranslationmodel( ,),zoominginonspeciﬁcareaswhere Bahdanauetal.2015\n",
      "            semanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\n",
      "                appearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\n",
      "           forthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\n",
      "          dimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\n",
      "              thattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\n",
      "          fromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone\n",
      "           word(anditscontext)andothersimilarwordsandcontexts.Thedistributed\n",
      "            representationthemodellearnsforeachwordenablesthissharingbyallowingthe\n",
      "             modeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\n",
      "worddog  andthewordcat       maptorepresentationsthatsharemanyattributes,then\n",
      "    sentencesthatcontainthewordcat       caninformthepredictionsthatwillbemade\n",
      "        bythemodelforsentencesthatcontaintheworddog     ,andviceversa.Becausethere\n",
      "            aremanysuchattributes,therearemanywaysinwhichgeneralizationcanhappen,\n",
      "         transferringinformationfromeachtrainingsentencetoanexponentiallylarge\n",
      "          numberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\n",
      "             modeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\n",
      "            length.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\n",
      "    exponentialnumberofsimilarsentences.\n",
      "     Wesometimescallthesewordrepresentations  wordembeddings  .Inthis\n",
      "             interpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal\n",
      "            tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\n",
      "            spaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\n",
      "           aone-hotvector,soeverypairofwordsisatEuclideandistance√\n",
      "2 fromeach\n",
      "           other.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n",
      "4 5 9  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "              (oranypairofwordssharingsome“features”learnedbythemodel)arecloseto\n",
      "           eachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\n",
      "              Figurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3\n",
      "            howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\n",
      "          Neuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,a\n",
      "          hiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”Usually\n",
      "           NLPpractitionersaremuchmoreinterestedinthisideaofembeddingsbecause\n",
      "            naturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\n",
      "             layerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\n",
      "represented.\n",
      "          Thebasicideaofusingdistributedrepresentationstoimprovemodelsfor\n",
      "            naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\n",
      "           usedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n",
      "      multiplelatentvariables( ,). MnihandHinton2007\n",
      "  12.4.3High-DimensionalOutputs\n",
      "           Inmanynaturallanguageapplications,weoftenwantourmodelstoproduce\n",
      "            words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\n",
      "          vocabularies,itcanbeverycomputationallyexpensivetorepresentanoutput\n",
      "              distributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\n",
      "applications, V          containshundredsofthousandsofwords.Thenaiveapproachto\n",
      "            representingsuchadistributionistoapplyanaﬃnetransformationfromahidden\n",
      "          representationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\n",
      "   wehaveavocabulary V withsize|| V      .Theweightmatrixdescribingthelinear\n",
      "           componentofthisaﬃnetransformationisverylarge,becauseitsoutputdimension\n",
      "is|| V             .Thisimposesahighmemory costtorepresentthematrix,andahigh\n",
      "            computationalcosttomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n",
      "|| V            outputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\n",
      "              timeaswellastesttime—wecannotcalculateonlythedotproductwiththeweight\n",
      "            vectorforthecorrectoutput.Thehighcomputationalcostsoftheoutputlayer\n",
      "             thusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and\n",
      "            attesttime(tocomputeprobabilitiesforallorselectedwords).Forspecialized\n",
      "            lossfunctions,thegradientcanbecomputedeﬃciently(Vincent2015etal.,),but\n",
      "           thestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\n",
      " manydiﬃculties.\n",
      " Supposethat h          isthetophiddenlayerusedtopredicttheoutputprobabilities\n",
      "ˆ y      .Ifweparametrizethetransformationfrom htoˆy  withlearnedweights W\n",
      "4 6 0  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "  andlearnedbiases b        ,thentheaﬃne-softmaxoutputlayerperformsthefollowing\n",
      "computations:\n",
      "a i= b i+\n",
      "jW i jh j        ∀∈{||} i1,..., V, (12.8)\n",
      "ˆy i=ea i\n",
      "|| V\n",
      "i =1eai  . (12.9)\n",
      "If hcontainsn h      elements,thentheaboveoperationisO(|| Vn h ).Withn h inthe\n",
      " thousandsand|| V        inthehundredsofthousands,thisoperationdominatesthe\n",
      "     computationofmostneurallanguagemodels.\n",
      "     12.4.3.1UseofaShortList\n",
      "              Theﬁrstneurallanguagemodels( ,,)dealtwiththehighcost Bengioetal.20012003\n",
      "              ofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\n",
      "            sizeto10,000or20,000words.SchwenkandGauvain2002 Schwenk2007 ()and ()\n",
      "       builtuponthisapproachbysplittingthevocabulary V intoa  shortlist L ofmost\n",
      "         frequentwords(handledbytheneuralnet)andatail T= V L\\   ofmorerarewords\n",
      "  (handledbyann         -grammodel). Tobeabletocombinethetwopredictions,the\n",
      "             neuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\n",
      "C               belongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\n",
      "     unittoprovideanestimateofP(    iC∈| T        ).Theextraoutputcanthenbeusedto\n",
      "             achieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V\n",
      "    PyiC (= |) =1 i∈ L              PyiC,iPiC (= |∈− L)(1 (∈| T ))\n",
      " +1 i∈ T             PyiC,iPiC, (= |∈ T)(∈| T ) (12.10)\n",
      "whereP(y=     iC,i|∈ L        )isprovidedbytheneurallanguagemodelandP(y= i|\n",
      "   C,i∈ T   ) isprovidedbythen      -grammodel.Withslightmodiﬁcation,thisapproach\n",
      "             canalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmax\n",
      "      layer,ratherthanaseparatesigmoidunit.\n",
      "            Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\n",
      "            alizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent\n",
      "          words,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulated\n",
      "         theexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\n",
      " describedbelow.\n",
      "4 6 1  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "  12.4.3.2HierarchicalSoftmax\n",
      "         Aclassicalapproach( ,)toreducingthecomputationalburden Goodman2001\n",
      "       ofhigh-dimensionaloutputlayersoverlargevocabularysets V  istodecompose\n",
      "        probabilitieshierarchically.Insteadofnecessitatinganumberofcomputations\n",
      " proportionalto|| V        (andalsoproportionaltothenumberofhiddenunits, n h),\n",
      "the|| V        factorcanbereducedtoaslowas  log|| V     . ()and Bengio2002 Morinand\n",
      "           Bengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\n",
      "models.\n",
      "            Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\n",
      "             ofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,andso\n",
      "              on.Thesenestedcategoriesformatree,withwordsattheleaves.Inabalanced\n",
      "    tree,thetreehasdepth O( log|| V        ). Theprobabilityofchoosingawordisgiven\n",
      "             bytheproductoftheprobabilitiesofchoosingthebranchleadingtothatword\n",
      "                 ateverynodeonapathfromtherootofthetreetotheleafcontainingtheword.\n",
      "          Figureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\n",
      "               howtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\n",
      "           thathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\n",
      "         summationoverallthepathsthatleadtothatword.\n",
      "            Topredicttheconditionalprobabilitiesrequiredateachnodeofthetree,we\n",
      "              typicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe\n",
      " samecontext C           asinputtoallthesemodels.Becausethecorrectoutputisencoded\n",
      "             inthetrainingset,wecanusesupervisedlearningtotrainthelogisticregression\n",
      "          models.Thisistypicallydoneusingastandardcross-entropyloss,corresponding\n",
      "         tomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\n",
      "          Becausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowas  log|| V\n",
      " ratherthan|| V          ),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnot\n",
      "            onlythegradientwithrespecttotheoutputparametersbutalsothegradients\n",
      "      withrespecttothehiddenlayeractivations.\n",
      "             Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\n",
      "          theexpectednumberofcomputations.Toolsfrominformationtheoryspecifyhow\n",
      "             tochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\n",
      "               doso,wecouldstructurethetreesothatthenumberofbitsassociatedwitha\n",
      "             wordisapproximatelyequaltothelogarithmofthefrequencyofthatword.In\n",
      "          practice,however,thecomputationalsavingsaretypicallynotworththeeﬀort\n",
      "             becausethecomputationoftheoutputprobabilitiesisonlyonepartofthetotal\n",
      "          computationintheneurallanguagemodel.Forexample,supposethereare lfully\n",
      "    connectedhiddenlayersofwidth n h .Let n b      betheweightedaverageofthenumber\n",
      "4 6 2  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "( 1) ( 0)\n",
      "       ( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\n",
      "w 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\n",
      "           Figure12.4:Illustrationofasimplehierarchyofwordcategories, with8wordsw 0,\n",
      "    ...,w 7            organizedintoathree-levelhierarchy.Theleavesofthetreerepresentactual\n",
      "             speciﬁcwords.Internalnodesrepresentgroupsofwords.Anynodecanbeindexedby\n",
      "                 thesequenceofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.\n",
      "     Super-classcontainstheclasses (0) (0,  0) (0and,     1),whichrespectivelycontainthesets\n",
      " ofwords{w 0 ,w 1}and{w 2 ,w 3}        ,andsimilarlysuper-classcontainstheclasses (1) (1,0)\n",
      " and(1,      1),whichrespectivelycontainthewords(w 4 ,w 5  ) (andw 6 ,w 7    ).Ifthetreeis\n",
      "            suﬃcientlybalanced,themaximumdepth(numberofbinarydecisions)isontheorder\n",
      "       ofthelogarithmofthenumberofwords|| V      :thechoiceofoneoutof|| V   wordscanbe\n",
      "                obtainedbydoing operations(oneforeachofthenodesonthepathfromthe O(log )|| V\n",
      "         root).Inthisexample,computingtheprobabilityofawordy    canbedonebymultiplying\n",
      "              threeprobabilities,associatedwiththebinarydecisionstomoveleftorrightateachnode\n",
      "        onthepathfromtheroottoanodey .Letbi(y ) bethei    -thbinarydecisionwhentraversing\n",
      "    thetreetowardthevaluey      .Theprobabilityofsamplinganoutputy   decomposesintoa\n",
      "           productofconditionalprobabilities,usingthechainruleforconditionalprobabilities,with\n",
      "            eachnodeindexedbythepreﬁxofthesebits.Forexample,node(1,   0)correspondstothe\n",
      " preﬁx(b 0(w4 ) = 1,b1(w4     ) = 0),andtheprobabilityofw 4    canbedecomposedasfollows:\n",
      "  Pw (= y 4 ) = (Pb 0 = 1,b 1 = 0,b 2  = 0) (12.11)\n",
      " = (Pb 0 = 1) (Pb 1 = 0 |b 0 = 1) (Pb 2 = 0 |b 0 = 1,b 1 = 0).(12.12)\n",
      "4 6 3  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "             ofbitsrequiredtoidentifyaword,withtheweightinggivenbythefrequency\n",
      "            ofthesewords.Inthisexample,thenumberofoperationsneededtocompute\n",
      "    thehiddenactivationsgrowsas O( l n2\n",
      "h      ),whiletheoutputcomputationsgrowas\n",
      "O( n h n b   ).Aslongas n b ≤ l n h       ,wecanreducecomputationmorebyshrinking n h\n",
      "  thanbyshrinking n b .Indeed, n b        isoftensmall.Becausethesizeofthevocabulary\n",
      "     rarelyexceedsamillionwordsandlog2(106)≈     20,itispossibletoreduce n bto\n",
      "  about,but20 n h     isoftenmuchlarger,around103    ormore.Ratherthancarefully\n",
      "               optimizingatreewithabranchingfactorof,onecaninsteaddeﬁneatreewith 2\n",
      "      depthtwoandabranchingfactorof\n",
      "|| V      .Suchatreecorrespondstosimply\n",
      "             deﬁningasetofmutuallyexclusivewordclasses.Thesimpleapproachbasedona\n",
      "            treeofdepthtwocapturesmostofthecomputationalbeneﬁtofthehierarchical\n",
      "strategy.\n",
      "            Onequestionthatremainssomewhatopenishowtobestdeﬁnetheseword\n",
      "             classes,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexisting\n",
      "            hierarchies( ,),butthehierarchycanalsobelearned,ideally MorinandBengio2005\n",
      "            jointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexact\n",
      "           optimizationofthelog-likelihoodappearsintractablebecausethechoiceofaword\n",
      "          hierarchyisadiscreteone,notamenabletogradient-basedoptimization.However,\n",
      "          onecouldusediscreteoptimizationtoapproximatelyoptimizethepartitionof\n",
      "   wordsintowordclasses.\n",
      "           Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\n",
      "                tionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewantto\n",
      "     computetheprobabilityofspeciﬁcwords.\n",
      "      Ofcourse,computingtheprobabilityofall|| V    wordswillremainexpensive\n",
      "          evenwiththehierarchicalsoftmax. Anotherimportantoperationisselectingthe\n",
      "            mostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\n",
      "        provideaneﬃcientandexactsolutiontothisproblem.\n",
      "            Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\n",
      "           testresultsthansampling-basedmethods,whichwedescribenext.Thismaybe\n",
      "       duetoapoorchoiceofwordclasses.\n",
      "  12.4.3.3ImportanceSampling\n",
      "              Onewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly\n",
      "             computingthecontributionofthegradientfromallthewordsthatdonotappear\n",
      "            inthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\n",
      "            model.Itcanbecomputationallycostlytoenumerateallthesewords.Instead,it\n",
      "              ispossibletosampleonlyasubsetofthewords.Usingthenotationintroducedin\n",
      "4 6 4  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "        equation,thegradientcanbewrittenasfollows: 12.8\n",
      "     ∂PyC log(|)\n",
      "∂θ=  ∂logsoftmax y() a\n",
      "∂θ(12.13)\n",
      "=∂\n",
      "∂θlogea y\n",
      "\n",
      "iea i(12.14)\n",
      "=∂\n",
      "∂θ(a y −log\n",
      "iea i ) (12.15)\n",
      "=∂a y\n",
      "∂θ−\n",
      "i    PyiC (= |)∂a i\n",
      "∂θ , (12.16)\n",
      "where a          isthevectorofpresoftmaxactivations(orscores),withoneelement\n",
      "     perword. Theﬁrsttermisthe  positivephase term,pushinga y  up,whilethe\n",
      "   secondtermisthe  negativephase term,pushinga i  downforalli  ,withweight\n",
      "P(  iC|             ).Sincethenegativephasetermisanexpectation,wecanestimateitwith\n",
      "            aMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\n",
      "     SamplingfromthemodelrequirescomputingP(  iC|  )foralli  inthevocabulary,\n",
      "        whichispreciselywhatwearetryingtoavoid.\n",
      "           Insteadofsamplingfromthemodel,wecansamplefromanotherdistribution,\n",
      " called theproposal distribution (denoted q  ),and use appropri ateweights to\n",
      "           correctforthebiasintroducedbysamplingfromthewrongdistribution(Bengio\n",
      "            andSénécal2003BengioandSénécal2008 ,; ,). Thisisanapplicationofamore\n",
      "  generaltechniquecalled  importancesampling      ,whichwedescribeinmoredetail\n",
      "          insection.Unfortunately,evenexactimportancesamplingisnoteﬃcient 17.2\n",
      "    becauseitrequirescomputingweightsp i/q i ,wherep i=P(  iC|  ),whichcan\n",
      "      onlybecomputedifallthescoresa i     arecomputed.Thesolutionadoptedfor\n",
      "   thisapplicationiscalled  biasedimportancesampling   ,wheretheimportance\n",
      "         weightsarenormalizedtosumto1.Whennegativewordn i  issampled,the\n",
      "    associatedgradientisweightedby\n",
      "w i=p n i/q n iN\n",
      "j =1p n j/q n j . (12.17)\n",
      "          Theseweightsareusedtogivetheappropriateimportancetothemnegative\n",
      " samplesfromq         usedtoformtheestimatednegativephasecontributiontothe\n",
      "gradient:\n",
      "|| V\n",
      "i =1   PiC(|)∂a i\n",
      "∂θ≈1\n",
      "mm\n",
      "i =1w i∂a n i\n",
      "∂θ . (12.18)\n",
      "4 6 5  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           Aunigramorabigramdistributionworkswellastheproposaldistributionq  .Itis\n",
      "            easytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\n",
      "             theparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently.\n",
      "           Importancesamplingisnotonlyusefulforspeedingupmodelswithlarge\n",
      "           softmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge\n",
      "             sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1n\n",
      "    choice.Anexampleisa  bagofwords        .Abagofwordsisasparsevector v\n",
      "wherev i       indicatesthepresenceorabsenceofwordi    fromthevocabularyinthe\n",
      " document.Alternately,v i       canindicatethenumberoftimesthatwordiappears.\n",
      "            Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\n",
      "              foravarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\n",
      "             maketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight\n",
      "             mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\n",
      "                everyelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\n",
      "           computationalbeneﬁttousingsparseoutputs,becausethemodelmaychooseto\n",
      "              makethemajorityoftheoutputnonzero,andallthesenon-zerovaluesneedtobe\n",
      "            comparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\n",
      "           Dauphin 2011etal.()demonstratedthatsuchmodelscanbeacceleratedusing\n",
      "         importancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionfor\n",
      "              the“positivewords”(thosethatarenonzerointhetarget)andanequalnumberof\n",
      "           “negativewords.”Thenegativewordsarechosenrandomly,usingaheuristicto\n",
      "            samplewordsthataremorelikelytobemistaken. Thebiasintroducedbythis\n",
      "        heuristicoversamplingcanthenbecorrectedusingimportanceweights.\n",
      "          Inallthesecases,thecomputationalcomplexityofgradientestimationfor\n",
      "             theoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\n",
      "         ratherthanproportionaltothesizeoftheoutputvector.\n",
      "     12.4.3.4Noise-ContrastiveEstimationandRankingLoss\n",
      "           Otherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\n",
      "           tionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\n",
      "           exampleistherankinglossproposedbyCollobertandWeston2008a(),which\n",
      "                viewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\n",
      "      makethescoreofthecorrectworda y       berankedhighincomparisontotheother\n",
      " scoresa i      .Therankinglossproposedthenis\n",
      " L=\n",
      "i   max(01,−a y +a i ). (12.19)\n",
      "4 6 6  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "     Thegradientiszeroforthe i        -thtermifthescoreoftheobservedword, a y ,is\n",
      "       greaterthanthescoreofthenegativeword a i       byamarginof1.Oneissuewith\n",
      "           thiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities,which\n",
      "          areusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n",
      "       (includingconditionaltextgenerationtaskssuchastranslation).\n",
      "           Amorerecentlyusedtrainingobjectiveforneurallanguagemodelsisnoise-\n",
      "          contrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\n",
      "           beensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih ,;\n",
      "  andKavukcuoglu2013,).\n",
      "      12.4.4CombiningNeuralLanguageModelswith-grams n\n",
      "   Amajoradvantageof n      -grammodelsoverneuralnetworksisthat n -grammodels\n",
      "           achievehighmodelcapacity(bystoringthefrequenciesofverymanytuples),\n",
      "           whilerequiringverylittlecomputationtoprocessanexample(bylookingup\n",
      "               onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\n",
      "       toaccessthecounts,thecomputationusedfor n   -gramsisalmostindependent\n",
      "          ofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameters\n",
      "         typicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\n",
      "             thatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\n",
      "            embeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\n",
      "           thecomputationtimeperexample.Someothermodels,suchastiledconvolutional\n",
      "           networks,canaddparameterswhilereducingthedegreeofparametersharingto\n",
      "          maintainthesameamountofcomputation.Typicalneuralnetworklayersbased\n",
      "          onmatrixmultiplication,however,useanamountofcomputationproportionalto\n",
      "   thenumberofparameters.\n",
      "              Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\n",
      "       consistingofaneurallanguagemodelandan n   -gramlanguagemodel(Bengio\n",
      "              etal.,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003\n",
      "          theensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearning\n",
      "         providesmanywaysofcombiningtheensemblemembers’predictions,including\n",
      "            uniformweightingandweightschosenonavalidationset.Mikolov 2011aetal.()\n",
      "              extendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\n",
      "              Itisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\n",
      "             trainbothjointly(Mikolov2011betal.,).Thisapproachcanbeviewedastraining\n",
      "              aneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\n",
      "              outputandnotconnectedtoanyotherpartofthemodel.Theextrainputsare\n",
      "     indicatorsforthepresenceofparticular n      -gramsintheinputcontext,sothese\n",
      "            variablesareveryhighdimensionalandverysparse.Theincreaseinmodelcapacity\n",
      "4 6 7  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "         ishuge—thenewportionofthearchitecturecontainsupto ||sVnparameters—but\n",
      "            theamountofaddedcomputationneededtoprocessaninputisminimalbecause\n",
      "     theextrainputsareverysparse.\n",
      "   12.4.5NeuralMachineTranslation\n",
      "             Machinetranslationisthetaskofreadingasentenceinonenaturallanguageand\n",
      "         emittingasentencewiththeequivalentmeaninginanotherlanguage. Machine\n",
      "           translationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\n",
      "          oftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\n",
      "           translationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.For\n",
      "           example,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish\n",
      "           directlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggests\n",
      "           manyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecond\n",
      "          componentofthetranslationsystem,alanguagemodel,evaluatestheproposed\n",
      "          translationsandcanscore“redapple”asbetterthan“applered.”\n",
      "         Theearliestexplorationofneuralnetworksformachinetranslationalready\n",
      "           incorporatedtheideaofencoderanddecoder( ; ; Allen1987Chrisman1991Forcada\n",
      "            andÑeco1997),whiletheﬁrstlarge-scalecompetitiveuseofneuralnetworksin\n",
      "             translationwastoupgradethelanguagemodelofatranslationsystembyusinga\n",
      "         neurallanguagemodel(Schwenk2006Schwenk2010 etal.,; ,). Previously,most\n",
      "     machinetranslationsystemshadusedann     -grammodelforthiscomponent.The\n",
      "n          -gram-basedmodelsusedformachinetranslationincludenotjusttraditional\n",
      "back-oﬀn          -grammodels( ,;,; , JelinekandMercer1980Katz1987ChenandGoodman\n",
      "  1999)butalso    maximumentropylanguagemodels     ( ,),in Bergeretal.1996\n",
      "            whichanaﬃne-softmaxlayerpredictsthenextwordgiventhepresenceoffrequent\n",
      "   n-gramsinthecontext.\n",
      "          Traditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\n",
      "         sentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven\n",
      "             aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\n",
      "            conditional.Asdescribedinsection ,itisstraightforwardtoextendamodel 6.2.1.1\n",
      "           thatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditional\n",
      "      distributionoverthatvariablegivenacontextC ,whereC    mightbeasinglevariable\n",
      "              oralistofvariables. ()beatthestate-of-the-artinsomestatistical Devlinetal.2014\n",
      "          machinetranslationbenchmarksbyusinganMLPtoscoreaphraset 1 ,t 2     ,...,t k\n",
      "      inthetargetlanguagegivenaphrases 1 ,s 2     ,...,s n    inthesourcelanguage.The\n",
      " MLPestimatesP (t 1 ,t 2     ,...,t k |s 1 ,s 2     ,...,s n      ).TheestimateformedbythisMLP\n",
      "       replacestheestimateprovidedbyconditional-grammodels. n\n",
      "4 6 8  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "DecoderOutput object (English \n",
      "sentence)\n",
      "Intermediate, semantic representation\n",
      "Source object (French sentence or image)Encoder\n",
      "            Figure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\n",
      "             representation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\n",
      "                Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping\n",
      "           fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\n",
      "              theinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\n",
      "            representationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\n",
      "             translatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\n",
      "         tomachinetranslationbutalsotocaptiongenerationfromimages.\n",
      "             AdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\n",
      "             preprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewould\n",
      "            liketouseamodelthatcanaccommo datevariablelengthinputsandvariable\n",
      "           lengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\n",
      "           ofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\n",
      "           givensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\n",
      "               whentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequence\n",
      "            andemitsadatastructurethatsummarizestheinputsequence.Wecallthis\n",
      "  summarythe“context” C  .Thecontext C          maybealistofvectors,oritmaybea\n",
      "          vectorortensor.Themodelthatreadstheinputtoproduce C   maybeanRNN\n",
      "              ( ,; Choetal.2014aSutskever2014Jean 2014 etal.,;etal.,)oraconvolutional\n",
      "          network(KalchbrennerandBlunsom2013,).Asecondmodel,usuallyanRNN,\n",
      "   thenreadsthecontext C        andgeneratesasentenceinthetargetlanguage.This\n",
      "          generalideaofanencoder-decoderframeworkformachinetranslationisillustrated\n",
      "  inﬁgure.12.5\n",
      "           Togenerateanentiresentenceconditionedonthesourcesentence,themodel\n",
      "             musthaveawaytorepresenttheentiresourcesentence.Earliermodelswereable\n",
      "           torepresentonlyindividualwordsorphrases.Fromarepresentationlearningpoint\n",
      "4 6 9  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "               ofview,itcanbeusefultolearnarepresentationinwhichsentencesthathavethe\n",
      "          samemeaninghavesimilarrepresentationsregardlessofwhethertheywerewritten\n",
      "             inthesourcelanguageorinthetargetlanguage.Thisstrategywasexploredﬁrst\n",
      "          usingacombinationofconvolutionsandRNNs(KalchbrennerandBlunsom2013,).\n",
      "            LaterworkintroducedtheuseofanRNNforscoringproposedtranslations(Cho\n",
      "            etal.,)andforgeneratingtranslatedsentences( 2014a Sutskever2014Jean etal.,).\n",
      "        etal.()scaledthesemodelstolargervocabularies. 2014\n",
      "         12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\n",
      "            Usingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofavery\n",
      "               longsentenceof,say,60wordsisverydiﬃcult.Itcanbeachievedbytraininga\n",
      "            suﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\n",
      "             etal.()and 2014a Sutskever2014etal.().Amoreeﬃcientapproach,however,is\n",
      "               toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n",
      "             isbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\n",
      "             focusingonadiﬀerentpartoftheinputsentencetogatherthesemanticdetails\n",
      "             requiredtoproducethenextoutputword.ThatisexactlytheideathatBahdanau\n",
      "            etal.()ﬁrstintroduced.Theattentionmechanismusedtofocusonspeciﬁc 2015\n",
      "             partsoftheinputsequenceateachtimestepisillustratedinﬁgure.12.6\n",
      "          Wecanthinkofanattention-basedsystemashavingthreecomponents:\n",
      "1.             Aprocessthatreadsrawdata(suchassourcewordsinasourcesentence)\n",
      "         andconvertsthemintodistributedrepresentations,withonefeaturevector\n",
      "    associatedwitheachwordposition.\n",
      "2.            Alistoffeaturevectorsstoring theoutputofthereader.Thiscanbe\n",
      "            understoodasa containingasequenceoffacts,whichcanberetrieved memory\n",
      "             later,notnecessarilyinthesameorder,withouthavingtovisitallofthem.\n",
      "3.           Aprocessthat thecontentofthememory tosequentiallyperform exploits\n",
      "              atask,ateachtimestephavingtheabilityputattentiononthecontentof\n",
      "         onememory element(orafew,withadiﬀerentweight).\n",
      "      Thethirdcomponentgeneratesthetranslatedsentence.\n",
      "            Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\n",
      "             ingwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\n",
      "           thecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\n",
      "            kindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe\n",
      "           wordembeddingsinanother(Kočiský2014etal.,),yieldingloweralignmenterror\n",
      "4 7 0  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "α( t − 1 )α( t − 1 )α( ) tα( ) tα( + 1 ) tα( + 1 ) t\n",
      "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\n",
      "× × × × × ×+\n",
      "             Figure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanauetal.2015\n",
      "      essentiallyaweightedaverage.Acontextvector c      isformedbytakingaweightedaverage\n",
      "  offeaturevectors h( ) t withweights α( ) t      .Insomeapplications,thefeaturevectors hare\n",
      "                hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\n",
      "weights α( ) t            areproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n",
      "[0 ,        1]andareintendedtoconcentratearoundjustone h( ) t    sothattheweightedaverage\n",
      "         approximatesreadingthatonespeciﬁctimestepprecisely.Theweights α( ) t areusually\n",
      "            producedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\n",
      "           ofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly\n",
      "  indexingthedesired h( ) t          ,butdirectindexingcannotbetrainedwithgradientdescent.The\n",
      "          attentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximation\n",
      "       thatcanbetrainedwithexistingoptimizationalgorithms.\n",
      "4 7 1  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            ratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\n",
      "            Thereisevenearlierworkonlearningcross-lingualwordvectors( , Klementievetal.\n",
      "           2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcient\n",
      "          cross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouwsetal.2014\n",
      "  12.4.6HistoricalPerspective\n",
      "          TheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart\n",
      "            etal.()inoneoftheﬁrstexplorationsofback-propagation,withsymbols 1986a\n",
      "           correspondingtotheidentityoffamilymembers,andtheneuralnetworkcapturing\n",
      "         therelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\n",
      "             suchas(Colin,Mother,Victoria).Theﬁrstlayeroftheneuralnetworklearneda\n",
      "           representationofeachfamilymember.Forexample,thefeaturesforColinmight\n",
      "              representwhichfamilytreeColinwasin,whatbranchofthattreehewasin,\n",
      "               whatgenerationhewasfrom,andsoon.Onecanthinkoftheneuralnetworkas\n",
      "          computinglearnedrulesrelatingtheseattributestogethertoobtainthedesired\n",
      "            predictions.Themodelcanthenmakepredictionssuchasinferringwhoisthe\n",
      "  motherofColin.\n",
      "               Theideaofforminganembeddingforasymbolwasextendedtotheideaofan\n",
      "            embeddingforawordby ().Theseembeddingswerelearned Deerwesteretal.1990\n",
      "          usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\n",
      "           Thehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\n",
      "           popularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Following\n",
      "             thisearlyworkonsymbolsandwords,someoftheearliestapplicationsofneural\n",
      "          networkstoNLP( ,; MiikkulainenandDyer1991SchmidhuberandHeil1996,)\n",
      "       representedtheinputasasequenceofcharacters.\n",
      "           Bengio 2001etal.()returnedthefocustomodelingwordsandintroduced\n",
      "        neurallanguagemodels,whichproduceinterpretablewordembeddings.These\n",
      "             neuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbols\n",
      "            inthe1980stomillionsofwords(includingpropernounsandmisspellings)in\n",
      "           modernapplications.Thiscomputationalscalingeﬀortledtotheinventionofthe\n",
      "    techniquesdescribedinsection .12.4.3\n",
      "            Initially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\n",
      "        improvedlanguagemodelingperformance( , ).To thisday, Bengioetal.2001\n",
      "         newtechniquescontinuallypushbothcharacter-basedmodels(Sutskeveretal.,\n",
      "            2011)andword-basedmodelsforward,withrecentwork(Gillick2015etal.,)even\n",
      "     modelingindividualbytesofUnicodecharacters.\n",
      "          Theideasbehindneurallanguagemodelshavebeenextendedintoseveral\n",
      "4 7 2  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "         naturallanguageprocessingapplications,suchasparsing( ,,; Henderson20032004\n",
      "        Collobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,and\n",
      "          soon,sometimesusingasinglemultitasklearningarchitecture(Collobertand\n",
      "            Weston2008aCollobert2011a ,; etal.,)inwhichthewordembeddingsareshared\n",
      " acrosstasks.\n",
      "         Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\n",
      "         alyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\n",
      "           reductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-\n",
      "         cationtovisualizationwordembeddingsbyJosephTurianin2009.\n",
      "  12.5OtherApplications\n",
      "              Inthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\n",
      "          arediﬀerentfromthestandardobjectrecognition,speechrecognition,andnatural\n",
      "            languageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\n",
      "         scopeevenfurthertotasksthatremainprimarilyresearchareas.\n",
      "  12.5.1RecommenderSystems\n",
      "            Oneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\n",
      "           technologysectoristheabilitytomakerecommendati onsofitemstopotential\n",
      "           usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\n",
      "         advertisinganditemrecommendati ons(oftentheserecommendati onsarestillfor\n",
      "            thepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\n",
      "              auserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\n",
      "             buyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\n",
      "               maydependonthevalueoftheproduct)ifanadisshownorarecommendati on\n",
      "           ismaderegardingthatproducttothatuser. Theinternetiscurrentlyﬁnanced\n",
      "             ingreatpartbyvariousformsofonlineadvertising.Majorpartsoftheeconomy\n",
      "          relyononlineshopping.CompaniesincludingAmazon andeBayusemachine\n",
      "        learning,includingdeeplearning,fortheirproductrecommendati ons.Sometimes,\n",
      "            theitemsarenotproductsthatareactuallyforsale.Examplesincludeselecting\n",
      "           poststodisplayonsocialnetworknewsfeeds,recommendi ngmoviestowatch,\n",
      "        recommendi ngjokes,recommendi ngadvicefromexperts,matchingplayersfor\n",
      "       videogames,ormatchingpeopleindatingservices.\n",
      "          Often,thisassociationproblemishandledlikeasupervisedlearningproblem:\n",
      "             givensomeinformationabouttheitemandabouttheuser,predicttheproxyof\n",
      "               interest(userclicksonad,userentersarating,userclicksona“like”button,user\n",
      "4 7 3  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            buysproduct,userspendssomeamountofmoneyontheproduct,userspends\n",
      "              timevisitingapagefortheproduct,andsoforth).Thisoftenendsupbeing\n",
      "          eitheraregressionproblem(predictingsomeconditionalexpectedvalue)ora\n",
      "        probabilisticclassiﬁcationproblem(predictingtheconditionalprobabilityofsome\n",
      " discreteevent).\n",
      "          Theearlyworkonrecommender systemsreliedonminimalinformationas\n",
      "              inputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\n",
      "               onlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\n",
      "             thetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1\n",
      "                  anduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n",
      "                user2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\n",
      "              cuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\n",
      "  thenameof  collaborativeﬁltering     . Bothnonparametricapproaches(suchas\n",
      "          nearestneighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\n",
      "         preferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\n",
      "           onlearningadistributedrepresentation(alsocalledanembedding)foreachuser\n",
      "               andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\n",
      "            simpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\n",
      "           ofstate-of-the-artsystems.Thepredictionisobtainedbythedotproductbetween\n",
      "           theuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\n",
      "           dependonlyoneithertheuserIDortheitemID).Letˆ R   bethematrixcontaining\n",
      " ourpredictions, A        amatrixwithuserembeddingsinitsrows,and B  amatrixwith\n",
      "     itemembeddingsinitscolumns.Let band c    bevectorsthatcontainrespectively\n",
      "              akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\n",
      "           ingeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\n",
      "     predictionisthusobtainedasfollows:\n",
      "ˆR u , i= b u +c i+\n",
      "jA u , jB j , i . (12.20)\n",
      "          Typicallyonewantstominimizethesquarederrorbetweenpredictedratings\n",
      "ˆR u , i  andactualratingsR u , i        .Userembeddingsanditemembeddingscanthenbe\n",
      "            convenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoor\n",
      "              three),ortheycanbeusedtocompareusersoritemsagainsteachother,just\n",
      "           likewordembeddings. Onewaytoobtaintheseembeddingsisbyperforminga\n",
      "     singularvaluedecompositionofthematrix R     ofactualtargets(suchasratings).\n",
      "   Thiscorrespondstofactorizing R= U D V     (oranormalizedvariant)intothe\n",
      "       productoftwofactors,thelowerrankmatrices A= U Dand B= V .One\n",
      "              problemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\n",
      "               asiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\n",
      "4 7 4  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            payinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum\n",
      "             ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\n",
      "           basedoptimization.TheSVDandthebilinearpredictionofequation both 12.20\n",
      "            performedverywellinthecompetitionfortheNetﬂixprize( , BennettandLanning\n",
      "            2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsby\n",
      "          alargesetofanonymoususers. Manymachinelearningexpertsparticipatedin\n",
      "             thiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\n",
      "         researchinrecommender systemsusingadvancedmachinelearningandyielded\n",
      "         improvementsinrecommender systems.Eventhoughthesimplebilinearprediction,\n",
      "               orSVD,didnotwinbyitself,itwasacomponentoftheensemblemodelspresented\n",
      "            bymostofthecompetitors,includingthewinners( ,;, Töscheretal.2009Koren\n",
      "2009).\n",
      "          Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrst\n",
      "            usesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirected\n",
      "          probabilisticmodel(Salakhutdinov2007etal.,).RBMswereanimportantelement\n",
      "             oftheensembleofmethodsthatwontheNetﬂixcompetition( ,; Töscheretal.2009\n",
      "            Koren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix\n",
      "          havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\n",
      " Mnih2008,).\n",
      "          Collaborativeﬁlteringsystemshaveabasiclimitation,however:whenanew\n",
      "                itemoranewuserisintroduced,itslackofratinghistorymeansthatthereisno\n",
      "              waytoevaluateitssimilaritywithotheritemsorusers,orthedegreeofassociation\n",
      "              between,say,thatnewuserandexistingitems.Thisiscalledtheproblemofcold-\n",
      "         startrecommendati ons.Ageneralwayofsolvingthecold-startrecommendati on\n",
      "            problemistointroduceextrainformationabouttheindividualusersanditems.For\n",
      "            example,thisextrainformationcouldbeuserproﬁleinformationorfeaturesofeach\n",
      "       item.Systemsthatusesuchinformationarecalled  content-basedrecommender\n",
      "systems              .Themappingfromarichsetofuserfeaturesoritemfeaturestoan\n",
      "            embeddingcanbelearnedthroughadeeplearningarchitecture( ,; Huangetal.2013\n",
      "   Elkahky2015etal.,).\n",
      "        Specializeddeeplearningarchitectures,suchasconvolutionalnetworks,have\n",
      "             alsobeenappliedtolearntoextractfeaturesfromrichcontent,suchasfrom\n",
      "           musicalaudiotracksformusicrecommendati on(vandenOörd 2013etal.,). In\n",
      "            thatwork,theconvolutionalnettakesacousticfeaturesasinputandcomputesan\n",
      "           embeddingfortheassociatedsong.Thedotproductbetweenthissongembedding\n",
      "                andtheembeddingforauseristhenusedtopredictwhetherauserwilllistento\n",
      " thesong.\n",
      "4 7 5  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "   12.5.1.1ExplorationversusExploitation\n",
      "           Whenmakingrecommendati onstousers,anissuearisesthatgoesbeyondordinary\n",
      "         supervisedlearningandintotherealmofreinforcementlearning. Manyrecom-\n",
      "       mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\n",
      "bandits              ( ,; ,).Theissueisthatwhenwe LangfordandZhang2008Luetal.2010\n",
      "            usetherecommendati onsystemtocollectdata,wegetabiasedandincomplete\n",
      "               viewofthepreferencesofusers:weseetheresponsesofusersonlytotheitems\n",
      "              recommended tothemandnottotheotheritems.Inaddition,insomecaseswe\n",
      "            maynotgetanyinformationonusersforwhomnorecommendati onhasbeen\n",
      "               made(forexample,withadauctions,itmaybethatthepriceproposedforanad\n",
      "               wasbelowaminimumpricethreshold,ordoesnotwintheauction,sotheadis\n",
      "            notshownatall).Moreimportantly,wegetnoinformationaboutwhatoutcome\n",
      "             wouldhaveresultedfromrecommendi nganyoftheotheritems.Thiswouldbelike\n",
      "      trainingaclassiﬁerbypickingoneclassˆ y   foreachtrainingexample x  (typicallythe\n",
      "             classwiththehighestprobabilityaccordingtothemodel)andthenonlygettingas\n",
      "            feedbackwhetherthiswasthecorrectclassornot.Clearly,eachexampleconveys\n",
      "          lessinformationthaninthesupervisedcase,wherethetruelabel y isdirectly\n",
      "             accessible,somoreexamplesarenecessary.Worse,ifwearenotcareful,wecould\n",
      "             endupwithasystemthatcontinuespickingthewrongdecisionsevenasmore\n",
      "             andmoredataiscollected,becausethecorrectdecisioninitiallyhadaverylow\n",
      "            probability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearnabout\n",
      "           thecorrectdecision.Thisissimilartothesituationinreinforcementlearning\n",
      "            whereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement\n",
      "            learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\n",
      "             scenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\n",
      "              asingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\n",
      "            sensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\n",
      "            thegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\n",
      "                havebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\n",
      " contextualbandits            referstothecasewheretheactionistakeninthecontextof\n",
      "             someinputvariablethatcaninformthedecision.Forexample,weatleastknow\n",
      "              theuseridentity,andwewanttopickanitem.Themappingfromcontextto\n",
      "    actionisalsocalledapolicy         .Thefeedbackloopbetweenthelearnerandthedata\n",
      "             distribution(whichnowdependsontheactionsofthelearner)isacentralresearch\n",
      "       issueinthereinforcementlearningandbanditsliterature.\n",
      "      Reinforcementlearningrequireschoosingatrade-oﬀbetweenexplorationand\n",
      "exploitation          .Exploitationreferstotakingactionsthatcomefromthecurrent,\n",
      "             bestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward.\n",
      "4 7 6  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           Explorationreferstotakingactionsspeciﬁcallytoobtainmoretrainingdata.If\n",
      "    weknowthatgivencontext x ,action a         givesusarewardof1,wedonotknow\n",
      "             whetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\n",
      "    policyandcontinuetakingaction a         toberelativelysureofobtainingarewardof1.\n",
      "         However,wemayalsowanttoexplorebytryingaction a     .Wedonotknowwhat\n",
      "     willhappenifwetryaction a             .Wehopetogetarewardof,butweruntherisk 2\n",
      "             ofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\n",
      "         Explorationcanbeimplementedinmanyways,rangingfromoccasionally\n",
      "            takingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\n",
      "           model-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\n",
      "         rewardandthemodel’samountofuncertaintyaboutthatreward.\n",
      "           Manyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\n",
      "              Oneofthemostprominentfactorsisthetimescaleweareinterestedin. Ifthe\n",
      "              agenthasonlyashortamountoftimetoaccruereward,thenweprefermore\n",
      "              exploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\n",
      "            moreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmore\n",
      "           knowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\n",
      " moreexploitation.\n",
      "        Supervisedlearninghasnotrade-oﬀbetweenexplorationandexploitation\n",
      "           becausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeach\n",
      "               input.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetter\n",
      "             thanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.\n",
      "          Anotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidesthe\n",
      "        exploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparing\n",
      "        diﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\n",
      "            andtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\n",
      "             evaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.The\n",
      "            policyitselfdetermineswhichinputswillbeseen. ()present Dudiketal.2011\n",
      "    techniquesforevaluatingcontextualbandits.\n",
      "     12.5.2KnowledgeRepresentation,ReasoningandQuestion\n",
      "Answering\n",
      "          Deeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine\n",
      "           translationandnaturallanguageprocessingbecauseoftheuseofembeddingsfor\n",
      "             symbols( ,)andwords( Rumelhartetal.1986a Deerwester1990Bengio etal.,;etal.,\n",
      "        2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\n",
      "            andconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\n",
      "4 7 7  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           relationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\n",
      "             thispurpose,butmuchmoreremainstobedonetoimprovethesemoreadvanced\n",
      "representations.\n",
      "     12.5.2.1Knowledge,RelationsandQuestionAnswering\n",
      "        Oneinterestingresearchdirectionisdetermininghowdistributedrepresentations\n",
      "     canbetrainedtocapturetherelations     betweentwoentities.Theserelations\n",
      "             allowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\n",
      "  Inmathematics,a binaryrelation        isasetoforderedpairsofobjects.Pairs\n",
      "                  thatareinthesetaresaidtohavetherelationwhilethosenotinthesetdonot.\n",
      "              Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities{1,2,3}\n",
      "      bydeﬁningthesetoforderedpairs S={(1,2),(1,3),(2,3)}    .Oncethisrelationis\n",
      "        deﬁned,wecanuseitlikeaverb. Because(1,2) ∈ S       ,wesaythat1islessthan\n",
      "  2.Because(2,1) ∈ S             ,wecannotsaythat2islessthan1.Ofcourse,theentities\n",
      "              thatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnearelation\n",
      "     i s _ a _ t y p e _ o fcontainingtupleslike(,). d o g m a m m a l\n",
      "               InthecontextofAI,wethinkofarelationasasentenceinasyntactically\n",
      "            simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\n",
      "              whiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\n",
      "        sentencestaketheformofatripletoftokens\n",
      "   (subjectverbobject) ,, (12.21)\n",
      " withvalues\n",
      "(entityi ,relation j ,entityk ). (12.22)\n",
      "    Wecanalsodeﬁneanattribute        ,aconceptanalogoustoarelation,buttaking\n",
      "  onlyoneargument:\n",
      "(entityi ,attribute j ). (12.23)\n",
      "     Forexample,wecoulddeﬁnethe h a s _ f u r       attribute,andapplyittoentitieslike\n",
      "d o g.\n",
      "        Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\n",
      "           Howshouldwebestdothiswithinthecontextofneuralnetworks?\n",
      "           Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\n",
      "         betweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\n",
      "          Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\n",
      "     structureforthesedatabasesisthe  relationaldatabase    ,whichstoresthissame\n",
      "       kindof information, albeitnot formatte dasthree tokensentences.Whena\n",
      "4 7 8  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "          databaseisintendedtoconveycommonsenseknowledgeabouteverydaylifeor\n",
      "           expertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,we\n",
      "   callthedatabasea  knowledgebase      .Knowledgebasesrangefromgeneralones\n",
      "likeFreebase,OpenCyc,WordNet,Wikibase,1     andsoforth,tomorespecialized\n",
      "  knowledgebaseslikeGeneOntology.2     Representationsforentitiesandrelationscan\n",
      "             belearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\n",
      "          andmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes\n",
      "  etal.,).2013a\n",
      "              Inadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.\n",
      "            Acommonapproachistoextendneurallanguagemodelstomodelentitiesand\n",
      "          relations.Neurallanguagemodelslearnavectorthatprovidesadistributed\n",
      "          representationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\n",
      "               suchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\n",
      "             ofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\n",
      "           anembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling\n",
      "           languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\n",
      "           havetrainedrepresentationsofsuchentitiesbyusingbothknowledgebasesand\n",
      "             naturallanguagesentences( ,,; Bordesetal.20112012Wang2014aetal.,),orby\n",
      "          combiningdatafrommultiplerelationaldatabases( ,).Many Bordesetal.2013b\n",
      "          possibilitiesexistfortheparticularparametrizationassociatedwithsuchamodel.\n",
      "          Earlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton\n",
      "        2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),\n",
      "             oftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities.\n",
      "             Forexample, ()and ()usedvectorsfor PaccanaroandHinton2000 Bordesetal.2011\n",
      "              entitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\n",
      "           onentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\n",
      "             etal.,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis 2012\n",
      "             putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\n",
      "       Apracticalshort-termapplicationofsuchmodelsis linkprediction :predict-\n",
      "              ingmissingarcsintheknowledgegraph.Thisisaformofgeneralizationtonew\n",
      "            factsbasedonoldfacts. Mostoftheknowledgebasesthatcurrentlyexisthave\n",
      "           beenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\n",
      "             themajorityoftruerelationsabsentfromtheknowledgebase.SeeWangetal.\n",
      "              (), (),and ()forexamplesofsuchan 2014bLinetal.2015 Garcia-Duranetal.2015\n",
      "application.\n",
      "1    Respectivelyavailable fromthesewebsites: f r e e b a s e . c o m, c y c . c o m / o p e n c y c, w o r d n e t .\n",
      " p r i n c e t o n . e d u w i k i b a . s e ,\n",
      "2g e n e o n t o l o g y . o r g\n",
      "479  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "            Evaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcult\n",
      "             becausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\n",
      "               betrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsure\n",
      "            whetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\n",
      "             fact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\n",
      "             modelranksaheld-outsetofknowntruepositivefactscomparedtootherfacts\n",
      "             thatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples\n",
      "              thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\n",
      "             factandcreatecorruptedversionsofthatfact,forexample,byreplacingoneentity\n",
      "             intherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat\n",
      "             10percentmetriccountshowmanytimesthemodelranksa“correct”factamong\n",
      "          thetop10percentofallcorruptedversionsofthatfact.\n",
      "        Anotherapplicationofknowledgebasesanddistributedrepresentationsfor\n",
      " themis  word-sensedisambiguation       (NavigliandVelardi2005Bordes ,; etal.,\n",
      "               2012),whichisthetaskofdecidingwhichsenseofawordistheappropriateone\n",
      "  insomecontext.\n",
      "          Eventually,knowledgeofrelationscombinedwithareasoningprocessandan\n",
      "           understandingofnaturallanguagecouldallowustobuildageneralquestion-\n",
      "          answeringsystem.Ageneralquestion-answeringsystemmustbeabletoprocess\n",
      "           inputinformationandrememberimportantfacts,organizedinawaythatenables\n",
      "             ittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblem\n",
      "         thatcanonlybesolvedinrestricted“toy” environments.Currently, thebest\n",
      "           approachtorememberingandretrievingspeciﬁcdeclarativefactsistousean\n",
      "          explicitmemory mechanism,asdescribedinsection.Memorynetworkswere 10.12\n",
      "            ﬁrstproposedtosolveatoyquestion-answering task(Weston 2014Kumar etal.,).\n",
      "             etal.()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015\n",
      "              theinputintothememory andtoproducetheanswergiventhecontentsofthe\n",
      "memory.\n",
      "           Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\n",
      "              describedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\n",
      "         beimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\n",
      "             ofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\n",
      "   asofthiswriting.\n",
      "          Thisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II\n",
      "         networks,comprisingallthemostsuccessfulmethods.Generallyspeaking,these\n",
      "              methodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofa\n",
      "          modelthatapproximatessomedesiredfunction.Withenoughtrainingdata,this\n",
      "               approachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\n",
      "4 8 0  C HAP T E R 1 2 . AP P L I C A T I O NS\n",
      "           territoryofresearch—methods thataredesignedtoworkwithlesstrainingdata\n",
      "             ortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcult\n",
      "              andnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\n",
      "4 8 1 P a rt I I I\n",
      "  D e e p Learning Re s e arc h\n",
      "482           Thispartofthebookdescribesthemoreambitiousandadvancedapproaches\n",
      "        todeeplearning,currentlypursuedbytheresearchcommunity.\n",
      "             Inthepreviouspartsofthebook,wehaveshownhowtosolvesupervised\n",
      "           learningproblems—howtolearntomaponevectortoanother,givenenough\n",
      "   examplesofthemapping.\n",
      "             Notallproblemswemightwanttosolvefallintothiscategory.Wemay\n",
      "             wishtogeneratenewexamples,ordeterminehowlikelysomepointis,orhandle\n",
      "             missingvaluesandtakeadvantageofalargesetofunlabeledexamplesorexamples\n",
      "             fromrelatedtasks.Ashortcomingofthecurrentstateoftheartforindustrial\n",
      "          applicationsisthatourlearningalgorithmsrequirelargeamountsofsupervised\n",
      "              datatoachievegoodaccuracy.Inthispartofthebook,wediscusssomeof\n",
      "          thespeculativeapproachestoreducingtheamountoflabeleddatanecessary\n",
      "             forexistingmodelstoworkwellandbeapplicableacrossabroaderrangeof\n",
      "          tasks.Accomplishingthesegoalsusuallyrequiressomeformofunsupervisedor\n",
      " semi-supervisedlearning.\n",
      "         Manydeeplearningalgorithmshavebeendesignedtotackleunsupervised\n",
      "             learningproblems,butnonehastrulysolvedtheprobleminthesamewaythat\n",
      "             deeplearninghaslargelysolvedthesupervisedlearningproblemforawidevarietyof\n",
      "             tasks.Inthispartofthebook,wedescribetheexistingapproachestounsupervised\n",
      "              learningandsomeofthepopularthoughtabouthowwecanmakeprogressinthis\n",
      "ﬁeld.\n",
      "            Acentralcauseofthediﬃcultieswithunsupervisedlearningisthehighdi-\n",
      "          mensionalityoftherandomvariablesbeingmodeled.Thisbringstwodistinct\n",
      "         challenges:astatisticalchallengeandacomputationalchallenge.Thestatistical\n",
      "          challengeregardsgeneralization:thenumberofconﬁgurationswemaywantto\n",
      "           distinguishcangrowexponentiallywiththenumberofdimensionsofinterest,and\n",
      "            thisquicklybecomesmuchlargerthanthenumberofexamplesonecanpossibly\n",
      "         have(orusewithboundedcomputationalresources).Thecomputationalchallenge\n",
      "        associatedwithhigh-dimensionaldistributionsarisesbecausemanyalgorithmsfor\n",
      "            learningorusingatrainedmodel(especiallythosebasedonestimatinganexplicit\n",
      "       probabilityfunction)involveintractablecomputationsthatgrowexponentially\n",
      "    withthenumberofdimensions.\n",
      "         Withprobabilisticmodels,thiscomputationalchallengearisesfromtheneed\n",
      "        toperformintractableinferenceortonormalizethedistribution.\n",
      "•           Intractableinference:inferenceisdiscussedmostlyinchapter.Itregards 19\n",
      "         thequestionofguessingtheprobablevaluesofsomevariablesa  ,givenother\n",
      "variablesb           ,withrespecttoamodelthatcapturesthejointdistributionover\n",
      "4 8 3a,bandc          .Inordertoevencomputesuchconditionalprobabilities,oneneeds\n",
      "       tosumoverthevaluesofthevariablesc      ,aswellascomputeanormalization\n",
      "         constantthatsumsoverthevaluesofaandc.\n",
      "•        Intractablenormalizationconstants(thepartitionfunction):thepartition\n",
      "          functionisdiscussedmostlyinchapter.Normalizingconstantsofproba- 18\n",
      "           bilityfunctionscomeupininference(above)aswellasinlearning. Many\n",
      "       probabilisticmodelsinvolvesuchanormalizingconstant.Unfortunately,\n",
      "           learningsuchamodeloftenrequirescomputingthegradientoftheloga-\n",
      "           rithmofthepartitionfunctionwithrespecttothemodelparameters.That\n",
      "         computationisgenerallyasintractableascomputingthepartitionfunction\n",
      "          itself.MonteCarloMarkovchain(MCMC)methods(chapter)areof-17\n",
      "            tenusedtodealwiththepartitionfunction(computingitoritsgradient).\n",
      "          Unfortunately,MCMCmethodssuﬀerwhenthemodesofthemodeldistribu-\n",
      "         tionarenumerousandwellseparated,especiallyinhigh-dimensionalspaces\n",
      " (section).17.5\n",
      "          Onewaytoconfronttheseintractablecomputationsistoapproximatethem,\n",
      "             andmanyapproacheshavebeenproposed,asdiscussedinthisthirdpartofthe\n",
      "         book.Anotherinterestingway, alsodiscussedhere, wouldbetoavoidthese\n",
      "          intractablecomputationsaltogetherbydesign,andmethodsthatdonotrequire\n",
      "          suchcomputationsarethusveryappealing.Severalgenerativemodelshavebeen\n",
      "          proposedinrecentyearswiththatmotivation. Awidevarietyofcontemporary\n",
      "        approachestogenerativemodelingarediscussedinchapter.20\n",
      "            Partisthemostimportantforaresearcher—someonewhowantstoun- III\n",
      "             derstandthebreadthofperspectivesthathavebeenbroughttotheﬁeldofdeep\n",
      "         learningandpushtheﬁeldforwardtowardtrueartiﬁcialintelligence.\n",
      "4 8 4 C h a p t e r 1 3\n",
      "  Li ne ar F act or Mo dels\n",
      "           Manyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilistic\n",
      "   modeloftheinput,p m o d e l(x        ).Suchamodelcan,inprinciple,useprobabilis-\n",
      "    tic inferenceto predict anyof the variables inits environment given anyof\n",
      "          theothervariables.Manyofthesemodelsalsohavelatentvariablesh ,with\n",
      "p m o d e l() = x E hp m o d e l  ( )xh|.       Theselatentvariablesprovideanothermeansofrep-\n",
      "         resentingthedata.Distributedrepresentationsbasedonlatentvariablescan\n",
      "            obtainalltheadvantagesofrepresentationlearningthatwehaveseenwithdeep\n",
      "   feedforwardandrecurrentnetworks.\n",
      "           Inthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\n",
      "           latentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\n",
      "           blocksofmixturemodels(Hinton 1995aGhahramaniandHinton1996 etal.,; ,;\n",
      "             Roweis2002 Tang 2012 etal.,)oroflarger,deepprobabilisticmodels(etal.,).\n",
      "           Theyalsoshowmanyofthebasicapproachesnecessarytobuildinggenerative\n",
      "         modelsthatthemoreadvanceddeepmodelswillextendfurther.\n",
      "              Alinearfactormodelisdeﬁnedbytheuseofastochasticlineardecoderfunction\n",
      "           thatgeneratesbyaddingnoisetoalineartransformationof. x h\n",
      "          Thesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\n",
      "             factorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\n",
      "             madethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied.\n",
      "          Alinearfactormodeldescribesthedata-generationprocessasfollows.First,\n",
      "        wesampletheexplanatoryfactors fromadistribution h\n",
      "   h∼p,()h (13.1)\n",
      "wherep(h     )isafactorialdistribution,withp(h) =\n",
      "ip(h i     ),sothatitiseasy\n",
      "485    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "x 1 x 1 x 2 x 2 x 3 x 3\n",
      "              x h n o i s e x h n o i s e = W + + b = W + + b\n",
      "            Figure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\n",
      "       whichweassumethatanobserveddatavectorx      isobtainedbyalinearcombinationof\n",
      "  independentlatentfactorsh        ,plussomenoise.Diﬀerentmodels,suchasprobabilistic\n",
      "               PCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandof\n",
      "  theprior.p()h\n",
      "           tosamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhe\n",
      "factors\n",
      "       xWhb = ++noise, (13.2)\n",
      "          wherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\n",
      "     Thisisillustratedinﬁgure.13.1\n",
      "     13.1ProbabilisticPCAandFactorAnalysis\n",
      "         ProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\n",
      "             factormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\n",
      "             diﬀerinthechoicesmadeforthenoisedistributionandthemodel’spriorover\n",
      "     latentvariablesbeforeobserving. h x\n",
      "In factoranalysis       ( ,; ,),thelatentvariable Bartholomew1987Basilevsky1994\n",
      "      priorisjusttheunitvarianceGaussian\n",
      "     h 0∼N(;h,,I) (13.3)\n",
      "   whiletheobservedvariablesx i   areassumedtobe  conditionallyindependent,\n",
      "givenh       .Speciﬁcally,thenoise isassumed tobe drawn froma diagonalco-\n",
      "  varianceGaussian distribution,with covariance matrixψ=diag(σ2 ),with\n",
      "σ2= [σ2\n",
      "1 ,σ2\n",
      "2     ,...,σ2\n",
      "n]    avectorofper-variablevariances.\n",
      "            Theroleofthelatentvariablesisthustocapturethedependenciesbetween\n",
      "   thediﬀerentobservedvariablesx i       .Indeed,itcaneasilybeshownthatx  isjusta\n",
      "    multivariatenormalrandomvariable,with\n",
      "     x∼N(;xbWW,  +)ψ. (13.4)\n",
      "4 8 6    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "            TocastPCAinaprobabilisticframework,wecanmakeaslightmodiﬁcation\n",
      "        tothefactoranalysismodel,makingtheconditionalvariancesσ2\n",
      "i  equaltoeach\n",
      "     other. Inthatcasethecovarianceofx isjust WW+σ2I ,whereσ2  isnowa\n",
      "     scalar.Thisyieldstheconditionaldistribution\n",
      "     x∼N(;xbWW, +σ2 I), (13.5)\n",
      " orequivalently\n",
      "       x h z = W ++bσ, (13.6)\n",
      "where  z∼N(z; 0,I          )isGaussiannoise.Then,as ()show, TippingandBishop1999\n",
      "             wecanuseaniterativeEMalgorithmforestimatingtheparametersandWσ2.\n",
      "Thisprobabilistic        PCAmodeltakesadvantageoftheobservationthatmost\n",
      "          variationsinthedatacanbecapturedbythelatentvariablesh    ,uptosomesmall\n",
      "residual   reconstructionerrorσ2       .Asshownby (), TippingandBishop1999\n",
      "    probabilisticPCAbecomesPCAas σ→      0.Inthatcase,theconditionalexpected\n",
      " valueofhgivenx     becomesanorthogonalprojectionof  xb−   ontothespace\n",
      "          spannedbythecolumnsof,asinPCA. d W\n",
      "As σ→          0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharp\n",
      " aroundthesed      dimensionsspannedbythecolumnsofW    .Thiscanmakethe\n",
      "              modelassignverylowlikelihoodtothedataifthedatadonotactuallycluster\n",
      "  nearahyperplane.\n",
      "    13.2IndependentComponentAnalysis(ICA)\n",
      "         Independentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\n",
      "      algorithms( , ; ,; ,; Herault andAns1984Juttenand Herault1991Comon1994\n",
      "             Hyvärinen1999Hyvärinen 2001aHinton 2001Teh 2003 ,; etal.,; etal.,;etal.,).\n",
      "             Itisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved\n",
      "            signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\n",
      "            theobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\n",
      "    merelydecorrelatedfromeachother.1\n",
      "          ManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariant\n",
      "              thatismostsimilartotheothergenerativemodelswehavedescribedhereisa\n",
      "            variant( ,)thattrainsafullyparametricgenerativemodel.The Phametal.1992\n",
      "     priordistributionovertheunderlyingfactors,p(h       ),mustbeﬁxedaheadoftimeby\n",
      "      theuser.Themodelthendeterministicallygeneratesx= Wh    .Wecanperforma\n",
      "1             Seesectionforadiscussionofthediﬀerencebetweenuncorrelatedvariablesandindepen- 3.8\n",
      " dentvariables.\n",
      "487    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "        nonlinearchangeofvariables(usingequation)todetermine 3.47 p(x) .Learning\n",
      "        themodelthenproceedsasusual,usingmaximumlikelihood.\n",
      "        Themotivationforthisapproachisthatbychoosing p(h   )tobeindependent,\n",
      "             wecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.This\n",
      "            iscommonlyused,nottocapturehigh-levelabstractcausalfactors,buttorecover\n",
      "           low-levelsignalsthathavebeenmixedtogether.Inthissetting,eachtraining\n",
      "      exampleisonemomentintime,each x i      isonesensor’sobservationofthemixed\n",
      "  signals,andeach h i           isoneestimateofoneoftheoriginalsignals.Forexample,we\n",
      " mighthave n     peoplespeakingsimultaneously.Ifwehave n  diﬀerentmicrophones\n",
      "            placedindiﬀerentlocations,ICAcandetectthechangesinthevolumebetween\n",
      "             eachspeakerasheardbyeachmicrophoneandseparatethesignalssothateach h i\n",
      "           containsonlyonepersonspeakingclearly.Thisiscommonlyusedinneuroscience\n",
      "        forelectroencephalography,atechnologyforrecordingelectricalsignalsoriginating\n",
      "            inthebrain.Multipleelectrodesensorsplacedonthesubject’sheadareused\n",
      "           tomeasuremanyelectricalsignalscomingfromthebody.Theexperimenteris\n",
      "            typicallyonlyinterestedinsignalsfromthebrain,butsignalsfromthesubject’s\n",
      "            heartandeyesarestrongenoughtoconfoundmeasurementstakenatthesubject’s\n",
      "             scalp.Thesignalsarriveattheelectrodesmixedtogether,soICAisnecessaryto\n",
      "            separatetheelectricalsignatureoftheheartfromthesignalsoriginatinginthe\n",
      "           brain,andtoseparatesignalsindiﬀerentbrainregionsfromeachother.\n",
      "            Asmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise\n",
      "   inthegenerationofx        ratherthanusingadeterministicdecoder.Mostdonot\n",
      "            usethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\n",
      "h=W− 1x          independentfromeachother.Manycriteriathataccomplishthisgoal\n",
      "        arepossible.Equationrequirestakingthedeterminantof 3.47 W   ,whichcanbe\n",
      "           anexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\n",
      "       problematicoperationbyconstrainingtobeorthogonal. W\n",
      "     AllvariantsofICArequirethat p(h      )benon-Gaussian.Thisisbecauseif p(h)\n",
      "       isanindependentpriorwithGaussiancomponents,thenW  isnotidentiﬁable.\n",
      "      Wecanobtainthesamedistributionover p(x    )formanyvaluesofW   .Thisisvery\n",
      "           diﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\n",
      "  whichoftenrequire p(h           )tobeGaussianinordertomakemanyoperationsonthe\n",
      "           modelhaveclosedformsolutions.Inthemaximumlikelihoodapproach,wherethe\n",
      "          userexplicitlyspeciﬁesthedistribution,atypicalchoiceistouse p( h i) =d\n",
      "d h iσ( h i).\n",
      "           Typicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\n",
      "            doestheGaussiandistribution,sowecanalsoseemostimplementationsofICA\n",
      "   aslearningsparsefeatures.\n",
      "              ManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\n",
      "4 8 8    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "        phrase.Inthisbook,agenerativemodeleitherrepresents p( x   ) orcandrawsamples\n",
      "           fromit.ManyvariantsofICAonlyknowhowtotransformbetween xand hbut\n",
      "      donothaveanywayofrepresenting p( h       ),andthusdonotimposeadistribution\n",
      "over p( x            ).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\n",
      "h= W− 1x     ,becausehighkurtosisindicatesthat p( h     )isnon-Gaussian,butthisis\n",
      "   accomplishedwithoutexplicitlyrepresenting p( h      ).ThisisbecauseICAismore\n",
      "            oftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\n",
      "    dataorestimatingitsdensity.\n",
      "           JustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\n",
      "            chapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\n",
      "    weuseanonlinearfunction f       togeneratetheobserveddata.SeeHyvärinenand\n",
      "             Pajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith\n",
      "           ensemblelearningbyRobertsandEverson2001 Lappalainen 2000 ()and etal.().\n",
      "        AnothernonlinearextensionofICAistheapproachof  nonlinearindependent\n",
      " componentsestimation           ,orNICE( ,),whichstacksaseriesof Dinhetal.2014\n",
      "         invertibletransformations(encoderstages)withthepropertythatthedeterminant\n",
      "           oftheJacobianofeachtransformationcanbecomputedeﬃciently.Thismakes\n",
      "            itpossibletocomputethelikelihoodexactly,andlikeICA,NICEattemptsto\n",
      "             transformthedataintoaspacewhereithasafactorizedmarginaldistribution,but\n",
      "             itismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\n",
      "            isassociatedwithadecoderthatisitsperfectinverse,generatingsamplesfrom\n",
      "       themodelisstraightforward(byﬁrstsamplingfrom p( h    )andthenapplyingthe\n",
      "decoder).\n",
      "           AnothergeneralizationofICAistolearngroupsoffeatures,withstatistical\n",
      "         dependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyvärinen\n",
      "             andHoyer1999Hyvärinen2001b ,; etal.,).Whenthegroupsofrelatedunitsare\n",
      "      chosentobenonoverlapping,thisiscalled   independentsubspaceanalysis  .Itis\n",
      "            alsopossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\n",
      "           groupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\n",
      "      features.Whenappliedtonaturalimages,this  topographicICA  approachlearns\n",
      "          Gaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationor\n",
      "         frequency. ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithin\n",
      "          eachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\n",
      "   13.3SlowFeatureAnalysis\n",
      "  Slowfeatureanalysis         (SFA)isalinearfactormodelthatusesinformationfrom\n",
      "         timesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\n",
      "4 8 9    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "           Slowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\n",
      "           principle.Theideaisthattheimportantcharacteristicsofsceneschangevery\n",
      "            slowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\n",
      "           scene.Forexample,incomputervision,individualpixelvaluescanchangevery\n",
      "              rapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel\n",
      "              willrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespass\n",
      "            overthepixel. Bycomparison,thefeatureindicatingwhetherazebraisinthe\n",
      "             imagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwill\n",
      "           changeslowly. Wethereforemaywishtoregularizeourmodeltolearnfeatures\n",
      "    thatchangeslowlyovertime.\n",
      "          Theslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\n",
      "             toawidevarietyofmodels(,; ,; ,; Hinton1989Földiák1989Mobahietal.2009\n",
      "             BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\n",
      "          diﬀerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\n",
      "           introducedbyaddingatermtothecostfunctionoftheform\n",
      "λ\n",
      "tLf(( x( +1 ) t )(,f x( ) t )), (13.7)\n",
      "whereλ         isahyperparameterdeterminingthestrengthoftheslownessregularization\n",
      "term,t        istheindexintoatimesequenceofexamples,f   isthefeatureextractor\n",
      "   toberegularized,andL       isalossfunctionmeasuringthedistancebetweenf( x( ) t)\n",
      " andf( x( +1 ) t          ).Acommonchoiceforisthemeansquareddiﬀerence. L\n",
      "          Slowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslowness\n",
      "              principle.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractorandcan\n",
      "               thusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\n",
      "              generativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninput\n",
      "              spaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthus\n",
      "        doesnotimposeadistribution oninputspace. p() x\n",
      "         TheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁningf( x; θ)\n",
      "         tobealineartransformation,thensolvingtheoptimizationproblem\n",
      "min\n",
      "θE t((f x( +1 ) t) i  −f( x( ) t) i)2(13.8)\n",
      "   subjecttotheconstraints\n",
      "E tf( x( ) t) i = 0 (13.9)\n",
      "and\n",
      "E t[(f x( ) t)2\n",
      "i ] = 1. (13.10)\n",
      "             Theconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\n",
      "             problemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\n",
      "4 9 0    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "            valuesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective.\n",
      "            Theconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\n",
      "            pathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\n",
      "             areordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\n",
      "    mustalsoaddtheconstraint\n",
      "   ∀i<j, E t[(f x( ) t) if( x( ) t) j ] = 0. (13.11)\n",
      "           Thisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelatedfromeach\n",
      "           other.Withoutthisconstraint,allthelearnedfeatureswouldsimplycapturethe\n",
      "           oneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\n",
      "     reconstructionerror,to forcethe featuresto diversify, butthis decorrelation\n",
      "             mechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\n",
      "           problemmaybesolvedinclosedformbyalinearalgebrapackage.\n",
      "            SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\n",
      " expansionto x         beforerunningSFA.Forexample, itiscommontoreplace x\n",
      "        withthequadraticbasisexpansion,avectorcontainingelementsx ix j foralli\n",
      "andj            .LinearSFAmodulesmaythenbecomposedtolearndeepnonlinearslow\n",
      "          featureextractorsbyrepeatedlylearningalinearSFAfeatureextractor,applying\n",
      "            anonlinearbasisexpansiontoitsoutput,andthenlearninganotherlinearSFA\n",
      "      featureextractorontopofthatexpansion.\n",
      "            Whentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\n",
      "         quadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\n",
      "            thoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\n",
      "         onvideosofrandommotionwithin3-Dcomputer-renderedenvironments,deep\n",
      "          SFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\n",
      "              byneuronsinratbrainsthatareusedfornavigation(Franzius2007etal.,).SFA\n",
      "        thusseemstobeareasonablybiologicallyplausiblemodel.\n",
      "             AmajoradvantageofSFAisthatitispossibletotheoreticallypredictwhich\n",
      "             featuresSFAwilllearn,eveninthedeepnonlinearsetting.Tomakesuchtheoretical\n",
      "            predictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\n",
      "      conﬁgurationspace (e.g., in thecaseof randommotion inthe 3-Drendered\n",
      "         environment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\n",
      "            distributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\n",
      "            theunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe\n",
      "          optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\n",
      "          appliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\n",
      "            Thisisincomparisontootherlearningalgorithms,wherethecostfunctiondepends\n",
      "            highlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhat\n",
      "    featuresthemodelwilllearn.\n",
      "4 9 1    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "             DeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\n",
      "            estimation(Franzius2008etal.,).Sofar,theslownessprinciplehasnotbecome\n",
      "            thebasisforanystate-of-the-artapplications.Itisunclearwhatfactorhaslimited\n",
      "            itsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and\n",
      "           that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\n",
      "               itwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\n",
      "                onetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\n",
      "            whethertheobject’svelocityishighorlow,buttheslownessprincipleencourages\n",
      "           themodeltoignorethepositionofobjectsthathavehighvelocity.\n",
      "  13.4SparseCoding\n",
      " Sparsecoding           ( ,)isalinearfactormodelthathas OlshausenandField1996\n",
      "          beenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction\n",
      "          mechanism. Strictlyspeaking,theterm“sparsecoding”referstotheprocessof\n",
      "   inferringthevalueofh         inthismodel,while“sparsemodeling”referstotheprocess\n",
      "              ofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedto\n",
      "  refertoboth.\n",
      "             Likemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\n",
      "  obtainreconstructionsofx        ,asspeciﬁedinequation.Morespeciﬁcally,sparse 13.2\n",
      "           codingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\n",
      "  isotropicprecision:β\n",
      "      p , ( ) = (; + xh|NxWhb1\n",
      "β I). (13.12)\n",
      " Thedistributionp(h           )ischosentobeonewithsharppeaksnear0(Olshausen\n",
      "          andField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\n",
      "Studentt          -distributions.Forexample,theLaplacepriorparametrizedintermsof\n",
      "       thesparsitypenaltycoeﬃcientisgivenby λ\n",
      "ph( i) = Laplace(h i ;0,2\n",
      "λ) =λ\n",
      "4e−1\n",
      "2λ h| i| , (13.13)\n",
      "     andtheStudentpriorbyt\n",
      "ph( i) ∝1\n",
      " (1+h2\n",
      "i\n",
      "ν)ν +1\n",
      "2 . (13.14)\n",
      "         Trainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\n",
      "           trainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\n",
      "4 9 2    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "            reconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtheras\n",
      "         aprincipledapproximationtomaximumlikelihoodlater,insection.19.3\n",
      "              FormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\n",
      " thatpredictsh          andconsistsonlyofmultiplicationbyaweightmatrix.Theencoder\n",
      "             thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\n",
      "           isanoptimizationalgorithm,whichsolvesanoptimizationprobleminwhichwe\n",
      "      seekthesinglemostlikelycodevalue:\n",
      "h∗ = () = argmax fx\n",
      "h   p. ( )hx| (13.15)\n",
      "           Whencombinedwithequation andequation,thisyieldsthefollowing 13.13 13.12\n",
      " optimizationproblem:\n",
      " argmax\n",
      "h   p( )hx| (13.16)\n",
      "  =argmax\n",
      "h    log( )phx| (13.17)\n",
      "  =argmin\n",
      "hλ||||h 1    +β||−||xWh2\n",
      "2 , (13.18)\n",
      "       wherewehavedroppedtermsnotdependingonh    anddividedbypositivescaling\n",
      "    factorstosimplifytheequation.\n",
      "     DuetotheimpositionofanL1 normonh      ,thisprocedurewillyieldasparse\n",
      "h∗  (seesection).7.1.2\n",
      "           Totrainthemodelratherthanjustperforminference,wealternatebetween\n",
      "   minimizationwithrespecttoh    andminimizationwithrespecttoW  .Inthis\n",
      "  presentation,wetreatβ          asahyperparameter.Typicallyitissetto1becauseits\n",
      "       roleinthisoptimizationproblemissharedwithλ       ,andthereisnoneedforboth\n",
      "      hyperparameters.Inprinciple,wecouldalsotreatβ     asaparameterofthemodel\n",
      "             andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\n",
      "onh   butdodependonβ  .Tolearnβ      ,thesetermsmustbeincluded,orβwill\n",
      "  collapseto.0\n",
      "        Notallapproachestosparsecodingexplicitlybuildap(h  )andap(  xh|).\n",
      "            Oftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\n",
      "           valuesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\n",
      "  Ifwesampleh            fromaLaplaceprior,itisinfactazeroprobabilityeventfor\n",
      "  anelementofh          toactuallybezero.Thegenerativemodelitselfisnotespecially\n",
      "           sparse;onlythefeatureextractoris. ()describeapproximate Goodfellowetal.2013d\n",
      "             inferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,for\n",
      "        whichsamplesfromthepriorusuallycontaintruezeros.\n",
      "4 9 3    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "         Thesparsecodingapproachcombinedwiththeuse ofthenonparametric\n",
      "          encodercaninprincipleminimizethecombinationofreconstructionerrorand\n",
      "          log-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthat\n",
      "            thereisnogeneralizationerrortotheencoder.Aparametricencodermustlearn\n",
      "  howtomap xto h      inawaythatgeneralizes.Forunusual x   thatdonotresemble\n",
      "           thetrainingdata,alearnedparametricencodermayfailtoﬁndan h  thatresultsin\n",
      "            accuratereconstructionorinasparsecode.Forthevastmajorityofformulations\n",
      "           ofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\n",
      "           procedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchas\n",
      "         replicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\n",
      "              canstillriseonunfamiliarpoints,butthisisduetogeneralizationerrorinthe\n",
      "            decoderweights,ratherthantogeneralizationerrorintheencoder.Thelackof\n",
      "        generalizationerrorinsparsecoding’soptimization-based encodingprocessmay\n",
      "             resultinbettergeneralizationwhensparsecodingisusedasafeatureextractorfor\n",
      "             aclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.Coates\n",
      "          andNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\n",
      "             objectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\n",
      "          encoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellowetal.\n",
      "            ()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\n",
      "            extractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\n",
      "  labelsperclass).\n",
      "          Theprimarydisadvantageofthenonparametricencoderisthatitrequires\n",
      "   greatertimetocompute hgiven x     becausethenonparametricapproachrequires\n",
      "         runninganiterativealgorithm.Theparametricautoencoderapproach,developedin\n",
      "             chapter,usesonlyaﬁxednumberoflayers,oftenonlyone.Anotherdisadvantage 14\n",
      "          isthatitisnotstraightforwardtoback-propagatethroughthenonparametric\n",
      "   encoder, which makes it diﬃcultto pretrainasparse coding model with an\n",
      "          unsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion.Modiﬁed\n",
      "            versionsofsparsecodingthatpermitapproximatederivativesdoexistbutarenot\n",
      "     widelyused( ,). BagnellandBradley2009\n",
      "           Sparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\n",
      "            showninﬁgure. Thishappensevenwhenthemodelisabletoreconstruct 13.2\n",
      "              thedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateach\n",
      "             individualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\n",
      "             resultsinthemodelincludingrandomsubsetsofallthefeaturesineachgenerated\n",
      "           sample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposea\n",
      "            nonfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentof\n",
      "   moresophisticatedshallowmodels.\n",
      "4 9 4    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "            Figure13.2: Example samplesandweightsfromaspikeandslabsparsecodingmodel\n",
      "              trainedontheMNISTdataset.(Left)Thesamplesfromthemodeldonotresemblethe\n",
      "              trainingexamples.Atﬁrstglance,onemightassumethemodelispoorlyﬁt. The (Right)\n",
      "            weightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\n",
      "              digits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\n",
      "            overfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets\n",
      "            areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\n",
      "           generativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\n",
      "       reproducedwithpermissionfrom (). Goodfellowetal.2013d\n",
      "4 9 5    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "    13.5ManifoldInterpretationofPCA\n",
      "           LinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\n",
      "            learningamanifold(Hinton 1997etal.,).WecanviewprobabilisticPCAas\n",
      "         deﬁningathinpancake-shapedregionofhighprobability—aGaussiandistribution\n",
      "                thatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsvertical\n",
      "               axis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\n",
      "           axes. Thisisillustratedinﬁgure. PCAcanbeinterpretedasaligningthis 13.3\n",
      "          pancakewithalinearmanifoldinahigher-dimensionalspace.Thisinterpretation\n",
      "             appliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\n",
      "matricesWandV       withthegoalofmakingthereconstructionofx   lieascloseto\n",
      "  xaspossible.\n",
      "   Lettheencoderbe\n",
      " hxW = (f) =    ( )xµ−. (13.19)\n",
      "      Theencodercomputesalow-dimensionalrepresentationofh   .Withtheautoencoder\n",
      "       view,wehaveadecodercomputingthereconstruction\n",
      "ˆ     xhbVh = (g) = +. (13.20)\n",
      "          Thechoicesoflinearencoderanddecoderthatminimizereconstructionerror\n",
      " E[||−xˆx||2 ] (13.21)\n",
      " correspondtoV=W,µ=b= E[x    ]andthecolumnsofW  formanorthonormal\n",
      "            basis,whichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\n",
      "matrix\n",
      "     Cxµxµ = [( E−)(−) ]. (13.22)\n",
      "       InthecaseofPCA,thecolumnsofW     aretheseeigenvectors,orderedbythe\n",
      "          magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\n",
      "     Onecanalsoshowthateigenvalueλ iofC     correspondstothevarianceofx\n",
      "    inthedirectionofeigenvectorv( ) i .If  x∈ RDand  h∈ Rdwith  d<D  ,thenthe\n",
      "           optimalreconstructionerror(choosing,,andasabove)is µbVW\n",
      "  min[ E||−xˆx||2] =D\n",
      "i d = +1λ i . (13.23)\n",
      "     Hence,ifthecovariancehasrankd  ,theeigenvaluesλ d +1toλ D   are0andrecon-\n",
      "   structionerroris0.\n",
      "4 9 6    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "         Figure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\n",
      "             manifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane,”\n",
      "             whichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifold\n",
      "               isverysmall(arrowpointingoutofplane)andcanbeconsidered“noise,”whiletheother\n",
      "              variancesarelarge(arrowsintheplane)andcorrespondto“signal”andtoacoordinate\n",
      "    systemforthereduced-dimensiondata.\n",
      "4 9 7    C HAP T E R 1 3 . L I NE AR F A C T O R M O D E L S\n",
      "            Furthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\n",
      "      maximizingthevariancesoftheelementsof h  ,underorthogonal W  ,insteadof\n",
      "  minimizingreconstructionerror.\n",
      "             Linearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe\n",
      "            simplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersand\n",
      "           linearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\n",
      "          factormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\n",
      "             modelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexible\n",
      " modelfamily.\n",
      "4 9 8 C h a p t e r 1 4\n",
      "Auto e nco ders\n",
      "An a ut o e nc o der            isaneuralnetworkthatistrainedtoattempttocopyitsinput\n",
      "       toitsoutput. Internally,ithasahiddenlayerh  thatdescribesa c o de usedto\n",
      "             representtheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\n",
      " encoderfunctionh= f(x       )andadecoderthatproducesareconstructionr= g(h).\n",
      "            Thisarchitectureispresentedinﬁgure.Ifanautoencodersucceedsinsimply 14.1\n",
      "  learningtoset g( f(x)) =x        everywhere,thenitisnotespeciallyuseful.Instead,\n",
      "             autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\n",
      "             restrictedinwaysthatallowthemtocopyonlyapproximately,andtocopyonly\n",
      "            inputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\n",
      "              whichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\n",
      "data.\n",
      "    Modern autoencoders havegeneralized the ideaof anencoder and ade-\n",
      "      coderbeyonddeterministicfunctionstostochasticmappings p e n c o d e r(  hx| )and\n",
      "p d e c o d e r  ( )xh|.\n",
      "            Theideaofautoencodershasbeenpartofthehistoricallandscapeofneural\n",
      "           networksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\n",
      "      1994).Traditionally, autoencoderswereused fordimensionalityreductionor\n",
      "       featurelearning.Recently,theoreticalconnectionsbetweenautoencodersand\n",
      "          latentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\n",
      "              modeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\n",
      "             aspecialcaseoffeedforwardnetworksandmaybetrainedwithallthesame\n",
      "       techniques,typicallyminibatchgradientdescentfollowinggradientscomputed\n",
      "       byback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\n",
      "   alsobetrainedusing r e c i r c ul a t i o n      (HintonandMcClelland1988,),alearning\n",
      "            algorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\n",
      "499  C HAP T E R 1 4 . A UT O E NC O D E R S\n",
      "xx rrh h\n",
      "f g\n",
      "              Figure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\n",
      " (calledreconstruction)r      throughaninternalrepresentationorcodeh  .Theautoencoder\n",
      "   hastwocomponents: theencoderf(mappingxtoh   )andthedecoderg(mappingh\n",
      " to).r\n",
      "           totheactivationsonthereconstructedinput.Recirculationisregardedasmore\n",
      "          biologicallyplausiblethanback-propagationbutisrarelyusedformachinelearning\n",
      "applications.\n",
      "  14.1UndercompleteAutoencoders\n",
      "             Copyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\n",
      "         interestedintheoutputofthe decoder.Instead, wehope thattrainingthe\n",
      "         autoencodertoperformtheinputcopyingtaskwillresultinh   takingonuseful\n",
      "properties.\n",
      "           Onewaytoobtainusefulfeaturesfromtheautoencoderistoconstrainhto\n",
      "    haveasmallerdimensionthanx       .Anautoencoderwhosecodedimensionisless\n",
      "     thantheinputdimensioniscalled under c o m p l e t e   .Learninganundercomplete\n",
      "           representationforcestheautoencodertocapturethemostsalientfeaturesofthe\n",
      " trainingdata.\n",
      "          Thelearningprocessisdescribedsimplyasminimizingalossfunction\n",
      "  L,gf, (x(()))x (14.1)\n",
      "whereL    isalossfunctionpenalizingg(f(x    ))forbeingdissimilarfromx  ,suchas\n",
      "   themeansquarederror.\n",
      "     WhenthedecoderislinearandL      isthemeansquarederror,anundercomplete\n",
      "             autoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\n",
      "            trainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\n",
      "     trainingdataasasideeﬀect.\n",
      "    Autoencoderswithnonlinearencoderfunctionsf   andnonlineardecoderfunc-\n",
      "tionsg          canthuslearnamorepowerfulnonlineargeneralizationofPCA.Unfortu-\n",
      "5 0 0  C HAP T E R 1 4 . A UT O E NC O D E R S\n",
      "            nately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\n",
      "           canlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\n",
      "           thedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\n",
      "            withaone-dimensionalcodebutaverypowerfulnonlinearencodercouldlearnto\n",
      "   representeachtrainingexample x( ) i  withthecode i     .Thedecodercouldlearnto\n",
      "            maptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples.This\n",
      "             speciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\n",
      "             codertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\n",
      "             thedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\n",
      "  14.2RegularizedAutoencoders\n",
      "         Undercompleteautoencoders,withcodedimensionlessthantheinputdimension,\n",
      "             canlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\n",
      "            theseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\n",
      "   giventoomuchcapacity.\n",
      "            Asimilarproblemoccursifthehiddencodeisallowedtohavedimension\n",
      "      equaltotheinput,andinthe o v e r c o m pl e t e      caseinwhichthehiddencodehas\n",
      "              dimensiongreaterthantheinput.Inthesecases,evenalinearencoderandalinear\n",
      "             decodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\n",
      "   aboutthedatadistribution.\n",
      "         Ideally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\n",
      "             thecodedimensionandthecapacityoftheencoderanddecoderbasedonthe\n",
      "         complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\n",
      "             abilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\n",
      "            anddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\n",
      "           functionthatencouragesthemodeltohaveotherpropertiesbesidestheability\n",
      "             tocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\n",
      "         representation,smallnessofthederivativeoftherepresentation,androbustness\n",
      "            tonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand\n",
      "           overcompletebutstilllearnsomethingusefulaboutthedatadistribution,evenif\n",
      "           themodelcapacityisgreatenoughtolearnatrivialidentityfunction.\n",
      "           Inadditiontothemethodsdescribedhere,whicharemostnaturallyinterpreted\n",
      "         asregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\n",
      "         andequippedwithaninferenceprocedure(forcomputinglatentrepresentations\n",
      "            giveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\n",
      "         modelingapproachesthatemphasizethisconnectionwithautoencodersarethe\n",
      "            descendantsoftheHelmholtzmachine(Hinton 1995betal.,),suchasthevariational\n",
      "5 0 1  C HAP T E R 1 4 . A UT O E NC O D E R S\n",
      "         autoencoder(section )andthegenerativestochasticnetworks(section). 20.10.3 20.12\n",
      "         Thesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\n",
      "            anddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\n",
      "          arenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\n",
      "              theprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\n",
      "  14.2.1SparseAutoencoders\n",
      "           Asparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa\n",
      "  sparsitypenaltyΩ(h    )onthecodelayerh      ,inadditiontothereconstructionerror:\n",
      "    L,gf , (x(()))+Ω()xh (14.2)\n",
      "whereg(h        )isthedecoderoutput,andtypicallywehaveh=f(x  ),theencoder\n",
      "output.\n",
      "           Sparseautoencodersaretypicallyusedtolearnfeaturesforanothertask,such\n",
      "           asclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemust\n",
      "             respondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\n",
      "             thansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe\n",
      "             copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\n",
      "   featuresasabyproduct.\n",
      "     Wecanthink ofthepenaltyΩ(h       )simplyasaregularizertermaddedto\n",
      "             afeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\n",
      "         (unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\n",
      "    (with asupervised learning objective) thatdepends on these sparsefeatures.\n",
      "           Unlikeotherregularizers,suchasweightdecay,thereisnotastraightforward\n",
      "          Bayesianinterpretationtothisregularizer.Asdescribedinsection,training 5.6.1\n",
      "           withweightdecayandotherregularizationpenaltiescanbeinterpretedasa\n",
      "         MAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\n",
      "          correspondingtoapriorprobabilitydistributionoverthemodelparameters.In\n",
      "       thisview,regularizedmaximumlikelihoodcorrespondstomaximizingp(  θx|),\n",
      "    whichisequivalenttomaximizing  logp(  xθ| )+  logp(θ). The  logp(  xθ| )term\n",
      "       istheusualdatalog-likelihoodterm,andthe  logp(θ    )term,thelog-priorover\n",
      "       parameters,incorporatesthepreferenceoverparticularvaluesofθ   .Thisviewis\n",
      "         describedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\n",
      "             becausetheregularizerdependsonthedataandisthereforebydeﬁnitionnota\n",
      "              priorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\n",
      "       termsasimplicitlyexpressingapreferenceoverfunctions.\n",
      "            Ratherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\n",
      "           task,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\n",
      "5 0 2  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "        maximumlikelihood trainingofagenerativemodel thathaslatentvariables.\n",
      "       Supposewehaveamodelwithvisiblevariablesx  andlatentvariablesh ,with\n",
      "   anexplicitjointdistributionp m o d e l( xh, )=p m o d e l(h)p m o d e l(  xh|   ).Wereferto\n",
      "p m o d e l(h          )asthemodel’spriordistributionoverthelatentvariables,representing\n",
      "     themodel’sbeliefspriortoseeingx        .Thisisdiﬀerentfromthewaywehave\n",
      "         previouslyusedtheword“prior,”torefertothedistributionp(θ  )encodingour\n",
      "            beliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.The\n",
      "    log-likelihoodcanbedecomposedas\n",
      " logp m o d e l() = logx\n",
      "hp m o d e l  ( )hx,. (14.3)\n",
      "             Wecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\n",
      "      forjustonehighlylikelyvalueforh        .Thisissimilartothesparsecodinggenerative\n",
      "    model(section),butwith 13.4h       beingtheoutputoftheparametricencoderrather\n",
      "          thantheresultofanoptimizationthatinfersthemostlikelyh    .Fromthispointof\n",
      "       view,withthischosen,wearemaximizing h\n",
      " logp m o d e l  ( ) = loghx,p m o d e l   ()+loghp m o d e l   ( )xh|. (14.4)\n",
      "  Thelogp m o d e l          ()htermcanbesparsityinducing.Forexample,theLaplaceprior,\n",
      "p m o d e l(h i) =λ\n",
      "2e−| λ h i| , (14.5)\n",
      "           correspondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\n",
      "    absolutevaluepenalty,weobtain\n",
      "Ω() = hλ\n",
      "i|h i |, (14.6)\n",
      " −logp m o d e l() =h\n",
      "i\n",
      "λh| i  |−logλ\n",
      "2\n",
      "   = Ω()+consth,(14.7)\n",
      "      wheretheconstanttermdependsonlyonλ andnoth   .Wetypicallytreatλ asa\n",
      "            hyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameter\n",
      "      learning.Otherpriors,suchastheStudentt     prior,canalsoinducesparsity.From\n",
      "           thispointofviewofsparsityasresultingfromtheeﬀectofp m o d e l(h  )onapproximate\n",
      "           maximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\n",
      "            all. Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables.\n",
      "             Thisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisaway\n",
      "            ofapproximatelytrainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor\n",
      "5 0 3  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "            whythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\n",
      "    variablesthatexplaintheinput.\n",
      "          Earlyworkonsparseautoencoders( ,,)explored Ranzatoetal.2007a2008\n",
      "           variousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\n",
      " andthe logZ         termthatariseswhenapplyingmaximumlikelihoodtoanundirected\n",
      " probabilisticmodelp(x) =1\n",
      "Z˜p(x     ).Theideaisthatminimizing logZ  preventsa\n",
      "         probabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity\n",
      "     on anautoencoder preventstheautoencoderfrom having lowreconstruction\n",
      "            erroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitive\n",
      "         understandingofageneralmechanismratherthanamathematicalcorrespondence.\n",
      "        Theinterpretationofthesparsitypenaltyascorrespondingto  logpmodel(h  )ina\n",
      "  directedmodelpmodel()hpmodel      ( )xh|ismoremathematicallystraightforward.\n",
      "      Onewaytoachieveactualzerosinh    forsparse(anddenoising)autoencoders\n",
      "               wasintroducedin ().Theideaistouserectiﬁedlinearunitsto Glorotetal.2011b\n",
      "            producethecodelayer.Withapriorthatactuallypushestherepresentationsto\n",
      "            zero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\n",
      "     numberofzerosintherepresentation.\n",
      "  14.2.2DenoisingAutoencoders\n",
      "              Ratherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder Ω\n",
      "            thatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\n",
      "function.\n",
      "    Traditionally,autoencodersminimizesomefunction\n",
      "  L,gf, (x(()))x (14.8)\n",
      "whereL    isalossfunctionpenalizingg(f(x    ))forbeingdissimilarfromx  ,suchas\n",
      "theL2    normoftheirdiﬀerence. Thisencourages  gf◦     tolearntobemerelyan\n",
      "         identityfunctioniftheyhavethecapacitytodoso.\n",
      "     A (DAE)insteadminimizes denoising autoencoder\n",
      " L,gf (x((˜ x))), (14.9)\n",
      "where˜x   isacopyofx         thathasbeencorruptedbysomeformofnoise.Denoising\n",
      "          autoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\n",
      "input.\n",
      "  Denoisingtrainingforcesfandg     toimplicitlylearnthestructureofpdata(x),\n",
      "        asshown by () and ().Denoising AlainandBengio2013 Bengio etal.2013c\n",
      "5 0 4  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "           autoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\n",
      "            asabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\n",
      "          howovercomplete,high-capacitymodelsmaybeusedasautoencodersaslong\n",
      "           ascareistakentopreventthemfromlearningtheidentityfunction. Denoising\n",
      "        autoencodersarepresentedinmoredetailinsection.14.5\n",
      "    14.2.3RegularizingbyPenalizingDerivatives\n",
      "              Anotherstrategyforregularizinganautoencoderistouseapenalty,asinsparse Ω\n",
      "autoencoders,\n",
      "     L,gf ,, ( x(()))+Ω( x h x) (14.10)\n",
      "      butwithadiﬀerentformof:Ω\n",
      " Ω( ) = h x,λ\n",
      "i||∇ xh i||2 . (14.11)\n",
      "             Thisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\n",
      "            changesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\n",
      "          theautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\n",
      "distribution.\n",
      "        Anautoencoderregularizedinthiswayiscalleda  contractiveautoencoder,\n",
      "         orCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,\n",
      "          manifoldlearning,andprobabilisticmodeling.TheCAEisdescribedinmore\n",
      "   detailinsection.14.7\n",
      "      14.3RepresentationalPower,LayerSizeandDepth\n",
      "             Autoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\n",
      "            decoder.However,thisisnotarequirement.Infact,usingdeepencodersand\n",
      "   decodersoﬀersmanyadvantages.\n",
      "             Recallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\n",
      "        wardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages\n",
      "           alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork,\n",
      "             asisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\n",
      "  beneﬁtfromdepth.\n",
      "          Onemajoradvantageofnontrivialdepthisthattheuniversalapproximator\n",
      "           theoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\n",
      "             layercanrepresentanapproximationofanyfunction(withinabroadclass)toan\n",
      "5 0 5  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "            arbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\n",
      "             thatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\n",
      "            functionalongthedomainofthedataarbitrarilywell.However,themappingfrom\n",
      "              inputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\n",
      "             constraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\n",
      "           leastoneadditionalhiddenlayerinsidetheencoderitself,canapproximateany\n",
      "          mappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.\n",
      "         Depthcanexponentiallyreducethecomputationalcostofrepresentingsome\n",
      "          functions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\n",
      "              neededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\n",
      "   depthinfeedforwardnetworks.\n",
      "        Experimentally,deepautoencodersyieldmuchbettercompressionthancorre-\n",
      "        spondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\n",
      "           Acommonstrategyfortrainingadeepautoencoderistogreedilypretrain\n",
      "            thedeeparchitecturebytrainingastackofshallowautoencoders,soweoften\n",
      "            encountershallowautoencoders,evenwhentheultimategoalistotrainadeep\n",
      "autoencoder.\n",
      "    14.4StochasticEncodersandDecoders\n",
      "          Autoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\n",
      "             unittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\n",
      "autoencoders.\n",
      "            Asdescribedinsection ,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\n",
      "             andthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistribution\n",
      "p(  yx|     )andminimizethenegativelog-likelihood  −log p(  yx|   ).Inthatsetting,y\n",
      "        isavectoroftargets,suchasclasslabels.\n",
      "  Inanautoencoder,x            isnowthetargetaswellastheinput.Yetwecanstill\n",
      "         applythesamemachineryasbefore.Givenahiddencodeh    ,wemaythinkof\n",
      "      thedecoderasprovidingaconditionaldistribution p d e c o d e r(  xh|   ).Wemaythen\n",
      "    traintheautoencoderbyminimizing  −log p d e c o d e r  ( )xh|     .Theexactformofthis\n",
      "        lossfunctionwillchangedependingontheformof p d e c o d e r  . Aswithtraditional\n",
      "            feedforwardnetworks,weusuallyuselinearoutputunitstoparametrizethemeanof\n",
      "   aGaussiandistributionifx        isrealvalued.Inthatcase,thenegativelog-likelihood\n",
      "       yieldsameansquarederrorcriterion.Similarly,binaryx  valuescorrespondto\n",
      "           aBernoullidistributionwhoseparametersaregivenbyasigmoidoutputunit,\n",
      "discretex          valuescorrespondtoasoftmaxdistribution,andsoon.Typically,the\n",
      "5 0 6  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "xx rrh h\n",
      "p e n c o d e r   ( ) h x | p d e c o d e r   ( ) x h |\n",
      "              Figure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\n",
      "            decoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat\n",
      "         theiroutputcanbeseenassampledfromadistribution,p e n c o d e r(  hx|   )fortheencoder\n",
      " andp d e c o d e r     ( )xh|forthedecoder.\n",
      "        outputvariablesaretreatedasbeingconditionallyindependentgivenh sothat\n",
      "          thisprobabilitydistributionisinexpensivetoevaluate,butsometechniques,such\n",
      "          asmixturedensityoutputs,allowtractablemodelingofoutputswithcorrelations.\n",
      "            Tomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\n",
      "        previously,wecanalsogeneralizethenotionofan   encodingfunctionf(x )to\n",
      "   anencodingdistributionp e n c o d e r       ( )hx|,asillustratedinﬁgure.14.2\n",
      "    Anylatentvariablemodelp m o d e l     ( )hx,deﬁnesastochasticencoder\n",
      "p e n c o d e r  ( ) = hx|p m o d e l   ( )hx| (14.12)\n",
      "   andastochasticdecoder\n",
      "p d e c o d e r  ( ) = xh|p m o d e l   ( )xh|. (14.13)\n",
      "          Ingeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\n",
      "      distributionscompatiblewithauniquejointdistributionp m o d e l( xh,   ).Alainetal.\n",
      "           ()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\n",
      "          willtendtomakethemcompatibleasymptotically(withenoughcapacityand\n",
      "examples).\n",
      "  14.5DenoisingAutoencoders\n",
      "The  denoising autoencoder        (DAE)isanautoencoderthatreceivesacorrupted\n",
      "             datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\n",
      "  asitsoutput.\n",
      "           TheDAEtrainingprocedureisillustratedinﬁgure.Weintroducea 14.3\n",
      " corruptionprocessC(˜  xx|      ), whichrepresentsaconditionaldistributionover\n",
      "5 0 7  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "˜ x˜ x L Lh h\n",
      "fg\n",
      "xxC ( ˜   x x | )\n",
      "            Figure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,\n",
      "        whichistrainedtoreconstructthecleandatapointx   fromitscorruptedversion˜x.\n",
      "      ThisisaccomplishedbyminimizingthelossL=  −logp d e c o d e r(  xh|=f(˜x )),where\n",
      "˜x       isacorruptedversionofthedataexamplex     ,obtainedthroughagivencorruption\n",
      "processC(˜  xx|   ).Typicallythedistributionp d e c o d e r     isafactorialdistributionwhosemean\n",
      "       parametersareemittedbyafeedforwardnetwork.g\n",
      " corruptedsamples˜x    ,givenadatasamplex     .Theautoencoderthenlearnsa\n",
      "  reconstructiondistributionp r e c o n s tr u c t( x|˜x    )estimatedfromtrainingpairs\n",
      "(x,˜  x)asfollows:\n",
      "        1. Sampleatrainingexamplefromthetrainingdata. x\n",
      "   2. Sampleacorruptedversion˜  xfromC(˜   xx|= )x.\n",
      "3. Use(x,˜x         )asatrainingexampleforestimatingtheautoencoderreconstruction\n",
      "distributionp r e c o n s tr u c t( x|˜x) =p d e c o d e r(  xh| )withh   theoutputofencoder\n",
      "f(˜  x)andp d e c o d e r      typicallydeﬁnedbyadecoder.g()h\n",
      "        Typicallywecansimplyperformgradient-basedapproximateminimization(such\n",
      "       asminibatchgradientdescent)onthenegativelog-likelihood  −logp d e c o d e r(  xh|).\n",
      "            Aslongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\n",
      "     network andmay be trainedwith exactlythesame techniques as anyother\n",
      " feedforwardnetwork.\n",
      "           WecanthereforeviewtheDAEasperformingstochasticgradientdescenton\n",
      "  thefollowingexpectation:\n",
      " − E x∼ ˆ p d a t a ( ) x E ˜x∼ C ( ˜x| x ) logp d e c o d e r   ( = (xh|f˜ x)), (14.14)\n",
      " whereˆp d a ta     ()xisthetrainingdistribution.\n",
      "5 0 8  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "   14.5.1EstimatingtheScore\n",
      "          Scorematching( ,)isanalternativetomaximumlikelihood.It Hyvärinen2005\n",
      "         providesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\n",
      "     themodeltohavethesamescore       asthedatadistributionateverytrainingpoint\n",
      "          x.Inthiscontext,thescoreisaparticulargradientﬁeld:\n",
      "∇ x  log()px. (14.15)\n",
      "           Scorematchingisdiscussedfurtherinsection.Forthepresentdiscussion, 18.4\n",
      "          regardingautoencoders,itissuﬃcienttounderstandthatlearningthegradient\n",
      "   ﬁeldoflogp d a ta        isonewaytolearnthestructureofp d a taitself.\n",
      "          AveryimportantpropertyofDAEsisthat theirtrainingcriterion(with\n",
      " conditionallyGaussianp(  xh|     ))makes theautoencoder learnavectorﬁeld\n",
      "(g(f(x)) −x           )thatestimatesthescoreofthedatadistribution.Thisisillustrated\n",
      "  inﬁgure.14.4\n",
      "          Denoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,\n",
      "           linearreconstructionunits)usingGaussiannoiseandmeansquarederrorasthe\n",
      "           reconstructioncostisequivalent( ,)totrainingaspeciﬁckindof Vincent2011\n",
      "          undirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.This\n",
      "              kindofmodelisdescribedindetailinsection ;forthepresentdiscussion,it 20.5.1\n",
      "           suﬃcestoknowthatitisamodelthatprovidesanexplicitp m o d e l(x;θ  ).Whenthe\n",
      "   RBMistrainedusing   denoising scorematching    ( ,), KingmaandLeCun2010\n",
      "          itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\n",
      "            autoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistent\n",
      "            estimator;itinsteadrecoversablurredversionofthedistribution.Ifthenoise\n",
      "            levelischosentoapproach0whenthenumberofexamplesapproachesinﬁnity,\n",
      "          however,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\n",
      "    moredetailinsection.18.5\n",
      "        OtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\n",
      "            appliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\n",
      "           combinedwitharegularizationtermsimilartothecontractivepenaltyofthe\n",
      "            CAE(Swersky2011BengioandDelalleau2009 etal.,). ()showedthatanautoen-\n",
      "         codergradientprovidesanapproximationtocontrastivedivergencetrainingof\n",
      "RBMs.\n",
      " Forcontinuous-valuedx       ,thedenoisingcriterionwithGaussiancorruptionand\n",
      "           reconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\n",
      "         generalencoderanddecoderparametrizations( ,).This AlainandBengio2013\n",
      "           meansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\n",
      "5 0 9  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "x˜ x\n",
      "  g f ◦\n",
      "˜ x\n",
      "C ( ˜   x x | )\n",
      "x\n",
      "            Figure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜x backto\n",
      "   theoriginaldatapointx    .Weillustratetrainingexamplesx     asredcrosseslyingneara\n",
      "           low-dimensionalmanifold,illustratedwiththeboldblackline.Weillustratethecorruption\n",
      "process C(˜  xx|          ) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\n",
      "            howonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.\n",
      "            Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\n",
      "|| g( f(˜x))−||x2  ,thereconstruction g( f(˜x)) estimates E x ,˜x∼ p dat a()( x C˜ x x|)[ x|˜x  ].Thevector\n",
      "g( f(˜x))−˜x         pointsapproximatelytowardthenearestpointonthemanifold,since g( f(˜x))\n",
      "        estimatesthecenterofmassofthecleanpointsx     thatcouldhavegivenriseto˜x .The\n",
      "     autoencoderthuslearnsavectorﬁeld g( f(x)) −x      indicatedbythegreenarrows.This\n",
      "    vectorﬁeldestimatesthescore∇ x log pdata(x        )uptoamultiplicativefactorthatisthe\n",
      "     averagerootmeansquarereconstructionerror.\n",
      "5 1 0  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "      bytrainingwiththesquarederrorcriterion\n",
      "||gf((˜  x x))−||2(14.16)\n",
      " andcorruption\n",
      "C(˜ x=˜ x x|) = (N˜   x x ;= µ,σΣ = 2 I) (14.17)\n",
      "   withnoisevarianceσ2          .Seeﬁgureforanillustrationofhowthisworks. 14.5\n",
      "        Ingeneral,thereisnoguaranteethatthereconstructiong(f( x  ))minusthe\n",
      "input x              correspondstothegradientofanyfunction,letalonetothescore.Thatis\n",
      "          whytheearlyresults(Vincent2011,)arespecializedtoparticularparametrizations,\n",
      "whereg(f( x)) − x         maybeobtainedbytakingthederivativeofanotherfunction.\n",
      "          KamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof ()by\n",
      "       identifyingafamilyofshallowautoencoderssuchthatg(f( x)) − x  correspondsto\n",
      "       ascoreforallmembersofthefamily.\n",
      "            Sofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\n",
      "           aprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoder\n",
      "            asagenerativemodelanddrawsamplesfromthisdistribution.Thisisdescribed\n",
      "  insection.20.11\n",
      "  14.5.1.1HistoricalPerspective\n",
      "              TheideaofusingMLPsfordenoisingdatesbacktotheworkof () LeCun1987\n",
      "            and (). ()alsousedrecurrentnetworkstodenoise Gallinarietal.1987Behnke2001\n",
      "           images.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\n",
      "            Thename“denoisingautoencoder,”however,referstoamodelthatisintendednot\n",
      "             merelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation\n",
      "       as asideeﬀect oflearningto denoise.This ideacame muchlater (Vincent\n",
      "             etal.,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\n",
      "         deeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\n",
      "        sparsecoding,contractiveautoencoders,andotherregularizedautoencoders,the\n",
      "            motivationforDAEswastoallowthelearningofaveryhigh-capacityencoder\n",
      "           whilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\n",
      "           PriortotheintroductionofthemodernDAE,InayoshiandKurita2005()\n",
      "             exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\n",
      "          minimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\n",
      "             noiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\n",
      "          generalizationbyintroducingthereconstructionerrorandtheinjectednoise.Their\n",
      "            methodwasbasedonalinearencoder,however,andcouldnotlearnfunction\n",
      "       familiesaspowerfulasthemodernDAEcan.\n",
      "5 1 1  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "    Figure 14.5:Vector ﬁeldlearned by adenoising autoencoder around a1-D curved\n",
      "             manifoldnearwhichthedataconcentrateina2-Dspace.Eacharrowisproportional\n",
      "            tothereconstructionminusinputvectoroftheautoencoderandpointstowardshigher\n",
      "          probabilityaccordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeld\n",
      "              haszerosatbothmaximaoftheestimateddensityfunction(onthedatamanifolds)and\n",
      "               atminimaofthatdensityfunction.Forexample,thespiralarmformsa1-Dmanifoldof\n",
      "              localmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\n",
      "               thegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelengthof\n",
      "             thearrows)islarge,probabilitycanbesigniﬁcantlyincreasedbymovinginthedirection\n",
      "               ofthearrow,andthatismostlythecaseinplacesoflowprobability.Theautoencoder\n",
      "          mapstheselowprobabilitypointstohigherprobabilityreconstructions.Whereprobability\n",
      "           ismaximal,thearrowsshrinkbecausethereconstructionbecomesmoreaccurate.Figure\n",
      "       reproducedwithpermissionfrom (). AlainandBengio2013\n",
      "5 1 2  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "    14.6LearningManifoldswithAutoencoders\n",
      "  Like many other machine learning algorithms, autoencoders exploittheidea\n",
      "            thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\n",
      "          manifolds,asdescribedinsection .Somemachinelearningalgorithmsexploit 5.11.3\n",
      "              thisideaonlyinsofarastheylearnafunctionthatbehavescorrectlyonthemanifold\n",
      "              butthatmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold.\n",
      "             Autoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\n",
      "          Tounderstandhowautoencodersdothis,wemustpresentsomeimportant\n",
      "  characteristicsofmanifolds.\n",
      "          Animportantcharacterizationofamanifoldisthesetofits  tangentplanes.\n",
      "  Atapoint x ona d        -dimensionalmanifold,thetangentplaneisgivenby dbasis\n",
      "            vectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\n",
      "           illustratedinﬁgure,theselocaldirectionsspecifyhowonecanchange 14.6 x\n",
      "     inﬁnitesimallywhilestayingonthemanifold.\n",
      "         Allautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\n",
      "1.   Learningarepresentation h   ofatrainingexample x suchthat x canbe\n",
      "  approximatelyrecoveredfrom h      throughadecoder.Thefactthat xis\n",
      "           drawnfromthetrainingdataiscrucial,becauseitmeanstheautoencoder\n",
      "          neednotsuccessfullyreconstructinputsthatarenotprobableunderthe\n",
      " data-generatingdistribution.\n",
      "          2. Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-\n",
      "            turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\n",
      "         aregularizationtermaddedtothereconstructioncost.Thesetechniques\n",
      "         generallyprefersolutionsthatarelesssensitivetotheinput.\n",
      "           Clearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutput\n",
      "               isnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\n",
      "          areusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\n",
      "         aboutthestructureofthedata-generatingdistribution.Theimportantprinciple\n",
      "             isthattheautoencodercanaﬀordtorepresentonlythevariationsthatareneeded\n",
      "        toreconstructtrainingexamples.Ifthedata-generatingdistributionconcentrates\n",
      "         nearalow-dimensionalmanifold,thisyieldsrepresentationsthatimplicitlycapture\n",
      "            alocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\n",
      " manifoldaround x     needtocorrespondtochangesin h = f ( x   ).Hencetheencoder\n",
      "      learnsamappingfromtheinputspace x      toarepresentationspace,amappingthat\n",
      "             isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\n",
      "    changesorthogonaltothemanifold.\n",
      "5 1 3  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "               Figure14.6:Anillustrationoftheconceptofatangenthyperplane.Herewecreatea1-D\n",
      "               manifoldin784-Dspace.WetakeanMNISTimagewith784pixelsandtransformitby\n",
      "          translatingitvertically. Theamountofverticaltranslationdeﬁnesacoordinatealong\n",
      "              a1-Dmanifoldthattracesoutacurvedpaththroughimagespace. Thi splotshowsa\n",
      "           fewpointsalongthismanifold. Forvisualization,wehaveprojectedthemanifoldinto\n",
      "    2-DspaceusingPCA.An n    -dimensionalmanifoldhasan n   -dimensionaltangentplane\n",
      "              ateverypoint.Thistangentplanetouchesthemanifoldexactlyatthatpointandis\n",
      "               orientedparalleltothesurfaceatthatpoint.Itdeﬁnesthespaceofdirectionsinwhich\n",
      "               itispossibletomovewhileremainingonthemanifold.This1-Dmanifoldhasasingle\n",
      "              tangentline.Weindicateanexampletangentlineatonepoint,withanimageshowing\n",
      "              howthistangentdirectionappearsinimagespace.Graypixelsindicatepixelsthatdonot\n",
      "              changeaswemovealongthetangentline,whitepixelsindicatepixelsthatbrighten,and\n",
      "     blackpixelsindicatepixelsthatdarken.\n",
      "5 1 4  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "           Aone-dimensionalexampleisillustratedinﬁgure,showingthat,bymaking 14.7\n",
      "          thereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\n",
      "          datapoints,wecausetheautoencodertorecoverthemanifoldstructure.\n",
      "           Tounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\n",
      "           structivetocomparethemtootherapproaches.Whatismostcommonlylearned\n",
      "     tocharacterizeamanifoldisa r e pr e s e n t a t i o n      ofthedatapointson(ornear)\n",
      "            themanifold.Sucharepresentationforaparticularexampleisalsocalledits\n",
      "           embedding.Itistypicallygivenbyalow-dimensionalvector,withfewerdimensions\n",
      "            thanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Some\n",
      "       algorithms(nonparametricmanifoldlearningalgorithms,discussedbelow)directly\n",
      "            learnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\n",
      "          mapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\n",
      "          pointintheambientspace(theinputspace)toitsembedding.\n",
      "x 0 x 1 x 2\n",
      "x0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Identity\n",
      " Optimalreconstruction\n",
      "             Figure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\n",
      "             perturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\n",
      "           themanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\n",
      "          lineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction\n",
      "            functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\n",
      "        arrowsatthebottomoftheplotindicatethe r( x) − x    reconstructiondirectionvectorat\n",
      "              thebaseofthearrow,ininputspace,alwayspointingtowardthenearest“manifold”(a\n",
      "            singledatapointinthe1-Dcase). Thedenoisingautoencoderexplicitlytriestomake\n",
      "     thederivativeofthereconstructionfunction r( x      )smallaroundthedatapoints.The\n",
      "           contractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x )is\n",
      "                askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\n",
      "            spacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\n",
      "            thereconstructionfunctionmusthavealargederivativetomapcorruptedpointsback\n",
      "  ontothemanifold.\n",
      "5 1 5  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "          Figure14.8:Nonparametricmanifoldlearningproceduresbuildanearestneighborgraph\n",
      "           inwhichnodesrepresenttrainingexamplesanddirectededgesindicatenearestneighbor\n",
      "          relationships. Variousprocedurescanthusobtainthetangentplaneassociatedwitha\n",
      "             neighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\n",
      "      examplewithareal-valuedvectorposition,orembedding     .Itispossibletogeneralize\n",
      "              sucharepresentationtonewexamplesbyaformofinterpolation. Aslongasthenumber\n",
      "              ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\n",
      "            approachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gongetal.\n",
      "2000).\n",
      "         Manifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\n",
      "           attempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\n",
      "      onlearningnonlinearmanifoldshasfocusedonnonparametric  methodsbased\n",
      " onthe   nearestneighborgraph        .Thisgraphhasonenodepertrainingexample\n",
      "          andedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopf\n",
      "             etal.,;1998RoweisandSaul2000Tenenbaum 2000Brand2003Belkin ,; etal.,;,;\n",
      "           andNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\n",
      "             andRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachnodewitha\n",
      "           tangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerence\n",
      "           vectorsbetweentheexampleanditsneighbors,asillustratedinﬁgure.14.8\n",
      "           Aglobalcoordinatesystemcanthenbeobtainedthroughanoptimizationor\n",
      "              bysolvingalinearsystem.Figureillustrateshowamanifoldcanbetiledby 14.9\n",
      "5 1 6  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "               Figure14.9:Ifthetangentplanes(seeﬁgure)ateachlocationareknown,thenthey 14.6\n",
      "               canbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\n",
      "                canbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or\n",
      "              “pancake,”withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\n",
      "             verylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.A\n",
      "            mixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold\n",
      "           Parzenwindowalgorithm( ,)orinitsnonlocalneural-net-based VincentandBengio2003\n",
      "    variant( ,). Bengioetal.2006c\n",
      "5 1 7  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "           alargenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausethe\n",
      "      Gaussiansareﬂatinthetangentdirections).\n",
      "         Afundamentaldiﬃcultywithsuchlocalnonparametricapproachestomanifold\n",
      "             learningisraisedin ():ifthemanifoldsarenotvery BengioandMonperrus2005\n",
      "             smooth(theyhavemanypeaksandtroughsandtwists),onemayneedavery\n",
      "            largenumberoftrainingexamplestocovereachoneofthesevariations,with\n",
      "           nochancetogeneralizetounseenvariations.Indeed,thesemethodscanonly\n",
      "          generalizetheshapeofthemanifoldbyinterpolatingbetweenneighboringexamples.\n",
      "          Unfortunately,themanifoldsinvolvedinAIproblemscanhaveverycomplicated\n",
      "           structuresthatcanbediﬃculttocapturefromonlylocalinterpolation.Consider\n",
      "            forexamplethemanifoldresultingfromtranslationshowninﬁgure.Ifwe 14.6\n",
      "       watchjustonecoordinatewithintheinputvector,x i     ,astheimageistranslated,\n",
      "              wewillobservethatonecoordinateencountersapeakoratroughinitsvalue\n",
      "              onceforeverypeakortroughinbrightnessintheimage.Inotherwords,the\n",
      "           complexityofthepatternsofbrightnessinanunderlyingimagetemplatedrives\n",
      "           thecomplexityofthemanifoldsthataregeneratedbyperformingsimpleimage\n",
      "         transformations.Thismotivatestheuseofdistributedrepresentationsanddeep\n",
      "    learningforcapturingmanifoldstructure.\n",
      "  14.7ContractiveAutoencoders\n",
      "          Thecontractiveautoencoder( ,,)introducesanexplicitregularizer Rifaietal.2011ab\n",
      "  onthecode h=f( x    ),encouragingthederivativesoff     tobeassmallaspossible:\n",
      "Ω() = hλ∂f() x\n",
      "∂ x2\n",
      "F . (14.18)\n",
      "  ThepenaltyΩ( h           )isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\n",
      "         Jacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\n",
      "          Thereisaconnectionbetweenthedenoisingautoencoderandthecontractive\n",
      "            autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\n",
      "    input noise, the denoi sing reconstruc tionerroris equivalent toacontractive\n",
      "      penaltyonthereconstructionfunctionthatmaps xto r=g(f( x  )).Inother\n",
      "         words,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\n",
      "         ﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\n",
      "         featureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.When\n",
      "       usingtheJacobian-basedcontractivepenaltytopretrainfeaturesf( x  )foruse\n",
      "           withaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthe\n",
      "5 1 8  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "  contractivepenaltyto f( x   )ratherthanto g( f( x    )).Acontractivepenaltyon f( x)\n",
      "           alsohascloseconnectionstoscorematching,asdiscussedinsection .14.5.1\n",
      " Thename c o n t r a c t i v e         arisesfromthewaythattheCAEwarpsspace.Speciﬁ-\n",
      "              cally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\n",
      "             tomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\n",
      "             Wecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput\n",
      "neighborhood.\n",
      "           Toclarify,theCAEiscontractiveonlylocally—allperturbationsofatraining\n",
      "point x   aremappednearto f( x    ).Globally,twodiﬀerentpoints xand x maybe\n",
      " mappedto f( x )and f( x         )pointsthatarefartherapartthantheoriginalpoints.\n",
      "   Itisplausiblethat f         couldbeexpandingin-betweenorfarfromthedatamanifolds\n",
      "              (see,forexample,whathappensinthe1-Dtoyexampleofﬁgure).Whenthe 14.7\n",
      "Ω( h              )penaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianis\n",
      "               tomakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01\n",
      "             inputpointswithextremevaluesofthesigmoid,whichmaybeinterpretedasa\n",
      "             binarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\n",
      "          mostofthehypercubethatitssigmoidalhiddenunitscanspan.\n",
      "      WecanthinkoftheJacobianmatrix J  atapoint x  asapproximatingthe\n",
      " nonlinearencoder f( x            )asbeingalinearoperator.Thisallowsustousetheword\n",
      "          “contractive”moreformally. Inthetheoryoflinearoperators,alinearoperator\n",
      "        issaidtobecontractiveifthenormof J x         remainslessthanorequaltoforall 1\n",
      "unit-norm x   .Inotherwords, J         iscontractiveifitsimageoftheunitsphereis\n",
      "             completelyencapsulatedbytheunitsphere.WecanthinkoftheCAEaspenalizing\n",
      "        theFrobeniusnormofthelocallinearapproximationof f( x   )ateverytraining\n",
      "point x            inordertoencourageeachoftheselocallinearoperatorstobecomea\n",
      "contraction.\n",
      "       Asdescribed insection, regularizedautoencoderslearnmanifoldsby 14.6\n",
      "             balancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare\n",
      "      reconstructionerrorandthecontractivepenaltyΩ( h   ).Reconstructionerroralone\n",
      "           wouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\n",
      "             alonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\n",
      "          Thecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\n",
      " ∂f ( ) x\n",
      "∂ x            aremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\n",
      "          smallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives.\n",
      "              ThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions x\n",
      " withlarge J x  rapidlychange h         ,sothesearelikelytobedirectionsthatapproximate\n",
      "           thetangentplanesofthemanifold. Experimentsby (,)show Rifaietal.2011ab\n",
      "         thattrainingtheCAEresultsinmostsingularvaluesof J    droppingbelowin1\n",
      "5 1 9  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "Input\n",
      "point Tangentvectors\n",
      "     LocalPCA(nosharingacrossregions)\n",
      " Contractiveautoencoder\n",
      "            Figure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\n",
      "              andbyacontractiveautoencoder.Thelocationonthemanifoldisdeﬁnedbytheinput\n",
      "            imageofadogdrawnfromtheCIFAR-10dataset. Thetangentvectorsareestimated\n",
      "        bytheleadingsingularvectorsoftheJacobianmatrix∂ h\n",
      "∂ x   oftheinput-to-codemapping.\n",
      "               AlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto\n",
      "           formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\n",
      "            sharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits. TheCAE\n",
      "             tangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\n",
      "            theheadorlegs).Imagesreproducedwithpermissionfrom (). Rifaietal.2011c\n",
      "         magnitudeandthereforebecomingcontractive.Somesingularvaluesremainabove\n",
      "           1,however,becausethereconstructionerrorpenaltyencouragestheCAEtoencode\n",
      "           thedirectionswiththemostlocalvariance.Thedirectionscorrespondingtothe\n",
      "           largestsingularvaluesareinterpretedasthetangentdirectionsthatthecontractive\n",
      "         autoencoderhaslearned.Ideally,thesetangentdirectionsshouldcorrespondto\n",
      "             realvariationsinthedata.Forexample,aCAEappliedtoimagesshouldlearn\n",
      "             tangentvectorsthatshowhowtheimagechangesasobjectsintheimagegradually\n",
      "           changepose,asshowninﬁgure.Visualizationsoftheexperimentallyobtained 14.6\n",
      "           singularvectorsdoseemtocorrespondtomeaningfultransformationsoftheinput\n",
      "     image,asshowninﬁgure.14.10\n",
      "           OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\n",
      "              ischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\n",
      "            muchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\n",
      "             Rifai 2011aetal.()istoseparatelytrainaseriesofsingle-layerautoencoders,each\n",
      "         trainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecomposition\n",
      "           oftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\n",
      "          separatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive\n",
      "               aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\n",
      "              theentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\n",
      "5 2 0  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "      capturesmanyofthedesirablequalitativecharacteristics.\n",
      "           Anotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\n",
      "               ifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\n",
      "         couldconsistofmultiplyingtheinputbyasmallconstant   ,andthedecoder\n",
      "      couldconsistofdividingthecodeby .As      approaches,theencoderdrivesthe 0\n",
      "  contractivepenaltyΩ(h         )toapproachwithouthavinglearnedanythingaboutthe 0\n",
      "        distribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\n",
      "          etal.(),thisispreventedbytyingtheweightsof 2011a fandg .Bothfandgare\n",
      "          standardneuralnetworklayersconsistingofanaﬃnetransformationfollowedby\n",
      "            anelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixofg\n",
      "         tobethetransposeoftheweightmatrixof.f\n",
      "   14.8PredictiveSparseDecomposition\n",
      "  Predictivesparsedecomposition         (PSD)isamodelthatisahybridofsparse\n",
      "         codingandparametricautoencoders(Kavukcuoglu 2008etal.,).Aparametric\n",
      "            encoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\n",
      "           appliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\n",
      "              (Kavukcuoglu20092010Jarrett2009Farabet2011 etal.,,; etal.,; etal.,),aswell\n",
      "            asforaudio( ,).Themodelconsistsofanencoder Henaﬀetal.2011 f(x  )anda\n",
      "decoderg(h      )thatarebothparametric.Duringtraining,h   iscontrolledbythe\n",
      "     optimizationalgorithm.Trainingproceedsbyminimizing\n",
      "  ||−||xg()h2 +λ||h 1   + ()γf||−hx||2 . (14.19)\n",
      "          Asinsparsecoding,thetrainingalgorithmalternatesbetweenminimizationwith\n",
      " respecttoh        andminimizationwithrespecttothemodelparameters.Minimization\n",
      "  withrespecttoh  isfastbecausef(x      )providesagoodinitialvalueofh  ,andthe\n",
      "  costfunctionconstrainsh  toremainnearf(x    )anyway.Simplegradientdescent\n",
      "           canobtainreasonablevaluesofinasfewastensteps. h\n",
      "            ThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparse\n",
      "    codingmodelandthentrainingf(x        )topredictthevaluesofthesparsecoding\n",
      "          features.ThePSDtrainingprocedureregularizesthedecodertouseparameters\n",
      "       forwhichcaninfergoodcodevalues. f()x\n",
      "      Predictivesparsecodingisanexampleof   learnedapproximateinference.\n",
      "             Insection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19\n",
      "             makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\n",
      "           probabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\n",
      "model.\n",
      "5 2 1  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "           InpracticalapplicationsofPSD,theiterativeoptimizationisusedonlyduring\n",
      "   training.Theparametricencoder f       isusedtocomputethelearnedfeatureswhen\n",
      "    themodelisdeployed.Evaluating f    iscomputationallyinexpensivecomparedto\n",
      "inferring h   viagradientdescent.Because f    isadiﬀerentiableparametricfunction,\n",
      "              PSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained\n",
      "  withanothercriterion.\n",
      "   14.9ApplicationsofAutoencoders\n",
      "         Autoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\n",
      "          mationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplications\n",
      "            ofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\n",
      "         forstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\n",
      "             astackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\n",
      "            withgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The\n",
      "           resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensions,and\n",
      "           thelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\n",
      "        underlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\n",
      "       Lower-dimensionalrepresentationscanimproveperformanceonmanytasks,\n",
      "           suchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemory andruntime.\n",
      "         Manyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\n",
      "            eachother,asobservedbySalakhutdinovandHinton2007b Torralba ()and etal.\n",
      "           ().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\n",
      "generalization.\n",
      "           Onetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionis\n",
      " informationretrieval           ,thetaskofﬁndingentriesinadatabasethatresemblea\n",
      "         queryentry. Thistaskderivestheusualbeneﬁtsfromdimensionalityreduction\n",
      "             thatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecome\n",
      "        extremelyeﬃcientincertainkindsoflow-dimensionalspaces.Speciﬁcally, if\n",
      "            wetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\n",
      "             dimensionaland,thenwecanstorealldatabaseentriesinahashtable binary\n",
      "            thatmapsbinarycodevectorstoentries. Thishashtableallowsustoperform\n",
      "           informationretrievalbyreturningalldatabaseentriesthathavethesamebinary\n",
      "        codeasthe query.Wecanalso search overslightlylesssimilar entries very\n",
      "           eﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery. This\n",
      "        approachtoinformationretrievalviadimensionalityreductionandbinarization\n",
      " iscalled  semantichashing       (SalakhutdinovandHinton2007b2009b ,,)andhas\n",
      "           beenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\n",
      "5 2 2  C HAP T E R 1 4 . AUT O E NC O D E R S\n",
      "            images(Torralba2008Weiss2008KrizhevskyandHinton2011 etal.,;etal.,; ,).\n",
      "           Toproducebinarycodesforsemantichashing,onetypicallyusesanencoding\n",
      "              functionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobe\n",
      "               saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\n",
      "            thisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\n",
      "            training.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthat\n",
      "            noiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\n",
      "          magnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\n",
      "            Theideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\n",
      "            directions,includingtheideaoftrainingtherepresentationstooptimizealossmore\n",
      "             directlylinkedtothetaskofﬁndingnearbyexamplesinthehashtable(Norouzi\n",
      "  andFleet2011,).\n",
      "5 2 3 C h a p t e r 1 5\n",
      " Re pre s e n t at i on Learning\n",
      "             Inthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhow\n",
      "            thenotionofrepresentationcanbeusefultodesigndeeparchitectures.Weexplore\n",
      "         howlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,including\n",
      "         usinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\n",
      "           representationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\n",
      "              learnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask\n",
      "            representationexists.Finally,westepbackandargueaboutthereasonsforthe\n",
      "         successofrepresentationlearning,startingwiththetheoreticaladvantagesof\n",
      "         distributedrepresentations( ,)anddeeprepresentations,ending Hintonetal.1986\n",
      "          withthemoregeneralideaofunderlyingassumptionsaboutthedata-generating\n",
      "         process,inparticularaboutunderlyingcausesoftheobserveddata.\n",
      "           Manyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdepending\n",
      "            onhowtheinformationisrepresented.Thisisageneralprincipleapplicableto\n",
      "             dailylife,tocomputerscienceingeneral,andtomachinelearning.Forexample,it\n",
      "             isstraightforwardforapersontodivide210by6usinglongdivision. Thetask\n",
      "           becomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\n",
      "           numeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\n",
      "            byVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\n",
      "            permittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\n",
      "          concretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing\n",
      "        appropriateorinappropriaterepresentations.Forexample,insertinganumber\n",
      "           intothecorrectpositioninasortedlistofnumbersisan O( n   )operationifthe\n",
      "        listisrepresentedasalinkedlist,butonly O( log n       )ifthelistisrepresentedasa\n",
      " red-blacktree.\n",
      "           Inthecontextofmachinelearning,whatmakesonerepresentationbetterthan\n",
      "524   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "           another?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\n",
      "            learningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\n",
      "    ofthesubsequentlearningtask.\n",
      "           Wecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\n",
      "            formingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetwork\n",
      "             istypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestof\n",
      "            thenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwitha\n",
      "           supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\n",
      "             moresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcation\n",
      "            taskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\n",
      "            featuresmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\n",
      "             lastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer\n",
      "          (SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\n",
      "          learndiﬀerentpropertiesdependingonthetypeofthelastlayer.\n",
      "         Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\n",
      "          anyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\n",
      "          learningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\n",
      "            someparticularway.Forexample,supposewewanttolearnarepresentationthat\n",
      "         makesdensityestimationeasier.Distributionswithmoreindependencesareeasier\n",
      "            tomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\n",
      "   oftherepresentationvector h      tobeindependent.Justlikesupervisednetworks,\n",
      "          unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\n",
      "            learnarepresentationasasideeﬀect.Regardlessofhowarepresentationwas\n",
      "           obtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\n",
      "          supervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\n",
      "representation.\n",
      "         Mostrepresentationlearningproblemsfaceatrade-oﬀbetweenpreservingas\n",
      "           muchinformationabouttheinputaspossibleandattainingniceproperties(such\n",
      " asindependence).\n",
      "      Representationlearningisparticularlyinterestingbecauseit provides one\n",
      "          waytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\n",
      "          largeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\n",
      "          data.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\n",
      "          resultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolve\n",
      "          thisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,\n",
      "            wecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese\n",
      "      representationstosolvethesupervisedlearningtask.\n",
      "             Humansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\n",
      "5 2 5   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "            notyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\n",
      "          performance—forexample,thebrainmayuseverylargeensemblesofclassiﬁers\n",
      "           orBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\n",
      "          abletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\n",
      "             toleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\n",
      "         unlabeleddatacanbeusedtolearnagoodrepresentation.\n",
      "    15.1GreedyLayer-WiseUnsupervisedPretraining\n",
      "            Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\n",
      "            networks,enablingresearchersfortheﬁrsttimetotrainadeepsupervisednetwork\n",
      "        withoutrequiringarchitecturalspecializationslikeconvolutionorrecurrence.We\n",
      "  callthisprocedure  unsupervisedpretraining   ,ormoreprecisely, greedylayer-\n",
      "  wiseunsupervisedpretraining        .Thisprocedureisacanonicalexampleofhow\n",
      "          arepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture\n",
      "            theshapeoftheinputdistribution)cansometimesbeusefulforanothertask\n",
      "      (supervisedlearningwiththesameinputdomain).\n",
      "        Greedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\n",
      "           tationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\n",
      "           codingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\n",
      "          pretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\n",
      "            andproducingasoutputanewrepresentationofthedata,whosedistribution(or\n",
      "            itsrelationtoothervariables,suchascategoriestopredict)ishopefullysimpler.\n",
      "      Seealgorithmforaformaldescription. 15.1\n",
      "         Greedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\n",
      "               beenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnet\n",
      "              forasupervisedtask.Thisapproachdatesbackatleastasfarastheneocognitron\n",
      "           (Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\n",
      "             thatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitializationfor\n",
      "              ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\n",
      "           tosuccessfullytrainevenfullyconnectedarchitectures(Hinton 2006Hinton etal.,;\n",
      "            andSalakhutdinov2006Hinton2006Bengio2007Ranzato2007a ,;,; etal.,; etal.,).\n",
      "           Priortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\n",
      "            resultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\n",
      "           thatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep\n",
      "          architectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodto\n",
      "succeed.\n",
      "    Greedylayer-wisepretrainingiscalledgreedy    becauseitisa greedyalgo-\n",
      "5 2 6   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "r i t hm           ,meaningthatitoptimizeseachpieceofthesolutionindependently,one\n",
      "            pieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi s e\n",
      "           becausetheseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedy\n",
      "         layer-wisepretrainingproceedsonelayeratatime,trainingthe k  -thlayerwhile\n",
      "            keepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrained\n",
      "            ﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -\n",
      "v i s e d          becauseeachlayeristrainedwithanunsupervisedrepresentationlearning\n",
      "     algorithm.However,itisalsocalled pr e t r a i n i n g      becauseitissupposedtobe\n",
      "           onlyaﬁrststepbeforeajointtrainingalgorithmisappliedto ﬁne-t une allthe\n",
      "             layerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\n",
      "          asaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\n",
      "        decreasingtrainingerror)andaformofparameterinitialization.\n",
      "              Itiscommontousetheword“pretraining”torefernotonlytothepretraining\n",
      "           stageitselfbuttotheentiretwo-phaseprotocolthatcombinesthepretraining\n",
      "           phaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\n",
      "             trainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,\n",
      "           oritmayinvolvesupervisedﬁne-tuning oftheentirenetworklearnedinthe\n",
      "          pretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\n",
      "             whatmodeltypeisemployed,inmostcases,theoveralltrainingschemeisnearly\n",
      "           thesame.Whilethechoiceofunsupervisedlearningalgorithmwillobviouslyaﬀect\n",
      "          thedetails,mostapplicationsofunsupervisedpretrainingfollowthisbasicprotocol.\n",
      "         Greedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\n",
      "         forotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\n",
      "          andSalakhutdinov2006,)andprobabilisticmodelswithmanylayersoflatent\n",
      "            variables.Suchmodelsincludedeepbeliefnetworks(Hinton 2006etal.,)anddeep\n",
      "        Boltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\n",
      "     modelsaredescribedinchapter.20\n",
      "           Asdiscussedinsection, itisalsopossibletohavegreedylayer-wise 8.7.4\n",
      "           supervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\n",
      "              iseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\n",
      "    contexts( ,). Erhanetal.2010\n",
      "       15.1.1WhenandWhyDoesUnsupervisedPretrainingWork?\n",
      "         Onmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\n",
      "          improvementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsible\n",
      "             fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\n",
      "5 2 7   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "      Algorithm15.1Greedylayer-wiseunsupervisedpretrainingprotocol\n",
      "     Giventhefollowing: UnsupervisedfeaturelearningalgorithmL   ,whichtakesa\n",
      "          trainingsetofexamplesandreturnsanencoderorfeaturefunctionf  .Theraw\n",
      "  inputdataisX      ,withonerowperexample,andf(1 )(X      )istheoutputoftheﬁrst\n",
      "  stageencoderonX           .Inthecasewhereﬁne-tuningisperformed,weusealearner\n",
      "T     ,whichtakesaninitialfunctionf  ,inputexamplesX   (andinthesupervised\n",
      "   ﬁne-tuningcase,associatedtargetsY       ),andreturnsatunedfunction.Thenumber\n",
      "   ofstagesis.m\n",
      "   f←Identityfunction\n",
      "˜ XX= \n",
      "        for do k,...,m = 1\n",
      "f( ) k= (L˜X)\n",
      "  ff←( ) k ◦f\n",
      "˜  X←f( ) k(˜X)\n",
      " endfor\n",
      "  ifﬁne-tuning t h e n\n",
      "      ff,,←T(XY)\n",
      " e nd i f\n",
      " Returnf\n",
      "             2006Bengio2007Ranzato2007a ; etal.,; etal.,).Onmanyothertasks,however,\n",
      "           unsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeable\n",
      "            harm. ()studiedtheeﬀectofpretrainingonmachinelearning Maetal.2015\n",
      "           modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\n",
      "          slightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervised\n",
      "           pretrainingissometimeshelpfulbutoftenharmful,itisimportanttounderstand\n",
      "               whenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\n",
      "task.\n",
      "              Attheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\n",
      "         togreedyunsupervisedpretraininginparticular.Thereareother,completely\n",
      "        diﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\n",
      "             suchasvirtualadversarialtraining,describedinsection.Itisalsopossibleto 7.13\n",
      "             trainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\n",
      "         Examplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\n",
      "              andBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\n",
      "               objectiveisanexplicitsumofthetwoterms(oneusingthelabels,andoneonly\n",
      "  usingtheinput).\n",
      "          Unsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof\n",
      "5 2 8   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "              theideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\n",
      "              asigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcan\n",
      "            improveoptimization).Second,itmakesuseofthemoregeneralideathatlearning\n",
      "            abouttheinputdistributioncanhelpwithlearningaboutthemappingfrominputs\n",
      " tooutputs.\n",
      "          Bothideasinvolvemanycomplicatedinteractionsbetweenseveralpartsofthe\n",
      "       machinelearningalgorithmthatarenotentirelyunderstood.\n",
      "             Theﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\n",
      "            canhaveastrongregularizingeﬀectonitsperformance,istheleastunderstood.\n",
      "            Atthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\n",
      "              modelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\n",
      "           another. Today,localminimaarenolongerconsideredtobeaseriousproblem\n",
      "           forneuralnetworkoptimization.Wenowknowthatourstandardneuralnetwork\n",
      "              trainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains\n",
      "           possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\n",
      "            beinaccessible—forexample,aregionthatissurroundedbyareaswherethecost\n",
      "            functionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\n",
      "              averynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\n",
      "           Hessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\n",
      "            verysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\n",
      "          pretrainedparametersareretainedduringthesupervisedtrainingstageislimited.\n",
      "          Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\n",
      "          learningandsupervisedlearningratherthantwosequentialstages.Onemay\n",
      "           alsoavoidstrugglingwiththesecomplicatedideasabouthowoptimizationinthe\n",
      "        supervisedlearningstagepreservesinformationfromtheunsupervisedlearning\n",
      "       stagebysimply freezing the parameters forthefeature extractorsand using\n",
      "            supervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures.\n",
      "            Theotheridea,thatalearningalgorithmcanuseinformationlearnedinthe\n",
      "           unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\n",
      "             understood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\n",
      "              taskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\n",
      "              agenerativemodelofimagesofcarsandmotorcycl es,itwillneedtoknowabout\n",
      "              wheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\n",
      "               therepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\n",
      "            learnertoaccess.Thisisnotyetunderstoodatamathematical,theoreticallevel,\n",
      "             soitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervised\n",
      "            learninginthisway.Manyaspectsofthisapproacharehighlydependenton\n",
      "              thespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron\n",
      "5 2 9   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "           topofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\n",
      "            separable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\n",
      "          isanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\n",
      "         preferable—theconstraintsimposedbytheoutputlayerarenaturallyincluded\n",
      "  fromthestart.\n",
      "           Fromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\n",
      "           wecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitial\n",
      "            representationispoor. Onekeyexampleofthisistheuseofwordembeddings.\n",
      "          Wordsrepresentedbyone-hotvectorsarenotveryinformativebecauseevery\n",
      "           twodistinctone-hotvectorsarethesamedistancefromeachother(squared L2\n",
      "         distanceof).Learnedwordembeddingsnaturallyencodesimilaritybetween 2\n",
      "           wordsbytheirdistancefromeachother.Becauseofthis,unsupervisedpretraining\n",
      "            isespeciallyusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,\n",
      "            perhapsbecauseimagesalreadylieinarichvectorspacewheredistancesprovide\n",
      "   alow-qualitysimilaritymetric.\n",
      "            Fromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\n",
      "           expectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\n",
      "           examplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\n",
      "          pretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\n",
      "         toperformbestwhen thenumber ofunlabeled examplesisverylarge.The\n",
      "        advantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\n",
      "          unlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin\n",
      "        2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\n",
      "            competitions( ,; Mesniletal.2011Goodfellow 2011etal.,),insettingswherethe\n",
      "              numberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\n",
      "           ofexamplesperclass).Theseeﬀectswerealsodocumentedincarefullycontrolled\n",
      "     experimentsby (). Paineetal.2014\n",
      "          Otherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\n",
      "              islikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.\n",
      "           Unsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnot\n",
      "            biasthelearnertowarddiscoveringasimplefunctionbutratherleadsthelearner\n",
      "          towarddiscoveringfeaturefunctionsthatareusefulfortheunsupervisedlearning\n",
      "            task.Ifthetrueunderlyingfunctionsarecomplicatedandshapedbyregularitiesof\n",
      "          theinputdistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\n",
      "          Thesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\n",
      "            pretrainingisknowntocauseanimprovementandexplainwhatisknownabout\n",
      "          whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenusedto\n",
      "             improveclassiﬁersandisusuallymostinterestingfromthepointofviewofreducing\n",
      "5 3 0   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "          testseterror.Unsupervisedpretrainingcanhelptasksotherthanclassiﬁcation,\n",
      "            however,andcanacttoimproveoptimizationratherthanbeingmerelyaregularizer.\n",
      "            Forexample,itcanimprovebothtrainandtestreconstructionerrorfordeep\n",
      "    autoencoders(HintonandSalakhutdinov2006,).\n",
      "          Erhan 2010etal.()performedmanyexperimentstoexplainseveralsuccesses\n",
      "       ofunsupervisedpretraining. Improvementstotrainingerrorandimprovements\n",
      "            totesterrormaybothbeexplainedintermsofunsupervisedpretrainingtaking\n",
      "           theparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\n",
      "           trainingisnondeterministicandconvergestoadiﬀerentfunctioneverytimeit\n",
      "             isrun. Trainingmayhaltatapointwherethegradientbecomessmall,apoint\n",
      "             whereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethe\n",
      "               gradientislarge,butitisdiﬃculttoﬁndadownhillstepbecauseofproblemssuch\n",
      "           asstochasticityorpoorconditioningoftheHessian.Neuralnetworksthatreceive\n",
      "          unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\n",
      "          whileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\n",
      "           ﬁgureforavisualizationofthisphenomenon.Theregionwherepretrained 15.1\n",
      "           networksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\n",
      "             estimationprocess,whichcaninturnreducetheriskofsevereoverﬁtting.In\n",
      "        otherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\n",
      "             aregionthattheydonotescape,andtheresultsfollowingthisinitializationare\n",
      "            moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\n",
      "            Erhan 2010etal.()alsoprovidesomeanswerstopretrainingworksbest— when\n",
      "             themeanandvarianceofthetesterrorweremostreducedbypretrainingfor\n",
      "           deepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\n",
      "          inventionandpopularizationofmoderntechniquesfortrainingverydeepnetworks\n",
      "            (rectiﬁedlinearunits,dropoutandbatchnormalization)solessisknownaboutthe\n",
      "        eﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.\n",
      "           Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.\n",
      "          Onehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\n",
      "           featuresthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\n",
      "          Thisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\n",
      "       pretrainingandisdescribedfurtherinsection.15.3\n",
      "        Comparedtootherformsofunsupervisedlearning,unsupervisedpretraining\n",
      "     has thedisadvantageof operating with t woseparate trainingphases.Many\n",
      "           regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\n",
      "           strengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\n",
      "            Unsupervisedpretrainingdoesnotoﬀeraclearwaytoadjustthestrengthof\n",
      "          theregularizationarisingfromtheunsupervisedstage.Instead,therearevery\n",
      "5 3 1   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "       \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000\u0000\u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\n",
      "           Figure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerent\n",
      "             neuralnetworksinfunctionspace(notparameterspace,toavoidtheissueofmany-to-one\n",
      "         mappingsfromparametervectorstofunctions),withdiﬀerentrandominitialization s\n",
      "           andwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerent\n",
      "            neuralnetworkataparticulartimeduringitstrainingprocess. Thisﬁgureisadapted\n",
      "              withpermissionfrom ().Acoordinateinfunctionspaceisaninﬁnite- Erhanetal.2010\n",
      "    dimensionalvectorassociatingeveryinput x  withanoutput y     . ()made Erhanetal.2010\n",
      "        alinearprojectiontohigh-dimensionalspacebyconcatenatingthe y  formanyspeciﬁc x\n",
      "            points.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum\n",
      "               etal.,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\n",
      "          (correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\n",
      "  overtheclass y          formostinputs).Overtime,learningmovesthefunctionoutward,to\n",
      "           pointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\n",
      "         usingpretrainingandinanother, nonoverlappingregionwhennotusingpretraining.\n",
      "             Isomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion\n",
      "         correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\n",
      "  hasreducedvariance.\n",
      "5 3 2   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "            manyhyperparameters,whoseeﬀectmaybemeasuredafterthefactbutisoften\n",
      "           diﬃculttopredictaheadoftime.Whenweperformunsupervisedandsupervised\n",
      "           learningsimultaneously,insteadofusingthepretrainingstrategy,thereisasingle\n",
      "        hyperparameter,usuallyacoeﬃcientattachedtotheunsupervisedcost, that\n",
      "         determineshowstronglytheunsupervisedobjectivewillregularizethesupervised\n",
      "          model.Onecanalwayspredictablyobtainlessregularizationbydecreasingthis\n",
      "          coeﬃcient. Inunsupervisedpretraining,thereisnotawayofﬂexiblyadapting\n",
      "          thestrengthoftheregularization—eitherthesupervisedmodelisinitializedto\n",
      "     pretrainedparameters,oritisnot.\n",
      "           Anotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\n",
      "           hasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\n",
      "             bepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposing\n",
      "            hyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedback\n",
      "             fromthesecondphase.Themostprincipledapproachistousevalidationseterror\n",
      "            inthesupervisedphasetoselectthehyperparametersofthepretrainingphase,as\n",
      "           discussedinLarochelle2009etal.().Inpractice,somehyperparameters,likethe\n",
      "          numberofpretrainingiterations,aremoreconvenientlysetduringthepretraining\n",
      "            phase,usingearlystoppingontheunsupervisedobjective,whichisnotidealbut\n",
      "        iscomputationallymuchcheaperthanusingthesupervisedobjective.\n",
      "         Today,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe\n",
      "           ﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\n",
      "          one-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\n",
      "              setsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\n",
      "             onceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\n",
      "            words),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\n",
      "             thenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthe\n",
      "         trainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered\n",
      "             byCollobertandWeston2008bTurian 2010 Collobert2011a (),etal.(),and etal.()\n",
      "     andremainsincommonusetoday.\n",
      "         Deeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\n",
      "           orbatchnormalization,areabletoachievehuman-levelperformanceonmanytasks,\n",
      "          butonlywithextremelylargelabeleddatasets.Thesesametechniquesoutperform\n",
      "         unsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10andMNIST,\n",
      "           whichhaveroughly5,000labeledexamplesperclass.Onextremelysmalldatasets,\n",
      "         suchasthealternativesplicingdataset,Bayesianmethodsoutperformmethods\n",
      "         basedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,the\n",
      "       popularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\n",
      "           pretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\n",
      "5 3 3   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "          andcontinuestoinﬂuencecontemporaryapproaches.Theideaofpretraininghas\n",
      "  beengeneralizedto  supervisedpretraining       ,discussedinsection,asavery 8.7.4\n",
      "         commonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\n",
      "             ispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\n",
      "         networkspretrainedontheImageNet dataset.Practitionerspublishtheparameters\n",
      "            ofthesetrainednetworksforthispurpose,justaspretrainedwordvectorsare\n",
      "            publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\n",
      "     15.2TransferLearningandDomainAdaptation\n",
      "            Transferlearninganddomainadaptationrefertothesituationwherewhathasbeen\n",
      "     learnedinonesetting(e.g.,distribution P 1     )isexploitedtoimprovegeneralization\n",
      "    inanothersetting(say,distribution P 2       ).Thisgeneralizestheideapresentedinthe\n",
      "        previoussection,wherewetransferredrepresentationsbetweenanunsupervised\n",
      "      learningtaskandasupervisedlearningtask.\n",
      "In  transferlearning         ,thelearnermustperformtwoormorediﬀerenttasks,\n",
      "            butweassumethatmanyofthefactorsthatexplainthevariationsin P 1are\n",
      "          relevanttothevariationsthatneedtobecapturedforlearning P 2   .Thisistypically\n",
      "             understoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\n",
      "               targetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetof\n",
      "              visualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnabouta\n",
      "             diﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.\n",
      "           Ifthereissigniﬁcantlymoredataintheﬁrstsetting(sampledfrom P 1 ),then\n",
      "            thatmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefrom\n",
      "     onlyveryfewexamplesdrawnfrom P 2     .Manyvisualcategoriessharelow-level\n",
      "            notionsofedgesandvisualshapes,theeﬀectsofgeometri cchanges,changesin\n",
      "           lighting,andsoon.Ingeneral,transferlearning,multitasklearning(section),7.7\n",
      "          anddomainadaptationcanbeachievedviarepresentationlearningwhenthere\n",
      "            existfeaturesthatareusefulforthediﬀerentsettingsortasks,correspondingto\n",
      "            underlyingfactorsthatappearinmorethanonesetting.Thisisillustratedin\n",
      "         ﬁgure,withsharedlowerlayersandtask-dependentupperlayers. 7.2\n",
      "      Sometimes, however, whatis sharedamong thediﬀerenttasksis notthe\n",
      "             semanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\n",
      "          recognitionsystemneedstoproducevalidsentencesattheoutputlayer, but\n",
      "            theearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversions\n",
      "          ofthesamephonemesorsubphonemicvocalizationsdependingonwhichperson\n",
      "              isspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\n",
      "5 3 4   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "Selection swi tch\n",
      "h(1)h(1)h(2)h(2)h(3)h(3)yy\n",
      "h(shared)h(shared)\n",
      "x(1)x(1)x(2)x(2)x(3)x(3)\n",
      "           Figure15.2:Examplearchitectureformultitaskortransferlearningwhentheoutput\n",
      "               variablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerent y x \n",
      "             meaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,each\n",
      " user),called x( 1 ),x( 2 )andx( 3 )         forthreetasks.Thelowerlevels(uptotheselection\n",
      "             switch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearnto\n",
      "         translatetheirtask-speciﬁcinputintoagenericsetoffeatures.\n",
      "            (neartheoutput)oftheneuralnetworkandhaveatask-speciﬁcpreprocessing,as\n",
      "   illustratedinﬁgure.15.2\n",
      "    Intherelatedcaseof  domainadaptation      ,thetask(andtheoptimalinput-to-\n",
      "           outputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\n",
      "           isslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,which\n",
      "          consistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\n",
      "           Commentspostedonthewebcomefrommanycategories.Adomainadaptation\n",
      "           scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\n",
      "             mediacontent,suchasbooks,videosandmusic,islaterusedtoanalyzecomments\n",
      "          aboutconsumerelectronics,suchastelevisionsorsmartphones.Onecanimagine\n",
      "            thatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\n",
      "             neutral,ornegative,butofcoursethevocabularyandstylemayvaryfromone\n",
      "           domaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simple\n",
      "          unsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery\n",
      "          successfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\n",
      "     Arelatedproblemisthatof  conceptdrift       ,whichwecanviewasaform\n",
      "            oftransferlearningduetogradualchangesinthedatadistributionovertime.\n",
      "5 3 5   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "            Bothconceptdriftandtransferlearningcanbeviewedasparticularformsof\n",
      "       multitasklearning.While thephrase“multitasklearning”typicallyrefers to\n",
      "           supervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\n",
      "       tounsupervisedlearningandreinforcementlearningaswell.\n",
      "              Inallthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrst\n",
      "            settingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\n",
      "           directlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation\n",
      "             learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\n",
      "           samerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthe\n",
      "       trainingdatathatisavailableforbothtasks.\n",
      "          Asmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\n",
      "          successinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\n",
      "             etal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe 2011\n",
      "            following.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(from\n",
      "distribution P 1         ),illustratingexamplesofsomesetofcategories.Theparticipants\n",
      "              mustusethistolearnagoodfeaturespace(mappingtherawinputtosome\n",
      "          representation),suchthatwhenweapplythislearnedtransformationtoinputs\n",
      "    fromthetransfersetting(distribution P 2       ),alinearclassiﬁercanbetrainedand\n",
      "            generalizewellfromfewlabeledexamples.Oneofthemoststrikingresultsfound\n",
      "             inthiscompetitionisthatasanarchitecturemakesuseofdeeperanddeeper\n",
      "          representations(learnedinapurelyunsupervisedwayfromdatacollectedin\n",
      "  theﬁrstsetting, P 1         ), thelearningcurveonthenewcategoriesofthesecond\n",
      " (transfer)setting P 2        becomesmuchbetter.Fordeeprepresentations,fewerlabeled\n",
      "           examplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\n",
      " generalizationperformance.\n",
      "      Twoextremeformsoftransferlearningare  one-shotlearningandzero-shot\n",
      "learning   ,sometimesalsocalled  zero-datalearning    .Onlyonelabeledexample\n",
      "             ofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare\n",
      "       givenatallforthezero-shotlearningtask.\n",
      "          One-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\n",
      "            learnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthe\n",
      "              transferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\n",
      "           possibletestexamplesthatallclusteraroundthesamepointinrepresentation\n",
      "            space.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\n",
      "            theseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned\n",
      "            representationspace,andthatwehavesomehowlearnedwhichfactorsdoanddo\n",
      "       notmatterwhendiscriminatingobjectsofcertaincategories.\n",
      "            Asanexampleofazero-shotlearningsetting,considertheproblemofhaving\n",
      "5 3 6   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "             alearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\n",
      "             Itmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseen\n",
      "              animageofthatobjectifthetextdescribestheobjectwellenough.Forexample,\n",
      "                havingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\n",
      "            guessthatanimageisacatwithouthavingseenacatbefore.\n",
      "         Zero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\n",
      "            etal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation\n",
      "            hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\n",
      "       asincludingthreerandomvariables:thetraditionalinputsx  ,thetraditional\n",
      "  outputsortargetsy        ,andanadditionalrandomvariabledescribingthetask,T.\n",
      "        Themodelistrainedtoestimatetheconditionaldistributionp(   yx|,T ),where\n",
      "T              isadescriptionofthetaskwewishthemodeltoperform. Inourexampleof\n",
      "            recognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\n",
      "withy    = 1indicating“yes”andy      = 0indicating“no.”ThetaskvariableTthen\n",
      "               representsquestionstobeanswered,suchas“Isthereacatinthisimage?”Ifwe\n",
      "            haveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\n",
      "  samespaceasT            ,wemaybeabletoinferthemeaningofunseeninstancesofT.\n",
      "               Inourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\n",
      "            importantthatwehavehadunlabeledtextdatacontainingsentencessuchas“cats\n",
      "       havefourlegs”or“catshavepointyears.”\n",
      "  Zero-shotlearningrequiresT         toberepresentedinawaythatallowssomesort\n",
      "   ofgeneralization.Forexample,T        cannotbejustaone-hotcodeindicatingan\n",
      "          objectcategory.Socher2013betal.()provideinsteadadistributedrepresentation\n",
      "            ofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\n",
      "  witheachcategory.\n",
      "          Asimilarphenomenonhappensinmachinetranslation( ,; Klementievetal.2012\n",
      "               Mikolov2013bGouws2014 etal.,;etal.,):wehavewordsinonelanguage,andthe\n",
      "           relationshipsbetweenwordscanbelearnedfromunilingualcorpora;ontheother\n",
      "             hand,wehavetranslatedsentencesthatrelatewordsinonelanguagewithwordsin\n",
      "           theother.EventhoughwemaynothavelabeledexamplestranslatingwordAin\n",
      "languageX towordB inlanguageY        ,wecangeneralizeandguessatranslationfor\n",
      "wordA           becausewehavelearnedadistributedrepresentationforwordsinlanguage\n",
      "X       andadistributedrepresentationforwordsinlanguageY    ,thencreatedalink\n",
      "          (possiblytwo-way)relatingthetwospaces,viatrainingexamplesconsistingof\n",
      "            matchedpairsofsentencesinbothlanguages.Thistransferwillbemostsuccessful\n",
      "           ifallthreeingredients(thetworepresentationsandtherelationsbetweenthem)\n",
      "  arelearnedjointly.\n",
      "           Zero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\n",
      "5 3 7   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "h x  = f x ( ) x\n",
      "x t e s t\n",
      "y t e s th y  = f y ( ) y\n",
      "y − s pa c e\n",
      "Relationship between embedded points within one of the domains\n",
      "Maps be tween representation spac es f x\n",
      "f y\n",
      "x − s pa c e\n",
      "      ( ) pa i r s i n t he t r a i ni ng s e t x y ,\n",
      "f x     : e nc o de r f unc t i o n f o r x\n",
      "f y     : e nc o de r f unc t i o n f o r y\n",
      "      Figure15.3:Transferlearningbetweentwodomains xand y   enableszero-shotlearning.\n",
      "    Labeledorunlabeledexamplesof x      allowonetolearnarepresentationfunctionf xand\n",
      "   similarlywithexamplesof y tolearnf y    .Eachapplicationofthef xandf yfunctions\n",
      "              appearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\n",
      "  applied.Distancein hx          spaceprovidesasimilaritymetricbetweenanypairofpointsin x\n",
      "        spacethatmaybemoremeaningfulthandistancein x   space.Likewise, distancein h y\n",
      "          spaceprovidesasimilaritymetricbetweenanypairofpointsin y   space.Bothofthese\n",
      "         similarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeledexamples\n",
      "     (dashedhorizontallines)arepairs( x y,         ) thatallowonetolearnaone-wayortwo-waymap\n",
      "     (solidbidirectionalarrow)betweentherepresentationsf x( x  ) andtherepresentationsf y( y)\n",
      "             andtoanchortheserepresentationstoeachother.Zero-datalearningisthenenabledas\n",
      "     follows.Onecanassociateanimage x t e s t  toaword y t e s t        ,evenifnoimageofthatwordwas\n",
      "     everpresented,simplybecausewordrepresentationsfy( yt e s t   )andimagerepresentations\n",
      "f x( x t e s t             )canberelatedtoeachotherviathemapsbetweenrepresentationspaces.It\n",
      "            worksbecause,althoughthatimageandthatwordwereneverpaired,theirrespective\n",
      " featurevectorsf x( x t e s t )andf y( y t e s t        )havebeenrelatedtoeachother.Figureinspired\n",
      "    fromsuggestionbyHrantKhachatrian.\n",
      "5 3 8   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "    explainshowonecanperform  multimodallearning    ,capturingarepresentationin\n",
      "             onemodality,arepresentationintheother,andtherelationship(ingeneralajoint\n",
      "   distribution)betweenpairs( xy,    )consistingofoneobservationx  inonemodality\n",
      "  andanotherobservationy      intheothermodality(SrivastavaandSalakhutdinov,\n",
      "        2012).Bylearningallthreesetsofparameters(fromx   toitsrepresentation,from\n",
      "y         toitsrepresentation,andtherelationshipbetweenthetworepresentations),\n",
      "            conceptsinonerepresentationareanchoredintheother,andviceversa,allowing\n",
      "         onetomeaningfullygeneralizeto newpairs.Theprocedureisillustrated in\n",
      " ﬁgure.15.3\n",
      "     15.3Semi-SupervisedDisentanglingofCausalFactors\n",
      "          Animportantquestionaboutrepresentationlearningis:whatmakesonerepre-\n",
      "           sentationbetterthananother?Onehypothesisisthatanidealrepresentationis\n",
      "           oneinwhichthefeatureswithintherepresentationcorrespondtotheunderly-\n",
      "            ingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature\n",
      "          spacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthe\n",
      "           causesfromoneanother.Thishypothesismotivatesapproachesinwhichweﬁrst\n",
      "    seekagoodrepresentationforp(x        ). Sucharepresentationmayalsobeagood\n",
      "  representationforcomputingp(  yx| )ify      isamongthemostsalientcausesof\n",
      "x             . Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\n",
      "            the1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,)inmoredetail.\n",
      "         Forotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure\n",
      "             supervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006\n",
      "          Inotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\n",
      "            witharepresentationthatiseasytomodel—forexample,onewhoseentriesare\n",
      "         sparseorindependentfromeachother. Arepresentationthatcleanlyseparates\n",
      "             theunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.\n",
      "         However,afurtherpartofthehypothesismotivatingsemi-supervisedlearning\n",
      "           viaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo\n",
      "          propertiescoincide: onceweareabletoobtaintheunderlyingexplanationsfor\n",
      "           whatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\n",
      "     theothers.Speciﬁcally,ifarepresentationh     representsmanyoftheunderlying\n",
      "   causesoftheobservedx   ,andtheoutputsy     areamongthemostsalientcauses,\n",
      "        thenitiseasytopredictfrom.yh\n",
      "          First,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\n",
      " learningofp(x      )isofnohelptolearningp(  yx|     ).Consider,forexample,thecase\n",
      "wherep(x        )isuniformlydistributedandwewanttolearnf(x) = E[  y|x ].Clearly,\n",
      "5 3 9   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "xp x()  y=1 y=2 y=3\n",
      "      Figure15.4: Mixtur emodel. Exampleofadensityoverx     thatisamixtureoverthree\n",
      "        components.Thecomponentidentityisanunderlyingexplanatoryfactor,y  .Becausethe\n",
      "           mixturecomponents(e.g.,naturalobjectclassesinimagedata)arestatisticallysalient,\n",
      " justmodelingp(x           )inanunsupervisedwaywithnolabeledexamplealreadyrevealsthe\n",
      " factor.y\n",
      "              observingatrainingsetofvaluesalonegivesusnoinformationabout . x p( )y x|\n",
      "            Next,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\n",
      "   Considerthesituationwherex       arisesfromamixture,withonemixturecomponent\n",
      "  pervalueofy          ,asillustratedinﬁgure. Ifthemixturecomponentsarewell 15.4\n",
      "  separated,thenmodelingp(x        )revealspreciselywhereeachcomponentis,anda\n",
      "            singlelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(  yx|).\n",
      "          Butmoregenerally,whatcouldtie andtogether? p( )y x|p()x\n",
      "Ify         iscloselyassociatedwithoneofthecausalfactorsofx ,thenp(x )and\n",
      "p(  yx|      )willbe stronglytied, andunsupervised represen tationlearningthat\n",
      "              triestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa\n",
      "  semi-supervisedlearningstrategy.\n",
      "   Considertheassumptionthaty      isoneofthecausalfactorsofx  ,andlet\n",
      "h            representallthosefactors.Thetruegenerativeprocesscanbeconceivedas\n",
      "            structuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\n",
      "    p,pp. (hx) = ( )xh|()h (15.1)\n",
      "       Asaconsequence,thedatahasmarginalprobability\n",
      "p() = x E h   p. ( )xh| (15.2)\n",
      "          Fromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\n",
      "ofx             (fromageneralizationpointofview)istheonethatuncoverstheabove“true”\n",
      "5 4 0   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      " structure,with h         asalatentvariablethatexplainstheobservedvariationsin x.\n",
      "          The“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatent\n",
      " factors.Ify               isoneofthese(orcloselyrelatedtooneofthem),thenitwillbeeasy\n",
      "   tolearntopredicty         fromsucharepresentation.Wealsoseethattheconditional\n",
      " distributionofygivenx          istiedbyBayes’ruletothecomponentsintheabove\n",
      "equation:\n",
      "  p( ) = yx|  pp ( )xy| ()y\n",
      "p()x . (15.3)\n",
      "  Thusthemarginalp(x     ) isintimatelytiedtotheconditionalp(  yx|  ),andknowledge\n",
      "              ofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\n",
      "       situationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\n",
      "performance.\n",
      "          Animportantresearchproblemregardsthefactthatmostobservationsare\n",
      "         formedbyanextremelylargenumberofunderlyingcauses.Supposey=h i ,but\n",
      "      theunsupervisedlearnerdoesnotknowwhichh i      .Thebruteforcesolutionisfor\n",
      "           anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\n",
      "  salientgenerativefactorsh j       anddisentanglesthemfromeachother,thusmaking\n",
      "          iteasytopredictfrom,regardlessofwhichh y h i   isassociatedwith.y\n",
      "              Inpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossibleto\n",
      "             captureallormostofthefactorsofvariationthatinﬂuenceanobservation.For\n",
      "            example,inavisualscene,shouldtherepresentationalwaysencodeallthesmallest\n",
      "         objectsinthebackground?Itisawell-documentedpsychologicalphenomenon\n",
      "            thathumanbeingsfailtoperceivechangesintheirenvironmentthatarenot\n",
      "          immediatelyrelevanttothetasktheyareperforming—see,forexampleSimons\n",
      "          andLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\n",
      "            determiningtoencodeineachsituation.Currently,twoofthemainstrategies what\n",
      "             fordealingwithalargenumberofunderlyingcausesaretouseasupervised\n",
      "             learningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\n",
      "              modelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch\n",
      "      largerrepresentationsifusingpurelyunsupervisedlearning.\n",
      "           Anemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionof\n",
      "         whichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\n",
      "            modelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomean\n",
      "          squarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient.\n",
      "            Forexample,meansquarederrorappliedtothepixelsofanimageimplicitly\n",
      "            speciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthe\n",
      "               brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\n",
      "            tosolveinvolvesinteractingwithsmallobjects.Seeﬁgureforanexample 15.5\n",
      "5 4 1   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      " Input Reconstruction\n",
      "             Figure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\n",
      "                failedtoreconstructapingpongball.Theexistenceofthepingpongballandallits\n",
      "           spatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\n",
      "          arerelevanttotheroboticstask. Unfortunately,theautoencoderhaslimitedcapacity,\n",
      "               andthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\n",
      "         salientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.\n",
      "               ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\n",
      "            pingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\n",
      "            objects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\n",
      "            Otherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixels\n",
      "            followsahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\n",
      "          brightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\n",
      "              Onewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydeveloped\n",
      " approachcalled   generativeadversarialnetworks    ( ,). Goodfellowetal.2014c\n",
      "             Inthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer.The\n",
      "          feedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerativemodel\n",
      "               asbeingfakeandallsamplesfromthetrainingsetasbeingreal.Inthisframework,\n",
      "           anystructuredpatternthatthefeedforwardnetworkcanrecognizeishighlysalient.\n",
      "           Thegenerativeadversarialnetworkisdescribedinmoredetailinsection .20.10.4\n",
      "             Forthepurposesofthepresentdiscussion,itissuﬃcienttounderstandthatthe\n",
      "             networkslearnhowtodeterminewhatissalient. ()showedthat Lotteretal.2015\n",
      "            modelstrainedtogenerateimagesofhumanheadswilloftenneglecttogenerate\n",
      "           theearswhentrainedwithmeansquarederror,butwillsuccessfullygenerate\n",
      "            theearswhentrainedwiththeadversarialframework.Becausetheearsarenot\n",
      "            extremelybrightordarkcomparedtothesurroundingskin,theyarenotespecially\n",
      "           salientaccordingtomeansquarederrorloss,buttheirhighlyrecognizableshape\n",
      "5 4 2   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "   GroundTruth MSE Adversarial\n",
      "           Figure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\n",
      "          learningwhichfeaturesaresalient. Inthisexample,thepredictivegenerativenetwork\n",
      "                 hasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁc\n",
      "            viewingangle.(Left)Groundtruth.Thisisthecorrectimage,whichthenetworkshould\n",
      "          emit. Imageproducedbyapredictivegenerativenetworktrainedwithmean (Center)\n",
      "             squarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightness\n",
      "              comparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearn\n",
      "            torepresentthem.(Right)Imageproducedbyamodeltrainedwithacombinationof\n",
      "            meansquarederrorandadversarialloss. Usingthislearnedcostfunction,theearsare\n",
      "           salientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\n",
      "             importantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\n",
      "      graciouslyprovidedby (). Lotteretal.2015\n",
      "            andconsistentpositionmeansthatafeedforwardnetworkcaneasilylearntodetect\n",
      "          them,makingthemhighlysalientunderthegenerativeadversarialframework.See\n",
      "           ﬁgureforexampleimages.Generativeadversarialnetworksareonlyonestep 15.6\n",
      "          towarddeterminingwhichfactorsshouldberepresented.Weexpectthatfuture\n",
      "           researchwilldiscoverbetterwaysofdeterminingwhichfactorstorepresentand\n",
      "         developmechanismsforrepresentingdiﬀerentfactorsdependingonthetask.\n",
      "            Abeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopf\n",
      "          etal.(),isthatifthetruegenerativeprocesshas 2012 x   asaneﬀectandyas\n",
      "   acause,thenmodeling p( x y|     )isrobusttochangesin p(y  ). Ifthecause-eﬀect\n",
      "           relationshipwerereversed,thiswouldnotbetrue,sincebyBayes’rule, p(  xy|)\n",
      "     wouldbesensitivetochangesin p(y       ).Veryoften,whenweconsiderchangesin\n",
      "         distributionduetodiﬀerentdomains,temporalnonstationarity,orchangesin\n",
      "             thenatureofthetask,thecausalmechanismsremaininvariant(“thelawsofthe\n",
      "          universeareconstant”),whilethemarginaldistributionovertheunderlyingcauses\n",
      "            canchange.Hence,bettergeneralizationandrobustnesstoallkindsofchangescan\n",
      "            beexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\n",
      "5 4 3   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "    factors and . h p ( )xh|\n",
      "  15.4DistributedRepresentation\n",
      "       Distributedrepresentationsofconcepts—representationscomposedofmanyele-\n",
      "             mentsthatcanbesetseparatelyfromeachother—areoneofthemostimportant\n",
      "        toolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\n",
      "  theycanusen  featureswithk  valuestodescribekn   diﬀerentconcepts.Aswe\n",
      "           haveseenthroughoutthisbook,neuralnetworkswithmultiplehiddenunitsand\n",
      "           probabilisticmodelswithmultiplelatentvariablesbothmakeuseofthestrategy\n",
      "         ofdistributedrepresentation.Wenowintroduceanadditionalobservation.Many\n",
      "           deeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\n",
      "            canlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\n",
      "          discussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\n",
      "            becauseeachdirectioninrepresentationspacecancorrespondtothevalueofa\n",
      "   diﬀerentunderlyingconﬁgurationvariable.\n",
      "         Anexampleofadistributedrepresentationisavectorofn  binaryfeatures,\n",
      "   whichcantake 2n      conﬁgurations,eachpotentiallycorrespondingtoadiﬀerent\n",
      "             regionininputspace,asillustratedinﬁgure.Thiscanbecomparedwith 15.7\n",
      "            asymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\n",
      "   category.Iftherearen       symbolsinthedictionary,onecanimaginenfeature\n",
      "           detectors,eachcorrespondingtothedetectionofthepresenceoftheassociated\n",
      "    category.Inthatcaseonlyn      diﬀerentconﬁgurationsoftherepresentationspace\n",
      "  arepossible,carvingn          diﬀerentregionsininputspace,asillustratedinﬁgure.15.8\n",
      "            Suchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\n",
      "      becapturedbyabinaryvectorwithn       bitsthataremutuallyexclusive(onlyoneof\n",
      "             themcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthebroader\n",
      "         classofnondistributedrepresentations,whicharerepresentationsthatmaycontain\n",
      "          manyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolovereachentry.\n",
      "         Thefollowingexamplesoflearningalgorithmsarebasedonnondistributed\n",
      "representations:\n",
      "•    Clusteringmethods,includingthek     -meansalgorithm:eachinputpointis\n",
      "    assignedtoexactlyonecluster.\n",
      " •k           -nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\n",
      "         areassociatedwithagiveninput.Inthecaseof k>  1,multiplevalues\n",
      "          describeeachinput,buttheycannotbecontrolledseparatelyfromeach\n",
      "          other,sothisdoesnotqualifyasatruedistributedrepresentation.\n",
      "5 4 4   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "h 1h 2 h 3\n",
      "   h = [ 1 , , 1 1 ]\n",
      "   h = [ 0 , , 1 1 ]   h = [ 1 , , 0 1 ]   h = [ 1 , , 1 0 ]\n",
      "   h = [ 0 , , 1 0 ]   h = [ 0 , , 0 1 ]   h = [ 1 , , 0 0 ]\n",
      "            Figure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\n",
      "              breaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\n",
      "h 1,h 2 ,andh 3            .Eachfeatureisdeﬁnedbythresholdingtheoutputofalearnedlinear\n",
      "   transformation.Eachfeaturedivides R2   intotwohalf-planes.Leth+\n",
      "i    bethesetofinput\n",
      "  pointsforwhichh i  =1,andh−\n",
      "i       bethesetofinputpointsforwhichh i   =0.Inthis\n",
      "        illustration,eachlinerepresentsthedecisionboundaryforoneh i   ,withthecorresponding\n",
      "   arrowpointingtotheh+\n",
      "i         sideoftheboundary.Therepresentationasawholetakes\n",
      "             onauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\n",
      "  representationvalue[1,1,1]   correspondstotheregionh+\n",
      "1 ∩h+\n",
      "2 ∩h+\n",
      "3    .Comparethistothe\n",
      "         non-distributedrepresentationsinﬁgure.Inthegeneralcaseof 15.8 d inputdimensions,\n",
      "   adistributedrepresentationdivides Rd     byintersectinghalf-spacesratherthanhalf-planes.\n",
      "   Thedistributedrepresentationwithn     featuresassignsuniquecodestoO(nd )diﬀerent\n",
      "      regions,whilethenearestneighboralgorithmwithn      examplesassignsuniquecodestoonly\n",
      "n           regions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\n",
      "           moreregionsthanthenondistributedone.Keepinmindthatnotallh  valuesarefeasible\n",
      "  (thereisnoh= 0            inthisexample),andthatalinearclassiﬁerontopofthedistributed\n",
      "            representationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;\n",
      "          evenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(  wwlog ),wherew\n",
      "            isthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\n",
      "               layerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearn\n",
      "                theconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclassto\n",
      "              aninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas\n",
      "            “manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfew\n",
      "h i             andencouragestolearntorepresenttheclassesinalinearlyseparableway. h\n",
      "5 4 5   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "•                Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\n",
      "     activatedwhenaninputisgiven.\n",
      "•           Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\n",
      "           expertsarenowassociatedwithadegreeofactivation.Aswiththe k-nearest\n",
      "         neighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but\n",
      "         thosevaluescannotreadilybecontrolledseparatelyfromeachother.\n",
      "•           KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\n",
      "           althoughthedegreeofactivationofeach“supportvector”ortemplateexample\n",
      "          isnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\n",
      "•  Language ortranslation models based on n   -grams:the setof contexts\n",
      "           (sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes.\n",
      "         Aleafmaycorrespondtothelasttwowordsbeing w 1and w 2  ,forexample.\n",
      "            Separateparametersareestimatedforeachleafofthetree(withsomesharing\n",
      " beingpossible).\n",
      "            Forsomeofthesenondistributedalgorithms,theoutputisnotconstantbyparts\n",
      "        butinsteadinterpolatesbetweenneighboringregions.Therelationshipbetween\n",
      "             thenumberofparameters(orexamples)andthenumberofregionstheycandeﬁne\n",
      " remainslinear.\n",
      "        Animportantrelatedconceptthatdistinguishesadistributedrepresentation\n",
      "            fromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween\n",
      "     diﬀerentconcepts.Aspuresymbols,“cat  ”and“dog      ”areasfarfromeachother\n",
      "            asanyothertwosymbols.However,ifoneassociatesthemwithameaningful\n",
      "            distributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\n",
      "           cangeneralizetodogsandviceversa.Forexample,ourdistributedrepresentation\n",
      "     maycontainentriessuchas“has_fur  ”or“number_of_legs    ”thathavethesame\n",
      "      valuefortheembeddingofboth“cat  ”and“dog    .”Neurallanguagemodelsthat\n",
      "          operateondistributedrepresentationsofwordsgeneralizemuchbetterthanother\n",
      "           modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\n",
      "          section.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\n",
      "            semanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\n",
      "    absentfrompurelysymbolicrepresentations.\n",
      "            Whenandwhycantherebeastatisticaladvantagefromusingadistributed\n",
      "          representationaspartofalearningalgorithm?Distributedrepresentationscanhave\n",
      "          astatisticaladvantagewhenanapparentlycomplicatedstructurecanbecompactly\n",
      "         representedusingasmallnumberofparameters.Sometraditionalnondistributed\n",
      "         learningalgorithmsgeneralizeonlyduetothesmoothnessassumption,which\n",
      "5 4 6   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "             Figure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\n",
      "            intodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\n",
      "        algorithmbasedonanondistributedrepresentation.Diﬀerentnon-distributedalgorithms\n",
      "         mayhave diﬀerentgeometry, but theytypicallybreaktheinputspaceintoregions,\n",
      "             withaseparatesetofparametersforeachregion.Theadvantageofanondistributed\n",
      "              approachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvinga\n",
      "           diﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutput\n",
      "          independentlyforeachregion.Thedisadvantageisthatsuchnondistributedmodels\n",
      "             generalizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicated\n",
      "            functionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\n",
      "      thiswithadistributedrepresentation,ﬁgure.15.7\n",
      "5 4 7   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "  statesthatif  uv≈    ,thenthetargetfunctionf      tobelearnedhasthepropertythat\n",
      "f(u) ≈f(v           )ingeneral.Therearemanywaysofformalizingsuchanassumption,\n",
      "           buttheendresultisthatifwehaveanexample( x,y     )forwhichweknowthat\n",
      "f(x) ≈y  , then we choose anestimator ˆfthat approximately satisﬁes these\n",
      "             constraintswhilechangingaslittleaspossiblewhenwemovetoanearbyinput\n",
      "x+             .Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseof\n",
      "           dimensionality:tolearnatargetfunctionthatincreasesanddecreasesmanytimes\n",
      "   inmanydiﬀerentregions,1           wemayneedanumberofexamplesthatisatleastas\n",
      "             largeasthenumberofdistinguishableregions.Onecanthinkofeachofthese\n",
      "              regionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomforeach\n",
      "            symbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbolto\n",
      "              value.However,thisdoesnotallowustogeneralizetonewsymbolsfornewregions.\n",
      "              Ifwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\n",
      "           smooth.Forexample,aconvolutionalnetworkwithmaxpoolingcanrecognizean\n",
      "            objectregardlessofitslocationintheimage,eventhoughspatialtranslationof\n",
      "           theobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\n",
      "           Letusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,\n",
      "           whichextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\n",
      "     binaryfeatureinthisrepresentationdivides Rd    intoapairofhalf-spaces, as\n",
      "          illustratedinﬁgure.Theexponentiallylargenumberofintersectionsof 15.7 n\n",
      "         ofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\n",
      "          representationlearnercandistinguish.Howmanyregionsaregeneratedbyan\n",
      " arrangementofn  hyperplanesin Rd       ?Byapplyingageneralresultconcerningthe\n",
      "           intersectionofhyperplanes(Zaslavsky1975,),onecanshow(Pascanu2014betal.,)\n",
      "           thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\n",
      "d \n",
      "j =0n\n",
      "j\n",
      "= (Ond ). (15.4)\n",
      "              Therefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\n",
      "    thenumberofhiddenunits.\n",
      "          Thisprovidesageometri cargumenttoexplainthegeneralizationpowerof\n",
      "  distributedrepresentation:withO(nd  )parameters(forn  linearthresholdfeatures\n",
      "in Rd    ),wecandistinctlyrepresentO(nd       ) regionsininputspace.Ifinsteadwemade\n",
      "             noassumptionatallaboutthedata,andusedarepresentationwithoneunique\n",
      "            symbolforeachregion,andseparateparametersforeachsymboltorecognizeits\n",
      "1              Potentially,wemaywanttolearnafunctionwhosebehaviorisdistinctinexponentiallymany\n",
      "  regions:ina d             -dimensionalspacewithatleast2diﬀerentvaluestodistinguishperdimension,we\n",
      "      mightwanttodiﬀerin f 2d   diﬀerentregions,requiring O(2d  )trainingexamples.\n",
      "548   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "  correspondingportionof Rd  ,thenspecifyingO(nd   )regionswouldrequireO(nd)\n",
      "          examples.Moregenerally,theargumentinfavorofthedistributedrepresentation\n",
      "             couldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\n",
      "           usenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\n",
      "            thedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\n",
      " transformationwithk    parameterscanlearnaboutr     regionsininputspace,with\n",
      "  kr              ,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\n",
      "            wecouldpotentiallygeneralizemuchbetterinthiswaythaninanondistributed\n",
      "    setting,wherewewouldneedO(r       )examplestoobtainthesamefeaturesand\n",
      "      associatedpartitioningoftheinputspaceintor     regions.Usingfewerparametersto\n",
      "             representthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequire\n",
      "      farfewertrainingexamplestogeneralizewell.\n",
      "            Afurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\n",
      "            tationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\n",
      "            distinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofa\n",
      "       neuralnetworkoflinearthresholdunitsisonlyO(  wwlog ),wherew  isthenumber\n",
      "            ofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\n",
      "            manyuniquecodestorepresentationspace,wecannotuseabsolutelyallthecode\n",
      "           space,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\n",
      "h  totheoutput y         usingalinearclassiﬁer.Theuseofadistributedrepresentation\n",
      "             combinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassesto\n",
      "            berecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors\n",
      " capturedby h            . Wewilltypicallywanttolearncategoriessuchasthesetofall\n",
      "                imagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\n",
      "            requirenonlinearXORlogic.Forexample,wetypicallydonotwanttopartition\n",
      "                   thedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\n",
      "       greencarsandredtrucksasanotherclass.\n",
      "            Theideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\n",
      "            validated. ()foundthathiddenunitsinadeepconvolutional Zhouetal.2015\n",
      "           networktrainedontheImageNet andPlacesbenchmarkdatasetslearnfeaturesthat\n",
      "           areofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\n",
      "             Inpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\n",
      "               thathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\n",
      "             toplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\n",
      "             commonisthatonecouldimaginelearningabouteachofthemwithouthavingto\n",
      "             seealltheconﬁgurationsofalltheothers. ()demonstratedthat Radfordetal.2015\n",
      "            agenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\n",
      "         directionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation.\n",
      "         Figuredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9\n",
      "5 4 9   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "- + =\n",
      "           Figure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\n",
      "              theconceptofgenderfromtheconceptofwearingglasses. Ifwebeginwiththerepre-\n",
      "              sentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\n",
      "             conceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconcept\n",
      "              ofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\n",
      "           withglasses.Thegenerativemodelcorrectlydecodesalltheserepresentationvectorsto\n",
      "             imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\n",
      "     permissionfrom (). Radfordetal.2015\n",
      "            towhetherthepersonismaleorfemale,whileanothercorrespondstowhether\n",
      "          thepersoniswearingglasses.Thesefeatureswerediscoveredautomatically,not\n",
      "              ﬁxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:\n",
      "          gradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\n",
      "             interestingfeatures,aslongasthetaskrequiressuchfeatures.Wecanlearnabout\n",
      "            thedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\n",
      "         glasses,withouthavingtocharacterizealltheconﬁgurationsofthe n − 1other\n",
      "           featuresbyexamplescoveringallthesecombinationsofvalues.Thisformof\n",
      "            statisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurationsofa\n",
      "        person’sfeaturesthathaveneverbeenseenduringtraining.\n",
      "    15.5ExponentialGainsfromDepth\n",
      "           Wehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\n",
      "           tors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\n",
      "           networkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\n",
      "           improvedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapply\n",
      "          moregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\n",
      "             Insection,wesawanexampleofagenerativemodelthatlearnedabout 15.4\n",
      "5 5 0   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "          theexplanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgender\n",
      "          andwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\n",
      "               thistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\n",
      "            ashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\n",
      "           betweentheseabstractexplanatoryfactorsandthepixelsintheimage. Inthis\n",
      "            andotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\n",
      "             eachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery\n",
      "              highlevelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\n",
      "          demandsdeepdistributedrepresentations,wherethehigherlevelfeatures(seenas\n",
      "            functionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\n",
      "    thecompositionofmanynonlinearities.\n",
      "          Ithasbeenprovedinmanydiﬀerentsettingsthatorganizingcomputation\n",
      "           throughthecompositionofmanynonlinearitiesandahierarchyofreusedfeatures\n",
      "            cangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponential\n",
      "           boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\n",
      "         withsaturatingnonlinearities,Booleangates,sum/products,orRBFunits)with\n",
      "            asinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\n",
      "            familythatisauniversalapproximatorcanapproximatealargeclassoffunctions\n",
      "           (includingallcontinuousfunctions)uptoanynonzerotolerancelevel,givenenough\n",
      "           hiddenunits. However,therequirednumberofhiddenunitsmaybeverylarge.\n",
      "          Theoreticalresultsconcerningtheexpressivepowerofdeeparchitecturesstatethat\n",
      "            therearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitecture\n",
      " ofdepthk           ,butthatwouldrequireanexponentialnumberofhiddenunits(with\n",
      "              respecttotheinputsize)withinsuﬃcientdepth(depth2ordepth).k− 1\n",
      "          Insection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\n",
      "         approximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\n",
      "          hiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\n",
      "         beliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux\n",
      "            andBengio20082010MontúfarandAy2011Montúfar2014Krause ,,; ,; ,; etal.,\n",
      "2013).\n",
      "            Insection,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhave 6.4.1\n",
      "             anexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\n",
      "           beobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\n",
      "  modelisthe  sum-productnetwork       ,orSPN( ,).These PoonandDomingos2011\n",
      "          modelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\n",
      "           setofrandomvariables. ()showedthatthereexist DelalleauandBengio2011\n",
      "            probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\n",
      "         needinganexponentiallylargemodel.Later, () MartensandMedabalimi2014\n",
      "5 5 1   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "           showedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsof\n",
      "             SPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\n",
      "  theirrepresentationalpower.\n",
      "           Anotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\n",
      "           poweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan\n",
      "            exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\n",
      "            toonlyapproximatethefunctioncomputedbythedeepcircuit( , Cohenetal.\n",
      "          2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\n",
      "         casewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\n",
      "      15.6ProvidingCluestoDiscoverUnderlyingCauses\n",
      "               Toclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\n",
      "           representationbetterthananother?Oneanswer,ﬁrstintroducedinsection,is15.3\n",
      "            thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\n",
      "            variationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\n",
      "         applications.Moststrategiesforrepresentationlearningarebasedonintroducing\n",
      "            cluesthathelpthelearningﬁndtheseunderlyingfactorsofvariations.Theclues\n",
      "           canhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\n",
      "       learningprovidesaverystrongclue:alabel y   ,presentedwitheach x  ,thatusually\n",
      "              speciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\n",
      "           tomakeuseofabundantunlabeleddata,representationlearningmakesuseof\n",
      "             other,lessdirecthintsabouttheunderlyingfactors.Thesehintstaketheformof\n",
      "            implicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\n",
      "              ordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\n",
      "          regularizationstrategiesarenecessarytoobtaingoodgeneralization.Whileitis\n",
      "           impossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeep\n",
      "             learningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicable\n",
      "                toawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\n",
      " tosolve.\n",
      "            Weprovideherealistofthesegenericregularizationstrategies.Thelistis\n",
      "           clearlynotexhaustivebutgivessomeconcreteexamplesofhowlearningalgorithms\n",
      "           canbeencouragedtodiscoverfeaturesthatcorrespondtounderlyingfactors.This\n",
      "              listwasintroducedinsection3.1of ()andhasbeenpartially Bengioetal.2013d\n",
      " expandedhere.\n",
      "•      Smoothness:Thisistheassumptionthat f( x+  d) ≈ f( x  )forunit dand\n",
      "small          .Thisassumptionallowsthelearnertogeneralizefromtraining\n",
      "5 5 2   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      "          examplestonearbypointsininputspace.Manymachinelearningalgorithms\n",
      "            leveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality.\n",
      "•         Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\n",
      "          variablesarelinear.Thisallowsthealgorithmtomakepredictionseven\n",
      "            veryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\n",
      "          predictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\n",
      "        smoothnessassumptioninsteadmakethelinearityassumption.Theseare\n",
      "        infactdiﬀerentassumptions—linearfunctionswithlargeweightsapplied\n",
      "           tohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\n",
      "           ()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\n",
      "•        Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\n",
      "           motivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\n",
      "            explanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\n",
      "            ofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3\n",
      "        supervisedlearningviarepresentationlearning.Learningthestructureof p(x)\n",
      "           requireslearningsomeofthesamefeaturesthatareusefulformodeling p( y|\n",
      "x           )becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\n",
      "          describeshowthisviewmotivatestheuseofdistributedrepresentations,with\n",
      "        separatedirectionsinrepresentationspacecorrespondingtoseparatefactors\n",
      " ofvariation.\n",
      "•             Causalfactors: Themodelisconstructedinsuchawaythatittreatsthe\n",
      "       factorsofvariationdescribedbythelearnedrepresentationh  asthecauses\n",
      "   oftheobserveddatax          ,andnotviceversa.Asdiscussedinsection,this15.3\n",
      "         isadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\n",
      "          morerobustwhenthedistributionovertheunderlyingcauseschangesor\n",
      "        whenweusethemodelforanewtask.\n",
      "•      Depthahierarchicalorganizationof explanatoryfactors , or :High-level,\n",
      "           abstractconceptscanbedeﬁnedintermsofsimpleconcepts,forminga\n",
      "    hierarchy.From another point ofview,the use ofa deep architecture\n",
      "           expressesourbeliefthatthetaskshouldbeaccomplishedviaamultistep\n",
      "          program, witheachstepreferringbacktotheoutputoftheprocessing\n",
      "   accomplishedviaprevioussteps.\n",
      "•           Sharedfactorsacrosstasks:Whenwehavemanytaskscorrespondingto\n",
      "diﬀerentyi     variablessharingthesameinput x      ,orwheneachtaskisassociated\n",
      "     withasubsetorafunction f( ) i( x    )ofaglobalinput x   ,theassumptionis\n",
      " thateachyi          isassociatedwithadiﬀerentsubsetfromacommonpoolof\n",
      "5 5 3   C HAP T E R 1 5 . R E P R E S E NT A T I O N L E ARNI N G\n",
      " relevantfactors h       .Becausethesesubsetsoverlap,learningallthe P(yi |x)\n",
      "    viaasharedintermediaterepresentation P(  hx|    )allowssharingofstatistical\n",
      "   strengthbetweenthetasks.\n",
      "•           Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\n",
      "           centratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\n",
      "         case,theseregionscanbeapproximatedbylow-dimensionalmanifoldswith\n",
      "           amuchsmallerdimensionalitythantheoriginalspacewherethedatalive.\n",
      "         Manymachinelearningalgorithmsbehavesensiblyonlyonthismanifold\n",
      "        ( ,).Somemachinelearningalgorithms,especially Goodfellowetal.2014b\n",
      "         autoencoders,attempttoexplicitlylearnthestructureofthemanifold.\n",
      "•         Naturalclustering:Manymachinelearningalgorithmsassumethateach\n",
      "             connectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\n",
      "           datamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\n",
      "          withineachoneofthese. Thisassumptionmotivatesavarietyoflearning\n",
      "       algorithms,includingtangentpropagation, doublebackprop,themanifold\n",
      "    tangentclassiﬁerandadversarialtraining.\n",
      "•          Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\n",
      "         maketheassumptionthatthemostimportantexplanatoryfactorschange\n",
      "              slowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\n",
      "          explanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\n",
      "        Seesectionforfurtherdescriptionofthisapproach. 13.3\n",
      "•           Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\n",
      "            inputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhen\n",
      "             representinganimageofacat.Itisthereforereasonabletoimposeaprior\n",
      "            thatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbe\n",
      "    absentmostofthetime.\n",
      "•         Simplicityoffactordependencies:Ingoodhigh-levelrepresentations,the\n",
      "          factorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\n",
      "   possibleismarginalindependence, P(h) =\n",
      "i P(h i   ),butlineardependencies\n",
      "          orthosecapturedbyashallowautoencoderarealsoreasonableassumptions.\n",
      "              Thiscanbeseeninmanylawsofphysicsandisassumedwhenplugginga\n",
      "           linearpredictororafactorizedpriorontopofalearnedrepresentation.\n",
      "           Theconceptofrepresentationlearningtiestogetherallthemanyformsof\n",
      "         deeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeepproba-\n",
      "         bilisticmodelsalllearnandexploitrepresentations. Learningthebestpossible\n",
      "      representationremainsanexcitingavenueofresearch.\n",
      "5 5 4 C h a p t e r 1 6\n",
      "  St r uct u r e d P r obab i l i s t i c Mo dels\n",
      "  for D e e p Learning\n",
      "            Deeplearningdrawsonmanymodelingformalismsthatresearcherscanusetoguide\n",
      "            theirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalismsisthe\n",
      " ideaof   structuredprobabilisticmodels    .Wediscussstructuredprobabilistic\n",
      "           modelsbrieﬂyinsection.Thatbriefpresentationissuﬃcienttounderstand 3.14\n",
      "             howtousestructuredprobabilisticmodelsasalanguagetodescribesomeofthe\n",
      "             algorithmsinpart.Now,inpart,structuredprobabilisticmodelsareakey II III\n",
      "            ingredientofmanyofthemostimportantresearchtopicsindeeplearning.To\n",
      "           preparetodiscusstheseresearchideas,inthischapter,wedescribestructured\n",
      "           probabilisticmodelsinmuchgreaterdetail. Thischapterisintendedtobeself-\n",
      "          contained; thereaderdoesnotneedtoreviewtheearlierintroductionbefore\n",
      "   continuingwiththischapter.\n",
      "           Astructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,\n",
      "           usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\n",
      "             interactwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—a\n",
      "              setofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\n",
      "               ofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoas\n",
      " graphicalmodels.\n",
      "          Thegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\n",
      "          diﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\n",
      "            providebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,\n",
      "              withanemphasisontheconceptsthathaveprovedmostusefultothedeeplearning\n",
      "           researchcommunity.Ifyoualreadyhaveastrongbackground ingraphicalmodels,\n",
      "             youmaywishtoskipmostofthischapter.However,evenagraphicalmodel\n",
      "555       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "             expertmaybeneﬁtfromreadingtheﬁnalsectionofthischapter,section,in16.7\n",
      "             whichwehighlightsomeoftheuniquewaysinwhichgraphicalmodelsareused\n",
      "           fordeeplearningalgorithms.Deeplearningpractitionerstendtouseverydiﬀerent\n",
      "         modelstructures,learningalgorithmsandinferenceproceduresthanarecommonly\n",
      "             usedbytherestofthegraphicalmodelsresearchcommunity.Inthischapter,we\n",
      "          identifythesediﬀerencesinpreferencesandexplainthereasonsforthem.\n",
      "         Weﬁrstdescribethechallengesofbuildinglarge-scaleprobabilisticmodels.\n",
      "              Next,wedescribehowtouseagraphtodescribethestructureofaprobability\n",
      "            distribution.Whilethisapproachallowsustoovercomemanychallenges,itisnot\n",
      "           withoutitsowncomplications.Oneofthemajordiﬃcultiesingraphicalmodeling\n",
      "             isunderstandingwhichvariablesneedtobeabletointeractdirectly,thatis,which\n",
      "             graphstructuresaremostsuitableforagivenproblem.Insection16.5,weoutline\n",
      "          twoapproachestoresolvingthisdiﬃcultybylearningaboutthedependencies.\n",
      "            Finally,weclosewithadiscussionoftheuniqueemphasisthatdeeplearning\n",
      "          practitionersplaceonspeciﬁcapproachestographicalmodeling,insection.16.7\n",
      "     16.1TheChallengeofUnstructuredModeling\n",
      "              Thegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\n",
      "           neededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-\n",
      "            dimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\n",
      "     beabletounderstandnaturalimages,1    audiowaveformsrepresentingspeech,and\n",
      "      documentscontainingmultiplewordsandpunctuationcharacters.\n",
      "          Classiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensional\n",
      "            distributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,\n",
      "              whatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\n",
      "            ofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesa\n",
      "            singleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\n",
      "              classiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,when\n",
      "              recognizinganobjectinaphoto,itisusuallypossibletoignorethebackground of\n",
      " thephoto.\n",
      "              Itispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\n",
      "          oftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultiple\n",
      "           outputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\n",
      "              theinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\n",
      "1A  n a t u r a l i m a ge             isanimagethatmightbecapturedbyacamerainareasonablyordinary\n",
      "              environment,asopposedtoasyntheticallyrenderedimage,ascreenshotofawebpage,etc.\n",
      "556       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      " •Density estimation  :Given aninput x ,the machine learning system\n",
      "     returnsanestimateof thetruedensity p( x   )underthedata-generating\n",
      "    distribution.This requiresonly asingle output,  but italso requires a\n",
      "            completeunderstandingoftheentireinput.Ifevenoneelementofthevector\n",
      "         isunusual,thesystemmustassignitalowprobability.\n",
      " •Denoising       :Givenadamagedorincorrectlyobservedinput˜ x  ,themachine\n",
      "         learningsystemreturnsanestimateoftheoriginalorcorrect x  .Forexample,\n",
      "           themachinelearningsystemmightbeaskedtoremovedustorscratches\n",
      "           fromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\n",
      "  estimatedcleanexample x        )andanunderstandingoftheentireinput(since\n",
      "            evenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged).\n",
      "   •Missingvalueimputation       :Giventheobservationsofsomeelementsof x,\n",
      "            themodelisaskedtoreturnestimatesoforaprobabilitydistributionover\n",
      "       someoralloftheunobservedelementsof x    .Thisrequiresmultipleoutputs.\n",
      "            Becausethemodelcouldbeaskedtorestoreanyoftheelementsof x ,it\n",
      "    mustunderstandtheentireinput.\n",
      " •Sampling       : Themodelgeneratesnewsamplesfromthedistribution p( x).\n",
      "         Applicationsincludespeechsynthesis,thatis,producingnewwaveformsthat\n",
      "           soundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\n",
      "             goodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\n",
      "         fromthewrongdistribution,thenthesamplingprocessiswrong.\n",
      "             Foranexampleofasamplingtaskusingsmallnaturalimages,seeﬁgure.16.1\n",
      "           Modelingarichdistributionoverthousandsormillionsofrandomvariablesis\n",
      "          achallengingtask,bothcomputationallyandstatistically.Supposewewantedto\n",
      "             modelonlybinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\n",
      "     seemsoverwhelming.Forasmall32×       32 2 pixelcolor(RGB)image,thereare3 0 7 2\n",
      "          possiblebinaryimagesofthisform.Thisnumberisover108 0 0  timeslargerthan\n",
      "       theestimatednumberofatomsintheuniverse.\n",
      "            Ingeneral,ifwewishtomodeladistributionoverarandomvector xcontaining\n",
      "n      discretevariablescapableoftakingon k      valueseach,thenthenaiveapproachof\n",
      "representing P( x           )bystoringalookuptablewithoneprobabilityvalueperpossible\n",
      "  outcomerequires knparameters!\n",
      "      Thisisnotfeasibleforseveralreasons:\n",
      "•            Memory—thecostofstoringtherepresentation:Forallbutverysmallvalues\n",
      "of nand k          ,representingthedistributionasatablewillrequiretoomany\n",
      "  valuestostore.\n",
      "5 5 7       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "         Figure16.1:Probabilisticmodelingofnaturalimages.(Top)Example32 ×  32pixelcolor\n",
      "         imagesfromtheCIFAR-10dataset( ,). Samples KrizhevskyandHinton2009(Bottom)\n",
      "            drawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\n",
      "                 atthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\n",
      "              space.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,\n",
      "             ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\n",
      "           adjustedfordisplay.Figurereproducedwithpermissionfrom (). Courvilleetal.2011\n",
      "5 5 8       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "•           Statisticaleﬃciency:Asthenumberofparametersinamodelincreases,\n",
      "             sodoestheamountoftrainingdataneededtochoosethevaluesofthose\n",
      "         parametersusingastatisticalestimator.Becausethetable-basedmodelhas\n",
      "          anastronomicalnumberofparameters,itwillrequireanastronomicallylarge\n",
      "             trainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetrainingsetvery\n",
      "          badlyunlessadditionalassumptionsaremadelinkingthediﬀerententriesin\n",
      "          thetable(asinback-oﬀorsmoothed-grammodels;section ). n 12.4.1\n",
      "•           Runtime—thecostofinference:Supposewewanttoperformaninference\n",
      "         taskwhereweuseourmodelofthejointdistribution P( x   )tocomputesome\n",
      "      otherdistribution,suchasthemarginaldistribution P(x 1   )ortheconditional\n",
      "distribution P(x 2 |x 1      ).Computingthesedistributionswillrequiresumming\n",
      "              acrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\n",
      "      intractablememory costofstoringthemodel.\n",
      "•            Runtime—thecostofsampling:Likewise,supposewewanttodrawasample\n",
      "             fromthemodel.Thenaivewaytodothisistosamplesomevalue  u∼ U(0 ,1),\n",
      "           theniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\n",
      "exceed u          andreturntheoutcomecorrespondingtothatpositioninthetable.\n",
      "             Thisrequiresreadingthroughthewholetableintheworstcase,soithas\n",
      "       thesameexponentialcostastheotheroperations.\n",
      "           Theproblemwiththetable-basedapproachisthatweareexplicitlymodeling\n",
      "           everypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\n",
      "           probabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\n",
      "       Usually,mostvariablesinﬂuenceeachotheronlyindirectly.\n",
      "             Forexample,considermodelingtheﬁnishingtimesofateaminarelayrace.\n",
      "              Supposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof\n",
      "             therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\n",
      "               herlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\n",
      "                lapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachof\n",
      "           theirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoes\n",
      "            notdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedepends\n",
      "              onAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\n",
      "            hascompletedhers. IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing\n",
      "            equal.Finally,Carol’sﬁnishingtimedependsonbothherteammates. IfAliceis\n",
      "              slow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitea\n",
      "               latestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,\n",
      "           Carol’sﬁnishingtimedependsonlyindirectlyonAlice’sﬁnishingtimeviaBob’s.\n",
      "              IfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’s\n",
      "5 5 9       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "            ﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeans\n",
      "              wecanmodeltherelayraceusingonlytwointeractions:Alice’seﬀectonBoband\n",
      "            Bob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\n",
      "    andCarolfromourmodel.\n",
      "         Structuredprobabilisticmodelsprovideaformalframeworkformodelingonly\n",
      "          directinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\n",
      "          signiﬁcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\n",
      "          Thesesmallermodelsalsohavedramaticallyreducedcomputationalcostinterms\n",
      "            ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\n",
      " themodel.\n",
      "      16.2UsingGraphstoDescribeModelStructure\n",
      "            Structuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or\n",
      "         “vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables.\n",
      "           Eachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\n",
      "          Thesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\n",
      "     interactionsneedtobeexplicitlymodeled.\n",
      "       Thereismore thanone wayto describe theinteractionsin aprobability\n",
      "            distributionusingagraph.Inthefollowingsections,wedescribesomeofthe\n",
      "           mostpopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\n",
      "           twocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\n",
      " undirectedgraphs.\n",
      "  16.2.1DirectedModels\n",
      "       Onekindofstructuredprobabilisticmodelisthe   directedgraphicalmodel,\n",
      "        otherwiseknownasthebeliefnetworkBayesiannetwork or2 (Pearl1985,).\n",
      "          Directedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,\n",
      "             thatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin\n",
      "            thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’s\n",
      "            probabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfrom\n",
      "atob       meansthatwedeﬁnetheprobabilitydistributionoverb  viaaconditional\n",
      " distribution,witha           asoneofthevariablesontherightsideoftheconditioning\n",
      "2            JudeaPearlsuggestedusingtheterm“Bayesiannetwork”whenonewishesto“emphasize\n",
      "               thejudgmental”natureofthevaluescomputedbythenetwork,i.e.,tohighlightthattheyusually\n",
      "        representdegreesofbeliefratherthanfrequenciesofevents.\n",
      "560       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "t 0 t 0 t 1 t 1 t 2 t 2  A l i c e B ob C ar ol\n",
      "            Figure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishing\n",
      "timet 0    inﬂuencesBob’sﬁnishingtimet 1         ,becauseBobdoesnotgettostartrunninguntil\n",
      "             Aliceﬁnishes.Likewise, CarolonlygetstostartrunningafterBobﬁnishes,soBob’s\n",
      "  ﬁnishingtimet 1      directlyinﬂuencesCarol’sﬁnishingtimet 2.\n",
      "             bar.Inotherwords,thedistributionoverbdependsonthevalueofa.\n",
      "           Continuingwiththerelayraceexamplefromsection,supposewename 16.1\n",
      "  Alice’sﬁnishingtimet 0   ,Bob’sﬁnishingtimet 1    ,andCarol’sﬁnishingtimet 2.\n",
      "      Aswesawearlier,ourestimateoft 1  dependsont 0   .Ourestimateoft 2depends\n",
      " directlyont 1   butonlyindirectlyont 0        .Wecandrawthisrelationshipinadirected\n",
      "     graphicalmodel,illustratedinﬁgure.16.2\n",
      "       Formally,adirectedgraphicalmodeldeﬁnedonvariables x   isdeﬁnedbya\n",
      "  directedacyclicgraphG         whoseverticesaretherandomvariablesinthemodel,and\n",
      "  asetof    localconditionalprobabilitydistributionsp(x i  |Pa G(x i )),where\n",
      " Pa G(x i    )givestheparentsofx iinG    .Theprobabilitydistributionover x isgiven\n",
      "by\n",
      "p() = Π x ip(x i  |Pa G(x i )). (16.1)\n",
      "              Inourrelayraceexample,thismeansthat,usingthegraphdrawninﬁgure,16.2\n",
      "p(t 0 ,t 1 ,t 2) = (pt 0)(pt 1 |t 0)(pt 2 |t 1 ). (16.2)\n",
      "            Thisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.We\n",
      "             canexaminethecostofusingit,toobservehowstructuredmodelinghasmany\n",
      "    advantagesrelativetounstructuredmodeling.\n",
      "           Supposewerepresentedtimebydiscretizingtimerangingfromminute0to\n",
      "       minute10into6-secondchunks.Thiswouldmaket 0,t 1andt 2   eachbeadiscrete\n",
      "         variablewith100possiblevalues.Ifweattemptedtorepresentp(t 0 ,t 1 ,t 2  )witha\n",
      "          table,itwouldneedtostore999,999values(100valuesoft 0×  100valuesoft 1×\n",
      "  100valuesoft 2            ,minus1,sincetheprobabilityofoneoftheconﬁgurationsismade\n",
      "              redundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\n",
      "            makeatableonlyforeachoftheconditionalprobabilitydistributions,thenthe\n",
      " distributionovert 0      requires99values,thetabledeﬁningt 1givent 0  requires9,900\n",
      "      values,andsodoesthetabledeﬁningt 2givent 1       .Thiscomestoatotalof19,899\n",
      "            values.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\n",
      "       parametersbyafactorofmorethan50!\n",
      "5 6 1       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "   Ingeneral,tomodeln    discretevariableseachhavingk    values,thecostofthe\n",
      "    singletableapproachscaleslikeO(kn       ),aswehaveobservedbefore.Nowsuppose\n",
      "         webuildadirectedgraphicalmodeloverthesevariables.Ifm  isthemaximum\n",
      "             numberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\n",
      "           conditionalprobabilitydistribution,thenthecostofthetablesforthedirected\n",
      "  modelscaleslikeO(km          ).Aslongaswecandesignamodelsuchthat  m<<n ,we\n",
      "   getverydramaticsavings.\n",
      "              Inotherwords,aslongaseachvariablehasfewparentsinthegraph,the\n",
      "         distributioncanberepresentedwithveryfewparameters. Somerestrictionson\n",
      "              thegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\n",
      "         operationslikecomputingmarginalorconditionaldistributionsoversubsetsof\n",
      "  variablesareeﬃcient.\n",
      "              Itisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\n",
      "          thegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables\n",
      "            areconditionallyindependentfromeachother.Itisalsopossibletomakeother\n",
      "         kindsofsimplifyingassumptions. Forexample,supposeweassumeBobalways\n",
      "           runsthesameregardlessofhowAliceperforms.(Inreality,Alice’sperformance\n",
      "        probablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlice\n",
      "              runsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\n",
      "           matchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy).\n",
      "               ThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’s\n",
      "              ﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.This\n",
      "       observationallowsustodeﬁneamodelwithO(k   )parametersinsteadofO(k2).\n",
      "  However,notethatt 0andt 1      arestilldirectlydependentwiththisassumption,\n",
      "becauset 1            representstheabsolutetimeatwhichBobﬁnishes,notthetotaltime\n",
      "            hespendsrunning.Thismeansourgraphmuststillcontainanarrowfromt 0\n",
      "tot 1          .TheassumptionthatBob’spersonalrunningtimeisindependentfrom\n",
      "         allotherfactorscannotbeencodedinagraphovert 0,t 1 ,andt 2  .Instead,we\n",
      "           encodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.The\n",
      "     conditionaldistributionisnolongera  kk × −    1elementtableindexedbyt 0andt 1\n",
      "         butisnowaslightlymorecomplicatedformulausingonly k −  1parameters.The\n",
      "            directedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne\n",
      "           ourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedto\n",
      "   takeinasarguments.\n",
      "  16.2.2UndirectedModels\n",
      "          Directedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\n",
      "       ticmodels.Anotherpopularlanguageisthatof  undirectedmodels ,otherwise\n",
      "5 6 2       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      " knownas   Markovrandomﬁelds  (MRFs)or  Markovnetworks(Kinder-\n",
      "           mann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\n",
      " areundirected.\n",
      "          Directedmodelsaremostnaturallyapplicabletosituationswherethereis\n",
      "             aclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\n",
      "            situationswhereweunderstandthecausality,andthecausalityﬂowsinonlyone\n",
      "            direction.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀectthe\n",
      "             ﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesof\n",
      " earlierrunners.\n",
      "              Notallsituationswemightwanttomodelhavesuchacleardirectiontotheir\n",
      "           interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto\n",
      "             operateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\n",
      "             Asanexampleofsuchasituation,supposewewanttomodeladistribution\n",
      "             overthreebinaryvariables:whetherornotyouaresick,whetherornotyour\n",
      "               coworkerissick,andwhetherornotyourroommate issick.Asintherelayrace\n",
      "           example,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\n",
      "            takeplace.Assumingthatyourcoworkerandyourroommate donotknoweach\n",
      "                 other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\n",
      "                colddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\n",
      "               it.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\n",
      "               thatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\n",
      "             acoldfromyourcoworkertoyourroommate bymodelingthetransmissionofthe\n",
      "               coldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\n",
      "roommate.\n",
      "                 Inthiscase,itisjustaseasyforyoutocauseyourroommate togetsickas\n",
      "               itisforyourroommate tomakeyousick,sothereisnotacleanunidirectional\n",
      "            narrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\n",
      "              Aswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\n",
      "           edge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\n",
      "            otherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\n",
      "         arrowandisnotassociatedwithaconditionalprobabilitydistribution.\n",
      "        Wedenotetherandomvariablerepresentingyourhealthash y  ,therandom\n",
      "     variablerepresentingyourroommate’ shealthash r    ,andtherandomvariable\n",
      "    representingyourcolleague’shealthash c        .Seeﬁgureforadrawingofthe 16.3\n",
      "   graphrepresentingthisscenario.\n",
      "         Formally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\n",
      "5 6 3       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "h r h r h y h y h c h c\n",
      "         Figure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthh r ,your\n",
      "healthh y     ,andyourworkcolleague’s healthh c      aﬀecteachother.Youandyourroommate\n",
      "                mightinfecteachotherwithacold,andyouandyourworkcolleague mightdothesame,\n",
      "              butassumingthatyourroommateandyourcolleague donotknoweachother,theycan\n",
      "      onlyinfecteachotherindirectlyviayou.\n",
      "    deﬁnedonanundirectedgraph G   .Foreachclique C  inthegraph,3a factorφ( C)\n",
      "  (alsocalleda cliquepotential         ) measurestheaﬃnityofthevariablesinthatclique\n",
      "              forbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe\n",
      "    nonnegative.Togethertheydeﬁnean   unnormalizedprobabilitydistribution\n",
      "˜p() = Π x C ∈ G φ.() C (16.3)\n",
      "           Theunnormalizedprobabilitydistributioniseﬃcienttoworkwithsolongas\n",
      "              allthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityare\n",
      "             morelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\n",
      "            deﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\n",
      "            togetherwillyieldavalidprobabilitydistribution.Seeﬁgureforanexample 16.4\n",
      "       ofreadingfactorizationinformationfromanundirectedgraph.\n",
      "           Ourexampleofthecoldspreadingbetweenyou,yourroommate, andyour\n",
      "      colleaguecontainstwocliques.Onecliquecontainsh yandh c    .Thefactorforthis\n",
      "            cliquecanbedeﬁnedbyatableandmighthavevaluesresemblingthese:\n",
      "h y  = 0h y= 1\n",
      "h c   = 02 1\n",
      "h c   = 1110\n",
      "              Astateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\n",
      "       (havingbeen infectedwith acold).Both ofyouareusually healthy, so the\n",
      "             correspondingstatehasthehighestaﬃnity.Thestatewhereonlyoneofyouis\n",
      "               sickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothof\n",
      "               youaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,\n",
      "           thoughstillnotascommonasthestatewherebotharehealthy.\n",
      "3                    Acliqueofthegraphisasubsetofnodesthatareallconnectedtoeachotherbyanedgeof\n",
      " thegraph.\n",
      "564       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "  a b c\n",
      "  d e f\n",
      "     Figure16.4: Thisgraphimpliesthatp(     abcdef,,,,,    )canbewrittenas\n",
      "1\n",
      "Zφ a b ,( ab,)φ b c ,( bc,)φ a d ,( ad,)φ b e ,( be,)φ e f ,( ef,      )foranappropriatechoiceoftheφfunc-\n",
      "tions.\n",
      "              Tocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthe\n",
      "  cliquecontainingh y andh r.\n",
      "   16.2.3ThePartitionFunction\n",
      "         Whiletheunnormalizedprobabilitydistributionisguaranteedtobenonnegative\n",
      "              everywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\n",
      "        probabilitydistribution,wemustusethecorrespondingnormalizedprobability\n",
      "distribution4\n",
      "p() = x1\n",
      "Z ˜p,() x (16.4)\n",
      "whereZ       isthe valuethatresultsinthe probability distributionsummingor\n",
      "  integratingto1:\n",
      " Z=\n",
      " ˜pd. () x x (16.5)\n",
      "   YoucanthinkofZ    asaconstantwhentheφ     functionsareheldconstant.Note\n",
      "  thatiftheφ    functionshaveparameters,thenZ     isafunctionofthoseparameters.\n",
      "       ItiscommonintheliteraturetowriteZ      withitsargumentsomittedtosavespace.\n",
      "  ThenormalizingconstantZ   isknownasthe  partitionfunction   ,atermborrowed\n",
      "  fromstatisticalphysics.\n",
      "SinceZ            isanintegralorsumoverallpossiblejointassignmentsofthestate x,\n",
      "             itisoftenintractabletocompute.Tobeabletoobtainthenormalizedprobability\n",
      "            distributionofanundirectedmodel,themodelstructureandthedeﬁnitionsofthe\n",
      "φ      functionsmustbeconducivetocomputingZ      eﬃciently.Inthecontextofdeep\n",
      "learning,Z        isusuallyintractable.BecauseoftheintractabilityofcomputingZ\n",
      "4             Adistributiondeﬁnedbynormalizingaproductofcliquepotentialsisalsocalleda Gi b b s\n",
      "d i s t r i b u t i on.\n",
      "565       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "          exactly,wemustresorttoapproximations. Suchapproximatealgorithmsarethe\n",
      "   topicofchapter.18\n",
      "          Oneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\n",
      "             isthatitispossibletospecifythefactorsinsuchawaythatZ  doesnotexist.\n",
      "              Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\n",
      "              of˜povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\n",
      "          scalarvariablexwithasinglecliquepotential ∈ R φxx () = 2   .Inthiscase,\n",
      " Z=\n",
      "x2 dx. (16.6)\n",
      "          Sincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\n",
      "  thischoiceofφ(x        ). Sometimesthechoiceofsomeparameteroftheφfunctions\n",
      "     determineswhetherthe probability distributionis deﬁned.Forexample, for\n",
      "φ(x;β )=exp\n",
      "−βx2\n",
      " ,theβ   parameterdetermineswhetherZ  exists.Positive\n",
      "β      resultsinaGaussiandistributionoverx     ,butallothervaluesofβmakeφ\n",
      "  impossibletonormalize.\n",
      "          Onekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthat\n",
      "          directedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfrom\n",
      "         thestart,whileundirectedmodelsaredeﬁnedmorelooselybyφ  functionsthat\n",
      "         arethenconvertedintoprobabilitydistributions.Thischangestheintuitions\n",
      "              onemustdeveloptoworkwiththesemodels.Onekeyideatokeepinmind\n",
      "             whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\n",
      "             hasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetofφ\n",
      "      functionscorrespondsto.Forexample,considerann   -dimensionalvectorvalued\n",
      " randomvariablex         andanundirectedmodelparametrizedbyavectorofbiases\n",
      "b         .Supposewehaveonecliqueforeachelementofx,φ( ) i(x i) =exp(b ix i ).What\n",
      "             kindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\n",
      "            nothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx.\n",
      "If  x∈ Rn    ,thentheintegraldeﬁningZ     diverges,andnoprobabilitydistribution\n",
      " exists.If  x∈{0,1}n ,thenp(x  )factorizesinton   independentdistributions,with\n",
      "p(x i= 1) =  sigmoid(b i)    .Ifthedomainofx      isthesetofelementarybasisvectors\n",
      "({[1,0    ,...,0],[0,1    ,...,0]    ,...,[0,0    ,...,1]} ),thenp(x )=softmax ( b   ),soalarge\n",
      " valueofb i  actuallyreducesp(x j  = 1)for j=i       .Often,itispossibletoleveragethe\n",
      "            eﬀectofacarefullychosendomainofavariabletoobtaincomplicatedbehavior\n",
      "     fromarelativelysimplesetofφ       functions.Weexploreapracticalapplicationof\n",
      "    thisideainsection.20.6\n",
      "5 6 6       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "  16.2.4Energy-Bas edModels\n",
      "          Manyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\n",
      " sumptionthat ∀ x,˜p( x)>          0.Aconvenientwaytoenforcethisconditionistouse\n",
      "    an (EBM)where energy-basedmodel\n",
      " ˜p E, () = exp( x −()) x (16.7)\n",
      "andE( x    )isknownasthe energyfunction .Becauseexp(z    )ispositiveforall\n",
      "z             ,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\n",
      "  foranystate x      .Beingcompletely freeto choosetheenergy functionmakes\n",
      "             learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\n",
      "        constrainedoptimizationtoarbitrarilyimposesomespeciﬁcminimalprobability\n",
      "          value.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\n",
      "           Theprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\n",
      "   butneverreachit.\n",
      "             Anydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\n",
      "mann distribution  .For this reason, manyenergy-based models are called\n",
      " Boltzmannmachines           (Fahlman 1983Ackley1985Hinton etal.,; etal.,; etal.,\n",
      "             1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\n",
      "             amodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\n",
      "           termBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusively\n",
      "          binaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\n",
      "         Boltzmannmachineincorporaterealvaluedvariablesaswell.WhileBoltzmann\n",
      "           machineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutla-\n",
      "            tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\n",
      "         modelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\n",
      "         aremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels.\n",
      "          Cliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\n",
      "  probabilityfunction.Becauseexp(a)exp(b) =exp(a+b    ),thismeansthatdiﬀerent\n",
      "            cliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergy\n",
      "             function.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov\n",
      "          network:theexponentiationmakeseachtermintheenergyfunctioncorrespond\n",
      "                 toafactorforadiﬀerentclique.Seeﬁgureforanexampleofhowtoreadthe 16.5\n",
      "             formoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan\n",
      "           energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\n",
      " ofexperts          (Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\n",
      "            anotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\n",
      "5               Forsomemodels,wemaystillneedtouseconstrainedoptimizationtomakesureexists. Z\n",
      "567       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "  a b c\n",
      "  d e f\n",
      "    Figure 16.5:ThisgraphimpliesthatE(     abcdef,,,,,  )can be writtenasE a b ,( ab, )+\n",
      "E b c ,( bc, )+E a d ,( ad, )+E b e ,( be, )+E e f ,( ef,       )foranappropriatechoiceoftheper-clique\n",
      "       energyfunctions.Notethatwecanobtaintheφ       functionsinﬁgurebysettingeach 16.4 φ\n",
      "        totheexponentialofthecorrespondingnegativeenergy,e.g.,φ a b ,( ab,) =   exp(()) −Eab,.\n",
      "            bethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraint\n",
      "           issatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\n",
      "          alow-dimensionalprojectionoftherandomvariables,butwhencombinedby\n",
      "         multiplicationofprobabilities,theexpertstogetherenforceacomplicatedhigh-\n",
      " dimensionalconstraint.\n",
      "            Onepartofthedeﬁnitionofanenergy-basedmodelservesnofunctionalpurpose\n",
      "       fromamachinelearningpointofview:the −    signinequation.This 16.7 −sign\n",
      "      couldbeincorporatedintothedeﬁnitionofE      .Formanychoicesofthefunction\n",
      "E              ,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\n",
      "−          signispresentprimarilytopreservecompatibilitybetweenthemachinelearning\n",
      "         literatureandthephysicsliterature.Manyadvancesinprobabilisticmodeling\n",
      "       wereoriginallydevelopedbystatisticalphysicists,forwhomE  referstoactual\n",
      "           physicalenergyanddoesnothavearbitrarysign.Terminologysuchas“energy”\n",
      "         and“partitionfunction”remainsassociatedwiththesetechniques,eventhough\n",
      "           theirmathematicalapplicabilityisbroaderthanthephysicscontextinwhichthey\n",
      "         weredeveloped.Somemachinelearningresearchers(e.g., [],who Smolensky1986\n",
      "    referredtonegativeenergyasharmony       )havechosentoomitthenegation,but\n",
      "     thisisnotthestandardconvention.\n",
      "          Manyalgorithmsthatoperateonprobabilisticmodelsneedtocomputenot\n",
      "p m o d e l( x  )butonlylog ˜p m o d e l( x      ).Forenergy-basedmodelswithlatentvariables h,\n",
      "            thesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\n",
      "   calledthe : freeenergy\n",
      "F − () = x log\n",
      "h    exp(( )) −E x h,. (16.8)\n",
      "         Inthisbook,weusuallypreferthemoregenerallog ˜p m o d e l () xformulation.\n",
      "5 6 8       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "  a s b   a s b\n",
      " (a) (b)\n",
      "       Figure16.6:(a)Thepathbetweenrandomvariablea  andrandomvariablebthroughsis\n",
      " active,becauses     isnotobserved.Thismeansthataandb    arenotseparated.(b)Heres\n",
      "             isshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\n",
      "throughs         ,andthatpathisinactive,wecanconcludethataandb  areseparatedgivens.\n",
      "   16.2.5SeparationandD-Separation\n",
      "             Theedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\n",
      "           needtoknowwhichvariablesindirectlyinteract.Someoftheseindirectinteractions\n",
      "            canbeenabledordisabledbyobservingothervariables.Moreformally,wewould\n",
      "           liketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach\n",
      "        other,giventhevaluesofothersubsetsofvariables.\n",
      "          Identifyingtheconditionalindependencesinagraphissimpleforundirected\n",
      "           models.Inthiscase,conditionalindependenceimpliedbythegraphiscalled\n",
      "s e pa r a t i o n      . Wesaythatasetofvariables Ais s e pa r a t e d   fromanothersetof\n",
      "variables B     givenathirdsetofvariables S     ifthegraphstructureimpliesthat A\n",
      "  isindependentfrom Bgiven S   .Iftwovariablesaandb    areconnectedbyapath\n",
      "           involvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno\n",
      "             pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\n",
      "           separated.Werefertopathsinvolvingonlyunobservedvariablesas“active”and\n",
      "      pathsincludinganobservedvariableas“inactive.”\n",
      "             Whenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\n",
      "              Seeﬁgureforadepictionofhowactiveandinactivepathsinanundirected 16.6\n",
      "              modellookwhendrawninthisway.Seeﬁgureforanexampleofreading 16.7\n",
      "    separationfromanundirectedgraph.\n",
      "    Similar concepts apply todirected models,except that inthe context of\n",
      "       directedmodels,theseconceptsarereferredtoas d-sepa r at i o n   .The“d”stands\n",
      "          for“dependence.” D-separationfordirectedgraphsisdeﬁnedthesameasseparation\n",
      "         forundirectedgraphs:Wesaythatasetofvariables A   isd-separatedfromanother\n",
      "  setofvariables B     givenathirdsetofvariables S    ifthegraphstructureimplies\n",
      "       thatisindependentfromgiven. A B S\n",
      "           Aswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\n",
      "              graphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\n",
      "              aredependentifthereisanactivepathbetweenthemandd-separatedifnosuch\n",
      "5 6 9       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "a\n",
      " b c\n",
      "d\n",
      "            Figure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\n",
      "b         isshadedtoindicatethatitisobserved.Becauseobservingb    blockstheonlypathfrom\n",
      "atoc   ,wesaythataandc     areseparatedfromeachothergivenb   .Theobservationofb\n",
      "    alsoblocksonepathbetweenaandd         ,butthereisasecond,activepathbetweenthem.\n",
      "        Therefore,aanddarenotseparatedgivenb.\n",
      "            pathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\n",
      "             morecomplicated.Seeﬁgureforaguidetoidentifyingactivepathsina 16.8\n",
      "             directedmodel.Seeﬁgureforanexampleofreadingsomepropertiesfroma 16.9\n",
      "graph.\n",
      "           Itisimportanttorememberthatseparationandd-separationtellusonly\n",
      "            aboutthoseconditionalindependencesthatareimpliedbythegraph.Thereisno\n",
      "           requirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\n",
      "              itisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\n",
      "         torepresentanydistribution.Infact,somedistributionscontainindependences\n",
      "         thatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\n",
      " speciﬁcindepende nces       areindependencesthatarepresentdependentonthe\n",
      "            valueofsomevariablesinthenetwork. Forexample,consideramodelofthree\n",
      " binaryvariables:a,bandc   .Supposethatwhena is0,bandc areindependent,\n",
      " butwhena is1,b   isdeterministicallyequaltoc    . Encodingthebehaviorwhen\n",
      "a     = 1requiresanedgeconnectingbandc       .Thegraphthenfailstoindicatethatb\n",
      "      andcareindependentwhena.= 0\n",
      "             Ingeneral,agraphwillneverimplythatanindependenceexistswhenitdoes\n",
      "         not.However,agraphmayfailtoencodeanindependence.\n",
      "      16.2.6ConvertingbetweenUndirectedandDirectedGraphs\n",
      "             Weoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected.\n",
      "             Forexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\n",
      "           Thischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\n",
      "           isinherentlydirectedorundirected.Instead,somemodelsaremosteasilydescribed\n",
      "           usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\n",
      "5 7 0       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "  a s b\n",
      "  a s b   a s b\n",
      " (a) (b)\n",
      "a\n",
      "sb   a s b\n",
      "c\n",
      " (c) (d)\n",
      "               Figure16.8:Allthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\n",
      "variablesaandb        .Anypathwitharrowsproceedingdirectlyfrom (a) atob  orviceversa.\n",
      "      Thiskindofpathbecomesblockedifs       isobserved. Wehavealreadyseenthiskindof\n",
      "       pathintherelayraceexample.Variables (b) aandb     areconnectedbyacommoncauses.\n",
      "  Forexample,supposes           isavariableindicatingwhetherornotthereisahurricane,anda\n",
      "andb             measurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifwe\n",
      "     observeveryhighwindsatstationa         ,wemightexpecttoalsoseehighwindsatb .This\n",
      "       kindofpathcanbeblockedbyobservings         .Ifwealreadyknowthereisahurricane,we\n",
      "     expecttoseehighwindsatb      ,regardlessofwhatisobservedata    .Alowerthanexpected\n",
      " windata          (forahurricane)wouldnotchangeourexpectationofwindsatb  (knowingthere\n",
      "    isahurricane).However,ifs   isnotobserved,thenaandb     aredependent,i.e.,thepathis\n",
      "  active.Variables (c) aandb   arebothparentsofs    .ThisiscalledaV-structure ,orthe\n",
      " collidercase  . TheV-structurecausesaandb    toberelatedbythe  explainingaway\n",
      "eﬀect         .Inthiscase,thepathisactuallyactivewhens    isobserved.Forexample,suppose\n",
      "s           isavariableindicatingthatyourcolleague isnotatwork. Thevariablearepresents\n",
      "   herbeingsick,whileb            representsherbeingonvacation.Ifyouobservethatsheisnot\n",
      "                atwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially\n",
      "                 likelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,\n",
      "                thisfactissuﬃcientto herabsence.Youcaninferthatsheisprobablynotalso explain\n",
      "           sick.Theexplainingawayeﬀecthappensevenifanydescendantof (d) s  isobserved!For\n",
      "  example,supposethatc         isavariablerepresentingwhetheryouhavereceivedareport\n",
      "              fromyourcolleague. Ifyounoticethatyouhavenotreceivedthereport,thisincreases\n",
      "                yourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit\n",
      "                 morelikelythatsheiseithersickoronvacation. Theonlywaytoblockapaththrougha\n",
      "           V-structureistoobservenoneofthedescendantsofthesharedchild.\n",
      "5 7 1       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      " a b\n",
      "c\n",
      " d e\n",
      "            Figure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\n",
      "include:\n",
      "         •aandbared-separatedgiventheemptyset\n",
      "       •aandeared-separatedgivenc\n",
      "       •dandeared-separatedgivenc\n",
      "              Wecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\n",
      "variables:\n",
      "        •aandbarenotd-separatedgivenc\n",
      "        •aandbarenotd-separatedgivend\n",
      "5 7 2       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "           Figure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\n",
      "            Hereweshowexampleswithfourrandomvariables.(Left)Thecompleteundirectedgraph.\n",
      "             Intheundirectedcase,thecompletegraphisunique. Acompletedirectedgraph. (Right)\n",
      "                Inthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\n",
      "                variablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe\n",
      "              ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\n",
      "              variables.Inthisexample,weorderthevariablesfromlefttoright,toptobottom.\n",
      "          Directedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\n",
      "         vantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\n",
      "             weshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\n",
      "            dependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\n",
      "           useeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\n",
      "          capturethemostindependencesintheprobabilitydistributionorwhichapproach\n",
      "            usesthefewestedgestodescribethedistribution.Otherfactorscanaﬀectthe\n",
      "            decisionofwhichlanguagetouse.Evenwhileworkingwithasingleprobabil-\n",
      "         itydistribution,wemaysometimesswitchbetweendiﬀerentmodelinglanguages.\n",
      "           Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserveacertain\n",
      "             subsetofvariables,orifwewishtoperformadiﬀerentcomputationaltask.For\n",
      "         example,thedirectedmodeldescriptionoftenprovidesastraightforwardapproach\n",
      "            toeﬃcientlydrawsamplesfromthemodel(describedinsection),whilethe 16.3\n",
      "         undirectedmodelformulationisoftenusefulforderivingapproximateinference\n",
      "              procedures(aswewillseeinchapter,wheretheroleofundirectedmodelsis 19\n",
      "   highlightedinequation).19.56\n",
      "           Everyprobabilitydistributioncanberepresentedbyeitheradirectedmodelor\n",
      "            anundirectedmodel.Intheworstcase,onecanalwaysrepresentanydistribution\n",
      "             byusinga“completegraph.”Foradirectedmodel,thecompletegraphisany\n",
      "            directedacyclicgraphinwhichweimposesomeorderingontherandomvariables,\n",
      "              andeachvariablehasallothervariablesthatprecedeitintheorderingasits\n",
      "             ancestorsinthegraph.Foranundirectedmodel,thecompletegraphissimplya\n",
      "           graphcontainingasinglecliqueencompassingallthevariables.Seeﬁgure16.10\n",
      "  foranexample.\n",
      "5 7 3       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "              Ofcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\n",
      "             variablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\n",
      "    doesnotimplyanyindependences.\n",
      "            Whenwerepresentaprobabilitydistributionwithagraph,wewanttochoose\n",
      "           agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\n",
      "     independencesthatdonotactuallyexist.\n",
      "           Fromthispointofview,somedistributionscanberepresentedmoreeﬃciently\n",
      "          usingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃciently\n",
      "   using undirectedmodels.In other words,directed models can encode som e\n",
      "        independencesthatundirectedmodelscannotencode,andviceversa.\n",
      "            Directedmodelsareabletouseonespeciﬁckindofsubstructurethatundirected\n",
      "        modelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.\n",
      "      Thestructureoccurswhentworandomvariablesaandb    arebothparentsofa\n",
      "  thirdrandomvariablec       ,andthereisnoedgedirectlyconnectingaandb ineither\n",
      "            direction.(Thename“immorality”mayseemstrange;itwascoinedinthegraphical\n",
      "            modelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\n",
      " withgraph D          intoanundirectedmodel,weneedtocreateanewgraph U. For\n",
      "   everypairofvariablesxandy      ,weaddanundirectededgeconnectingxandyto\n",
      "U         ifthereisadirectededge(ineitherdirection)connectingxandyin D orifx\n",
      "andy   arebothparentsin D   ofathirdvariablez  .Theresulting U   isknownasa\n",
      " moralizedgraph          .Seeﬁgureforexamplesofconvertingdirectedmodelsto 16.11\n",
      "   undirectedmodelsviamoralization.\n",
      "         Likewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\n",
      "      canrepresentperfectly.Speciﬁcally,adirectedgraph D    cannotcaptureallthe\n",
      "      conditionalindependencesimpliedbyanundirectedgraph Uif U  containsaloop\n",
      "          oflengthgreaterthanthree,unlessthatloopalsocontainsachord   .Aloopis\n",
      "            asequenceofvariablesconnectedbyundirectededges,withthelastvariablein\n",
      "              thesequenceconnectedbacktotheﬁrstvariableinthesequence.Achordisa\n",
      "          connectionbetweenanytwononconsecutivevariablesinthesequencedeﬁninga\n",
      " loop.If U             hasloopsoflengthfourorgreateranddoesnothavechordsforthese\n",
      "               loops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\n",
      "           thesechordsdiscardssomeoftheindependenceinformationthatwasencodedin U.\n",
      "      Thegraphformedbyaddingchordsto U   isknownasachordal ,ortriangulated,\n",
      "             graph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\n",
      "     loops.Tobuildadirectedgraph D        fromthechordalgraph,weneedtoalsoassign\n",
      "              directionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein D,\n",
      "              ortheresultwillnotdeﬁneavaliddirectedprobabilisticmodel.Onewaytoassign\n",
      "    directionstotheedgesin D         istoimposeanorderingontherandomvariables,then\n",
      "5 7 4       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3 a b\n",
      "ca\n",
      "cb\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3 a b\n",
      "ca\n",
      "cb\n",
      "          Figure16.11: Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\n",
      "            (bottomrow)byconstructingmoralizedgraphs.(Left)Thissimplechaincanbeconverted\n",
      "             toamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\n",
      "           resultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional\n",
      "      independences. This graphis thesimplest directedmodel thatcannot be (Center)\n",
      "           convertedtoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsists\n",
      "     entirelyofasingleimmorality.Becauseaandb  areparentsofc     ,theyareconnectedbyan\n",
      "  activepathwhenc         isobserved.Tocapturethisdependence,theundirectedmodelmust\n",
      "              includeacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthat\n",
      "ab ⊥              . Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany (Right)\n",
      "          impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\n",
      "             edgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\n",
      " directdependences.\n",
      "5 7 5       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      " a b\n",
      " d c a b\n",
      " d c a b\n",
      " d c\n",
      "            Figure16.12:Convertinganundirectedmodeltoadirectedmodel.(Left)Thisundirected\n",
      "                modelcannotbeconvertedtoadirectedmodelbecauseithasaloopoflengthfourwith\n",
      "          nochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthat\n",
      "     nodirectedmodelcancapturesimultaneously:   acbd⊥|{,}and   bdac⊥|{,}. (Center)\n",
      "             Toconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,\n",
      "                 byensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecan\n",
      "    eitheraddanedgeconnectingaandc      orwecanaddanedgeconnectingbandd  .Inthis\n",
      "       example,wechoosetoaddtheedgeconnectingaandc     . Toﬁnishtheconversion (Right)\n",
      "                process,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\n",
      "               directedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\n",
      "                andalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode\n",
      "               thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\n",
      " alphabeticalorder.\n",
      "               pointeachedgefromthenodethatcomesearlierintheorderingtothenodethat\n",
      "          comeslaterintheordering.Seeﬁgureforademonstration. 16.12\n",
      "  16.2.7FactorGraphs\n",
      " Factorgraphs         areanotherwayofdrawingundirectedmodelsthatresolvean\n",
      "          ambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\n",
      "      anundirectedmodel,thescopeofeveryφ        functionmustbeaofsomeclique subset\n",
      "              inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\n",
      "         acorrespondingfactorwhosescopeencompassestheentireclique—forexample,\n",
      "             acliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\n",
      "              ormaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\n",
      "           Factorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeachφ\n",
      "           function.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirected\n",
      "             modelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\n",
      "            ascircles.Thesenodescorrespondtorandomvariables,asinastandardundirected\n",
      "           model. Therestofthenodesaredrawnassquares. Thesenodescorrespondto\n",
      " thefactorsφ       oftheunnormalizedprobabilitydistribution.Variablesandfactors\n",
      "            maybeconnectedwithundirectededges.Avariableandafactorareconnected\n",
      "5 7 6       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      " a b\n",
      "c a b\n",
      "cf 1 f 1 a b\n",
      "cf 1 f 1f 2 f 2\n",
      "f 3 f 3\n",
      "              Figure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation\n",
      "           ofundirectednetworks.(Left)Anundirectednetworkwithacliqueinvolvingthree\n",
      "variables:a,bandc         . Afactorgraphcorrespondingtothesameundirected (Center)\n",
      "             model.Thisfactorgraphhasonefactoroverallthreevariables. Anothervalid (Right)\n",
      "            factorgraphforthesameundirectedmodel. Thisfactorgraphhasthreefactors,each\n",
      "          overonlytwovariables.Representation,inference,andlearningareallasymptotically\n",
      "               cheaperinthisfactorgraphthaninthefactorgraphdepictedinthecenter,eventhough\n",
      "       bothrequirethesameundirectedgraphtorepresent.\n",
      "                 inthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\n",
      "          theunnormalizedprobabilitydistribution.Nofactormaybeconnectedtoanother\n",
      "               factorinthegraph,norcanavariablebeconnectedtoavariable.Seeﬁgure16.13\n",
      "             foranexampleofhowfactorgraphscanresolveambiguityintheinterpretationof\n",
      " undirectednetworks.\n",
      "    16.3SamplingfromGraphicalModels\n",
      "           Graphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\n",
      "            Oneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientproce-\n",
      " durecalled  ancestralsampling       canproduceasamplefromthejointdistribution\n",
      "   representedbythemodel.\n",
      "       Thebasicideaistosortthevariablesx i      inthegraphintoatopologicalordering,\n",
      "   sothatforalliandj,j  isgreaterthaniifx i   isaparentofx j  .Thevariables\n",
      "           canthenbesampledinthisorder. Inotherwords,weﬁrstsamplex 1 ∼P(x 1),\n",
      " thensampleP(x 2  |Pa G(x 2       )),andsoon,untilﬁnallywesampleP(x n  |Pa G(x n)).\n",
      "     Solongaseachconditionaldistributionp(x i  |Pa G(x i     ))iseasytosamplefrom,\n",
      "            thenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\n",
      "           guaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\n",
      "            samplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto\n",
      "       sampleavariablebeforeitsparentsareavailable.\n",
      "5 7 7       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "          Forsomegraphs,morethanonetopologicalorderingispossible.Ancestral\n",
      "         samplingmaybeusedwithanyofthesetopologicalorderings.\n",
      "          Ancestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\n",
      "    tionaliseasy)andconvenient.\n",
      "            Onedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\n",
      "           models.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\n",
      "              operation.Whenwewishtosamplefromasubsetofthevariablesinadirected\n",
      "            graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\n",
      "             ingvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\n",
      "           Inthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\n",
      "         speciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionswe\n",
      "           needtosamplefromaretheposteriordistributionsgiventheobservedvariables.\n",
      "         Theseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrized\n",
      "            inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\n",
      "         thisisthecase,ancestralsamplingisnolongereﬃcient.\n",
      "         Unfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\n",
      "            cansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\n",
      "         oftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\n",
      "            distributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\n",
      "          somanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling\n",
      "             fromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemsto\n",
      "         requireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\n",
      "            variable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\n",
      "          drawingsamplesfromanundirectedgraphicalmodelisanexpensive,multipass\n",
      "     process.Theconceptuallysimplestapproachis Gibbssampling  .Supposewe\n",
      "     haveagraphicalmodeloveran n     -dimensionalvectorofrandomvariables x .We\n",
      "   iterativelyvisiteachvariablex i        anddrawasampleconditionedonalltheother\n",
      " variables,from p (x i |x − i        ).Duetotheseparationpropertiesofthegraphical\n",
      "         model,wecanequivalentlyconditionononlytheneighborsofx i .Unfortunately,\n",
      "            afterwehavemadeonepassthroughthegraphicalmodelandsampledall n\n",
      "         variables,westilldonothaveafairsamplefrom p ( x     ).Instead,wemustrepeatthe\n",
      "   processandresampleall n        variablesusingtheupdatedvaluesoftheirneighbors.\n",
      "         Asymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\n",
      "            thecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshave\n",
      "         reachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Sampling\n",
      "            techniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\n",
      " chapter.17\n",
      "5 7 8       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "    16.4AdvantagesofStructuredModeling\n",
      "           Theprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\n",
      "            ustodramaticallyreducethecostofrepresentingprobabilitydistributionsaswellas\n",
      "            learningandinference.Samplingisalsoacceleratedinthecaseofdirectedmodels,\n",
      "          whilethesituationcanbecomplicatedwithundirectedmodels.Theprimary\n",
      "            mechanismthatallowsalltheseoperationstouselessruntimeandmemory is\n",
      "         choosingtonotmodelcertaininteractions.Graphicalmodelsconveyinformation\n",
      "             byleavingedgesout.Anywherethereisnotanedge,themodelspeciﬁesthe\n",
      "          assumptionthatwedonotneedtomodeladirectinteraction.\n",
      "          Alessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthat\n",
      "           theyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\n",
      "           knowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\n",
      "           developanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\n",
      "          inferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,\n",
      "             wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\n",
      "           data.Wecanthencombinethesediﬀerentalgorithmsandstructuresandobtain\n",
      "            aCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃcultto\n",
      "      designend-to-endalgorithmsforeverypossiblesituation.\n",
      "   16.5LearningaboutDependencies\n",
      "           Agoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\n",
      "   observed,or“visible,”variablesv     .Oftenthediﬀerentelementsofv arehighly\n",
      "            dependentoneachother.Inthecontextofdeeplearning,theapproachmost\n",
      "           commonlyusedtomodelthesedependenciesistointroduceseverallatentor\n",
      " “hidden”variables,h         .Themodelcanthencapturedependenciesbetweenanypair\n",
      " ofvariablesv iandv j     indirectly,viadirectdependenciesbetweenv iandh ,and\n",
      "    directdependenciesbetweenandv h j.\n",
      "   Agoodmodelofv         whichdidnotcontainanylatentvariableswouldneedto\n",
      "              haveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\n",
      "          cliquesinaMarkovnetwork.Justrepresentingthesehigher-orderinteractionsis\n",
      "          costly—bothinacomputationalsense,becausethenumberofparametersthat\n",
      "             mustbestoredinmemory scalesexponentiallywiththenumberofmembersina\n",
      "            clique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\n",
      "       requiresawealthofdatatoestimateaccurately.\n",
      "          Whenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\n",
      "            withdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\n",
      "5 7 9       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "            graphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\n",
      "           omitedgesbetweenothervariables.Anentireﬁeldofmachinelearningcalled\n",
      " structurelearning          isdevotedtothisproblem.Foragoodreferenceonstructure\n",
      "          learning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare\n",
      "              aformofgreedysearch.Astructureisproposedandamodelwiththatstructure\n",
      "             istrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\n",
      "          penalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\n",
      "              addedorremovedarethenproposedasthenextstepofthesearch.Thesearch\n",
      "           proceedstoanewstructurethatisexpectedtoincreasethescore.\n",
      "           Usinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\n",
      "            discretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisibleand\n",
      "           hiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunitsto\n",
      "         imposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameterlearning\n",
      "             techniques,wecanlearnamodelwithaﬁxedstructurethatimputestheright\n",
      "    structureonthemarginal.p()v\n",
      "         Latentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturing\n",
      "p(v   ).Thenewvariablesh     alsoprovideanalternativerepresentationforv .For\n",
      "            example,asdiscussedinsection,themixtureofGaussiansmodellearnsa 3.9.6\n",
      "            latentvariablethatcorrespondstothecategoryofexamplestheinputwasdrawn\n",
      "              from.ThismeansthatthelatentvariableinamixtureofGaussiansmodelcanbe\n",
      "            usedtodoclassiﬁcation.Inchapterwesawhowsimpleprobabilisticmodels 14\n",
      "             likesparsecodinglearnlatentvariablesthatcanbeusedasinputfeaturesfor\n",
      "             aclassiﬁer,orascoordinatesalongamanifold.Othermodelscanbeusedin\n",
      "            thissameway,butdeepermodelsandmodelswithdiﬀerentkindsofinteractions\n",
      "          cancreateevenricherdescriptionsoftheinput.Manyapproachesaccomplish\n",
      "          featurelearningbylearninglatentvariables.Often,givensomemodelofvandh,\n",
      "   experimentalobservationsshowthat E[  hv| ]orargmax hp( hv,    )isagoodfeature\n",
      "  mappingfor.v\n",
      "    16.6InferenceandApproximateInference\n",
      "               Oneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\n",
      "               howvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\n",
      "              whatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\n",
      " extractfeatures E[  hv|    ]describingtheobservedvariablesv   .Sometimesweneed\n",
      "              tosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\n",
      "      usingtheprincipleofmaximumlikelihood.Because\n",
      " log() = pv E h h∼ p (| v )         [log( )log( )] phv,−phv|, (16.9)\n",
      "5 8 0       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "    weoftenwanttocompute p(  h| v         )inordertoimplementalearningrule.Allthese\n",
      "  areexamplesof i nf e r e nce          problemsinwhichwemustpredictthevalueofsome\n",
      "          variablesgivenothervariables,orpredicttheprobabilitydistributionoversome\n",
      "      variablesgiventhevalueofothervariables.\n",
      "         Unfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\n",
      "            intractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The\n",
      "        graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\n",
      "             withareasonablenumberofparameters,butthegraphsusedfordeeplearningare\n",
      "        usuallynotrestrictiveenoughtoalsoalloweﬃcientinference.\n",
      "            Itisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\n",
      "             graphicalmodelis#Phard.Thecomplexityclass#Pisageneralizationofthe\n",
      "           complexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\n",
      "              hasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecounting\n",
      "           thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\n",
      "            wedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem. We\n",
      "            canimposeauniformdistributionoverthesevariables.Wecanthenaddone\n",
      "           binarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed.\n",
      "            Wecanthenaddanotherlatentvariableindicatingwhetheralltheclausesare\n",
      "             satisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\n",
      "             treeoflatentvariables,witheachnodeinthetreereportingwhethertwoother\n",
      "             variablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause.\n",
      "            Therootofthetreereportswhethertheentireproblemissatisﬁed. Becauseof\n",
      "           theuniformdistributionovertheliterals,themarginaldistributionovertheroot\n",
      "           ofthereductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.\n",
      "            Whilethisisacontrivedworst-caseexample,NPhardgraphscommonlyarisein\n",
      "  practicalreal-worldscenarios.\n",
      "         Thismotivatestheuse ofapproximateinference.Inthecontextof deep\n",
      "           learning,thisusuallyreferstovariationalinference,inwhichweapproximatethe\n",
      " truedistribution p(  h| v     )byseekinganapproximatedistribution q(hv|   )thatisas\n",
      "              closetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\n",
      "  inchapter.19\n",
      "      16.7TheDeepLearningApproachtoStructured\n",
      " ProbabilisticModels\n",
      "          Deeplearningpractitionersgenerallyusethesamebasiccomputationaltoolsas\n",
      "         othermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\n",
      "5 8 1       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "            Inthecontextofdeeplearning,however,weusuallymakediﬀerentdesigndecisions\n",
      "            abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\n",
      "         haveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels.\n",
      "           Deeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\n",
      "               contextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthe\n",
      "            graphicalmodelgraphratherthanthecomputationalgraph.Wecanthinkofa\n",
      " latentvariable h i   asbeingatdepth j    iftheshortestpathfrom h i  toanobserved\n",
      " variableis j            steps.Weusuallydescribethedepthofthemodelasbeingthegreatest\n",
      "   depthofanysuch h i           .Thiskindofdepthisdiﬀerentfromthedepthinducedby\n",
      "           thecomputationalgraph.Manygenerativemodelsusedfordeeplearninghaveno\n",
      "            latentvariablesoronlyonelayeroflatentvariablesbutusedeepcomputational\n",
      "        graphstodeﬁnetheconditionaldistributionswithinamodel.\n",
      "           Deeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\n",
      "           tations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\n",
      "            shallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways\n",
      "            haveasinglelargelayeroflatentvariables.Deeplearningmodelstypicallyhave\n",
      "        morelatentvariablesthanobservedvariables.Complicatednonlinearinteractions\n",
      "         betweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthrough\n",
      "  multiplelatentvariables.\n",
      "         Bycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\n",
      "             areatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\n",
      "         randomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order\n",
      "         termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\n",
      "           variables.Iftherearelatentvariables,theyareusuallyfewinnumber.\n",
      "            Thewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.The\n",
      "           deeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\n",
      "            takeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreeto\n",
      "             inventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\n",
      "             usuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\n",
      "          techniquesmayallowsomeroughcharacterizationofwhattheyrepresent.When\n",
      "            latentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\n",
      "           oftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,\n",
      "            theintelligenceofastudent,thediseasecausingapatient’ssymptoms,andso\n",
      "           forth.Thesemodelsareoftenmuchmoreinterpretablebyhumanpractitionersand\n",
      "             oftenhavemoretheoreticalguarantees,yettheyarelessabletoscaletocomplex\n",
      "             problemsandarenotreusableinasmanydiﬀerentcontextsasdeepmodelsare.\n",
      "            Anotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthedeep\n",
      "           learningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunitsthat\n",
      "5 8 2       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "             areallconnectedtoothergroupsofunits,sothattheinteractionsbetweentwo\n",
      "            groupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodelshavevery\n",
      "            fewconnections,andthechoiceofconnectionsforeachvariablemaybeindividually\n",
      "             designed.Thedesignofthemodelstructureistightlylinkedwiththechoiceof\n",
      "         inferencealgorithm.Traditionalapproachestographicalmodelstypicallyaimto\n",
      "           maintainthetractabilityofexactinference.Whenthisconstraintistoolimiting,\n",
      "      apopularapproximateinferencealgorithmiscalled  loopybeliefpropagation.\n",
      "          Bothapproachesoftenworkwellwithsparselyconnectedgraphs.Bycomparison,\n",
      "          modelsusedindeeplearningtendtoconnecteachvisibleunitv i  tomanyhidden\n",
      "unitsh j  ,sothat h     canprovideadistributedrepresentationofv i (andprobably\n",
      "        severalotherobservedvariablestoo).Distributedrepresentationshavemany\n",
      "           advantages,butfromthepointofviewofgraphicalmodelsandcomputational\n",
      "        complexity,distributedrepresentationshavethedisadvantageofusuallyyielding\n",
      "            graphsthatarenotsparseenoughforthetraditionaltechniquesofexactinference\n",
      "             andloopybeliefpropagationtoberelevant.Asaconsequence,oneofthemost\n",
      "          strikingdiﬀerencesbetweenthelargergraphicalmodelscommunityandthedeep\n",
      "           graphicalmodelscommunityisthatloopybeliefpropagationisalmostneverused\n",
      "            fordeeplearning.MostdeepmodelsareinsteaddesignedtomakeGibbssampling\n",
      "         orvariationalinferencealgorithmseﬃcient.Anotherconsiderationisthatdeep\n",
      "           learningmodelscontainaverylargenumberoflatentvariables,makingeﬃcient\n",
      "          numericalcodeessential.Thisprovidesanadditionalmotivation,besidesthechoice\n",
      "            ofhigh-levelinferencealgorithm,forgroupingtheunitsintolayerswithamatrix\n",
      "            describingtheinteractionbetweentwolayers.Thisallowstheindividualstepsofthe\n",
      "          algorithmtobeimplementedwitheﬃcientmatrixproductoperations,orsparsely\n",
      "        connectedgeneralizations,likeblockdiagonalmatrixproductsorconvolutions.\n",
      "          Finally,thedeeplearningapproachtographicalmodelingischaracterizedby\n",
      "           amarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\n",
      "             allquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\n",
      "               themodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\n",
      "           whosemarginaldistributionscannotbecomputedandaresatisﬁedsimplytodraw\n",
      "           approximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\n",
      "           objectivefunctionthatwecannotevenapproximateinareasonableamountof\n",
      "              time,butwearestillabletoapproximatelytrainthemodelifwecaneﬃciently\n",
      "             obtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach\n",
      "            isoftentoﬁgureoutwhattheminimumamountofinformationweabsolutely\n",
      "              needis,andthentoﬁgureouthowtogetareasonableapproximationofthat\n",
      "    informationasquicklyaspossible.\n",
      "5 8 3       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\n",
      "        Figure16.14:AnRBMdrawnasaMarkovnetwork.\n",
      "     16.7.1Example:TheRestrictedBoltzmannMachine\n",
      "The   restrictedBoltzmannmachine    (RBM)( ,),or Smolensky1986 harmo-\n",
      "nium            ,isthequintessentialexampleofhowgraphicalmodelsareusedfordeep\n",
      "              learning.TheRBMisnotitselfadeepmodel.Instead,ithasasinglelayer\n",
      "             oflatentvariablesthatmaybeusedtolearnarepresentationfortheinput. In\n",
      "               chapter,wewillseehowRBMscanbeusedtobuildmanydeepermodels.Here, 20\n",
      "               weshowhowtheRBMexempliﬁesmanyofthepracticesusedinawidevarietyof\n",
      "            deepgraphicalmodels:itsunitsareorganizedintolargegroupscalledlayers,the\n",
      "           connectivitybetweenlayersisdescribedbyamatrix,theconnectivityisrelatively\n",
      "            dense,themodelisdesignedtoalloweﬃcientGibbssampling,andtheemphasis\n",
      "             ofthemodeldesignisonfreeingthetrainingalgorithmtolearnlatentvariables\n",
      "             whosesemanticswerenotspeciﬁedbythedesigner.Insection,werevisitthe 20.2\n",
      "   RBMinmoredetail.\n",
      "           ThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\n",
      "    units.Itsenergyfunctionis\n",
      " E,(vhb ) = −  vc− hv−  Wh, (16.10)\n",
      "whereb,c ,andW       areunconstrained,realvalued,learnableparameters.Wecan\n",
      "          seethatthemodelisdividedintotwogroupsofunits:vandh   ,andtheinteraction\n",
      "      betweenthemisdescribedbyamatrixW      .Themodelisdepictedgraphicallyin\n",
      "              ﬁgure.Asthisﬁguremakesclear,animportantaspectofthismodelisthat 16.14\n",
      "             therearenodirectinteractionsbetweenanytwovisibleunitsorbetweenanytwo\n",
      "          hiddenunits(hence“restricted”;ageneralBoltzmannmachinemayhavearbitrary\n",
      "connections).\n",
      "         TherestrictionsontheRBMstructureyieldtheniceproperties\n",
      "  p( ) = Π hv| ip(h i  |v) (16.11)\n",
      "and\n",
      "  p( ) = Π vh| ip(v i  |h). (16.12)\n",
      "5 8 4       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "             Figure16.15:SamplesfromatrainedRBManditsweights.(Left)Samplesfroma\n",
      "             modeltrainedonMNIST,drawnusingGibbssampling.EachcolumnisaseparateGibbs\n",
      "             samplingprocess.Eachrowrepresentstheoutputofanother1,000stepsofGibbssampling.\n",
      "         Successivesamplesarehighlycorrelatedwithoneanother. (Right)Thecorresponding\n",
      "              weightvectors.Comparethistothesamplesandweightsofalinearfactormodel,shown\n",
      "            inﬁgure.ThesamplesherearemuchbetterbecausetheRBMprior 13.2 p(h  )isnot\n",
      "            constrainedtobefactorial.TheRBMcanlearnwhichfeaturesshouldappeartogether\n",
      "        whensampling.Ontheotherhand,theRBMposteriorp(  hv|    )isfactorial,whilethe\n",
      "  sparsecodingposteriorp(  hv|           ) isnot,sothesparsecodingmodelmaybebetterforfeature\n",
      "        extraction. Othermodelsareabletohavebothanonfactorialp(h   )andanonfactorial\n",
      "         p( )hv|.ImagereproducedwithpermissionfromLISA(2008).\n",
      "            Theindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\n",
      " weobtain\n",
      " P(h i = 1 ) = | vσ\n",
      "vW : , i +c i\n",
      " , (16.13)\n",
      " P(h i   = 0 ) = 1| v−σ\n",
      "vW : , i +c i\n",
      " . (16.14)\n",
      "     Togetherthesepropertiesallowforeﬃcient  blockGibbssampling  ,whichalter-\n",
      "    natesbetweensamplingallof h     simultaneouslyandsamplingallof vsimultane-\n",
      "            ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\n",
      " ﬁgure.16.15\n",
      "              Sincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\n",
      "      easytotakeitsderivatives.Forexample,\n",
      "∂\n",
      "∂W i , j E,( v h) = −v ih j . (16.15)\n",
      "       Thesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—make\n",
      "            trainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\n",
      "          trainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\n",
      "     Trainingthemodelinducesarepresentationh  ofthedatav    .Wecanoftenuse\n",
      "E h h∼ p (| v )        []hasasetoffeaturestodescribe.v\n",
      "5 8 5       C HAP T E R 1 6 . S T R UC T URE D P R O B AB I L I S T I C M O D E L S F O R D E E P L E ARNI N G\n",
      "          Overall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\n",
      "        icalmodels: representationlearningaccomplishedvialayersoflatentvariables,\n",
      "        combinedwitheﬃcientinteractionsbetweenlayersparametrizedbymatrices.\n",
      "           Thelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguage\n",
      "           fordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\n",
      "           amongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\n",
      "5 8 6 C h a p t e r 1 7\n",
      "  Mon t e C arlo Metho ds\n",
      "          Randomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsand\n",
      "          MonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrect\n",
      "           answer(orreportthattheyfailed).Thesealgorithmsconsumearandomamount\n",
      "           ofresources,usuallymemory ortime.Incontrast,MonteCarloalgorithmsreturn\n",
      "             answerswitharandomamountoferror.Theamountoferrorcantypicallybe\n",
      "           reducedbyexpendingmoreresources(usuallyrunningtimeandmemory). Forany\n",
      "          ﬁxedcomputationalbudget,aMonteCarloalgorithmcanprovideanapproximate\n",
      "answer.\n",
      "             Manyproblemsinmachinelearningaresodiﬃcultthatwecanneverexpectto\n",
      "          obtainpreciseanswerstothem.Thisexcludesprecisedeterministicalgorithmsand\n",
      "         LasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithms\n",
      "         orMonteCarloapproximations.Bothapproachesareubiquitousinmachine\n",
      "         learning.Inthischapter,wefocusonMonteCarlomethods.\n",
      "     17.1SamplingandMonteCarloMethods\n",
      "          Manyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebased\n",
      "           ondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplesto\n",
      "        formaMonteCarloestimateofsomedesiredquantity.\n",
      "  17.1.1WhySampling?\n",
      "            Wemaywishtodrawsamplesfromaprobabilitydistributionformanyreasons.\n",
      "           Samplingprovidesaﬂexiblewaytoapproximatemanysumsandintegralsat\n",
      "587    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "             reducedcost.Sometimesweusethistoprovideasigniﬁcantspeeduptoacostly\n",
      "              buttractablesum,asinthecasewhenwesubsamplethefulltrainingcostwith\n",
      "           minibatches.Inothercases,ourlearningalgorithmrequiresustoapproximatean\n",
      "              intractablesumorintegral,suchasthegradientofthelogpartitionfunctionofan\n",
      "             undirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthesense\n",
      "             thatwewanttotrainamodelthatcansamplefromthetrainingdistribution.\n",
      "     17.1.2BasicsofMonteCarloSampling\n",
      "             Whenasumoranintegralcannotbecomputedexactly(forexample,thesum\n",
      "             hasanexponentialnumberofterms,andnoexactsimpliﬁcationisknown),itis\n",
      "             oftenpossibletoapproximateitusingMonteCarlosampling.Theideaistoview\n",
      "              thesumorintegralasifitwereanexpectationundersomedistributionandto\n",
      "       approximatetheexpectationbyacorrespondingaverage.Let\n",
      " s=\n",
      "xpfE ()x() = x p  [()]f x (17.1)\n",
      "or\n",
      " s=\n",
      " pfdE ()x()xx= p  [()]f x (17.2)\n",
      "             bethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraint\n",
      "thatp            isaprobabilitydistribution(forthesum)oraprobabilitydensity(forthe\n",
      "    integral)overrandomvariable. x\n",
      "  Wecanapproximates bydrawingnsamplesx(1 )     ,...,x( ) nfromp andthen\n",
      "   formingtheempiricalaverage\n",
      "ˆs n=1\n",
      "nn \n",
      "i =1f(x( ) i ). (17.3)\n",
      "           Thisapproximationisjustiﬁedbyafewdiﬀerentproperties.Theﬁrsttrivial\n",
      "        observationisthattheestimator ˆsisunbiased,since\n",
      "E[ˆs n] =1\n",
      "nn \n",
      "i =1E[(fx( ) i)] =1\n",
      "nn \n",
      "i =1  ss.= (17.4)\n",
      "   Butinaddition,the   lawoflargenumbers    statesthatifthesamplesx( ) iare\n",
      "          i.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:\n",
      "limn → ∞ˆs n = s, (17.5)\n",
      "5 8 8    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "       providedthatthevarianceoftheindividualterms,Var[f( x( ) i    )],isbounded.Tosee\n",
      "      thismoreclearly,considerthevarianceofˆs nasn   increases.ThevarianceVar[ˆs n]\n",
      "        decreasesandconvergesto0,solongasVar[(f x( ) i )] < ∞:\n",
      "Var[ˆs n] =1\n",
      "n2n\n",
      "i =1 Var[()]f x (17.6)\n",
      "=Var[()]f x\n",
      "n . (17.7)\n",
      "             ThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonte\n",
      "            CarloaverageorequivalentlytheamountofexpectederroroftheMonteCarlo\n",
      "        approximation.Wecomputeboththeempiricalaverageofthef( x( ) i  )andtheir\n",
      " empiricalvariance,1         andthendividetheestimatedvariancebythenumberof\n",
      "samplesn    toobtainanestimatorofVar[ˆs n]. The  centrallimittheoremtells\n",
      "      usthatthedistributionoftheaverage, ˆs n     ,convergestoanormaldistribution\n",
      " withmeans andvarianceV a r [ ( )] f x\n",
      "n       .Thisallowsustoestimateconﬁdenceintervals\n",
      "   aroundtheestimate ˆs n        ,usingthecumulativedistributionofthenormaldensity.\n",
      "            Allthisreliesonourabilitytoeasilysamplefromthebasedistributionp( x),\n",
      "              butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefromp ,an\n",
      "            alternativeistouseimportancesampling,presentedinsection.Amoregeneral 17.2\n",
      "            approachistoformasequenceofestimatorsthatconvergetowardthedistribution\n",
      "            ofinterest.ThatistheapproachofMonteCarloMarkovchains(section).17.3\n",
      "  17.2ImportanceSampling\n",
      "             Animportantstepinthedecompositionoftheintegrand(orsummand)usedbythe\n",
      "             MonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould 17.2\n",
      "    playtheroleofprobabilityp( x         )andwhichpartoftheintegrandshouldplaythe\n",
      "   roleofthequantityf( x       ) whoseexpectedvalue(underthatprobabilitydistribution)\n",
      "         istobeestimated.Thereisnouniquedecompositionbecausep( x)f( x  )canalways\n",
      "  berewrittenas\n",
      "pfq () x() = x () xpf() x() x\n",
      "q() x , (17.8)\n",
      "    wherewenowsamplefromq andaveragep f\n",
      "q       .Inmanycases,wewishtocompute\n",
      "    anexpectationforagivenp andanf        ,andthefactthattheproblemisspeciﬁed\n",
      "        fromthestartasanexpectationsuggeststhatthispandf   wouldbeanatural\n",
      "1              Theunbiasedestimatorofthevarianceisoftenpreferred,inwhichthesumofsquared\n",
      "         diﬀerencesisdividedbyinsteadof. n−1 n\n",
      "589    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "          choiceofdecomposition.However,theoriginalspeciﬁcationoftheproblemmay\n",
      "               notbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtain\n",
      "          agivenlevelofaccuracy. Fortunately,theformoftheoptimalchoiceq∗ canbe\n",
      "   derivedeasily.Theoptimalq∗      correspondstowhatiscalledoptimalimportance\n",
      "sampling.\n",
      "           Becauseoftheidentityshowninequation,anyMonteCarloestimator 17.8\n",
      "ˆs p=1\n",
      "nn \n",
      "i , =1 x( ) i∼ pf( x( ) i ) (17.9)\n",
      "       canbetransformedintoanimportancesamplingestimator\n",
      "ˆs q=1\n",
      "nn\n",
      "i , =1 x( ) i∼ qp( x( ) i)(f x( ) i)\n",
      "q( x( ) i) . (17.10)\n",
      "              Weseereadilythattheexpectedvalueoftheestimatordoesnotdependon:q\n",
      "E q[ˆs q] = E q[ˆs p ] = s. (17.11)\n",
      "           Thevarianceofanimportancesamplingestimator,however,canbegreatlysensitive\n",
      "         tothechoiceof.Thevarianceisgivenby q\n",
      "Var[ˆs q] = Var[pf() x() x\n",
      "q() x ]/n. (17.12)\n",
      "      Theminimumvarianceoccurswhenisq\n",
      "q∗() = xpf() x|() x|\n",
      "Z , (17.13)\n",
      "whereZ      isthenormalizationconstant,chosensothatq∗( x    )sumsorintegratesto\n",
      "          1asappropriate.Betterimportancesamplingdistributionsputmoreweightwhere\n",
      "      theintegrandislarger.Infact,whenf( x    )doesnotchangesign,Var[ˆs q∗  ]=0,\n",
      "            meaningthat whentheoptimaldistributionisused. asinglesampleissuﬃcient\n",
      "        Ofcourse,thisisonlybecausethecomputationofq∗   hasessentiallysolvedthe\n",
      "              originalproblem,soitisusuallynotpracticaltousethisapproachofdrawinga\n",
      "     singlesamplefromtheoptimaldistribution.\n",
      "    Anychoiceofsamplingdistributionq       isvalid(inthesenseofyieldingthe\n",
      "   correctexpectedvalue),andq∗         istheoptimalone(inthesenseofyieldingminimum\n",
      "  variance).Samplingfromq∗      isusuallyinfeasible,butotherchoicesofq canbe\n",
      "      feasiblewhilestillreducingthevariancesomewhat.\n",
      "5 9 0    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "    Anotherapproachistouse  biasedimportancesampling   ,whichhasthe\n",
      "    advantageofnotrequiringnormalizedporq       .Inthecaseofdiscretevariables,the\n",
      "      biasedimportancesamplingestimatorisgivenby\n",
      "ˆs B I S=n\n",
      "i =1p ( x( ) i)\n",
      "q ( x( ) i )f( x( ) i)\n",
      "n\n",
      "i =1p ( x( ) i )\n",
      "q ( x( ) i )(17.14)\n",
      "=n\n",
      "i =1p ( x( ) i)\n",
      "˜ q ( x( ) i )f( x( ) i)\n",
      "n\n",
      "i =1p ( x( ) i )\n",
      "˜ q ( x( ) i )(17.15)\n",
      "=n\n",
      "i =1˜ p ( x( ) i)\n",
      "˜ q ( x( ) i)f( x( ) i)\n",
      "n\n",
      "i =1˜ p ( x( ) i )\n",
      "˜ q ( x( ) i ) , (17.16)\n",
      "where˜pand˜q    aretheunnormalizedformsofpandq  ,andthe x( ) i  arethesamples\n",
      "fromq     .Thisestimatorisbiasedbecause E[ˆs B I S]=s   ,exceptasymptoticallywhen\n",
      "  n→∞           andthedenominatorofequation convergesto1.Hencethisestimator 17.14\n",
      "   iscalledasymptoticallyunbiased.\n",
      "    Althoughagoodchoiceofq       cangreatlyimprovetheeﬃciencyofMonteCarlo\n",
      "    estimation,apoorchoiceofq        canmaketheeﬃciencymuchworse.Goingbackto\n",
      "         equation,weseethatiftherearesamplesof 17.12 q forwhichp f ( ) x| ( ) x|\n",
      "q ( ) x islarge,\n",
      "             thenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhenq( x)\n",
      "   istinywhileneitherp( x) norf( x      ) aresmallenoughtocancelit.Theqdistribution\n",
      "                isusuallychosentobeasimpledistributionsothatitiseasytosamplefrom.When\n",
      "x     ishighdimensional,thissimplicityinq   causesittomatchporpf|| poorly.When\n",
      "q( x( ) i) p( x( ) i)|f( x( ) i)|      ,importancesamplingcollectsuselesssamples(summing\n",
      "        tinynumbersorzeros).Ontheotherhand,whenq( x( ) i) p( x( ) i)|f( x( ) i)| ,which\n",
      "            willhappenmorerarely,theratiocanbehuge. Becausetheselattereventsare\n",
      "            rare,theymaynotshowupinatypicalsample,yieldingtypicalunderestimation\n",
      "ofs           ,compensatedrarelybygrossoverestimation. Suchverylargeorverysmall\n",
      "   numbersaretypicalwhen x       ishighdimensional,becauseinhighdimensionthe\n",
      "        dynamicrangeofjointprobabilitiescanbeverylarge.\n",
      "            Inspiteofthisdanger,importancesamplinganditsvariantshavebeenfound\n",
      "          veryusefulinmanymachinelearningalgorithms,includingdeeplearningalgo-\n",
      "            rithms.Forexample,seetheuseofimportancesamplingtoacceleratetrainingin\n",
      "           neurallanguagemodelswithalargevocabulary(section )orotherneural 12.4.3.3\n",
      "             netswithalargenumberofoutputs.Seealsohowimportancesamplinghasbeen\n",
      "           usedtoestimateapartitionfunction(thenormalizationconstantofaprobability\n",
      "5 9 1    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "           distribution)insection,andtoestimatethelog-likelihoodindeepdirected 18.7\n",
      "          models,suchasthevariationalautoencoder,insection .Importancesam- 20.10.3\n",
      "               plingmayalsobeusedtoimprovetheestimateofthegradientofthecostfunction\n",
      "          usedtotrainmodelparameterswithstochasticgradientdescent,particularlyfor\n",
      "              models,suchasclassiﬁers,inwhichmostofthetotalvalueofthecostfunction\n",
      "          comesfromasmallnumberofmisclassiﬁedexamples.Samplingmorediﬃcult\n",
      "            examplesmorefrequentlycanreducethevarianceofthegradientinsuchcases\n",
      " (Hinton2006,).\n",
      "     17.3MarkovChainMonteCarloMethods\n",
      "               Inmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractable\n",
      "       methodfordrawingexactsamplesfromthedistributionp m o d e l( x    )orfromagood\n",
      "    (lowvariance)importancesamplingdistributionq( x     ).Inthecontextofdeep\n",
      "     learning,thismostoftenhappenswhenp m o d e l( x     )isrepresentedbyanundirected\n",
      "          model.Inthesecases,weintroduceamathematicaltoolcalleda  Markovchain\n",
      "   toapproximatelysamplefromp m o d e l( x       ).ThefamilyofalgorithmsthatuseMarkov\n",
      "       chainstoperformMonteCarloestimatesiscalled    MarkovchainMonteCarlo\n",
      "methods          (MCMC).MarkovchainMonteCarlomethodsformachinelearningare\n",
      "          describedatgreaterlengthinKollerandFriedman2009(). Themoststandard,\n",
      "          genericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodel\n",
      "            doesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenient\n",
      "   to present these techniques assampling froman energy-basedmodel (EBM)\n",
      "p( x)  ∝−exp(E()) x        asdescribedinsection .IntheEBMformulation, 16.2.4\n",
      "            everystateisguaranteedtohavenonzeroprobability.MCMCmethodsareinfact\n",
      "           morebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthat\n",
      "         containzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthe\n",
      "            behaviorofMCMCmethodsmustbeprovedonacase-by-casebasisfordiﬀerent\n",
      "              familiesofsuchdistributions.Inthecontextofdeeplearning,itismostcommonto\n",
      "           relyonthegeneraltheoreticalguaranteesthatnaturallyapplytoallenergy-based\n",
      "models.\n",
      "          Tounderstandwhydrawingsamplesfromanenergy-basedmodelisdiﬃcult,\n",
      "             consideranEBMoverjusttwovariables,deﬁningadistributionab.Inorder p(,)\n",
      " tosamplea   ,wemustdrawafromp(  ab|     ),andinordertosampleb  ,wemust\n",
      "  drawitfromp(  ba|        ).Itseemstobeanintractablechicken-and-eggproblem.\n",
      "            Directedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperform\n",
      " ancestralsampling          ,onesimplysampleseachofthevariablesintopologicalorder,\n",
      "           conditioningoneachvariable’sparents,whichareguaranteedtohavealreadybeen\n",
      "5 9 2    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "         sampled(section).Ancestralsamplingdeﬁnesaneﬃcient,single-passmethod 16.3\n",
      "   ofobtainingasample.\n",
      "            InanEBM,wecanavoidthischicken-and-eggproblembysamplingusinga\n",
      "             Markovchain.ThecoreideaofaMarkovchainistohaveastatex thatbegins\n",
      "        asanarbitraryvalue.Overtime,werandomlyupdatex  repeatedly.Eventually\n",
      "x       becomes(verynearly)afairsamplefromp(x     ).Formally,aMarkovchainis\n",
      "    deﬁnedbyarandomstatex   andatransitiondistributionT(x |x )specifying\n",
      "         theprobabilitythatarandomupdatewillgotostatex    ifitstartsinstatex.\n",
      "        RunningtheMarkovchainmeansrepeatedlyupdatingthestatex  toavaluex\n",
      "   sampledfromT(x |x).\n",
      "           TogainsometheoreticalunderstandingofhowMCMCmethodswork,itis\n",
      "            usefultoreparametrizetheproblem.First,werestrictourattentiontothecase\n",
      "   wheretherandomvariablex       hascountablymanystates.Wecanthenrepresent\n",
      "      thestateasjustapositiveintegerx    . Diﬀerentintegervaluesofx  mapbackto\n",
      "      diﬀerentstatesintheoriginalproblem. x\n",
      "           ConsiderwhathappenswhenweruninﬁnitelymanyMarkovchainsinparallel.\n",
      "            AllthestatesofthediﬀerentMarkovchainsaredrawnfromsomedistribution\n",
      "q( ) t(x ),wheret           indicatesthenumberoftimestepsthathaveelapsed.Atthe\n",
      "beginning,q(0 )        issomedistributionthatweusedtoarbitrarilyinitializex foreach\n",
      " Markovchain. Later,q( ) t         isinﬂuencedbyalltheMarkovchainstepsthathave\n",
      "       runsofar.Ourgoalisforq( ) t    ()xtoconvergeto.px()\n",
      "           Becausewehavereparametrizedtheproblemintermsofapositiveintegerx,\n",
      "           wecandescribetheprobabilitydistributionusingavectorwith q v\n",
      " qiv (= x ) = i . (17.17)\n",
      "          ConsiderwhathappenswhenweupdateasingleMarkovchain’sstatex toa\n",
      "  newstatex          .Theprobabilityofasinglestatelandinginstatex  isgivenby\n",
      "q( +1 ) t(x) =\n",
      "xq( ) t ()(xTx  |x.) (17.18)\n",
      "           Usingourintegerparametrization,wecanrepresenttheeﬀectofthetransition\n",
      "          operatorusingamatrix.Wedeﬁnesothat T A A\n",
      "A i , j = (Tx    = = )i|xj. (17.19)\n",
      "             Usingthisdeﬁnition,wecannowrewriteequation.Ratherthanwritingitin 17.18\n",
      " termsofqandT           tounderstandhowasinglestateisupdated,wemaynowusev\n",
      "5 9 3    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "andA           todescribehowtheentiredistributionoverallthediﬀerentMarkovchains\n",
      "        (runninginparallel)shiftsasweapplyanupdate:\n",
      "v( ) t= Av( 1 ) t − . (17.20)\n",
      "          ApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythe\n",
      "matrixA            repeatedly.Inotherwords,wecanthinkoftheprocessasexponentiating\n",
      "  thematrix:A\n",
      "v( ) t= Atv(0 ) . (17.21)\n",
      " ThematrixA         hasspecialstructurebecauseeachofitscolumnsrepresentsa\n",
      "     probabilitydistribution.Suchmatricesarecalled  stochasticmatrices  .Ifthere\n",
      "        isanonzeroprobabilityoftransitioningfromanystatex   toanyotherstatexfor\n",
      " somepowert        ,thenthePerron-Frobeniustheorem(,; Perron1907Frobenius1908,)\n",
      "              guaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan 1\n",
      "      seethatalltheeigenvaluesareexponentiated:\n",
      "v( ) t=\n",
      " VλVdiag()− 1tv(0 ) = ()VdiagλtV− 1v(0 ) . (17.22)\n",
      "                Thisprocesscausesalltheeigenvaluesthatarenotequaltotodecaytozero.Under 1\n",
      "   someadditionalmildconditions,A      isguaranteedtohaveonlyoneeigenvector\n",
      "        witheigenvalue.Theprocessthusconvergestoa 1  stationarydistribution,\n",
      "       sometimesalsocalledthe .Atconvergence, equilibriumdistribution\n",
      "v  = = Avv, (17.23)\n",
      "            andthissameconditionholdsforeveryadditionalstep.Thisisaneigenvector\n",
      "     equation.Tobeastationarypoint,v     mustbeaneigenvectorwithcorresponding\n",
      "           eigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary 1\n",
      "         distribution,repeatedapplicationsofthetransitionsamplingproceduredonot\n",
      "            changethe overthestatesofallthevariousMarkovchains(although distribution\n",
      "         thetransitionoperatordoeschangeeachindividualstate,ofcourse).\n",
      "   IfwehavechosenT     correctly,thenthestationarydistributionq  willbeequal\n",
      "  tothedistributionp         wewishtosamplefrom.WedescribehowtochooseTin\n",
      " section.17.4\n",
      "          MostpropertiesofMarkovchainswithcountablestatescanbegeneralized\n",
      "            tocontinuousvariables.Inthissituation,someauthorscalltheMarkovchaina\n",
      " Harrischain           ,butweusethetermMarkovchaintodescribebothconditions.\n",
      "       Ingeneral,aMarkovchainwithtransitionoperatorT   willconverge,undermild\n",
      "        conditions,toaﬁxedpointdescribedbytheequation\n",
      "q( x) = E x ∼ q T( x  | x), (17.24)\n",
      "5 9 4    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "          whichinthediscretecaseisjustrewritingequation.When 17.23 x isdiscrete,\n",
      "       theexpectationcorrespondstoasum,andwhen x   iscontinuous,theexpectation\n",
      "   correspondstoanintegral.\n",
      "           Regardlessofwhetherthestateiscontinuousordiscrete,allMarkovchain\n",
      "          methodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythestate\n",
      "          beginstoyieldsamplesfromtheequilibriumdistribution.RunningtheMarkov\n",
      "        chainuntilitreachesitsequilibriumdistributioniscalled  burningin theMarkov\n",
      "           chain.Afterthechainhasreachedequilibrium,asequenceofinﬁnitelymany\n",
      "          samplesmaybedrawnfromtheequilibriumdistribution.Theyareidentically\n",
      "            distributed,butanytwosuccessivesampleswillbehighlycorrelatedwitheachother.\n",
      "             Aﬁnitesequenceofsamplesmaythusnotbeveryrepresentativeoftheequilibrium\n",
      "           distribution.Onewaytomitigatethisproblemistoreturnonlyevery nsuccessive\n",
      "            samples,sothatourestimateofthestatisticsoftheequilibriumdistributionis\n",
      "             notasbiasedbythecorrelationbetweenanMCMCsampleandthenextseveral\n",
      "             samples.Markovchainsarethusexpensivetousebecauseofthetimerequiredto\n",
      "            burnintotheequilibriumdistributionandthetimerequiredtotransitionfrom\n",
      "         onesampletoanotherreasonablydecorrelatedsampleafterreachingequilibrium.\n",
      "           Ifonedesirestrulyindependentsamples,onecanrunmultipleMarkovchains\n",
      "          inparallel.Thisapproachusesextraparallelcomputationtoeliminatelatency.\n",
      "              ThestrategyofusingonlyasingleMarkovchaintogenerateallsamplesandthe\n",
      "             strategyofusingoneMarkovchainforeachdesiredsamplearetwoextremes;deep\n",
      "             learningpractitionersusuallyuseanumberofchainsthatissimilartothenumber\n",
      "              ofexamplesinaminibatchandthendrawasmanysamplesasareneededfrom\n",
      "              thisﬁxedsetofMarkovchains.AcommonlyusednumberofMarkovchainsis100.\n",
      "             Anotherdiﬃcultyisthatwedonotknowinadvancehowmanystepsthe\n",
      "          Markovchainmustrunbeforereachingitsequilibriumdistribution.Thislength\n",
      "    oftimeiscalledthe mixingtime       .TestingwhetheraMarkovchainhasreached\n",
      "              equilibriumisalsodiﬃcult.Wedonothaveapreciseenoughtheoryforguidingus\n",
      "             inansweringthisquestion.Theorytellsusthatthechainwillconverge,butnot\n",
      "               muchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrix A\n",
      "     actingonavectorofprobabilities v        ,thenweknowthatthechainmixeswhen At\n",
      "      haseﬀectivelylostalltheeigenvaluesfrom A      besidestheuniqueeigenvalueof. 1\n",
      "           Thismeansthatthemagnitudeofthesecond-largesteigenvaluewilldeterminethe\n",
      "           mixingtime.Inpractice,though,wecannotactuallyrepresentourMarkovchain\n",
      "              intermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisit\n",
      "             isexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresent v,\n",
      "A    ,ortheeigenvaluesof A         .Becauseoftheseandotherobstacles,weusuallydo\n",
      "             notknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkov\n",
      "              chainforanamountoftimethatweroughlyestimatetobesuﬃcient,anduse\n",
      "5 9 5    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "          heuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristic\n",
      "        methodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween\n",
      " successivesamples.\n",
      "  17.4GibbsSampling\n",
      "           Sofarwehavedescribedhowtodrawsamplesfromadistributionq(x  )byrepeatedly\n",
      "updating  xx← ∼T(x |x        ).Wehavenotdescribedhowtoensurethatq(x  )isa\n",
      "           usefuldistribution.Twobasicapproachesareconsideredinthisbook.Theﬁrst\n",
      "   oneistoderiveT   fromagivenlearnedp m o d e l      ,describedbelowwiththecaseof\n",
      "         samplingfromEBMs.ThesecondoneistodirectlyparametrizeT   andlearnit,so\n",
      "      thatitsstationarydistributionimplicitlydeﬁnesthep m o d e l  ofinterest.Examples\n",
      "          ofthissecondapproacharediscussedinsectionsand. 20.1220.13\n",
      "            Inthecontextofdeeplearning,wecommonlyuseMarkovchainstodraw\n",
      "       samplesfromanenergy-basedmodeldeﬁningadistributionp m o d e l(x   ).Inthiscase,\n",
      "  wewanttheq(x      )fortheMarkovchaintobep m o d e l(x    ).Toobtainthedesired\n",
      "       q()x,wemustchooseanappropriateT(x |x).\n",
      "          AconceptuallysimpleandeﬀectiveapproachtobuildingaMarkovchain\n",
      "  thatsamplesfromp m o d e l(x   )istouse Gibbssampling    ,inwhichsamplingfrom\n",
      "T( x | x      )isaccomplishedbyselectingonevariablex i   andsamplingitfromp m o d e l\n",
      "       conditionedonitsneighborsintheundirectedgraphG     deﬁningthestructureofthe\n",
      "             energy-basedmodel.Wecanalsosampleseveralvariablesatthesametimeaslong\n",
      "            astheyareconditionallyindependentgivenalltheirneighbors.Asshowninthe\n",
      "              RBMexampleinsection ,allthehiddenunitsofanRBMmaybesampled 16.7.1\n",
      "         simultaneouslybecausetheyareconditionallyindependentfromeachothergiven\n",
      "            allthevisibleunits.Likewise,allthevisibleunitsmaybesampledsimultaneously\n",
      "           becausetheyareconditionallyindependentfromeachothergivenallthehidden\n",
      "         units.Gibbssamplingapproachesthatupdatemanyvariablessimultaneouslyin\n",
      "      thiswayarecalledblockGibbssampling.\n",
      "        AlternateapproachestodesigningMarkovchainstosamplefromp m o d e lare\n",
      "          possible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinother\n",
      "           disciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,\n",
      "            itisraretouseanyapproachotherthanGibbssampling.Improvedsampling\n",
      "     techniquesareonepossibleresearchfrontier.\n",
      "5 9 6    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "      17.5TheChallengeofMixingbetweenSeparated\n",
      "Modes\n",
      "            TheprimarydiﬃcultyinvolvedwithMCMCmethodsisthattheyhaveatendency\n",
      "to m i x           poorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample\n",
      "fromp( x           )wouldbecompletelyindependentfromeachotherandwouldvisitmany\n",
      "  diﬀerentregionsin x      spaceproportionaltotheirprobability.Instead,especially\n",
      "         inhigh-dimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer\n",
      "             tosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswith\n",
      "          slowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisy\n",
      "            gradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingonthe\n",
      "            probability,withrespecttothestateofthechain(therandomvariablesbeing\n",
      "              sampled). Thechaintendstotakesmallsteps(inthespaceofthestateofthe\n",
      "    Markovchain),fromaconﬁguration x( 1 ) t −  toaconﬁguration x( ) t   ,withtheenergy\n",
      "E( x( ) t        )generallylowerorapproximatelyequaltotheenergyE( x( 1 ) t −  ),witha\n",
      "           preferenceformovesthatyieldlowerenergyconﬁgurations.Whenstartingfroma\n",
      "         ratherimprobableconﬁguration(higherenergythanthetypicalonesfromp( x)),\n",
      "             thechaintendstograduallyreducetheenergyofthestateandonlyoccasionally\n",
      "              movetoanothermode.Oncethechainhasfoundaregionoflowenergy(for\n",
      "                example,ifthevariablesarepixelsinanimage,aregionoflowenergymightbea\n",
      "              connectedmanifoldofimagesofthesameobject),whichwecallamode,thechain\n",
      "               willtendtowalkaroundthatmode(followingakindofrandomwalk).Onceina\n",
      "                  whileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitﬁndsanescape\n",
      "           route)movetowardanothermode.Theproblemisthatsuccessfulescaperoutes\n",
      "            arerareformanyinterestingdistributions,sotheMarkovchainwillcontinueto\n",
      "       samplethesamemodelongerthanitshould.\n",
      "            ThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4\n",
      "              Inthiscontext,considertheprobabilityofgoingfromonemodetoanearbymode\n",
      "             withinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshape\n",
      "         ofthe“energybarrier” betweenthesemodes.Transitionsbetweentwomodes\n",
      "             thatareseparatedbyahighenergybarrier(aregionoflowprobability)are\n",
      "             exponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisis\n",
      "            illustratedinﬁgure.Theproblemariseswhentherearemultiplemodeswith 17.1\n",
      "           highprobabilitythatareseparatedbyregionsoflowprobability,especiallywhen\n",
      "            eachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhose\n",
      "       valuesarelargelydeterminedbytheothervariables.\n",
      "          Asasimpleexample,consideranenergy-basedmodelovertwovariablesaand\n",
      "b          ,whicharebothbinarywithasign,takingonvalues −  1 1and.IfE( ab,) = −wab\n",
      "5 9 7    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "    forsomelargepositivenumberw        ,thenthemodelexpressesastrongbeliefthata\n",
      "andb     havethesamesign.Considerupdatingb     usingaGibbssamplingstepwith\n",
      "a    = 1. Theconditionaldistributionoverb  isgivenbyP(b= 1 |a  = 1)=σ(w).\n",
      "Ifw          islarge,thesigmoidsaturates,andtheprobabilityofalsoassigningb tobe\n",
      "      1iscloseto1.Likewise,ifa=−    1,theprobabilityofassigningb tobe− 1is\n",
      "    closeto1.AccordingtoP m o d e l( ab,        ),bothsignsofbothvariablesareequallylikely.\n",
      " AccordingtoP m o d e l(  ab|         ),bothvariablesshouldhavethesamesign.Thismeans\n",
      "            thatGibbssamplingwillonlyveryrarelyﬂipthesignsofthesevariables.\n",
      "            Inmorepracticalscenarios,thechallengeisevengreaterbecausewecareabout\n",
      "           makingtransitionsnotonlybetweentwomodesbutmoregenerallybetweenall\n",
      "             themanymodesthatarealmodelmightcontain.Ifseveralsuchtransitionsare\n",
      "            diﬃcultbecauseofthediﬃcultyofmixingbetweenmodes,thenitbecomesvery\n",
      "             expensivetoobtainareliablesetofsamplescoveringmostofthemodes,and\n",
      "          convergenceofthechaintoitsstationarydistributionisveryslow.\n",
      "           Sometimesthisproblemcanberesolvedbyﬁndinggroupsofhighlydependent\n",
      "          unitsandupdatingallofthemsimultaneouslyinablock. Unfortunately,when\n",
      "           thedependenciesarecomplicated,itcanbecomputationallyintractabletodrawa\n",
      "             samplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginally\n",
      "             introducedtosolveisthisproblemofsamplingfromalargegroupofvariables.\n",
      "            Inthecontextofmodelswithlatentvariables,whichdeﬁneajointdistribution\n",
      "p m o d e l( xh,     ),weoftendrawsamplesofx    byalternatingbetweensamplingfrom\n",
      "p m o d e l(  xh|   )andsamplingfromp m o d e l(  hx|       ).Fromthepointofviewofmixing\n",
      "   rapidly,wewouldlikep m o d e l(  hx|        )tohavehighentropy.Fromthepointof\n",
      "      viewoflearningausefulrepresentationofh   ,wewouldlikeh  toencodeenough\n",
      " informationaboutx      toreconstructitwell,whichimpliesthathandx  shouldhave\n",
      "             highmutualinformation.Thesetwogoalsareatoddswitheachother.Weoften\n",
      "      learngenerativemodelsthatverypreciselyencodexintoh     butarenotabletomix\n",
      "         verywell.ThissituationarisesfrequentlywithBoltzmannmachines—thesharper\n",
      "             thedistributionaBoltzmannmachinelearns,theharderitisforaMarkovchain\n",
      "            samplingfromthemodeldistributiontomixwell.Thisproblemisillustratedin\n",
      " ﬁgure.17.2\n",
      "            AllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinterest\n",
      "             hasamanifoldstructurewithaseparatemanifoldforeachclass:thedistributionis\n",
      "           concentratedaroundmanymodes,andthesemodesareseparatedbyvastregions\n",
      "             ofhighenergy.Thistypeofdistributioniswhatweexpectinmanyclassiﬁcation\n",
      "           problems,anditwouldmakeMCMCmethodsconvergeveryslowlybecauseof\n",
      "   poormixingbetweenmodes.\n",
      "5 9 8    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "            Figure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkov\n",
      "           chaininitializedatthemodeinbothcases. ( L e f t )Amultivariatenormaldistribution\n",
      "           withtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesare\n",
      "        independent. Amultivariatenormaldistributionwithhighlycorrelatedvariables. ( C e n t e r )\n",
      "             ThecorrelationbetweenvariablesmakesitdiﬃcultfortheMarkovchaintomix.Because\n",
      "             theupdateforeachvariablemustbeconditionedontheothervariable,thecorrelation\n",
      "              reducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint.\n",
      "            ( R i g h t )AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxisaligned.\n",
      "             Gibbssamplingmixesveryslowlybecauseitisdiﬃculttochangemodeswhilealtering\n",
      "     onlyonevariableatatime.\n",
      "5 9 9    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "     17.5.1TemperingtoMixbetweenModes\n",
      "            Whenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsof\n",
      "             lowprobability,itisdiﬃculttomixbetweenthediﬀerentmodesofthedistribution.\n",
      "          Severaltechniquesforfastermixingarebasedonconstructingalternativeversions\n",
      "              ofthetargetdistributioninwhichthepeaksarenotashighandthesurrounding\n",
      "            valleysarenotaslow.Energy-based modelsprovideaparticularlysimplewayto\n",
      "             doso.Sofar,wehavedescribedanenergy-basedmodelasdeﬁningaprobability\n",
      "distribution\n",
      "    p E. () exp( x∝−()) x (17.25)\n",
      "        Energy-based modelsmaybeaugmentedwithanextraparameterβcontrolling\n",
      "     howsharplypeakedthedistributionis:\n",
      "p β    () exp( ()) x∝−βE x. (17.26)\n",
      "Theβ          parameterisoftendescribedasbeingthereciprocalofthe t e m p e r a t ur e,\n",
      "          reﬂectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthe\n",
      "    temperaturefallstozero,andβ      risestoinﬁnity,theenergy-basedmodelbecomes\n",
      "            Figure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels.\n",
      "             Eachpanelshouldbereadlefttoright,toptobottom. ( L e f t )Consecutivesamplesfrom\n",
      "            GibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset.\n",
      "            Consecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformed\n",
      "              inadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticthanrawvisual\n",
      "                features,butitisstilldiﬃcultfortheGibbschaintotransitionfromonemodeofthe\n",
      "          distributiontoanother,forexample,bychangingthedigitidentity. Consecutive ( R i g h t )\n",
      "         ancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsampling\n",
      "           generateseachsampleindependentlyfromtheothers,thereisnomixingproblem.\n",
      "6 0 0    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "       deterministic.Whenthetemperaturerisestoinﬁnity,andβ   fallstozero,the\n",
      "     distribution(fordiscrete)becomesuniform. x\n",
      "        Typically,amodelistrainedtobeevaluatedatβ    = 1.However,wecanmake\n",
      "      useofothertemperatures,particularlythosewhere β<1.Tempering  isageneral\n",
      "      strategyofmixingbetweenmodesofp 1        rapidlybydrawingsampleswith.β<1\n",
      "   Markovchainsbasedon  temperedtransitions  (,)temporarily Neal1994\n",
      "         samplefromhigher-temperature distributionstomixtodiﬀerentmodes,then\n",
      "         resumesamplingfromtheunittemperaturedistribution.Thesetechniqueshave\n",
      "           beenappliedtomodelssuchasRBMs(Salakhutdinov2010,).Anotherapproachis\n",
      " touse  paralleltempering        (,),inwhichtheMarkovchainsimulatesmany Iba2001\n",
      "         diﬀerentstatesinparallel,atdiﬀerenttemperatures.Thehighesttemperature\n",
      "           statesmixslowly,whilethelowesttemperaturestates,attemperature,provide 1\n",
      "         accuratesamplesfromthemodel.Thetransitionoperatorincludesstochastically\n",
      "           swappingstatesbetweentwodiﬀerenttemperaturelevels,sothatasuﬃcientlyhigh-\n",
      "           probabilitysamplefromahigh-temperatureslotcanjumpintoalowertemperature\n",
      "             slot.ThisapproachhasalsobeenappliedtoRBMs( ,; Desjardinsetal.2010Cho\n",
      "              etal.,).Althoughtemperingisapromisingapproach,atthispointithasnot 2010\n",
      "            allowedresearcherstomakeastrongadvanceinsolvingthechallengeofsampling\n",
      "         fromcomplexEBMs.Onepossiblereasonisthatthereare criticaltemperatures\n",
      "            aroundwhichthetemperaturetransitionmustbeveryslow(asthetemperatureis\n",
      "      graduallyreduced)fortemperingtobeeﬀective.\n",
      "    17.5.2DepthMayHelpMixing\n",
      "       Whendrawingsamplesfromalatentvariablemodelp( hx,     ),wehaveseenthatif\n",
      "p(  hx| )encodesx    toowell,thensamplingfromp(  xh|   )willnotchangexvery\n",
      "              much,andmixingwillbepoor.Onewaytoresolvethisproblemistomakeha\n",
      "  deeprepresentation,encodingxintoh         insuchawaythataMarkovchaininthe\n",
      " spaceofh         canmixmoreeasily.Manyrepresentationlearningalgorithms,suchas\n",
      "         autoencodersandRBMs,tendtoyieldamarginaldistributionoverh  thatismore\n",
      "         uniformandmoreunimodalthantheoriginaldatadistributionoverx   .Itcanbe\n",
      "            arguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusingall\n",
      "        theavailablerepresentationspace,becauseminimizingreconstructionerrorover\n",
      "           thetrainingexampleswillbebetterachievedwhendiﬀerenttrainingexamplesare\n",
      "     easilydistinguishablefromeachotherinh     -space,andthuswellseparated.Bengio\n",
      "           etal.()observedthatdeeperstacksofregularizedautoencodersorRBMs 2013a\n",
      "     yieldmarginaldistributionsinthetop-levelh     -spacethatappearedmorespreadout\n",
      "             andmoreuniform,withlessofagapbetweentheregionscorrespondingtodiﬀerent\n",
      "          modes(categories,intheexperiments).TraininganRBMinthathigher-level\n",
      "6 0 1    C HAP T E R 1 7 . M O NT E C ARL O M E T HO D S\n",
      "           spaceallowedGibbssamplingtomixfasterbetweenmodes.Itremainsunclear,\n",
      "             however,howtoexploitthisobservationtohelpbettertrainandsamplefromdeep\n",
      " generativemodels.\n",
      "           Despitethediﬃcultyofmixing,MonteCarlotechniquesareusefulandare\n",
      "             oftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfront\n",
      "        theintractablepartitionfunctionofundirectedmodels,discussednext.\n",
      "6 0 2 C h a p t e r 1 8\n",
      "  C on f r on t i ng t he P art i t i on\n",
      "F unction\n",
      "            Insection wesawthatmanyprobabilisticmodels(commonlyknownasundi- 16.2.2\n",
      "         rectedgraphicalmodels)aredeﬁnedbyanunnormalizedprobabilitydistribution\n",
      "˜p( x;θ   ).Wemustnormalize˜p     bydividingbyapartitionfunctionZ(θ   )toobtaina\n",
      "  validprobabilitydistribution:\n",
      " p(;) = xθ1\n",
      "Z()θ  ˜p. (;) xθ (18.1)\n",
      "            Thepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscrete\n",
      "       variables)overtheunnormalizedprobabilityofallstates:\n",
      "\n",
      " ˜pd()xx (18.2)\n",
      "or \n",
      "x ˜p.()x (18.3)\n",
      "       Thisoperationisintractableformanyinterestingmodels.\n",
      "              Aswewillseeinchapter,severaldeeplearningmodelsaredesignedtohave 20\n",
      "              atractablenormalizingconstant,oraredesignedtobeusedinwaysthatdonot\n",
      " involvecomputingp( x          )atall.Yet,othermodelsdirectlyconfrontthechallengeof\n",
      "         intractablepartitionfunctions. Inthischapter,wedescribetechniquesusedfor\n",
      "        trainingandevaluatingmodelsthathaveintractablepartitionfunctions.\n",
      "603     C HAP T E R 1 8 . C O NFR O NT I NG T HE P AR T I T I O N F UNC T I O N\n",
      "   18.1TheLog-LikelihoodGradient\n",
      "   What makes learning undirectedmodels bymaximumlikelihood particularly\n",
      "            diﬃcultisthatthepartitionfunctiondependsontheparameters.Thegradientof\n",
      "            thelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothe\n",
      "    gradientofthepartitionfunction:\n",
      "∇ θ  log(;) = p x θ∇ θ    log ˜p(;) x θ−∇ θ  log()Z θ. (18.4)\n",
      "      Thisisawell-knowndecompositionintothe  positivephaseandnegative\n",
      "  phaseoflearning.\n",
      "           Formostundirectedmodelsofinterest,thenegativephaseisdiﬃcult.Models\n",
      "           withnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypically\n",
      "            haveatractablepositivephase.Thequintessentialexampleofamodelwitha\n",
      "            straightforwardpositivephaseandadiﬃcultnegativephaseistheRBM,whichhas\n",
      "           hiddenunitsthatareconditionallyindependentfromeachothergiventhevisible\n",
      "           units.Thecasewherethepositivephaseisdiﬃcult,withcomplicatedinteractions\n",
      "           betweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses 19\n",
      "      onthediﬃcultiesofthenegativephase.\n",
      "          Letuslookmorecloselyatthegradientof: logZ\n",
      "∇ θ  logZ (18.5)\n",
      "=∇ θZ\n",
      "Z(18.6)\n",
      "=∇ θ\n",
      "x˜p() x\n",
      "Z(18.7)\n",
      "=\n",
      "x∇ θ˜p() x\n",
      "Z . (18.8)\n",
      "   Formodelsthatguaranteep( x)>  0forall x   ,wecansubstitute  exp(log ˜p()) x\n",
      " for˜p() x:\n",
      "x∇ θ  exp(log ˜p()) x\n",
      "Z(18.9)\n",
      "=\n",
      "x   exp(log ˜p()) x∇ θlog ˜p() x\n",
      "Z(18.10)\n",
      "=\n",
      "x˜p() x∇ θlog ˜p() x\n",
      "Z(18.11)\n",
      "=\n",
      "xp() x∇ θ  log ˜p() x (18.12)\n",
      "6 0 4     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "= E x x ∼ p ( )∇ θ log ˜p.() x (18.13)\n",
      "       Thisderivationmadeuseofsummationoverdiscretex    ,butasimilarresult\n",
      "    appliesusingintegrationovercontinuousx      .Inthecontinuousversionofthe\n",
      "            derivation,weuseLeibniz’srulefordiﬀerentiationundertheintegralsigntoobtain\n",
      " theidentity\n",
      "∇ θ\n",
      " ˜pd() xx=\n",
      "∇ θ ˜pd. () xx (18.14)\n",
      "         Thisidentityisapplicableonlyundercertainregularityconditionson˜pand∇ θ˜p( x).\n",
      "          Inmeasuretheoreticterms,theconditionsare:(1)Theunnormalizeddistribution\n",
      "˜p     mustbeaLebesgue-integrablefunctionofx   foreveryvalueofθ   .(2)Thegradient\n",
      "∇ θ˜p( x    )mustexistforallθ  andalmostallx      .(3)Theremustexistanintegrable\n",
      "functionR(x  )thatbounds∇ θ˜p( x    )inthesensethatmax i|∂\n",
      "∂ θ i˜p( x)  |≤R(x  )forall\n",
      "θ  andalmostallx        .Fortunately,mostmachinelearningmodelsofinteresthave\n",
      " theseproperties.\n",
      " Thisidentity\n",
      "∇ θ  log = Z E x x ∼ p ( )∇ θ  log ˜p() x (18.15)\n",
      "            isthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizing\n",
      "       thelikelihoodofmodelswithintractablepartitionfunctions.\n",
      "          TheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitive\n",
      "             frameworkinwhichwecanthinkofboththepositivephaseandthenegative\n",
      "      phase.Inthepositivephase,weincreaselog ˜p( x )forx    drawnfromthedata.In\n",
      "         thenegativephase,wedecreasethepartitionfunctionbydecreasinglog ˜p( x) drawn\n",
      "   fromthemodeldistribution.\n",
      "         Inthedeeplearningliterature,itiscommontoparametrizelog ˜p  intermsof\n",
      "            anenergyfunction(equation).Inthiscase,wecaninterpretthepositive 16.7\n",
      "             phaseaspushingdownontheenergyoftrainingexamplesandthenegativephase\n",
      "              aspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedin\n",
      " ﬁgure.18.1\n",
      "     18.2StochasticMaximumLikelihoodandContrastive\n",
      "Divergence\n",
      "             Thenaivewayofimplementingequation istocomputeitbyburningin 18.15\n",
      "             asetofMarkovchainsfromarandominitializationeverytimethegradientis\n",
      "          needed.Whenlearningisperformedusingstochasticgradientdescent,thismeans\n",
      "              thechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe\n",
      "6 0 5     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      " Algorithm18.1       AnaiveMCMCalgorithmformaximizingthelog-likelihood\n",
      "       withanintractablepartitionfunctionusinggradientascent\n",
      "         Set,thestepsize,toasmallpositivenumber. \n",
      "Setk              ,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100to\n",
      "       trainanRBMonasmallimagepatch.\n",
      "   whilenotconvergeddo\n",
      "      Sampleaminibatchofexamplesm{x(1 )     ,...,x( ) m    }fromthetrainingset\n",
      " g←1\n",
      "mm\n",
      "i =1∇ θlog ˜p(x( ) i ;)θ.\n",
      "   Initializeasetofmsamples{˜x(1 )    ,...,˜x( ) m}    torandomvalues(e.g.,from\n",
      "          auniformornormaldistribution,orpossiblyadistributionwithmarginals\n",
      "    matchedtothemodel’smarginals).\n",
      "     for doik = 1to\n",
      "     for do jm = 1to\n",
      "˜x( ) j ←gibbs_update(˜x( ) j).\n",
      " endfor\n",
      " endfor\n",
      "   gg←−1\n",
      "mm\n",
      "i =1∇ θlog ˜p(˜x( ) i ;)θ.\n",
      "    θθ← +.g\n",
      " endwhile\n",
      "            trainingprocedurepresentedinalgorithm.Thehighcostofburninginthe 18.1\n",
      "          Markovchainsintheinnerloopmakesthisprocedurecomputationallyinfeasible,\n",
      "            butthisprocedureisthestartingpointthatothermorepracticalalgorithmsaim\n",
      " toapproximate.\n",
      "            WecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachieve\n",
      "            balancebetweentwoforces,onepushinguponthemodeldistributionwherethe\n",
      "            dataoccurs,andanotherpushingdownonthemodeldistributionwherethemodel\n",
      "           samplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto 18.1\n",
      "maximizinglog ˜p andminimizing logZ     .Severalapproximations tothenegative\n",
      "           phasearepossible.Eachoftheseapproximationscanbeunderstoodasmaking\n",
      "            thenegativephasecomputationallycheaperbutalsomakingitpushdowninthe\n",
      " wronglocations.\n",
      "          Becausethenegativephaseinvolvesdrawingsamplesfromthemodel’sdistri-\n",
      "              bution,wecanthinkofitasﬁndingpointsthatthemodelbelievesinstrongly.\n",
      "            Becausethenegativephaseactstoreducetheprobabilityofthosepoints,they\n",
      "           aregenerallyconsideredtorepresentthemodel’sincorrectbeliefsabouttheworld.\n",
      "           Theyarefrequentlyreferredtointheliteratureas“hallucinations”or“fantasy\n",
      "            particles.”Infact,thenegativephasehasbeenproposedasapossibleexplanation\n",
      "6 0 6     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "xp (x )  Thepositivephase\n",
      "p mo d e l ( ) x\n",
      "p d a t a ( ) x\n",
      "xp (x )  Thenegativephase\n",
      "p mo d e l ( ) x\n",
      "p d a t a ( ) x\n",
      "              Figure18.1:Theviewofalgorithmashavinga“positivephase”anda“negative 18.1\n",
      "            phase.” ( L e f t )Inthepositivephase,wesamplepointsfromthedatadistributionand\n",
      "             pushupontheirunnormalizedprobability.Thismeanspointsthatarelikelyinthe\n",
      "             datagetpusheduponmore. ( R i g h t )Inthenegativephase,wesamplepointsfromthe\n",
      "          modeldistributionandpushdownontheirunnormalizedprobability.Thiscounteracts\n",
      "             thepositivephase’stendencytojustaddalargeconstanttotheunnormalizedprobability\n",
      "            everywhere.Whenthedatadistributionandthemodeldistributionareequal,thepositive\n",
      "                  phasehasthesamechancetopushupatapointasthenegativephasehastopushdown.\n",
      "             Whenthisoccurs,thereisnolongeranygradient(inexpectation),andtrainingmust\n",
      "terminate.\n",
      "            fordreaminginhumansandotheranimals(CrickandMitchison1983,),theidea\n",
      "             beingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollowsthe\n",
      " gradientoflog ˜ p         whenexperiencingrealeventswhileawakeandfollowsthenegative\n",
      " gradientoflog ˜ p tominimize log Z     whilesleepingandexperiencingeventssampled\n",
      "             fromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedtodescribe\n",
      "               algorithmswithapositiveandanegativephase,butithasnotbeenprovedtobe\n",
      "          correctwithneuroscientiﬁcexperiments.Inmachinelearningmodels,itisusually\n",
      "           necessarytousethepositiveandnegativephasesimultaneously,ratherthanin\n",
      "             separateperiodsofwakefulnessandREMsleep.Aswewillseeinsection,19.5\n",
      "          othermachinelearningalgorithmsdrawsamplesfromthemodeldistributionfor\n",
      "            otherpurposes,andsuchalgorithmscouldalsoprovideanaccountforthefunction\n",
      "  ofdreamsleep.\n",
      "             Giventhisunderstandingoftheroleofthepositiveandthenegativephaseof\n",
      "            learning,wecanattempttodesignalessexpensivealternativetoalgorithm.18.1\n",
      "               ThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkov\n",
      "             chainsfromarandominitializationateachstep.Anaturalsolutionistoinitialize\n",
      "6 0 7     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      " Algorithm18.2       Thecontrastivedivergencealgorithm,usinggradientascentas\n",
      "  theoptimizationprocedure\n",
      "         Set,thestepsize,toasmallpositivenumber. \n",
      "Setk             ,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\n",
      "fromp(x;θ     )tomixwheninitializedfromp d a ta      .Perhaps1–20totrainanRBM\n",
      "    onasmallimagepatch.\n",
      "   whilenotconvergeddo\n",
      "      Sampleaminibatchofexamplesm{x(1 )     ,...,x( ) m    }fromthetrainingset\n",
      " g←1\n",
      "mm\n",
      "i =1∇ θlog ˜p(x( ) i ;)θ.\n",
      "     for do im = 1to\n",
      "˜x( ) i ←x( ) i.\n",
      " endfor\n",
      "     for doik = 1to\n",
      "     for do jm = 1to\n",
      "˜x( ) j ←gibbs_update(˜x( ) j).\n",
      " endfor\n",
      " endfor\n",
      "   gg←−1\n",
      "mm\n",
      "i =1∇ θlog ˜p(˜x( ) i ;)θ.\n",
      "    θθ← +.g\n",
      " endwhile\n",
      "             theMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,\n",
      "           sothattheburninoperationdoesnottakeasmanysteps.\n",
      "The  contrastivedivergence  (CD,orCD-k   toindicateCDwithk Gibbssteps)\n",
      "            algorithminitializestheMarkovchainateachstepwithsamplesfromthedata\n",
      "          distribution(Hinton20002010,,).Thisapproachispresentedasalgorithm.18.2\n",
      "           Obtainingsamplesfromthedatadistributionisfree,becausetheyarealready\n",
      "             availableinthedataset.Initially,thedatadistributionisnotclosetothemodel\n",
      "           distribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositive\n",
      "            phasecanstillaccuratelyincreasethemodel’sprobabilityofthedata.Afterthe\n",
      "              positivephasehashadsometimetoact,themodeldistributionisclosertothe\n",
      "         datadistribution,andthenegativephasestartstobecomeaccurate.\n",
      "            Ofcourse,CDisstillanapproximationtothecorrectnegativephase.The\n",
      "            mainwayinwhichCDqualitativelyfailstoimplementthecorrectnegativephase\n",
      "              isthatitfailstosuppressregionsofhighprobabilitythatarefarfromactual\n",
      "          trainingexamples.Theseregionsthathavehighprobabilityunderthemodel\n",
      "        butlowprobabilityunderthedata-generatingdistributionarecalledspurious\n",
      "modes           .Figureillustrateswhythishappens.Essentially,modesinthemodel 18.2\n",
      "6 0 8     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "xp (x )p mo d e l ( ) x\n",
      "p d a t a ( ) x\n",
      "             Figure18.2:Aspuriousmode.Anillustrationofhowthenegativephaseofcontrastive\n",
      "            divergence(algorithm)canfailtosuppressspuriousmodes.Aspuriousmodeis 18.2\n",
      "              amodethatispresentinthemodeldistributionbutabsentinthedatadistribution.\n",
      "            BecausecontrastivedivergenceinitializesitsMarkovchainsfromdatapointsandrunsthe\n",
      "                  Markovchainforonlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefar\n",
      "              fromthedatapoints.Thismeansthatwhensamplingfromthemodel,wewillsometimes\n",
      "               getsamplesthatdonotresemblethedata. Italsomeansthatduetowastingsomeof\n",
      "             itsprobabilitymassonthesemodes,themodelwillstruggletoplacehighprobability\n",
      "              massonthecorrectmodes.Forthepurposeofvisualization,thisﬁgureusesasomewhat\n",
      "             simpliﬁedconceptofdistance—thespuriousmodeisfarfromthecorrectmodealongthe\n",
      "  numberlinein R            .ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswith\n",
      " asinglex  variablein R          .Formostdeepprobabilisticmodels,theMarkovchainsarebased\n",
      "             onGibbssamplingandcanmakenonlocalmovesofindividualvariablesbutcannotmove\n",
      "             allthevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsiderthe\n",
      "           editdistancebetweenmodes,ratherthantheEuclideandistance.However,editdistance\n",
      "           inahigh-dimensionalspaceisdiﬃculttodepictina2-Dplot.\n",
      "             distributionthatarefarfromthedatadistributionwillnotbevisitedbyMarkov\n",
      "         chainsinitializedattrainingpoints,unlessisverylarge. k\n",
      "      Carreira-PerpiñanandHinton2005()showed experimentallythat theCD\n",
      "            estimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatit\n",
      "          convergestodiﬀerentpointsthanthemaximumlikelihoodestimator.Theyargue\n",
      "               thatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitialize\n",
      "            amodelthatcouldlaterbeﬁne-tunedviamoreexpensiveMCMCmethods.Bengio\n",
      "            andDelalleau2009()showthatCDcanbeinterpretedasdiscardingthesmallest\n",
      "          termsofthecorrectMCMCupdategradient,whichexplainsthebias.\n",
      "             CDisusefulfortrainingshallowmodelslikeRBMs.Thesecaninturnbe\n",
      "             stackedtoinitializedeepermodelslikeDBNsorDBMs.ButCDdoesnotprovide\n",
      "6 0 9     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "             muchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdiﬃcultto\n",
      "             obtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethe\n",
      "            hiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannot\n",
      "               solvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstill\n",
      "             needtoburninaMarkovchainsamplingfromthedistributionoverthehidden\n",
      "     unitsconditionedonthosevisiblesamples.\n",
      "             TheCDalgorithmcanbethoughtofaspenalizingthemodelforhavinga\n",
      "             Markovchainthatchangestheinputrapidlywhentheinputcomesfromthedata.\n",
      "         ThismeanstrainingwithCDsomewhatresemblesautoencodertraining.Even\n",
      "              thoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbe\n",
      "            usefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecause\n",
      "             theearliestmodelsinthestackareencouragedtocopymoreinformationupto\n",
      "            theirlatentvariables,therebymakingitavailabletothelatermodels.Thisshould\n",
      "              bethoughtofmoreasanoften-exploitablesideeﬀectofCDtrainingratherthana\n",
      "  principleddesignadvantage.\n",
      "            SutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthe\n",
      "            gradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,\n",
      "        butinpracticethisisnotaseriousproblem.\n",
      "             AdiﬀerentstrategythatresolvesmanyoftheproblemswithCDistoinitial-\n",
      "             izetheMarkovchainsateachgradientstepwiththeirstatesfromtheprevious\n",
      "         gradientstep.Thisapproachwasﬁrstdiscoveredunderthename  stochasticmax-\n",
      " imumlikelihood        (SML)intheappliedmathematicsandstatisticscommunity\n",
      "        (Younes1998,)andlaterindependentlyrediscoveredunderthenamepersistent\n",
      " contrastivedivergence  (PCD,orPCD- k    toindicatetheuseof k Gibbssteps\n",
      "           perupdate)inthedeeplearningcommunity( ,).Seealgorithm. Tieleman2008 18.3\n",
      "                Thebasicideaofthisapproachisthat,aslongasthestepstakenbythestochastic\n",
      "             gradientalgorithmaresmall,themodelfromthepreviousstepwillbesimilarto\n",
      "            themodelfromthecurrentstep. Itfollowsthatthesamplesfromtheprevious\n",
      "            model’sdistributionwillbeveryclosetobeingfairsamplesfromthecurrent\n",
      "           model’sdistribution,soaMarkovchaininitializedwiththesesampleswillnot\n",
      "    requiremuchtimetomix.\n",
      "         BecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearning\n",
      "             process,ratherthanrestartedateachgradientstep,thechainsarefreetowander\n",
      "             farenoughtoﬁndallthemodel’smodes.SMListhusconsiderablymoreresistant\n",
      "             toformingmodelswithspuriousmodesthanCDis.Moreover,becauseitispossible\n",
      "             tostorethestateofallthesampledvariables,whethervisibleorlatent,SML\n",
      "             providesaninitializationpointforboththehiddenandthevisibleunits.CDis\n",
      "            onlyabletoprovideaninitializationforthevisibleunits,andthereforerequires\n",
      "6 1 0     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      " Algorithm18.3      Thestochasticmaximumlikelihood/persistentcontrastive\n",
      "        divergencealgorithmusinggradientascentastheoptimizationprocedure\n",
      "         Set,thestepsize,toasmallpositivenumber. \n",
      "Setk             ,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\n",
      "fromp(x;θ+g       )toburnin,startingfromsamplesfromp(x;θ   ).Perhaps1for\n",
      "               RBMonasmallimagepatch,or5–50foramorecomplicatedmodellikeaDBM.\n",
      "   Initializeasetofmsamples{˜x(1 )    ,...,˜x( ) m}     torandomvalues(e.g.,froma\n",
      "          uniformornormaldistribution,orpossiblyadistributionwithmarginalsmatched\n",
      "   tothemodel’smarginals).\n",
      "   whilenotconvergeddo\n",
      "      Sampleaminibatchofexamplesm{x(1 )     ,...,x( ) m    }fromthetrainingset\n",
      " g←1\n",
      "mm\n",
      "i =1∇ θlog ˜p(x( ) i ;)θ.\n",
      "     for doik = 1to\n",
      "     for do jm = 1to\n",
      "˜x( ) j ←gibbs_update(˜x( ) j).\n",
      " endfor\n",
      " endfor\n",
      "   gg←−1\n",
      "mm\n",
      "i =1∇ θlog ˜p(˜x( ) i ;)θ.\n",
      "    θθ← +.g\n",
      " endwhile\n",
      "            burn-infordeepmodels.SMLisabletotraindeepmodelseﬃciently.Marlin\n",
      "             etal.()comparedSMLtomanyothercriteriapresentedinthischapter.They 2010\n",
      "               foundthatSMLresultsinthebesttestsetlog-likelihoodforanRBM,andthatif\n",
      "              theRBM’shiddenunitsareusedasfeaturesforanSVMclassiﬁer,SMLresultsin\n",
      "   thebestclassiﬁcationaccuracy.\n",
      "          SMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithm\n",
      "             canmovethemodelfasterthantheMarkovchaincanmixbetweensteps.This\n",
      "  canhappenifk   istoosmallor        istoolarge.Thepermissiblerangeofvaluesis\n",
      "           unfortunatelyhighlyproblemdependent.Thereisnoknownwaytotestformally\n",
      "           whetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearning\n",
      "                rateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeableto\n",
      "           observemuchmorevarianceinthenegativephasesamplesacrossgradientsteps\n",
      "           thanacrossdiﬀerentMarkovchains.Forexample,amodeltrainedonMNIST\n",
      "             mightsampleexclusively7sononestep.Thelearningprocesswillthenpushdown\n",
      "            stronglyonthemodecorrespondingto7s,andthemodelmightsampleexclusively\n",
      "    9sonthenextstep.\n",
      "6 1 1     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "            Caremustbetakenwhenevaluatingthesamplesfromamodeltrainedwith\n",
      "             SML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain\n",
      "            initializedfromarandomstartingpointafterthemodelisdonetraining.The\n",
      "           samplespresentinthepersistentnegativechainsusedfortraininghavebeen\n",
      "             inﬂuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodel\n",
      "        appeartohavegreatercapacitythanitactuallydoes.\n",
      "          BerglundandRaiko2013()performedexperimentstoexaminethebiasand\n",
      "              varianceintheestimateofthegradientprovidedbyCDandSML.CDprovesto\n",
      "            havelowervariancethantheestimatorbasedonexactsampling.SMLhashigher\n",
      "              variance.ThecauseofCD’slowvarianceisitsuseofthesametrainingpoints\n",
      "             inboththepositiveandnegativephase.Ifthenegativephaseisinitializedfrom\n",
      "            diﬀerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedon\n",
      " exactsampling.\n",
      "             AllthesemethodsbasedonusingMCMCtodrawsamplesfromthemodelcan\n",
      "             inprinciplebeusedwithalmostanyvariantofMCMC.Thismeansthattechniques\n",
      "             suchasSMLcanbeimprovedbyusinganyoftheenhancedMCMCtechniques\n",
      "            describedinchapter,suchasparalleltempering( ,; 17 Desjardinsetal.2010Cho\n",
      "  etal.,).2010\n",
      "          Oneapproachtoacceleratingmixingduringlearningreliesnotonchanging\n",
      "          theMonteCarlosamplingtechnologybutratheronchangingtheparametrization\n",
      "      ofthemodelandthecostfunction. FastPCD     ,orFPCD( , TielemanandHinton\n",
      "    2009)involvesreplacingtheparametersθ      ofatraditionalmodelwithanexpression\n",
      " θθ= ( ) s l o w +θ( ) f a s t . (18.16)\n",
      "             Therearenowtwiceasmanyparametersasbefore,andtheyareaddedtogether\n",
      "           element-wisetoprovidetheparametersusedbytheoriginalmodeldeﬁnition.The\n",
      "             fastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowing\n",
      "              ittoadaptrapidlyinresponsetothenegativephaseoflearningandpushthe\n",
      "             Markovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,though\n",
      "             thiseﬀectoccursonlyduringlearningwhilethefastweightsarefreetochange.\n",
      "           Typicallyonealsoappliessigniﬁcantweightdecaytothefastweights,encouraging\n",
      "             themtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslong\n",
      "        enoughtoencouragetheMarkovchaintochangemodes.\n",
      "            OnekeybeneﬁttotheMCMC-basedmethodsdescribedinthissectionisthat\n",
      "       theyprovideanestimateofthegradientof  logZ     ,andthuswecanessentially\n",
      "    decomposetheproblemintothelog ˜p   contributionandthe logZcontribution.\n",
      "        Wecanthenuseanyothermethodtotacklelog ˜p( x     )andjustaddournegative\n",
      "           phasegradientontotheothermethod’sgradient.Inparticular,thismeansthat\n",
      "6 1 2     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "              ourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon\n",
      "˜p        .Mostoftheothermethodsofdealingwith  logZ     presentedinthischapterare\n",
      "     incompatiblewithbound-basedpositivephasemethods.\n",
      " 18.3Pseudolikelihood\n",
      "          MonteCarloapproximationstothepartitionfunctionanditsgradientdirectly\n",
      "          confrontthepartitionfunction.Otherapproachessidesteptheissue,bytraining\n",
      "           themodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesare\n",
      "              basedontheobservationthatitiseasytocomputeratiosofprobabilitiesinan\n",
      "          undirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsin\n",
      "           boththenumerator andthedenominatoroftheratioandcancelsout:\n",
      "p()x\n",
      "p()y=1\n",
      "Z˜p()x\n",
      "1\n",
      "Z˜p()y=˜p()x\n",
      "˜p()y . (18.17)\n",
      "         Thepseudolikelihoodisbasedontheobservationthatconditionalprobabilities\n",
      "            takethisratio-basedformandthuscanbecomputedwithoutknowledgeofthe\n",
      "     partitionfunction.Supposethatwepartitionxintoa,bandc ,whereacontains\n",
      "         thevariableswewanttoﬁndtheconditionaldistributionover,b  containsthe\n",
      "      variableswewanttoconditionon,andc       containsthevariablesthatarenotpart\n",
      "  ofourquery:\n",
      "  p( ) = ab| p,(ab)\n",
      "p()b= p,(ab)\n",
      "a c ,  p,,(abc)= ˜p,(ab)\n",
      "a c ,  ˜p,,(abc) . (18.18)\n",
      "    Thisquantityrequiresmarginalizingouta       ,whichcanbeaveryeﬃcientoperation\n",
      " providedthataandc        donotcontainmanyvariables.Intheextremecase,acan\n",
      "    beasinglevariableandc        canbeempty,makingthisoperationrequireonlyas\n",
      "            manyevaluationsof˜pastherearevaluesofasinglerandomvariable.\n",
      "          Unfortunately,inordertocomputethelog-likelihood,weneedtomarginalize\n",
      "       outlargesetsofvariables.Iftherearen       variablestotal,wemustmarginalizeaset\n",
      "          ofsize.Bythechainruleofprobability, n−1\n",
      "  log() = log( pxpx 1   )+log(px 2 |x 1    )+ +(···px n |x 1 : 1 n − ). (18.19)\n",
      "     Inthiscase,wehavemadea   maximallysmall,butc    canbeaslargeasx 2 : n .What\n",
      "   ifwesimplymovecintob       toreducethecomputationalcost?Thisyieldsthe\n",
      "pseudolik e l i ho o d         (,)objectivefunction,basedonpredictingthevalue Besag1975\n",
      "6 1 3     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "  offeaturex i     givenalltheotherfeatures x − i:\n",
      "n \n",
      "i =1 log(px i | x − i ). (18.20)\n",
      "    Ifeachrandomvariablehask     diﬀerentvalues,thisrequiresonlykn×evaluations\n",
      "of˜p     tocompute,asopposedtothekn     evaluationsneededtocomputethepartition\n",
      "function.\n",
      "             Thismaylooklikeanunprincipledhack,butitcanbeprovedthatestimation\n",
      "        bymaximizingthepseudolikelihoodisasymptoticallyconsistent(,). Mase1995\n",
      "              Ofcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,\n",
      "        pseudolikelihoodmaydisplaydiﬀerentbehaviorfromthemaximumlikelihood\n",
      "estimator.\n",
      "        Itispossible totradecomputationalcomplexityfordeviationfrom maxi -\n",
      "     mumlikelihoodbehaviorbyusingthe   generalizedpseudolik elihoodestima-\n",
      "tor         ( ,).Thegeneralizedpseudolikelihoodestimatoruses HuangandOgata2002\n",
      "m  diﬀerentsets S( ) i ,i =1     ,...,m      ofindicesofvariablesthatappeartogether\n",
      "         ontheleftside ofthe conditioningbar.Intheextremecase ofm   =1and\n",
      "S(1 )=     1,...,n        ,thegeneralizedpseudolikelihoodrecoversthelog-likelihood.Inthe\n",
      "  extremecaseofm=nand S( ) i={}i    ,thegeneralizedpseudolikelihoodrecovers\n",
      "        thepseudolikelihood.Thegeneralizedpseudolikelihoodobjectivefunctionisgiven\n",
      "by\n",
      "m \n",
      "i =1 log(p xS( ) i | x− S( ) i ). (18.21)\n",
      "        Theperformanceofpseudolikelihood-basedapproachesdependslargelyonhow\n",
      "            themodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthat\n",
      "       requireagoodmodelofthefulljointp( x      ),suchasdensityestimationandsampling.\n",
      "            Itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonlythe\n",
      "           conditionaldistributionsusedduringtraining,suchasﬁllinginsmallamountsof\n",
      "        missingvalues.Generalizedpseudolikelihoodtechniquesareespeciallypowerfulif\n",
      "       thedatahasregularstructurethatallowsthe S      indexsetstobedesignedtocapture\n",
      "           themostimportantcorrelationswhileleavingoutgroupsofvariablesthathave\n",
      "           onlynegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidely\n",
      "          separatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihood\n",
      "            canbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow. S\n",
      "           Oneweaknessofthepseudolikelihoodestimatoristhatitcannotbeused\n",
      "         withotherapproximations thatprovideonlyalowerboundon˜p( x  ),suchas\n",
      "         variationalinference,whichiscoveredinchapter. Thisisbecause 19 ˜pappears\n",
      "6 1 4     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "            inthedenominator.Alowerboundonthedenominatorprovidesonlyanupper\n",
      "              boundontheexpressionasawhole,andthereisnobeneﬁttomaximizingan\n",
      "           upperbound.Thismakesitdiﬃculttoapplypseudolikelihoodapproachestodeep\n",
      "           modelssuchasdeepBoltzmannmachines,sincevariationalmethodsareoneof\n",
      "          thedominantapproachestoapproximatelymarginalizingoutthemanylayersof\n",
      "         hiddenvariablesthatinteractwitheachother.Nonetheless,pseudolikelihoodis\n",
      "             stillusefulfordeeplearning,becauseitcanbeusedtotrainsingle-layermodels\n",
      "            ordeepmodelsusingapproximateinferencemethodsthatarenotbasedonlower\n",
      "bounds.\n",
      "            PseudolikelihoodhasamuchgreatercostpergradientstepthanSML,dueto\n",
      "         itsexplicitcomputationofalltheconditionals.Butgeneralizedpseudolikelihood\n",
      "            andsimilarcriteriacanstillperformwellifonlyonerandomlyselectedcondi-\n",
      "           tionaliscomputedperexample( ,),therebybringingthe Goodfellowetal.2013b\n",
      "       computationalcostdowntomatchthatofSML.\n",
      "       Thoughthepseudolikelihoodestimatordoesnotexplicitlyminimize logZ ,it\n",
      "            canstillbethoughtofashavingsomethingresemblinganegativephase.The\n",
      "         denominatorsofeachconditionaldistributionresultinthelearningalgorithm\n",
      "            suppressingtheprobabilityofallstatesthathaveonlyonevariablediﬀeringfrom\n",
      "  atrainingexample.\n",
      "            SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\n",
      "  eﬃciencyofpseudolikelihood.\n",
      "     18.4ScoreMatchingandRatioMatching\n",
      "          Scorematching( ,)providesanotherconsistentmeansoftraininga Hyvärinen2005\n",
      "  modelwithoutestimatingZ       oritsderivatives.Thenamescorematchingcomes\n",
      "             fromterminologyinwhichthederivativesofalogdensitywithrespecttoits\n",
      "argument,∇ x logp( x   ),arecalledits s c o r e      .Thestrategyusedbyscorematching\n",
      "           istominimizetheexpectedsquareddiﬀerencebetweenthederivativesofthe\n",
      "              model’slogdensitywithrespecttotheinputandthederivativesofthedata’slog\n",
      "     densitywithrespecttotheinput:\n",
      " L,( x θ) =1\n",
      "2||∇ x  logp m o d e l  (;) x θ,−∇ x logp d a ta() x||2\n",
      "2 ,(18.22)\n",
      "J() = θ1\n",
      "2E p d a t a ( ) x  L,, ( x θ) (18.23)\n",
      "θ∗= min\n",
      "θ J.() θ (18.24)\n",
      "        Thisobjectivefunctionavoidsthediﬃcultiesassociatedwithdiﬀerentiating\n",
      "6 1 5     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "  thepartitionfunctionZbecauseZ    isnotafunctionof x andtherefore ∇ xZ= 0.\n",
      "          Initially,scorematchingappearstohaveanewdiﬃculty: computingthescore\n",
      "          ofthedatadistributionrequiresknowledgeofthetruedistributiongenerating\n",
      "   thetrainingdata,p d a ta         .Fortunately,minimizingtheexpectedvalueof isL,( x θ)\n",
      "      equivalenttominimizingtheexpectedvalueof\n",
      "˜ L,( x θ) =n \n",
      "j =1\n",
      "∂2\n",
      "∂x2\n",
      "j logp m o d e l  (;)+ x θ1\n",
      "2∂\n",
      "∂x j logp m o d e l (;) x θ2\n",
      " ,(18.25)\n",
      "      whereisthedimensionalityof. n x\n",
      "        Becausescorematchingrequirestakingderivativeswithrespectto x   ,itisnot\n",
      "              applicabletomodelsofdiscretedatabutthelatentvariablesinthemodelmaybe\n",
      "discrete.\n",
      "           Likepseudolikelihood,scorematchingonlyworkswhenweareabletoevaluate\n",
      "log ˜p( x           ) anditsderivativesdirectly.Itisnotcompatiblewithmethodsthatprovide\n",
      "    onlyalowerboundonlog ˜p( x      ),becausescorematchingrequiresthederivatives\n",
      "   andsecondderivativesoflog ˜p( x        ),andalowerboundconveysnoinformationabout\n",
      "           itsderivatives.Thismeansthatscorematchingcannotbeappliedtoestimating\n",
      "          modelswithcomplicatedinteractionsbetweenthehiddenunits,suchassparse\n",
      "           codingmodelsordeepBoltzmannmachines.Whilescorematchingcanbeused\n",
      "               topretraintheﬁrsthiddenlayerofalargermodel,ithasnotbeenappliedas\n",
      "             apretrainingstrategyforthedeeperlayersofalargermodel.Thisisprobably\n",
      "           becausethehiddenlayersofsuchmodelsusuallycontainsomediscretevariables.\n",
      "            Whilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbe\n",
      "             viewedasaversionofcontrastivedivergenceusingaspeciﬁckindofMarkovchain\n",
      "            ( ,).TheMarkovchaininthiscaseisnotGibbssampling,but Hyvärinen2007a\n",
      "            ratheradiﬀerentapproachthatmakeslocalmovesguidedbythegradient.Score\n",
      "               matchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthe\n",
      "   localmovesapproacheszero.\n",
      "             Lyu2009()generalizedscorematchingtothediscretecase(butmadeanerrorin\n",
      "              thederivationthatwascorrectedby []). ()found Marlinetal.2010Marlinetal.2010\n",
      "that   generalizedscorematching      (GSM)doesnotworkinhigh-dimensional\n",
      "          discretespaceswheretheobservedprobabilityofmanyeventsis0.\n",
      "           Amoresuccessfulapproachtoextendingthebasicideasofscorematching\n",
      "   todiscretedatais ratiomatching     ( ,).Ratiomatchingapplies Hyvärinen2007b\n",
      "           speciﬁcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageover\n",
      "6 1 6     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "     examplesofthefollowingobjectivefunction:\n",
      "L( ) RM () =xθ,n \n",
      "j =1\n",
      "1\n",
      " 1+p m o d e l ( ; ) x θ\n",
      "p m o d e l ( ( ); ) f x , j θ\n",
      "2\n",
      " , (18.26)\n",
      "             where returnswiththebitatpositionﬂipped.Ratiomatchingavoids f,j(x) x j\n",
      "            thepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:ina\n",
      "            ratiooftwoprobabilities,thepartitionfunctioncancelsout. () Marlinetal.2010\n",
      "          foundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMinterms\n",
      "             oftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages.\n",
      "      Likethepseudolikelihoodestimator,ratiomatchingrequiresn  evaluationsof˜p\n",
      "         perdatapoint,makingitscomputationalcostperupdateroughlyn timeshigher\n",
      "   thanthatofSML.\n",
      "           Aswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofas\n",
      "             pushingdownonallfantasystatesthathaveonlyonevariablediﬀerentfroma\n",
      "          trainingexample.Sinceratiomatchingappliesspeciﬁcallytobinarydata,this\n",
      "              meansthatitactsonallfantasystateswithinHammingdistance1ofthedata.\n",
      "            Ratiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensional\n",
      "              sparsedata,suchaswordcountvectors.Thiskindofdataposesachallengefor\n",
      "          MCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentin\n",
      "             denseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodel\n",
      "            haslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio\n",
      "          ()overcamethisissuebydesigninganunbiasedstochasticapproximationto 2013\n",
      "          ratiomatching.Theapproximationevaluatesonlyarandomlyselectedsubsetof\n",
      "             thetermsoftheobjectiveanddoesnotrequirethemodeltogeneratecomplete\n",
      " fantasysamples.\n",
      "            SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\n",
      "   eﬃciencyofratiomatching.\n",
      "   18.5DenoisingScoreMatching\n",
      "             Insomecaseswemaywishtoregularizescorematching,byﬁttingadistribution\n",
      "p s m o o the d() =x\n",
      "p d a ta   ()( )yqxy|dy (18.27)\n",
      "   ratherthanthetruep d a ta  .Thedistributionq(  xy|    ) isacorruptionprocess,usually\n",
      "            onethatformsbyaddingasmallamountofnoiseto. x y\n",
      "6 1 7     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "           Denoisingscorematchingisespeciallyusefulbecauseinpractice,weusuallydo\n",
      "     nothaveaccesstothetruep d a ta      butratheronlyanempiricaldistributiondeﬁned\n",
      "           bysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,make\n",
      "p m o d e l           intoasetofDiracdistributionscenteredonthetrainingpoints.Smoothing\n",
      "byq            helpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty\n",
      "           describedinsection. ()introducedaprocedurefor 5.4.5KingmaandLeCun2010\n",
      "       performingregularizedscorematchingwiththesmoothingdistributionqbeing\n",
      "  normallydistributednoise.\n",
      "         Recallfromsection thatseveralautoencodertrainingalgorithmsare 14.5.1\n",
      "         equivalenttoscorematchingordenoisingscorematching.Theseautoencoder\n",
      "      trainingalgorithmsare thereforea wayof overcomingthepartition function\n",
      "problem.\n",
      "  18.6Noise-ContrastiveEstimation\n",
      "          Mosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonot\n",
      "            provideanestimateofthepartitionfunction.SMLandCDestimateonlythe\n",
      "           gradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself.\n",
      "         Scorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothe\n",
      "  partitionfunctionaltogether.\n",
      " Noise-contrastiveestimation      (NCE)(GutmannandHyvarinen2010,)takes\n",
      "         adiﬀerentstrategy. Inthisapproach,theprobabilitydistributionestimatedby\n",
      "     themodelisrepresentedexplicitlyas\n",
      " logp m o d e l() = log ˜ xp m o d e l    (;)+ x θc, (18.28)\n",
      "wherec      isexplicitlyintroducedasanapproximationof  −logZ( θ  ).Ratherthan\n",
      " estimatingonly θ      ,thenoisecontrastiveestimationproceduretreatsc asjust\n",
      "   anotherparameterandestimates θandc     simultaneously,usingthesamealgorithm\n",
      "   forboth.Theresulting  logp m o d e l( x        )thusmaynotcorrespondexactlytoavalid\n",
      "             probabilitydistribution,butitwillbecomecloserandclosertobeingvalidasthe\n",
      "   estimateofimproves.c1\n",
      "           Suchanapproachwouldnotbepossibleusingmaximumlikelihoodasthe\n",
      "           criterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetoset\n",
      "            c c arbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution.\n",
      "1              NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisno\n",
      "     needtointroducetheextraparameter c          .However,ithasgeneratedthemostinterestasameans\n",
      "      ofestimatingmodelswithdiﬃcultpartitionfunctions.\n",
      "618     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "         NCEworksbyreducingtheunsupervisedlearningproblemofestimatingp(x)\n",
      "             tothatoflearningaprobabilisticbinaryclassiﬁerinwhichoneofthecategories\n",
      "           correspondstothedatageneratedbythemodel.Thissupervisedlearningproblem\n",
      "         isconstructed insuchawaythatmaximumlikelihoodestimation deﬁnesan\n",
      "      asymptoticallyconsistentestimatoroftheoriginalproblem.\n",
      "      Speciﬁcally,weintroduceaseconddistribution,the  noisedistributionpnoise(x).\n",
      "           Thenoisedistributionshouldbetractabletoevaluateandtosamplefrom. We\n",
      "      cannowconstructamodeloverbothx     andanew,binaryclassvariabley  .Inthe\n",
      "     newjointmodel,wespecifythat\n",
      "pjoint (= 1) =y1\n",
      "2 , (18.29)\n",
      "pjoint   ( = 1) = x|yp m o d e l ()x, (18.30)\n",
      "and\n",
      "pjoint   ( = 0) = x|ypnoise ()x. (18.31)\n",
      "  Inotherwords,y         isaswitchvariablethatdetermineswhetherwewillgeneratex\n",
      "       fromthemodelorfromthenoisedistribution.\n",
      "             Wecanconstructasimilarjointmodeloftrainingdata.Inthiscase,the\n",
      "     switchvariabledetermineswhetherwedrawx fromthedata   orfromthenoise\n",
      " distribution.Formally,ptrain(y  =1)=1\n",
      "2,ptrain(  x|y  =1)=pdata(x), and\n",
      "ptrain   ( = 0) = x|ypnoise()x.\n",
      "          Wecannowjustusestandardmaximumlikelihoodlearningonthesupervised\n",
      "    learningproblemofﬁttingpjoint toptrain:\n",
      "   θ,c= argmax\n",
      "θ , cE x , py ∼ t r a i n logpjoint   ( )y|x. (18.32)\n",
      " Thedistributionpjoint        isessentiallyalogisticregressionmodelappliedtothe\n",
      "          diﬀerenceinlogprobabilitiesofthemodelandthenoisedistribution:\n",
      "pjoint  (= 1 ) =y|xp m o d e l()x\n",
      "p m o d e l  ()+xpnoise()x(18.33)\n",
      "=1\n",
      " 1+p n o i se ( ) x\n",
      "p m o d e l ( ) x(18.34)\n",
      "=1\n",
      "  1+exp\n",
      "logp n o i se ( ) x\n",
      "p m o d e l ( ) x (18.35)\n",
      "6 1 9     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "= σ\n",
      "−logp n o i s e() x\n",
      "p m o d e l() x\n",
      "(18.36)\n",
      "  = (logσp m o d e l   ()log x−p n o i s e   ()) x. (18.37)\n",
      "        NCEisthussimpletoapplyaslongaslog ˜p m o d e l   iseasytoback-propagate\n",
      "    through,and,asspeciﬁedabove,p n o i s e       iseasytoevaluate(inordertoevaluate\n",
      "p j o i n t        )andsamplefrom(togeneratethetrainingdata).\n",
      "           NCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,\n",
      "                butitcanworkwellevenifthoserandomvariablescantakeonahighnumberof\n",
      "           values.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditional\n",
      "            distributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,\n",
      "              2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyone\n",
      "word.\n",
      "            WhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomesless\n",
      "           eﬃcient.Thelogisticregressionclassiﬁercanrejectanoisesamplebyidentifying\n",
      "            anyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdown\n",
      " greatlyafterp m o d e l        haslearnedthebasicmarginalstatistics.Imaginelearninga\n",
      "         modelofimagesoffaces,usingunstructuredGaussiannoiseasp n o i s e .Ifp m o d e l\n",
      "           learnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithout\n",
      "         havinglearnedanythingaboutotherfacialfeatures,suchasmouths.\n",
      "  Theconstraintthatp n o i s e         mustbeeasytoevaluateandeasytosamplefrom\n",
      "    canbeoverlyrestrictive.Whenp n o i s e        issimple,mostsamplesarelikelytobetoo\n",
      "       obviouslydistinctfromthedatatoforcep m o d e l  toimprovenoticeably.\n",
      "            Likescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalower\n",
      " boundon˜p            isavailable.Suchalowerboundcouldbeusedtoconstructalower\n",
      " boundonp j o i n t(y= 1 | x            ),butitcanonlybeusedtoconstructanupperboundon\n",
      "p j o i n t(y= 0 | x           ),whichappearsinhalfthetermsoftheNCEobjective.Likewise,\n",
      "   alowerboundonp n o i s e          isnotuseful,becauseitprovidesonlyanupperboundon\n",
      "p j o i n t  (= 1 )y| x.\n",
      "            Whenthemodeldistributioniscopiedtodeﬁneanewnoisedistributionbefore\n",
      "       eachgradientstep,NCEdeﬁnesaprocedurecalled  self-contrastiveestimation,\n",
      "   whose expected gradi entis equivalentto the expected gradi ent ofmaximum\n",
      "           likelihood( ,).ThespecialcaseofNCEwherethenoisesamples Goodfellow2014\n",
      "   are thosegenerated by themodel suggests that maximum likelihood can be\n",
      "            interpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguish\n",
      "          realityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachieves\n",
      "           somereducedcomputationalcostbyonlyforcingthemodeltodistinguishreality\n",
      "      fromaﬁxedbaseline(thenoisemodel).\n",
      "6 2 0     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "          Usingthesupervisedtaskofclassifyingbetweentrainingsamplesandgenerated\n",
      "            samples(withthemodelenergyfunctionusedindeﬁningtheclassiﬁer)toprovide\n",
      "             agradientonthemodelwasintroducedearlierinvariousforms(Wellingetal.,\n",
      "  2003bBengio2009 ;,).\n",
      "    Noise contrastiveestimationis based onthe idea thata good generative\n",
      "       modelshould be abletodistinguish datafrom noise.A closelyrelatedidea\n",
      "          isthat agoodgenerativemodelshould beabletogenerate samplesthatno\n",
      "          classiﬁercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks\n",
      " (section ).20.10.4\n",
      "    18.7EstimatingthePartitionFunction\n",
      "            Whilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneeding\n",
      "     tocomputetheintractablepartitionfunctionZ( θ    )associatedwithanundirected\n",
      "           graphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimating\n",
      "  thepartitionfunction.\n",
      "           Estimatingthepartitionfunctioncanbeimportantbecausewerequireitif\n",
      "             wewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantin\n",
      "         evaluatingthemodel,monitoringtrainingperformance,andcomparingmodelsto\n",
      " eachother.\n",
      "       Forexample,imaginewehavetwomodels:modelM A   deﬁningaprobabil-\n",
      " itydistributionp A( x; θ A )=1\n",
      "Z A˜p A( x; θ A  )andmodelM B   deﬁningaprobability\n",
      "distributionp B( x; θ B )=1\n",
      "Z B˜p B( x; θ B       ).Acommonwaytocomparethemodels\n",
      "             istoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d.\n",
      "       testdataset.Supposethetestsetconsistsofmexamples{ x(1 )     ,..., x( ) m} .If\n",
      "ip A(x( ) i ; θ A) >\n",
      "ip B(x( ) i ; θ B   ),orequivalentlyif\n",
      "\n",
      "i logp A(x( ) i ; θ A )−\n",
      "i logp B(x( ) i ; θ B  ) 0>, (18.38)\n",
      "   thenwesaythatM A    isabettermodelthanM B       (or,atleast,itisabettermodel\n",
      "              ofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,\n",
      "          testingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction.\n",
      "          Indeed,equation seemstorequireevaluatingthelog-probabilitythatthe 18.38\n",
      "            modelassignstoeachpoint,whichinturnrequiresevaluatingthepartitionfunction.\n",
      "             Wecansimplifythesituationslightlybyrearrangingequation intoaformin 18.38\n",
      "6 2 1     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "             whichweneedtoknowonlytheofthetwomodel’spartitionfunctions: ratio\n",
      "\n",
      "i logp A(x( ) i ; θ A)−\n",
      "i logp B(x( ) i ; θ B) =\n",
      "i\n",
      "log˜p A(x( ) i ; θ A)\n",
      "˜p B(x( ) i ; θ B)\n",
      " −mlogZ( θ A)\n",
      "Z( θ B).\n",
      "(18.39)\n",
      "    WecanthusdeterminewhetherM A    isabettermodelthanM B  withoutknowing\n",
      "              thepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,\n",
      "            wecanestimatethisratiousingimportancesampling,providedthatthetwomodels\n",
      " aresimilar.\n",
      "             If,however,wewantedtocomputetheactualprobabilityofthetestdataunder\n",
      "eitherM AorM B           ,wewouldneedtocomputetheactualvalueofthepartition\n",
      "           functions.Thatsaid,ifweknewtheratiooftwopartitionfunctions,r=Z ( θ B )\n",
      "Z ( θ A ),\n",
      "            andweknewtheactualvalueofjustoneofthetwo,sayZ( θ A   ),wecouldcompute\n",
      "    thevalueoftheother:\n",
      "Z( θ B) = (rZ θ A) =Z( θ B)\n",
      "Z( θ A)Z( θ A ). (18.40)\n",
      "           Asimplewaytoestimatethepartitionfunctionistousea Monte Carlo\n",
      "           methodsuchassimpleimportancesampling.Wepresenttheapproachinterms\n",
      "            ofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscrete\n",
      "           variablesbyreplacingtheintegralswithsummation.Weuseaproposaldistribution\n",
      "p 0(x) =1\n",
      "Z 0˜p 0(x        ),whichsupportstractablesamplingandtractableevaluationof\n",
      "    boththepartitionfunctionZ 0    andtheunnormalizeddistribution˜p 0()x.\n",
      "Z 1=\n",
      "˜p 1  ()xdx (18.41)\n",
      "=p 0()x\n",
      "p 0()x˜p 1  ()xdx (18.42)\n",
      "= Z 0\n",
      "p 0()x˜p 1()x\n",
      "˜p 0()x d,x (18.43)\n",
      "ˆZ 1=Z 0\n",
      "KK \n",
      "k =1˜p 1(x( ) k)\n",
      "˜p 0(x( ) k)  st:..x( ) k ∼p 0 . (18.44)\n",
      "         Inthelastline,wemakeaMonteCarloestimator, ˆZ 1     ,oftheintegralusingsamples\n",
      " drawnfromp 0(x           ),andthenweighteachsamplewiththeratiooftheunnormalized\n",
      "˜p 1   andtheproposalp 0.\n",
      "     This approach alsoallowsus toestimatethe ratio between thepartition\n",
      "6 2 2     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      " functionsas\n",
      "1\n",
      "KK\n",
      "k =1˜p 1( x( ) k)\n",
      "˜p 0( x( ) k)  st:.. x( ) k ∼p 0 . (18.45)\n",
      "             Thisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedin\n",
      " equation.18.39\n",
      "  Ifthedistributionp 0  isclosetop 1        ,equation canbeaneﬀectivewayof 18.44\n",
      "          estimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime\n",
      "p 1         isbothcomplicated(usuallymultimodal)anddeﬁnedoverahigh-dimensional\n",
      "       space.Itisdiﬃculttoﬁndatractablep 0      thatissimpleenoughtoevaluatewhile\n",
      "    stillbeingcloseenoughtop 1      toresultinahigh-qualityapproximation.Ifp 0and\n",
      "p 1     arenotclose,mostsamplesfromp 0    willhavelowprobabilityunderp 1and\n",
      "          thereforemake(relatively)negligiblecontributionstothesuminequation.18.44\n",
      "          Havingfewsamples withsigniﬁcantweightsinthissumwillresult inan\n",
      "             estimatorthatisofpoorqualitybecauseofhighvariance.Thiscanbeunderstood\n",
      "         quantitativelythroughanestimateofthevarianceofourestimate ˆZ 1:\n",
      "ˆVar\n",
      "ˆZ 1\n",
      "=Z 0\n",
      "K2K \n",
      "k =1\n",
      "˜p 1( x( ) k)\n",
      "˜p 0( x( ) k)−ˆZ 12\n",
      " . (18.46)\n",
      "             Thisquantityislargestwhenthereissigniﬁcantdeviationinthevaluesofthe\n",
      " importanceweights˜ p 1 ( x( ) k)\n",
      "˜ p 0 ( x( ) k ).\n",
      "            Wenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-\n",
      "          ingtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-\n",
      "       dimensionalspaces: annealedimportancesamplingandbridgesampling.Both\n",
      "          startwiththesimpleimportancesamplingstrategyintroducedabove,andboth\n",
      "       attempttoovercometheproblemoftheproposalp 0   beingtoofarfromp 1by\n",
      "         introducingintermediatedistributionsthatattemptto between bridgethegapp 0\n",
      " andp 1.\n",
      "   18.7.1AnnealedImportanceSampling\n",
      "  InsituationswhereD K L(p 0p 1         )islarge(i.e.,wherethereislittleoverlapbetween\n",
      "p 0andp 1   ),astrategycalled   annealedimportancesampling  (AIS)attempts\n",
      "          tobridgethegapbyintroducingintermediatedistributions( ,;, Jarzynski1997Neal\n",
      "     2001).Considerasequenceofdistributionsp η 0     ,...,p η n  ,with0 =η 0 <η 1  <<···\n",
      "η n − 1 <η n            = 1sothattheﬁrstandlastdistributionsinthesequencearep 0and\n",
      "p 1 ,respectively.\n",
      "6 2 3     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "           Thisapproachenablesustoestimatethepartitionfunctionofamultimodal\n",
      "          distributiondeﬁnedoverahigh-dimensionalspace(suchasthedistributiondeﬁned\n",
      "              byatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction\n",
      "              (suchasanRBMwithzerosforweights)andestimatetheratiobetweenthetwo\n",
      "           model’spartitionfunctions. Theestimateofthisratioisbasedontheestimate\n",
      "              oftheratiosofasequenceofmanysimilardistributions,suchasthesequenceof\n",
      "         RBMswithweightsinterpolatingbetweenzeroandthelearnedweights.\n",
      "     WecannowwritetheratioZ 1\n",
      "Z 0as\n",
      "Z 1\n",
      "Z 0=Z 1\n",
      "Z 0Z η 1\n",
      "Z η 1···Z η n − 1\n",
      "Z η n − 1(18.47)\n",
      "=Z η 1\n",
      "Z 0Z η 2\n",
      "Z η 1···Z η n − 1\n",
      "Z η n − 2Z 1\n",
      "Z η n − 1(18.48)\n",
      "=n − 1\n",
      "j =0Z η j +1\n",
      "Z η j . (18.49)\n",
      "  Providedthedistributionsp η jandp η j +1   ,forall0    ≤≤−jn  1,aresuﬃciently\n",
      "        close,wecanreliablyestimateeachofthefactorsZ η j +1\n",
      "Z η j  usingsimpleimportance\n",
      "         samplingandthenusethesetoobtainanestimateofZ 1\n",
      "Z 0.\n",
      "          Wheredotheseintermediatedistributionscomefrom?Justastheoriginal\n",
      " proposaldistributionp 0         isadesignchoice,soisthesequenceofdistributions\n",
      "p η 1   ...p η n − 1            .Thatis,itcanbespeciﬁcallyconstructedtosuittheproblemdomain.\n",
      "           Onegeneralpurposeandpopularchoicefortheintermediatedistributionsisto\n",
      "        usetheweightedgeometri caverageofthetargetdistributionp 1  andthestarting\n",
      "         proposaldistribution(forwhichthepartitionfunctionisknown)p 0:\n",
      "p η j ∝pη j\n",
      "1p1 − η j\n",
      "0 . (18.50)\n",
      "            Inordertosamplefromtheseintermediatedistributions,wedeﬁneaseriesof\n",
      "   MarkovchaintransitionfunctionsT η j(x |x    ) thatdeﬁnetheconditionalprobability\n",
      "   distributionoftransitioningtox    givenwearecurrentlyatx  .Thetransition\n",
      " operatorT η j(x      |x)isdeﬁnedtoleavep η j ()xinvariant:\n",
      "p η j() =x\n",
      "p η j(x)T η j  (xx| )dx . (18.51)\n",
      "           ThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod\n",
      "       (e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepasses\n",
      "         throughalltherandomvariablesorotherkindsofiterations.\n",
      "6 2 4     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "  The AIS samplingstrategy is then togenerate samples fromp 0and use\n",
      "         thetransitionoperatorstosequentiallygeneratesamplesfromtheintermediate\n",
      "          distributionsuntilwearriveatsamplesfromthetargetdistributionp 1:\n",
      "       •fork...K = 1\n",
      "  –Sample x( ) k\n",
      "η 1 ∼p 0() x\n",
      "  –Sample x( ) k\n",
      "η 2 ∼T η 1( x( ) k\n",
      "η 2 | x( ) k\n",
      "η 1)\n",
      "   –...\n",
      "  –Sample x( ) k\n",
      "η n − 1 ∼T η n − 2( x( ) k\n",
      "η n − 1 | x( ) k\n",
      "η n − 2)\n",
      "  –Sample x( ) k\n",
      "η n ∼T η n − 1( x( ) k\n",
      "η n | x( ) k\n",
      "η n − 1)\n",
      " •end\n",
      " Forsamplek          ,wecanderivetheimportanceweightbychainingtogetherthe\n",
      "          importanceweightsforthejumpsbetweentheintermediatedistributionsgivenin\n",
      " equation:18.49\n",
      "w( ) k=˜p η 1( x( ) k\n",
      "η 1)\n",
      "˜p 0( x( ) k\n",
      "η 1)˜p η 2( x( ) k\n",
      "η 2)\n",
      "˜p η 1( x( ) k\n",
      "η 2)  ...˜p 1( x( ) k\n",
      "1)\n",
      "˜p η n − 1( x( ) k\n",
      "η n) . (18.52)\n",
      "            Toavoidnumericalissuessuchasoverﬂow,itisprobablybesttocompute logw( ) kby\n",
      "       addingandsubtractinglogprobabilities,ratherthancomputingw( ) k bymultiplying\n",
      "  anddividingprobabilities.\n",
      "          Withthesamplingprocedurethusdeﬁnedandtheimportanceweightsgiven\n",
      "             inequation,theestimateoftheratioofpartitionfunctionsisgivenby: 18.52\n",
      "Z 1\n",
      "Z 0≈1\n",
      "KK \n",
      "k =1w( ) k . (18.53)\n",
      "           Toverifythatthisproceduredeﬁnesavalidimportancesamplingscheme,we\n",
      "           canshow(,)thattheAISprocedurecorrespondstosimpleimportance Neal2001\n",
      "            samplingonanextendedstatespace,withpointssampledovertheproductspace\n",
      "[ x η 1     ,..., x η n − 1 , x 1           ].Todothis,wedeﬁnethedistributionovertheextendedspace\n",
      "as\n",
      "˜p( x η 1     ,..., x η n − 1 , x 1 ) (18.54)\n",
      " =˜p 1( x 1)˜T η n − 1( x η n − 1 | x 1)˜T η n − 2( x η n − 2 | x η n − 1   )...˜T η 1( x η 1 | x η 2 ),(18.55)\n",
      "6 2 5     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "where˜T a        isthereverseofthetransitionoperatordeﬁnedbyT a  (viaanapplication\n",
      "  ofBayes’rule):\n",
      "˜T a(x |x) =p a(x)\n",
      "p a()xT a  (xx|) =˜p a(x)\n",
      "˜p a()xT a  (xx| ). (18.56)\n",
      "            Pluggingtheaboveintotheexpressionforthejointdistributionontheextended\n",
      "       statespacegiveninequation,weget: 18.55\n",
      "˜p(x η 1     ,...,x η n − 1 ,x 1 ) (18.57)\n",
      " =˜p 1(x 1)˜p η n − 1(x η n − 1)\n",
      "˜p η n − 1(x 1)T η n − 1(x 1 |x η n − 1)n − 2 \n",
      "i =1˜p η i(x η i)\n",
      "˜p η i(x η i +1)T η i(x η i +1 |x η i)\n",
      "(18.58)\n",
      "=˜p 1(x 1)\n",
      "˜p η n − 1(x 1)T η n − 1(x 1 |x η n − 1 )˜p η 1(x η 1)n − 2\n",
      "i =1˜p η i +1(x η i +1)\n",
      "˜p η i(x η i +1)T η i(x η i +1 |x η i).\n",
      "(18.59)\n",
      "           Wenowhavemeansofgeneratingsamplesfromthejointproposaldistribution\n",
      "q            overtheextendedsampleviaasamplingschemegivenabove,withthejoint\n",
      "  distributiongivenby\n",
      "q(x η 1     ,...,x η n − 1 ,x 1) = p 0(x η 1)T η 1(x η 2 |x η 1    )...T η n − 1(x 1 |x η n − 1 ).(18.60)\n",
      "             Wehaveajointdistributionontheextendedspacegivenbyequation.Taking 18.59\n",
      "q(x η 1     ,...,x η n − 1 ,x 1          )astheproposaldistributionontheextendedstatespacefrom\n",
      "           whichwewilldrawsamples,itremainstodeterminetheimportanceweights:\n",
      "w( ) k=˜p(x η 1     ,...,x η n − 1 ,x 1)\n",
      "q(x η 1     ,...,x η n − 1 ,x 1)=˜p 1(x( ) k\n",
      "1)\n",
      "˜p η n − 1(x( ) k\n",
      "η n − 1)  ...˜p η 2(x( ) k\n",
      "η 2)\n",
      "˜p 1(x( ) k\n",
      "η 1)˜p η 1(x( ) k\n",
      "η 1)\n",
      "˜p 0(x( ) k\n",
      "0) .(18.61)\n",
      "              TheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISas\n",
      "           simpleimportancesamplingappliedtoanextendedstate,anditsvalidityfollows\n",
      "      immediatelyfromthevalidityofimportancesampling.\n",
      "         Annealedimportancesamplingwasﬁrstdiscoveredby ()and Jarzynski1997\n",
      "          thenagain, independentl y,by().Itiscurrentlythemostcommon Neal2001\n",
      "          wayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.The\n",
      "              reasonsforthismayhavemoretodowiththepublicationofaninﬂuentialpaper\n",
      "         (SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthe\n",
      "          partitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthan\n",
      "6 2 6     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "           withanyinherentadvantagethemethodhasovertheothermethoddescribed\n",
      "below.\n",
      "            AdiscussionofthepropertiesoftheAISestimator(e.g.,itsvarianceand\n",
      "      eﬃciency)canbefoundin(). Neal2001\n",
      "  18.7.2BridgeSampling\n",
      "           Bridgesampling( ,)isanothermethodthat,likeAIS,addressesthe Bennett1976\n",
      "          shortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof\n",
      "        intermediatedistributions,bridgesamplingreliesonasingledistributionp ∗ ,known\n",
      "           asthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,\n",
      "p 0   ,andadistributionp 1         forwhichwearetryingtoestimatethepartitionfunction\n",
      "Z 1.\n",
      "    BridgesamplingestimatestheratioZ 1/Z 0      astheratiooftheexpectedimpor-\n",
      "   tanceweightsbetween˜p 0 and˜p ∗  andbetween˜p 1 and˜p ∗:\n",
      "Z 1\n",
      "Z 0≈K \n",
      "k =1˜p ∗( x( ) k\n",
      "0)\n",
      "˜p 0( x( ) k\n",
      "0)K \n",
      "k =1˜p ∗( x( ) k\n",
      "1)\n",
      "˜p 1( x( ) k\n",
      "1) . (18.62)\n",
      "   Ifthebridgedistributionp ∗         ischosencarefullytohavealargeoverlapofsupport\n",
      " withbothp 0andp 1         ,thenbridgesamplingcanallowthedistancebetweentwo\n",
      "   distributions(ormoreformally,D K L(p 0 p 1       ))tobemuchlargerthanwithstandard\n",
      " importancesampling.\n",
      "           Itcanbeshownthattheoptimalbridgingdistributionisgivenbyp( ) o p t\n",
      "∗( x) ∝\n",
      "˜ p 0 ( ) ˜ x p 1 ( ) x\n",
      "r ˜ p 0 ( )+ ˜ x p 1 ( ) x ,wherer=Z 1/Z 0         .Atﬁrst,thisappearstobeanunworkablesolution\n",
      "             asitwouldseemtorequiretheveryquantitywearetryingtoestimate,Z 1/Z 0.\n",
      "          However,itispossibletostartwithacoarseestimateofr   andusetheresulting\n",
      "           bridgedistributiontoreﬁneourestimateiteratively(,).Thatis,we Neal2005\n",
      "             iterativelyreestimatetheratioanduseeachiterationtoupdatethevalueof.r\n",
      "  Linkedimportancesampling        BothAISandbridgesamplinghavetheirad-\n",
      " vantages.IfD K L(p 0 p 1     )isnottoolarge(becausep 0andp 1  aresuﬃcientlyclose),\n",
      "             bridgesamplingcanbeamoreeﬀectivemeansofestimatingtheratioofpartition\n",
      "              functionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingle\n",
      "distributionp ∗            tobridgethegap,thenonecanatleastuseAISwithpotentially\n",
      "       manyintermediatedistributionstospanthedistancebetweenp 0andp 1 .Neal\n",
      "           ()showedhowhislinkedimportancesamplingmethodleveragedthepowerof 2005\n",
      "           thebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIS\n",
      "       andsigniﬁcantlyimprovetheoverallpartitionfunctionestimates.\n",
      "6 2 7     C HAP T E R 1 8 . C O NFRO NTI NG T HE P AR T I T I O N F UNC T I O N\n",
      "     Estimatingthepartitionfunctionwhiletraining    WhileAIShasbecome\n",
      "           acceptedasthestandardmethodforestimatingthepartitionfunctionformany\n",
      "        undirectedmodels, itissuﬃcientlycomputationallyintensivethatitremains\n",
      "          infeasibletouseduringtraining.Alternativestrategieshavebeenexploredto\n",
      "        maintainanestimateofthepartitionfunctionthroughouttraining.\n",
      "          Usingacombinationofbridgesampling,short-chainAISandparalleltempering,\n",
      "             Desjardins 2011etal.()devisedaschemetotrackthepartitionfunctionofan\n",
      "            RBMthroughoutthetrainingprocess.Thestrategyisbasedonthemaintenanceof\n",
      "           independentestimatesofthepartitionfunctionsoftheRBMateverytemperature\n",
      "          operatingintheparalleltemperingscheme.Theauthorscombinedbridgesampling\n",
      "           estimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.,from\n",
      "             paralleltempering)withAISestimatesacrosstimetocomeupwithalowvariance\n",
      "         estimateofthepartitionfunctionsateveryiterationoflearning.\n",
      "           Thetoolsdescribedinthischapterprovidemanydiﬀerentwaysofovercoming\n",
      "           theproblemofintractablepartitionfunctions,buttherecanbeseveralother\n",
      "          diﬃcultiesinvolvedintrainingandusinggenerativemodels.Foremostamongthese\n",
      "         istheproblemofintractableinference,whichweconfrontnext.\n",
      "6 2 8 C h a p t e r 1 9\n",
      " Appro x i ma t e I nfe r e n ce\n",
      "            Manyprobabilisticmodelsarediﬃculttotrainbecauseitisdiﬃculttoperform\n",
      "               inferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisible\n",
      "variablesv     andasetoflatentvariablesh     .Thechallengeofinferenceusually\n",
      "      referstothediﬃcultproblemofcomputing p(  hv|    )ortakingexpectationswith\n",
      "            respecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihood\n",
      "learning.\n",
      "           Manysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestricted\n",
      "           BoltzmannmachinesandprobabilisticPCA,aredeﬁnedinawaythatmakes\n",
      "   inferenceoperationslikecomputing p(  hv|     ),ortakingexpectationswithrespect\n",
      "           toit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhidden\n",
      "        variableshaveintractableposteriordistributions.Exactinferencerequiresan\n",
      "             exponentialamountoftimeinthesemodels.Evensomemodelswithonlyasingle\n",
      "       layer,suchassparsecoding,havethisproblem.\n",
      "           Inthischapter,weintroduceseveralofthetechniquesforconfrontingthese\n",
      "        intractableinferenceproblems.Inchapter, wedescribe howto usethese 20\n",
      "          techniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,such\n",
      "       asdeepbeliefnetworksanddeepBoltzmannmachines.\n",
      "         Intractableinferenceproblemsindeeplearningusuallyarisefrominteractions\n",
      "            betweenlatentvariablesinastructuredgraphicalmodel.Seeﬁgureforsome 19.1\n",
      "          examples.Theseinteractionsmaybeduetodirectinteractionsinundirected\n",
      "          modelsor“explainingaway”interactionsbetweenmutualancestorsofthesame\n",
      "    visibleunitindirectedmodels.\n",
      "629   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "            Figure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultof\n",
      "          interactionsbetweenlatentvariablesinastructuredgraphicalmodel.Theseinteractions\n",
      "              canbeduetoedgesdirectlyconnectingonelatentvariabletoanotherorlongerpaths\n",
      "           thatareactivatedwhenthechildofaV-structureisobserved. ( L e f t )Asemi-restricted\n",
      " Boltzmannmachine        ( ,)withconnectionsbetweenhidden OsinderoandHinton2008\n",
      "          units.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistribution\n",
      "           intractablebecauseoflargecliquesoflatentvariables. AdeepBoltzmannmachine, ( C e n t e r )\n",
      "           organizedintolayersofvariableswithoutintralayerconnections,stillhasanintractable\n",
      "        posteriordistributionbecauseoftheconnectionsbetweenlayers. Thisdirected ( R i g h t )\n",
      "           modelhasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,\n",
      "           becauseeverytwolatentvariablesarecoparents.Someprobabilisticmodelsareable\n",
      "             toprovidetractableinferenceoverthelatentvariablesdespitehavingoneofthegraph\n",
      "          structuresdepictedabove.Thisispossibleiftheconditionalprobabilitydistributions\n",
      "           arechosentointroduceadditionalindependencesbeyondthosedescribedbythegraph.\n",
      "              Forexample,probabilisticPCAhasthegraphstructureshownintherightyetstillhas\n",
      "           simpleinferencebecauseofspecialpropertiesofthespeciﬁcconditionaldistributionsit\n",
      "       uses(linear-Gaussianconditionalswithmutuallyorthogonalbasisvectors).\n",
      "6 3 0   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "   19.1InferenceasOptimization\n",
      "           Manyapproachestoconfrontingtheproblemofdiﬃcultinferencemakeuseof\n",
      "           theobservationthatexactinferencecanbedescribedasanoptimizationproblem.\n",
      "         Approximateinferencealgorithmsmaythenbederivedbyapproximatingthe\n",
      "  underlyingoptimizationproblem.\n",
      "          Toconstructtheoptimizationproblem,assumewehaveaprobabilisticmodel\n",
      "   consistingofobservedvariablesv  andlatentvariablesh     .Wewouldliketocompute\n",
      "     thelog-probabilityoftheobserveddata,  logp(v;θ     ).Sometimesitistoodiﬃcult\n",
      " tocompute logp(v;θ       )ifitiscostlytomarginalizeouth    .Instead,wecancompute\n",
      "  alowerboundL(  vθ,,q )on  logp(v;θ     ).Thisboundiscalledthe  evidencelower\n",
      "bound            (ELBO).Anothercommonlyusednameforthislowerboundisthenegative\n",
      "  variationalfreeenergy         .Speciﬁcally,theevidencelowerboundisdeﬁnedtobe\n",
      "      L − ( ) = log(;) vθ,,qpvθD K L       (( )( ;)) qhv|phv|θ, (19.1)\n",
      "        whereisanarbitraryprobabilitydistributionover. q h\n",
      "   Becausethediﬀerencebetween  logp(v )andL(  vθ,,q     )isgivenbytheKL\n",
      "            divergence,andbecausetheKLdivergenceisalwaysnonnegative,wecanseethat\n",
      "L              alwayshasatmostthesamevalueasthedesiredlog-probability.Thetwoare\n",
      "             equalifandonlyifisthesamedistributionas . q p( )hv|\n",
      "Surprisingly,L        canbeconsiderablyeasiertocomputeforsomedistributionsq.\n",
      "      SimplealgebrashowsthatwecanrearrangeL     intoamuchmoreconvenientform:\n",
      "      L − ( ) =log(;) vθ,,qpvθD K L      (( )( ;)) qhv|phv|θ (19.2)\n",
      "     =log(;)pvθ− E h ∼ qlog  q( )hv|\n",
      "  p( )hv|(19.3)\n",
      "     =log(;)pvθ− E h ∼ qlog  q( )hv|\n",
      "p , ( h v θ ; )\n",
      "p ( ; ) v θ(19.4)\n",
      "     =log(;)pvθ− E h ∼ q              [log( )log( ;)+log(;)] qhv|−phv,θpvθ(19.5)\n",
      "  =− E h ∼ q           [log( )log( ;)] qhv|−phv,θ. (19.6)\n",
      "          Thisyieldsthemorecanonicaldeﬁnitionoftheevidencelowerbound,\n",
      "  L( ) = vθ,,q E h ∼ q      [log( )]+() phv,Hq. (19.7)\n",
      "    Foranappropriatechoiceofq,L      istractabletocompute.Foranychoice\n",
      "ofq,L        providesalowerboundonthelikelihood.Forq(  hv|   )thatarebetter\n",
      "6 3 1   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      " approximationsofp(  hv|   ),thelowerboundL     willbetighter,inotherwords,\n",
      " closerto  logp(v). Whenq(  hv| )=p(  hv|     ),theapproximationisperfect,and\n",
      "    L( ) = log(;) vθ,,qpvθ.\n",
      "           Wecanthusthinkofinferenceastheprocedureforﬁndingtheq thatmaximizes\n",
      "L   .ExactinferencemaximizesL        perfectlybysearchingoverafamilyoffunctionsq\n",
      " thatincludesp(  hv|         ).Throughoutthischapter,weshowhowtoderivediﬀerent\n",
      "         formsofapproximateinferencebyusingapproximateoptimizationtoﬁndq .We\n",
      "          canmaketheoptimizationprocedurelessexpensivebutapproximatebyrestricting\n",
      "   thefamilyofdistributionsq         thattheoptimizationisallowedtosearchoverorby\n",
      "         usinganimperfectoptimizationprocedurethatmaynotcompletelymaximizeL\n",
      "        butmaymerelyincreaseitbyasigniﬁcantamount.\n",
      "    Nomatterwhatchoiceofq weuse,L       isalowerbound.Wecangettighter\n",
      "            orlooserboundsthatarecheaperormoreexpensivetocomputedependingon\n",
      "           howwechoosetoapproachthisoptimizationproblem. Wecanobtainapoorly\n",
      "matchedq         butreducethecomputationalcostbyusinganimperfectoptimization\n",
      "            procedure,orbyusingaperfectoptimizationprocedureoverarestrictedfamilyof\n",
      " qdistributions.\n",
      "  19.2ExpectationMaximization\n",
      "          TheﬁrstalgorithmweintroducebasedonmaximizingalowerboundL isthe\n",
      " expectationmaximization      (EM)algorithm,apopulartrainingalgorithmfor\n",
      "            modelswithlatentvariables.WedescribehereaviewontheEMalgorithm\n",
      "            developedby ().Unlikemostoftheotheralgorithmswe NealandHinton1999\n",
      "            describeinthischapter,EMisnotanapproachtoapproximateinference,but\n",
      "        ratheranapproachtolearningwithanapproximateposterior.\n",
      "          TheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:\n",
      "•TheE-step   (expectationstep):Letθ(0 )     denotethevalueoftheparameters\n",
      "      atthebeginningofthestep.Setq(h( ) i |v )=p(h( ) i |v( ) i;θ(0 )  )forall\n",
      "indicesi   ofthetrainingexamplesv( ) i       wewanttotrainon(bothbatchand\n",
      "       minibatchvariantsarevalid).Bythiswemeanq     isdeﬁnedintermsofthe\n",
      "   currentparametervalueofθ(0 )   ;ifwevaryθ ,thenp(  hv|;θ  )willchange,\n",
      "           but willremainequalto q( )hv| p( ;hv|θ(0 )).\n",
      "        •The (maximizationstep):Completelyorpartiallymaximize M-step\n",
      "\n",
      "iL(v( ) i   ,,qθ) (19.8)\n",
      "6 3 2   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "         withrespecttousingyouroptimizationalgorithmofchoice. θ\n",
      "          ThiscanbeviewedasacoordinateascentalgorithmtomaximizeL  .Onone\n",
      "  step,wemaximizeL  withrespectto q      ,andontheother,wemaximizeLwith\n",
      "  respectto.θ\n",
      "            Stochasticgradientascentonlatentvariablemodelscanbeseenasaspecial\n",
      "             caseoftheEMalgorithmwheretheM-stepconsistsoftakingasinglegradient\n",
      "             step.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsome\n",
      "           modelfamilies,theM-stepcanevenbeperformedanalytically,jumpingallthe\n",
      "          waytotheoptimalsolutionforgiventhecurrent. θ q\n",
      "            EventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEM\n",
      "          algorithmasusingapproximateinferenceinsomesense.Speciﬁcally,theM-step\n",
      "     assumesthatthesamevalueof q      canbeusedforallvaluesofθ   .Thiswillintroduce\n",
      "  agapbetweenL  andthetrue log p(v       )astheM-stepmovesfurtherandfurther\n",
      "   awayfromthevalueθ(0 )        usedintheE-step.Fortunately,theE-stepreducesthe\n",
      "            gaptozeroagainasweentertheloopforthenexttime.\n",
      "            TheEMalgorithmcontainsafewdiﬀerentinsights.First,thereisthebasic\n",
      "            structureofthelearningprocess,inwhichweupdatethemodelparametersto\n",
      "           improvethelikelihoodofacompleteddataset,whereallmissingvariableshave\n",
      "           theirvaluesprovidedbyanestimateoftheposteriordistribution.Thisparticular\n",
      "             insightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescentto\n",
      "          maximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradient\n",
      "         computationsrequiretakingexpectationswithrespecttotheposteriordistribution\n",
      "             overthehiddenunits. Another keyinsightintheEMalgorithmisthatwecan\n",
      "     continuetouseonevalueof q         evenafterwehavemovedtoadiﬀerentvalueofθ.\n",
      "           Thisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelarge\n",
      "            M-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplex\n",
      "             toadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecond\n",
      "           insight,whichismoreuniquetotheEMalgorithm,israrelyused.\n",
      "     19.3MAPInferenceandSparseCoding\n",
      "            Weusuallyusetheterminferencetorefertocomputingtheprobabilitydistribution\n",
      "           overonesetofvariablesgivenanother.Whentrainingprobabilisticmodelswith\n",
      "       latentvariables,weareusuallyinterestedincomputing p(  hv|  ).Analternative\n",
      "              formofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,\n",
      "             ratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext\n",
      "6 3 3   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "      oflatentvariablemodels,thismeanscomputing\n",
      "h∗ = argmax\n",
      "h   p. ( )hv| (19.9)\n",
      "   Thisisknownas   maximumaposteriori     inference,abbreviatedasMAPinference.\n",
      "          MAPinferenceisusuallynotthoughtofasapproximateinference—itdoes\n",
      "      computetheexactmostlikelyvalueofh∗       .However,ifwewishtodevelopa\n",
      "    learningprocessbasedonmaximizingL(  vh,,q       ),thenitishelpfultothinkof\n",
      "         MAPinferenceasaprocedurethatprovidesavalueofq    . Inthissense,wecan\n",
      "            thinkofMAPinferenceasapproximateinference,becauseitdoesnotprovidethe\n",
      " optimal.q\n",
      "         Recallfromsectionthatexactinferenceconsistsofmaximizing 19.1\n",
      "  L( ) = vθ,,q E h ∼ q      [log( )]+() phv,Hq (19.10)\n",
      "  withrespecttoq       overanunrestrictedfamilyofprobabilitydistributions,using\n",
      "            anexactoptimizationalgorithm.WecanderiveMAPinferenceasaformof\n",
      "       approximateinferencebyrestrictingthefamilyofdistributionsq  maybedrawn\n",
      "          from.Speciﬁcally,werequiretotakeonaDiracdistribution: q\n",
      "     qδ. ( ) = hv| ( )hµ− (19.11)\n",
      "      Thismeansthatwecannowcontrolq  entirelyviaµ   .DroppingtermsofLthat\n",
      "           donotvarywith,weareleftwiththeoptimizationproblem µ\n",
      "µ∗ = argmax\n",
      "µ    log(= )phµv,, (19.12)\n",
      "       whichisequivalenttotheMAPinferenceproblem\n",
      "h∗ = argmax\n",
      "h   p. ( )hv| (19.13)\n",
      "             WecanthusjustifyalearningproceduresimilartoEM,inwhichwealternate\n",
      "     betweenperformingMAPinferencetoinferh∗  andthenupdateθ toincrease\n",
      " logp(h∗ ,v           ).AswithEM,thisisaformofcoordinateascentonL  ,wherewe\n",
      "   alternatebetweenusing inferenceto optimizeL withrespect toq andusing\n",
      "   parameterupdatestooptimizeL  withrespecttoθ      .Theprocedureasawholecan\n",
      "     bejustiﬁedbythefactthatL    isalowerboundon  logp(v     ).InthecaseofMAP\n",
      "           inference,thisjustiﬁcationisrathervacuous,becausetheboundisinﬁnitelyloose,\n",
      "          duetotheDiracdistribution’sdiﬀerentialentropyofnegativeinﬁnity.Adding\n",
      "        noisetowouldmaketheboundmeaningfulagain. µ\n",
      "6 3 4   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "            MAPinferenceiscommonlyusedindeeplearningasbothafeatureextractor\n",
      "           andalearningmechanism.Itisprimarilyusedforsparsecodingmodels.\n",
      "             Recallfromsectionthatsparsecodingisalinearfactormodelthatimposes 13.4\n",
      "             asparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplace\n",
      " prior,with\n",
      "ph( i) =λ\n",
      "2e−| λ h i| . (19.14)\n",
      "           Thevisibleunitsarethengeneratedbyperformingalineartransformationand\n",
      " addingnoise:\n",
      "       p ,β ( ) = (; + xh|NvWhb− 1 I). (19.15)\n",
      "   Computingorevenrepresentingp(  hv|      )isdiﬃcult.Everypairofvariablesh i\n",
      "andh j   arebothparentsofv    .Thismeansthatwhenv   isobserved,thegraphical\n",
      "     modelcontainsanactivepathconnectingh iandh j     .Allthehiddenunitsthus\n",
      "     participateinonemassivecliqueinp(  hv|     ). IfthemodelwereGaussian,then\n",
      "           theseinteractionscouldbemodeledeﬃcientlyviathecovariancematrix,butthe\n",
      "     sparsepriormakestheseinteractionsnon-Gaussian.\n",
      "Becausep(  hv|          )isintractable,soisthecomputationofthelog-likelihoodand\n",
      "           itsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,we\n",
      "            useMAPinferenceandlearntheparametersbymaximizingtheELBOdeﬁnedby\n",
      "        theDiracdistributionaroundtheMAPestimateof.h\n",
      "    Ifweconcatenatealltheh        vectorsinthetrainingsetintoamatrixH ,and\n",
      "  concatenateallthev    vectorsintoamatrixV     ,thenthesparsecodinglearning\n",
      "   processconsistsofminimizing\n",
      "  J,(HW) =\n",
      "i , j|H i , j |+\n",
      "i , j\n",
      "  VHW−2\n",
      "i , j . (19.16)\n",
      "             Mostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthe\n",
      "    normsofthecolumnsofW       ,topreventthepathologicalsolutionwithextremely\n",
      "     smallandlarge. H W\n",
      "  WecanminimizeJ      byalternatingbetweenminimizationwithrespecttoH\n",
      "    andminimizationwithrespecttoW       .Bothsubproblemsareconvex.Infact,the\n",
      "   minimizationwithrespecttoW      isjustalinearregressionproblem.Minimization\n",
      "             ofwithrespecttobotharguments,however,isusuallynotaconvexproblem. J\n",
      "   MinimizationwithrespecttoH      requiresspecializedalgorithmssuchasthe\n",
      "      feature-signsearchalgorithm( ,). Leeetal.2007\n",
      "6 3 5   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "    19.4VariationalInferenceandLearning\n",
      "We have seen how the evidence l ower boundL(  vθ,,q )is a lower bound on\n",
      " logp(v;θ       ),howinferencecanbeviewedasmaximizingL  withrespecttoq ,and\n",
      "      howlearningcanbeviewedasmaximizingL  withrespecttoθ   .Wehaveseen\n",
      "             thattheEMalgorithmenablesustomakelargelearningstepswithaﬁxedqand\n",
      "             thatlearningalgorithmsbasedonMAPinferenceenableustolearnusingapoint\n",
      " estimateofp(  hv|         )ratherthaninferringtheentiredistribution.Nowwedevelop\n",
      "      themoregeneralapproachtovariationallearning.\n",
      "          ThecoreideabehindvariationallearningisthatwecanmaximizeL overa\n",
      "   restrictedfamilyofdistributionsq          .Thisfamilyshouldbechosensothatitiseasy\n",
      " tocompute E q logp( hv,         ). Atypicalwaytodothisistointroduceassumptions\n",
      "   abouthowfactorizes. q\n",
      "           Acommonapproachtovariationallearningistoimposetherestrictionthatq\n",
      "   isafactorialdistribution:\n",
      "  q( ) =hv|\n",
      "iqh( i  |v). (19.17)\n",
      "   Thisiscalledthe meanﬁeld        approach.Moregenerally,wecanimposeanygraphi-\n",
      "     calmodelstructurewechooseonq       ,toﬂexiblydeterminehowmanyinteractionswe\n",
      "          wantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproach\n",
      "        iscalledstructuredvariationalinference( ,). SaulandJordan1996\n",
      "             Thebeautyofthevariationalapproachisthatwedonotneedtospecify\n",
      "    aspeciﬁcparametricformforq        .Wespecifyhowitshouldfactorize,butthen\n",
      "        theoptimizationproblemdeterminestheoptimalprobabilitydistributionwithin\n",
      "      thosefactorization constrai nts.For discretelatentvariables, thisjustmeans\n",
      "           thatweusetraditionaloptimizationtechniquestooptimizeaﬁnitenumberof\n",
      "  variablesdescribingtheq       distribution.Forcontinuouslatentvariables,thismeans\n",
      "            thatweuseabranchofmathematicscalledcalculusofvariationstoperform\n",
      "          optimizationoveraspaceoffunctionsandactuallydeterminewhichfunction\n",
      "    shouldbeusedtorepresentq         . Calculusofvariationsistheoriginofthenames\n",
      "         “variationallearning”and“variationalinference,”thoughthesenamesapplyeven\n",
      "            whenthelatentvariablesarediscreteandcalculusofvariationsisnotneeded.\n",
      "          Withcontinuouslatentvariables,calculusofvariationsisapowerfultechnique\n",
      "            thatremovesmuchoftheresponsibilityfromthehumandesignerofthemodel,\n",
      "     whonowmustspecifyonlyhowq        factorizes,ratherthanneedingtoguesshowto\n",
      "         designaspeciﬁcthatcanaccuratelyapproximatetheposterior. q\n",
      "BecauseL(  vθ,,q    )isdeﬁnedtobe  logp(v;θ) −D K L(q(  hv|)p(  hv|;θ )),we\n",
      "   canthinkofmaximizingL  withrespecttoq asminimizingD K L(q(  hv|)p(  hv|)).\n",
      "6 3 6   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "     Inthissense,weareﬁttingqtop         .However,wearedoingsowiththeoppositedi-\n",
      "              rectionoftheKLdivergencethanweareusedtousingforﬁttinganapproximation.\n",
      "             Whenweusemaximumlikelihoodlearningtoﬁtamodeltodata,weminimize\n",
      "D K L(p d a tap m o d e l          ).Asillustratedinﬁgure,thismeansthatmaximumlikelihood 3.6\n",
      "            encouragesthemodeltohavehighprobabilityeverywherethatthedatahashigh\n",
      "      probability,whileouroptimization-based inferenceprocedureencouragesq tohave\n",
      "          lowprobabilityeverywherethetrueposteriorhaslowprobability.Bothdirections\n",
      "            oftheKLdivergencecanhavedesirableandundesirableproperties.Thechoiceof\n",
      "             whichtousedependsonwhichpropertiesarethehighestpriorityforeachapplica-\n",
      "         tion.Intheinferenceoptimizationproblem,wechoosetouseD K L(q(  hv|)p( h|\n",
      "v     ))forcomputationalreasons.Speciﬁcally,computingD K L(q(  hv|)p(  hv| ))in-\n",
      "     volvesevaluatingexpectationswithrespecttoq   ,sobydesigningq   tobesimple,we\n",
      "           cansimplifytherequiredexpectations.TheoppositedirectionoftheKLdivergence\n",
      "          wouldrequirecomputingexpectationswithrespecttothetrueposterior.Because\n",
      "              theformofthetrueposteriorisdeterminedbythechoiceofmodel,wecannot\n",
      "      designareduced-costapproachtocomputingD K L     (( )( )) phv|qhv|exactly.\n",
      "   19.4.1DiscreteLatentVariables\n",
      "        Variationalinferencewithdiscretelatentvariablesisrelativelystraightforward.\n",
      "   Wedeﬁneadistributionq      ,typicallyonewhereeachfactorofq  isjustdeﬁned\n",
      "         byalookuptableoverdiscretestates. Inthesimplestcase,h   isbinaryandwe\n",
      "           makethemeanﬁeldassumptionthatfactorizesovereachindividual q h i  .Inthis\n",
      "   casewecanparametrizeq  withavectorˆh    whoseentriesareprobabilities.Then\n",
      "qh( i = 1 ) =|v ˆh i.\n",
      "    Afterdetermininghowtorepresentq      ,wesimplyoptimizeitsparameters.With\n",
      "           discretelatentvariables,thisisjustastandardoptimizationproblem.Inprinciple\n",
      "  theselectionofq         couldbedonewithanyoptimizationalgorithm,suchasgradient\n",
      "descent.\n",
      "            Becausethisoptimizationmustoccurintheinnerloopofalearningalgorithm,\n",
      "             itmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimization\n",
      "           algorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsin\n",
      "            fewiterations.Apopularchoiceistoiterateﬁxed-pointequations,inotherwords,\n",
      " tosolve∂\n",
      "∂ˆh i  L= 0 (19.18)\n",
      "forˆh i      .Werepeatedlyupdatediﬀerentelementsofˆh    untilwesatisfyaconvergence\n",
      "criterion.\n",
      "6 3 7   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "            Tomakethismoreconcrete,weshowhowtoapplyvariationalinferenceto\n",
      "the   binarysparsecodingmodel      (wepresentherethemodeldevelopedby\n",
      "           Henniges 2010etal.[]butdemonstratetraditional,genericmeanﬁeldappliedto\n",
      "          themodel,whiletheyintroduceaspecializedalgorithm).Thisderivationgoes\n",
      "            intoconsiderablemathematicaldetailandisintendedforthereaderwhowishesto\n",
      "          fullyresolveanyambiguityinthehigh-levelconceptualdescriptionofvariational\n",
      "              inferenceandlearningwehavepresentedsofar.Readerswhodonotplantoderive\n",
      "           orimplementvariationallearningalgorithmsmaysafelyskiptothenextsection\n",
      "           withoutmissinganynewhigh-levelconcepts.Readerswhoproceedwiththebinary\n",
      "            sparsecodingexampleareencouragedtoreviewthelistofusefulpropertiesof\n",
      "           functionsthatcommonlyariseinprobabilisticmodelsinsection.Weuse 3.10\n",
      "        thesepropertiesliberallythroughoutthefollowingderivationswithouthighlighting\n",
      "     exactlywhereweuseeachone.\n",
      "       Inthebinarysparsecodingmodel,theinput  v∈ Rn   isgeneratedfromthe\n",
      "        modelbyaddingGaussiannoisetothesumofm   diﬀerentcomponents,which\n",
      "              caneachbepresentorabsent.Eachcomponentisswitchedonoroﬀbythe\n",
      "       correspondinghiddenunitinh∈{}01,m:\n",
      "ph( i= 1) = (σb i ), (19.19)\n",
      "     p , ( ) = (; vh|NvWhβ− 1 ), (19.20)\n",
      "whereb     isalearnablesetofbiases,W     isalearnableweightmatrix,andβ isa\n",
      "   learnable,diagonalprecisionmatrix.\n",
      "         Trainingthismodelwithmaximumlikelihoodrequirestakingthederivative\n",
      "             withrespecttotheparameters.Considerthederivativewithrespecttooneofthe\n",
      "biases:\n",
      "∂\n",
      "∂b i  log()pv (19.21)\n",
      "=∂\n",
      "∂ b ip()v\n",
      "p()v(19.22)\n",
      "=∂\n",
      "∂ b i\n",
      "h p,(hv)\n",
      "p()v(19.23)\n",
      "=∂\n",
      "∂ b i\n",
      "h  pp()h( )vh|\n",
      "p()v(19.24)\n",
      "=\n",
      "h  p( )vh|∂\n",
      "∂ b ip()h\n",
      "p()v(19.25)\n",
      "6 3 8   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\n",
      "h 1 h 1\n",
      "h 2 h 2h 3 h 3\n",
      "h 4 h 4\n",
      "              Figure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits.\n",
      "   ( L e f t )Thegraphstructureofp( hv,          ).Notethattheedgesaredirected,andthateverytwo\n",
      "           hiddenunitsarecoparentsofeveryvisibleunit. Thegraphstructureof ( R i g h t ) p(  hv|).\n",
      "            Toaccount fortheactivepathsbetweencoparents,theposteriordistributionneedsan\n",
      "     edgebetweenallthehiddenunits.\n",
      "=\n",
      "h  p( )hv|∂\n",
      "∂ b ip()h\n",
      "p()h(19.26)\n",
      "= E h∼ | p ( h v )∂\n",
      "∂b i  log()ph. (19.27)\n",
      "      Thisrequirescomputingexpectationswithrespecttop(  hv| ).Unfortunately,\n",
      "p(  hv|            )isacomplicateddistribution.Seeﬁgureforthegraphstructureof 19.2\n",
      "p( hv, )andp(  hv|        ).Theposteriordistributioncorrespondstothecompletegraph\n",
      "             overthehiddenunits,sovariableeliminationalgorithmsdonothelpustocompute\n",
      "       therequiredexpectationsanyfasterthanbruteforce.\n",
      "          Wecanresolvethisdiﬃcultybyusingvariationalinferenceandvariational\n",
      " learninginstead.\n",
      "      Wecanmakeameanﬁeldapproximation:\n",
      "  q( ) =hv|\n",
      "iqh( i  |v). (19.28)\n",
      "             Thelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresent\n",
      " afactorialq    wesimplyneedtomodelm  Bernoullidistributionsq(h i |v  ).Anatural\n",
      "            waytorepresentthemeansoftheBernoullidistributionsiswithavectorˆhof\n",
      " probabilities,withq(h i =1 |v )=ˆh i     .Weimposearestrictionthatˆh i isnever\n",
      "               equalto0orto1,inordertoavoiderrorswhencomputing,forexample,logˆh i.\n",
      "             Wewillseethatthevariationalinferenceequationsneverassignorto 01 ˆh i\n",
      "         analytically.Inasoftwareimplementation,however,machineroundingerrorcould\n",
      "              resultinorvalues.Insoftware,wemaywishtoimplementbinarysparse 0 1\n",
      "6 3 9   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "       codingusinganunrestrictedvectorofvariationalparametersz andobtainˆhvia\n",
      " therelationˆh=σ(z     ).Wecanthussafelycomputelogˆh i    onacomputerbyusing\n",
      "   theidentitylog(σz i) = (−ζ−z i      ),relatingthesigmoidandthesoftplus.\n",
      "           Tobeginourderivationofvariationallearninginthebinarysparsecoding\n",
      "            model,weshowthattheuseofthismeanﬁeldapproximationmakeslearning\n",
      "tractable.\n",
      "      Theevidencelowerboundisgivenby\n",
      "   L( )vθ,,q (19.29)\n",
      "= E h ∼ q     [log( )]+() phv,Hq (19.30)\n",
      "= E h ∼ q            [log()+log( )log( )] phpvh|−qhv| (19.31)\n",
      "= E h ∼ qm\n",
      "i =1 log(ph i )+n\n",
      "i =1 log(pv i  |−h)m\n",
      "i =1 log(qh i |v)\n",
      "(19.32)\n",
      "=m\n",
      "i =1\n",
      "ˆh i (log(σb i  )log− ˆh i   )+(1−ˆh i  )(log(σ−b i   )log(1−−ˆh i))\n",
      "(19.33)\n",
      " + E h ∼ qn\n",
      "i =1log\n",
      "β i\n",
      "2πexp\n",
      "−β i\n",
      "2(v i −W i , :h)2\n",
      "(19.34)\n",
      "=m\n",
      "i =1\n",
      "ˆh i (log(σb i  )log− ˆh i   )+(1−ˆh i  )(log(σ−b i   )log(1−−ˆh i))\n",
      "(19.35)\n",
      "+1\n",
      "2n\n",
      "i =1\n",
      "logβ i\n",
      "2π −β i\n",
      "v2\n",
      "i −2v iW i , :ˆ h+\n",
      "j\n",
      "W2\n",
      "i , jˆh j+\n",
      "k j =W i , jW i , kˆh jˆh k\n",
      "\n",
      "\n",
      ".\n",
      "(19.36)\n",
      "         Whiletheseequationsaresomewhatunappealingaesthetically,theyshowthatL\n",
      "            canbeexpressedinasmallnumberofsimplearithmeticoperations.Theevidence\n",
      " lowerboundL     isthereforetractable.WecanuseL    asareplacementforthe\n",
      " intractablelog-likelihood.\n",
      "         Inprinciple,wecouldsimplyrungradientascentonbothvandh  ,andthis\n",
      "         wouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm.\n",
      "             Usually,however,wedonotdothis,fortworeasons.First,thiswouldrequire\n",
      "storingˆh foreachv         .Wetypicallypreferalgorithmsthatdonotrequireper\n",
      "            examplememory.Itisdiﬃculttoscalelearningalgorithmstobillionsofexamples\n",
      "           ifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample.\n",
      "          Second,wewouldliketobeabletoextractthefeaturesˆh    veryquickly,inorderto\n",
      "   recognizethecontentofv         . Inarealisticdeployedsetting,wewouldneedtobe\n",
      "6 4 0   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "  abletocomputeˆ   hinrealtime.\n",
      "            Forboththesereasons,wetypicallydonotusegradientdescenttocompute\n",
      "   themeanﬁeldparametersˆh       .Instead,werapidlyestimatethemwithﬁxed-point\n",
      "equations.\n",
      "            Theideabehindﬁxed-pointequationsisthatweareseekingalocalmaximum\n",
      "  withrespecttoˆh, where∇ hL( vθ,,ˆh )= 0     .Wecannoteﬃcientlysolvethis\n",
      "     equationwithrespecttoallofˆh        simultaneously.However,wecansolveforasingle\n",
      "variable:\n",
      "∂\n",
      "∂ˆh i L(vθ,,ˆ h) = 0. (19.37)\n",
      "          Wecantheniterativelyapplythesolutiontotheequationfori =1     ,...,m,\n",
      "           andrepeatthecycleuntilwesatisfyaconvergencecriterion.Commonconvergence\n",
      "           criteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveL bymore\n",
      "          thansometoleranceamount,orwhenthecycledoesnotchangeˆh  bymorethan\n",
      " someamount.\n",
      "           Iteratingmeanﬁeldﬁxed-pointequationsisageneraltechniquethatcanprovide\n",
      "             fastvariationalinferenceinabroadvarietyofmodels.Tomakethismoreconcrete,\n",
      "              weshowhowtoderivetheupdatesforthebinarysparsecodingmodelinparticular.\n",
      "           First,wemustwriteanexpressionforthederivativeswithrespecttoˆh i .To\n",
      "            doso,wesubstituteequation intotheleftsideofequation: 19.36 19.37\n",
      "∂\n",
      "∂ˆh i L(vθ,,ˆ h) (19.38)\n",
      "=∂\n",
      "∂ˆh i\n",
      "m \n",
      "j =1\n",
      "ˆh j  (log(σb j  )log− ˆh j   )+(1−ˆh j  )(log(σ−b j   )log(1−−ˆh j))\n",
      "(19.39)\n",
      "+1\n",
      "2n\n",
      "j =1\n",
      "logβ j\n",
      "2π −β j\n",
      "v2\n",
      "j −2v jW j , :ˆ h+\n",
      "k\n",
      "W2\n",
      "j , kˆh k+\n",
      "l k =W j , kW j , lˆh kˆh l\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(19.40)\n",
      "  =log(σb i  )log− ˆh i    −− 1+log(1ˆh i     )+1log(−σ−b i ) (19.41)\n",
      "+n \n",
      "j =1\n",
      "β j\n",
      "v jW j , i−1\n",
      "2W2\n",
      "j , i−\n",
      "k i =W j , kW j , iˆh k\n",
      "\n",
      " (19.42)\n",
      "=b i −logˆh i  +log(1−ˆh i  )+vβW : , i−1\n",
      "2W\n",
      ": , iβW : , i−\n",
      "j i =W\n",
      ": , jβW : , iˆh j .(19.43)\n",
      "6 4 1   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "          Toapplytheﬁxed-pointupdateinferencerule,wesolvefortheˆh i thatsets\n",
      "   equation to0: 19.43\n",
      "ˆh i= σ\n",
      "b i +vβW : , i −1\n",
      "2W\n",
      ": , iβW : , i −\n",
      "j i =W\n",
      ": , jβW : , iˆh j\n",
      " .(19.44)\n",
      "             Atthispoint,wecanseethatthereisacloseconnectionbetweenrecurrent\n",
      "          neuralnetworksandinferenceingraphicalmodels.Speciﬁcally,themeanﬁeld\n",
      "           ﬁxed-pointequationsdeﬁnedarecurrentneuralnetwork.Thetaskofthisnetwork\n",
      "             istoperforminference.Wehavedescribedhowtoderivethisnetworkfroma\n",
      "            modeldescription,butitisalsopossibletotraintheinferencenetworkdirectly.\n",
      "          Severalideasbasedonthisthemearedescribedinchapter.20\n",
      "             Inthecaseofbinarysparsecoding,wecanseethattherecurrentnetwork\n",
      "          connectionspeciﬁedbyequation consistsofrepeatedlyupdatingthehidden 19.44\n",
      "            unitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinput\n",
      "     alwayssendsaﬁxedmessageofvβW       tothehiddenunits,butthehiddenunits\n",
      "           constantlyupdatethemessagetheysendtoeachother.Speciﬁcally,twounitsˆh i\n",
      "andˆh j             inhibiteachotherwhentheirweightvectorsarealigned.Thisisaformof\n",
      "           competition—betweentwohiddenunitsthatbothexplaintheinput,onlytheone\n",
      "             thatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionis\n",
      "          themeanﬁeldapproximation’sattempttocapturetheexplainingawayinteractions\n",
      "           inthebinarysparsecodingposterior.Theexplainingawayeﬀectactuallyshould\n",
      "           causeamultimodalposterior, sothatifwedrawsamplesfromtheposterior,\n",
      "             somesampleswillhaveoneunitactive,othersampleswillhavetheotherunit\n",
      "           active,butveryfewsampleswillhavebothactive.Unfortunately,explainingaway\n",
      "      interactionscannotbemodeledbythefactorialq      usedformeanﬁeld,sothemean\n",
      "              ﬁeldapproximationisforcedtochooseonemodetomodel.Thisisaninstanceof\n",
      "     thebehaviorillustratedinﬁgure.3.6\n",
      "            Wecanrewriteequation intoanequivalentformthatrevealssomefurther 19.44\n",
      "insights:\n",
      "ˆh i= σ\n",
      "b i+\n",
      " v −\n",
      "j i =W : , jˆh j\n",
      "\n",
      "βW : , i −1\n",
      "2W\n",
      ": , iβW : , i\n",
      " . (19.45)\n",
      "            Inthisreformulation,weseetheinputateachstepasconsistingof v −\n",
      "j i =W : , jˆh j\n",
      " ratherthanv      .Wecanthusthinkofuniti     asattemptingtoencodetheresidual\n",
      " errorinv               giventhecodeoftheotherunits.Wecanthusthinkofsparsecodingasan\n",
      "6 4 2   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "         iterativeautoencoder,whichrepeatedlyencodesanddecodesitsinput,attempting\n",
      "        toﬁxmistakesinthereconstructionaftereachiteration.\n",
      "              Inthisexample,wehavederivedanupdaterulethatupdatesasingleunitat\n",
      "             atime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously.\n",
      "            Somegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsucha\n",
      "        waythatwecansolveformanyentriesofˆh   simultaneously.Unfortunately,binary\n",
      "             sparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristic\n",
      " techniquecalleddamping       toperformblockupdates.Inthedampingapproach,\n",
      "          wesolvefortheindividuallyoptimalvaluesofeveryelementofˆh   ,thenmoveall\n",
      "              thevaluesinasmallstepinthatdirection.Thisapproachisnolongerguaranteed\n",
      " toincreaseL            ateachstep,butitworkswellinpracticeformanymodels.See\n",
      "           KollerandFriedman2009()formoreinformationaboutchoosingthedegreeof\n",
      "      synchronyanddampingstrategiesinmessage-passi ngalgorithms.\n",
      "   19.4.2CalculusofVariations\n",
      "          Beforecontinuingwithourpresentationofvariationallearning,wemustbrieﬂy\n",
      "          introduceanimportantsetofmathematicaltoolsusedinvariationallearning:\n",
      "  calculusofvariations.\n",
      "         ManymachinelearningtechniquesarebasedonminimizingafunctionJ(θ )by\n",
      "   ﬁndingtheinputvector  θ∈ Rn         forwhichittakesonitsminimalvalue.Thiscan\n",
      "           beaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthe\n",
      "  criticalpointswhere∇ θJ(θ) = 0          .Insomecases,weactuallywanttosolvefora\n",
      "functionf(x            ),suchaswhenwewanttoﬁndtheprobabilitydensityfunctionover\n",
      "            somerandomvariable.Thisiswhatcalculusofvariationsenablesustodo.\n",
      "  Afunction ofa functionf  isknown asa  functionalJ[f  ].Muchas we\n",
      "             cantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-\n",
      "    valuedargument,wecantake  functionalderivatives   ,alsoknownasvariational\n",
      "derivatives   ,ofafunctionalJ[f        ]withrespecttoindividualvaluesofthefunction\n",
      "f(x     )atanyspeciﬁcvalueofx      .ThefunctionalderivativeofthefunctionalJwith\n",
      "            respecttothevalueofthefunctionatpointisdenoted fxδ\n",
      "δ f x ( )J.\n",
      "           Acompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeof\n",
      "             thisbook.Forourpurposes,itissuﬃcienttostatethatfordiﬀerentiablefunctions\n",
      "         f gy, ()xanddiﬀerentiablefunctions(x)withcontinuousderivatives,that\n",
      "δ\n",
      "δf()x\n",
      "     gf,d (()xx)x=∂\n",
      "∂y  gf,. (()xx) (19.46)\n",
      "          Togainsomeintuitionforthisidentity,onecanthinkoff(x    )asbeingavector\n",
      "6 4 3   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "        withuncountablymanyelements,indexedbyarealvectorx   .Inthis(somewhat\n",
      "           incomplete)view,theidentityprovidingthefunctionalderivativesisthesameas\n",
      "         whatwewouldobtainforavectorθ∈ Rn   indexedbypositiveintegers:\n",
      "∂\n",
      "∂θ i\n",
      "jgθ( j ,j) =∂\n",
      "∂θ igθ( i  ,i.) (19.47)\n",
      "           Manyresultsinothermachinelearningpublicationsarepresentedusingthemore\n",
      "general  Euler-Lagrangeequation  ,whichallowsg    todependonthederivatives\n",
      "off     aswellasthevalueoff           ,butwedonotneedthisfullygeneralformforthe\n",
      "    resultspresentedinthisbook.\n",
      "              Tooptimizeafunctionwithrespecttoavector,wetakethegradientofthe\n",
      "              functionwithrespecttothevectorandsolveforthepointwhereeveryelementof\n",
      "              thegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingfor\n",
      "            thefunctionwherethefunctionalderivativeateverypointisequaltozero.\n",
      "             Asanexampleofhowthisprocessworks,considertheproblemofﬁndingthe\n",
      "   probabilitydistributionfunctionover  x∈ R    thathasmaximaldiﬀerentialentropy.\n",
      "           Recallthattheentropyofaprobabilitydistribution isdeﬁnedas px()\n",
      "Hp[] = − E x  log()px. (19.48)\n",
      "       Forcontinuousvalues,theexpectationisanintegral:\n",
      "Hp[] = −\n",
      "   pxpxdx. ()log() (19.49)\n",
      "   WecannotsimplymaximizeH[p    ] withrespecttothefunctionp(x  ),becausethe\n",
      "            resultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrange\n",
      "     multiplierstoaddaconstraintthatp(x       )integrateto1.Also,theentropyshould\n",
      "           increasewithoutboundasthevarianceincreases.Thismakesthequestionof\n",
      "          whichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhich\n",
      "      distributionhasmaximalentropyforﬁxedvarianceσ2   .Finally,theproblem\n",
      "         isunderdeterminedbecausethedistributioncanbeshiftedarbitrarilywithout\n",
      "             changingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthe\n",
      "    meanofthedistributionbeµ     . TheLagrangianfunctionalforthisoptimization\n",
      " problemis\n",
      "L[] = pλ 1 \n",
      "  pxdx()−1\n",
      "+λ 2   ([] )+ Ex−µλ 3  E[( )xµ−2  ]−σ2 +[]Hp(19.50)\n",
      "=\n",
      "λ 1   pxλ ()+ 2  pxxλ ()+ 3  pxxµ ()(−)2   −pxpx ()log()\n",
      "  dxλ− 1 −µλ 2 −σ2λ 3.\n",
      "(19.51)\n",
      "6 4 4   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "      TominimizetheLagrangianwithrespecttop     ,wesetthefunctionalderivatives\n",
      "  equalto0:\n",
      "∀x,δ\n",
      "δpx() L= λ 1 +λ 2  xλ+ 3  ( )xµ−2    −−1log() = 0px. (19.52)\n",
      "        Thisconditionnowtellsusthefunctionalformofp(x  ).Byalgebraically\n",
      "    rearrangingtheequation,weobtain\n",
      "px() = exp\n",
      "λ 1 +λ 2  xλ+ 3  ( )xµ−2 −1\n",
      " . (19.53)\n",
      "    Weneverassumeddirectlythatp(x      )wouldtakethisfunctionalform;we\n",
      "          obtainedtheexpressionitselfbyanalyticallyminimizingafunctional.Toﬁnish\n",
      "      theminimizationproblem,wemustchoosetheλ     valuestoensurethatallour\n",
      "        constraintsaresatisﬁed.Wearefreetochooseanyλ    values,becausethegradient\n",
      "      oftheLagrangianwithrespecttotheλ        variablesiszeroaslongastheconstraints\n",
      "         aresatisﬁed.Tosatisfyalltheconstraints,wemaysetλ 1= 1  −logσ√\n",
      "2π,λ 2= 0,\n",
      " andλ 3= −1\n",
      "2 σ2 toobtain\n",
      "  pxxµ,σ () = (N;2 ). (19.54)\n",
      "              Thisisonereasonforusingthenormaldistributionwhenwedonotknowthe\n",
      "          truedistribution.Becausethenormaldistributionhasthemaximumentropy,we\n",
      "          imposetheleastpossibleamountofstructurebymakingthisassumption.\n",
      "           WhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,\n",
      "           wefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyfor\n",
      "         ﬁxedvariance.Whatabouttheprobabilitydistributionfunctionthat m i n i m i z e s\n",
      "             theentropy?Whydidwenotﬁndasecondcriticalpointcorrespondingtothe\n",
      "           minimum?Thereasonisthatnospeciﬁcfunctionachievesminimalentropy.As\n",
      "        functionsplacemoreprobabilitydensityonthetwopointsx=µ+σandx=µσ−,\n",
      "         andplacelessprobabilitydensityonallothervaluesofx    ,theyloseentropywhile\n",
      "          maintainingthedesiredvariance.However,anyfunctionplacingexactlyzeromass\n",
      "               onallbuttwopointsdoesnotintegratetooneandisnotavalidprobability\n",
      "         distribution.Thusthereisnosingleminimalentropyprobabilitydistribution\n",
      "             function,muchasthereisnosingleminimalpositiverealnumber.Instead,wecan\n",
      "           saythatthereisasequenceofprobabilitydistributionsconvergingtowardputting\n",
      "             massonlyonthesetwopoints.Thisdegeneratescenariomaybedescribedasa\n",
      "           mixtureofDiracdistributions.BecauseDiracdistributionsarenotdescribedbya\n",
      "          singleprobabilitydistributionfunction,noDiracormixtureofDiracdistribution\n",
      "          correspondstoasinglespeciﬁcpointinfunctionspace. Thesedistributionsare\n",
      "             thusinvisibletoourmethodofsolvingforaspeciﬁcpointwherethefunctional\n",
      "6 4 5   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "             derivativesarezero.Thisisalimitationofthemethod.Distributionssuchasthe\n",
      "             Diracmustbefoundbyothermethods,suchasguessingthesolutionandthen\n",
      "    provingthatitiscorrect.\n",
      "   19.4.3ContinuousLatentVariables\n",
      "           Whenourgraphicalmodelcontainscontinuouslatentvariables,wecanstillperform\n",
      "     variationalinferenceandlearningbymaximizingL     .However,wemustnowuse\n",
      "           calculusofvariationswhenmaximizingwithrespectto . L q( )hv|\n",
      "           Inmostcases,practitionersneednotsolveanycalculusofvariationsproblems\n",
      "           themselves.Instead,thereisageneralequationforthemeanﬁeldﬁxed-point\n",
      "       updates.Ifwemakethemeanﬁeldapproximation\n",
      "  q( ) =hv|\n",
      "iqh( i  |v), (19.55)\n",
      " andﬁxq(h j |v  )forall j=i   ,thentheoptimalq(h i |v    )maybeobtainedby\n",
      "   normalizingtheunnormalizeddistribution\n",
      "˜qh( i |v) = exp\n",
      "E h − i∼ q ( h − i| v ) log ˜p,(vh) , (19.56)\n",
      "  aslongasp          doesnotassignprobabilitytoanyjointconﬁgurationofvariables. 0\n",
      "           Carryingouttheexpectationinsidetheequationwillyieldthecorrectfunctional\n",
      " formofq(h i |v    ).Derivingfunctionalformsofq     directlyusingcalculusofvariations\n",
      "             isonlynecessaryifonewishestodevelopanewformofvariationallearning;\n",
      "          equation yieldsthemeanﬁeldapproximationforanyprobabilisticmodel. 19.56\n",
      "           Equation isaﬁxed-pointequation,designedtobeiterativelyappliedfor 19.56\n",
      "  eachvalueofi          repeatedlyuntilconvergence.However,italsotellsusmorethan\n",
      "             that.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whether\n",
      "             wearrivetherebyﬁxed-pointequationsornot.Thismeanswecantakethe\n",
      "              functionalformfromthatequationbutregardsomeofthevaluesthatappearinit\n",
      "           asparameters,whichwecanoptimizewithanyoptimizationalgorithmwelike.\n",
      "          Asanexample,considerasimpleprobabilisticmodel,withlatentvariables\n",
      "  h∈ R2    andjustonevisiblevariable,v  .Supposethatp(h )=N(h ;0 ,I )and\n",
      "p(  v|h )=N(v;wh         ;1).Wecouldactuallysimplifythismodelbyintegrating\n",
      "outh        ;theresultisjustaGaussiandistributionoverv    . Themodelitselfisnot\n",
      "            interesting;wehaveconstructeditonlytoprovideasimpledemonstrationofhow\n",
      "        calculusofvariationscanbeappliedtoprobabilisticmodeling.\n",
      "          Thetrueposteriorisgiven,uptoanormalizingconstant,by\n",
      "   p( )hv| (19.57)\n",
      "6 4 6   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "  ∝p,(hv) (19.58)\n",
      " =(ph 1)(ph 2   )( )pvh| (19.59)\n",
      "∝exp\n",
      "−1\n",
      "2\n",
      "h2\n",
      "1 +h2\n",
      "2  +(vh− 1w 1 −h 2w 2)2\n",
      "(19.60)\n",
      " =exp\n",
      "−1\n",
      "2\n",
      "h2\n",
      "1 +h2\n",
      "2 +v2 +h2\n",
      "1w2\n",
      "1 +h2\n",
      "2w2\n",
      "2 −2vh 1w 1 −2vh 2w 2 +2h 1w 1h 2w 2\n",
      ".\n",
      "(19.61)\n",
      "       Becauseofthepresenceofthetermsmultiplyingh 1andh 2    together,wecansee\n",
      "        thatthetrueposteriordoesnotfactorizeoverh 1 andh 2.\n",
      "     Applyingequation,weﬁndthat 19.56\n",
      "˜qh( 1  |v) (19.62)\n",
      " =exp\n",
      "Eh 2∼ q ( h 2| v ) log ˜p,(vh)\n",
      "(19.63)\n",
      " =exp\n",
      "−1\n",
      "2E h 2∼ q ( h 2| v )\n",
      "h2\n",
      "1 +h2\n",
      "2 +v2 +h2\n",
      "1w2\n",
      "1 +h2\n",
      "2w2\n",
      "2 (19.64)\n",
      "−2vh 1w 1 −2vh 2w 2 +2h 1w 1h 2w 2]\n",
      " . (19.65)\n",
      "               Fromthis,wecanseethatthereareeﬀectivelyonlytwovaluesweneedtoobtain\n",
      "fromq(h 2 |v): Eh 2∼ | q ( h v )[h 2 ]and Eh 2∼ | q ( h v )[h2\n",
      "2   ]. Writingtheseash 2andh2\n",
      "2,\n",
      " weobtain\n",
      "˜qh( 1 |v) = exp\n",
      "−1\n",
      "2\n",
      "h2\n",
      "1 +h2\n",
      "2  +v2 +h2\n",
      "1w2\n",
      "1 +h2\n",
      "2w2\n",
      "2 (19.66)\n",
      "−2vh 1w 1 −2vh 2w 2 +2h 1w 1h 2w 2]\n",
      " . (19.67)\n",
      "     Fromthis,wecanseethat˜q        hasthefunctionalformofaGaussian.Wecan\n",
      " thusconcludeq(  hv| )=N(h; µβ,− 1 )whereµ anddiagonalβ arevariational\n",
      "            parameters,whichwecanoptimizeusinganytechniquewechoose.Itisimportant\n",
      "        torecallthatwedidnoteverassumethatq    wouldbeGaussian;itsGaussian\n",
      "          formwasderivedautomaticallybyusingcalculusofvariationstomaximizeqwith\n",
      " respecttoL            .Usingthesameapproachonadiﬀerentmodelcouldyieldadiﬀerent\n",
      "   functionalformof.q\n",
      "           Thiswas,ofcourse,justasmallcaseconstructedfordemonstrationpurposes.\n",
      "          Forexamplesofrealapplicationsofvariationallearningwithcontinuousvariables\n",
      "          inthecontextofdeeplearning,see (). Goodfellowetal.2013d\n",
      "6 4 7   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "     19.4.4InteractionsbetweenLearningandInference\n",
      "           Usingapproximateinferenceaspartofalearningalgorithmaﬀectsthelearning\n",
      "           process,andthisinturnaﬀectstheaccuracyoftheinferencealgorithm.\n",
      "             Speciﬁcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakes\n",
      "       theapproximatingassumptionsunderlyingtheapproximateinferencealgorithm\n",
      "         becomemoretrue.Whentrainingtheparameters,variationallearningincreases\n",
      "E h ∼ q   log( )pvh,. (19.68)\n",
      "  Foraspeciﬁcv  ,thisincreasesp(  hv|   )forvaluesofh   thathavehighprobability\n",
      "underq(  hv|  )anddecreasesp(  hv|   )forvaluesofh   thathavelowprobability\n",
      "   under .q( )hv|\n",
      "        Thisbehaviorcausesourapproximatingassumptionstobecomeself-fulﬁlling\n",
      "            prophecies.Ifwetrainthemodelwithaunimodalapproximateposterior,wewill\n",
      "               obtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewould\n",
      "        haveobtainedbytrainingthemodelwithexactinference.\n",
      "            Computingthetrueamountofharmimposedonamodelbyavariational\n",
      "          approximationisthusverydiﬃcult.Thereexistseveralmethodsforestimating\n",
      " logp(v   ).Weoftenestimate  logp(v;θ       )aftertrainingthemodelandﬁndthat\n",
      "  thegapwithL(  vθ,,q          )issmall.Fromthis,wecanconcludethatourvariational\n",
      "       approximationisaccurateforthespeciﬁcvalueofθ    thatweobtainedfromthe\n",
      "          learningprocess.Weshouldnotconcludethatourvariationalapproximationis\n",
      "            accurateingeneralorthatthevariationalapproximationdidlittleharmtothe\n",
      "            learningprocess.Tomeasurethetrueamountofharminducedbythevariational\n",
      "     approximation,wewouldneedtoknowθ∗=max θ logp(v;θ   ). Itispossiblefor\n",
      "L(  vθ,,q)  ≈logp(v;θ )and  logp(v;θ)  logp(v;θ∗    )toholdsimultaneously.If\n",
      "max qL( vθ,∗ ,q)  logp(v;θ∗ ),becauseθ∗     inducestoocomplicatedofaposterior\n",
      "  distributionforourq       familytocapture, thenthelearningprocesswillnever\n",
      "approachθ∗             .Suchaproblemisverydiﬃculttodetect,becausewecanonlyknow\n",
      "              forsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanﬁndθ∗\n",
      " forcomparison.\n",
      "   19.5LearnedApproximateInference\n",
      "            Wehaveseenthatinferencecanbethoughtofasanoptimizationprocedure\n",
      "      thatincreasesthevalueofafunctionL    .Explicitlyperformingoptimizationvia\n",
      "        iterativeproceduressuchasﬁxed-pointequationsorgradient-basedoptimization\n",
      "           isoftenveryexpensiveandtimeconsuming.Manyapproachestoinferenceavoid\n",
      "6 4 8   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "         thisexpensebylearningtoperformapproximateinference. Speciﬁcally,wecan\n",
      "       thinkoftheoptimizationprocessasafunctionf   thatmapsaninput v toan\n",
      " approximatedistributionq∗=  argmaxq L( v,q      ).Oncewethinkofthemultistep\n",
      "            iterativeoptimizationprocessasjustbeingafunction,wecanapproximateitwith\n",
      "      aneuralnetworkthatimplementsanapproximation ˆ f(;) v θ.\n",
      " 19.5.1Wake-Sleep\n",
      "          Oneofthemaindiﬃcultieswithtrainingamodeltoinfer hfrom v  isthatwe\n",
      "              donothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givena v,\n",
      "     wedonotknowtheappropriate h   .Themappingfrom vto h   dependsonthe\n",
      "          choiceofmodelfamily,andevolvesthroughoutthelearningprocessas θchanges.\n",
      "            Thewake-sleepalgorithm(Hinton 1995bFrey 1996 etal.,;etal.,)resolvesthis\n",
      "     problembydrawingsamplesofboth hand v   fromthemodeldistribution. For\n",
      "            example,inadirectedmodel,thiscanbedonecheaplybyperformingancestral\n",
      "  samplingbeginningat h  andendingat v      .Theinferencenetworkcanthenbe\n",
      "      trainedtoperformthereversemapping: predictingwhich h   causedthepresent\n",
      "v                .Themaindrawbacktothisapproachisthatwewillonlybeabletotrainthe\n",
      "    inferencenetworkonvaluesof v       thathavehighprobabilityunderthemodel.Early\n",
      "            inlearning,themodeldistributionwillnotresemblethedatadistribution,sothe\n",
      "            inferencenetworkwillnothaveanopportunitytolearnonsamplesthatresemble\n",
      "data.\n",
      "              Insectionwesawthatonepossibleexplanationfortheroleofdreamsleep 18.2\n",
      "            inhumanbeingsandanimalsisthatdreamscouldprovidethenegativephase\n",
      "          samplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegative\n",
      "          gradientofthelogpartitionfunctionofundirectedmodels.Anotherpossible\n",
      "          explanationforbiologicaldreamingisthatitisprovidingsamplesfromp( h v,)\n",
      "          whichcanbeusedtotrainaninferencenetworktopredict hgiven v  .Insome\n",
      "          senses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation.\n",
      "             MonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonly\n",
      "             thepositivephaseofthegradientforseveralstepsthenwithonlythenegative\n",
      "            phaseofthegradientforseveralsteps.Humanbeingsandanimalsareusually\n",
      "            awakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis\n",
      "            notreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofan\n",
      "      undirectedmodel.Learningalgorithmsbasedonmaximizing L   canberunwith\n",
      "   prolongedperiodsofimprovingq    andprolongedperiodsofimproving θ ,however.\n",
      "           Iftheroleofbiologicaldreamingistotrainnetworksforpredictingq  ,thenthis\n",
      "             explainshowanimalsareabletoremainawakeforseveralhours(thelongerthey\n",
      "      areawake,thegreaterthegapbetween Land  logp( v ),but L   willremainalower\n",
      "6 4 9   C HAP T E R 1 9 . AP P R O XI M A T E I NFE R E NC E\n",
      "             bound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnot\n",
      "          modiﬁedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,these\n",
      "             ideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreaming\n",
      "          accomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearning\n",
      "         ratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromthe\n",
      "            animal’stransitionmodel,onwhichtotraintheanimal’spolicy.Orsleepmay\n",
      "           servesomeotherpurposenotyetanticipatedbythemachinelearningcommunity.\n",
      "     19.5.2OtherFormsofLearnedInference\n",
      "           Thisstrategyoflearnedapproximateinferencehasalsobeenappliedtoother\n",
      "           models.SalakhutdinovandLarochelle2010()showedthatasinglepassina\n",
      "           learnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanﬁeld\n",
      "            ﬁxed-pointequationsinaDBM.Thetrainingprocedureisbasedonrunningthe\n",
      "            inferencenetwork,thenapplyingonestepofmeanﬁeldtoimproveitsestimates,\n",
      "            andtrainingtheinferencenetworktooutputthisreﬁnedestimateinsteadofits\n",
      " originalestimate.\n",
      "           Wehavealreadyseeninsectionthatthepredictivesparsedecomposition 14.8\n",
      "             modeltrainsashallowencodernetworktopredictasparsecodefortheinput.\n",
      "              Thiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itis\n",
      "           possibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencoder\n",
      "           maybeviewedasperforminglearnedapproximateMAPinference.Duetoits\n",
      "            shallowencoder,PSDisnotabletoimplementthekindofcompetitionbetween\n",
      "             unitsthatwehaveseeninmeanﬁeldinference.However,thatproblemcanbe\n",
      "           remediedbytrainingadeepencodertoperformlearnedapproximateinference,as\n",
      "       intheISTAtechnique( ,). GregorandLeCun2010b\n",
      " Learned approximate inference has recently become one of the dominant\n",
      "          approachestogenerativemodeling,intheformofthevariationalautoencoder\n",
      "              ( ,; ,).Inthiselegantapproach,thereisnoneedto Kingma2013Rezendeetal.2014\n",
      "          constructexplicittargetsfortheinferencenetwork.Instead,theinferencenetwork\n",
      "    issimplyusedtodeﬁne L         ,andthentheparametersoftheinferencenetworkare\n",
      "            adaptedtoincrease.Thismodelisdescribedindepthinsection . L 20.10.3\n",
      "             Usingapproximateinference,itispossibletotrainanduseawidevarietyof\n",
      "          models.Manyofthesemodelsaredescribedinthenextchapter.\n",
      "6 5 0 C h a p t e r 2 0\n",
      "  D e e p Gene r at i v e Mo dels\n",
      "             Inthischapter,wepresentseveralofthespeciﬁckindsofgenerativemodelsthat\n",
      "           canbebuiltandtrainedusingthetechniquespresentedinchapters–.All 1619\n",
      "         thesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsome\n",
      "          way.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly.\n",
      "           Othersdonotallowtheevaluationoftheprobabilitydistributionfunctionbut\n",
      "           supportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamples\n",
      "          fromthedistribution.Someofthesemodelsarestructuredprobabilisticmodels\n",
      "            describedintermsofgraphsandfactors,usingthelanguageofgraphicalmodels\n",
      "             presentedinchapter.Otherscannotbeeasilydescribedintermsoffactorsbut 16\n",
      "   representprobabilitydistributionsnonetheless.\n",
      "  20.1BoltzmannMachines\n",
      "         Boltzmannmachineswereoriginallyintroducedasageneral“connectionist”ap-\n",
      "         proachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlman\n",
      "              etal.,;1983Ackley1985Hinton 1984HintonandSejnowski1986 etal.,; etal.,; ,).\n",
      "            VariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelong\n",
      "            agosurpassedthepopularityoftheoriginal.Inthissectionwebrieﬂyintroduce\n",
      "             thebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingto\n",
      "      trainandperforminferenceinthemodel.\n",
      "      WedeﬁnetheBoltzmannmachineovera d    -dimensionalbinaryrandomvector\n",
      " x ∈{ 0 , 1}d        . TheBoltzmannmachineisanenergy-basedmodel(section ),16.2.4\n",
      "651    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "          meaningwedeﬁnethejointprobabilitydistributionusinganenergyfunction:\n",
      " P() =x exp(())−Ex\n",
      "Z , (20.1)\n",
      "whereE(x     )istheenergyfunction,andZ     isthepartitionfunctionthatensures\n",
      "that\n",
      "x           P() = 1x.TheenergyfunctionoftheBoltzmannmachineisgivenby\n",
      "E() = x−x  Uxb− x, (20.2)\n",
      "whereU       isthe“weight”matrixofmodelparametersandb    isthevectorofbias\n",
      "parameters.\n",
      "              InthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftraining\n",
      "    examples,eachofwhicharen      -dimensional.Equationdescribesthejoint 20.1\n",
      "          probabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainly\n",
      "            viable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablesto\n",
      "            thosedescribedbytheweightmatrix.Speciﬁcally,itmeansthattheprobabilityof\n",
      "               oneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesof\n",
      "  theotherunits.\n",
      "           TheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesare\n",
      "              observed.Inthiscase,thelatentvariablescanactsimilarlytohiddenunitsina\n",
      "         multilayerperceptronandmodelhigher-orderinteractionsamongthevisibleunits.\n",
      "              JustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresults\n",
      "           intheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachine\n",
      "           withhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetween\n",
      "         variables.Instead,theBoltzmannmachinebecomesauniversalapproximatorof\n",
      "          probabilitymassfunctionsoverdiscretevariables( ,). LeRouxandBengio2008\n",
      "    Formally,wedecomposetheunitsx     intotwosubsets:thevisibleunitsvand\n",
      "         thelatent(orhidden)units.Theenergyfunctionbecomes h\n",
      " E,(vhv ) = −  Rvv−   Whh− Shb−  vc− h. (20.3)\n",
      "  BoltzmannMachineLearning     LearningalgorithmsforBoltzmannmachines\n",
      "          areusuallybasedonmaximumlikelihood.AllBoltzmannmachineshavean\n",
      "          intractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-\n",
      "       proximatedusingthetechniquesdescribedinchapter.18\n",
      "         OneinterestingpropertyofBoltzmannmachineswhentrainedwithlearning\n",
      "            rulesbasedonmaximumlikelihoodisthattheupdateforaparticularweight\n",
      "            connectingtwounitsdependsonlyonthestatisticsofthosetwounits,collected\n",
      "  underdiﬀerentdistributions:P m o d e l(v )andˆP d a ta(v)P m o d e l(  hv|    ).Therestofthe\n",
      "6 5 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           networkparticipatesinshapingthosestatistics,buttheweightcanbeupdated\n",
      "             withoutknowinganythingabouttherestofthenetworkorhowthosestatisticswere\n",
      "           produced.Thismeansthatthelearningruleis“local,”whichmakesBoltzmann\n",
      "         machinelearningsomewhatbiologicallyplausible. Itisconceivablethatifeach\n",
      "            neuronwerearandomvariableinaBoltzmannmachine,thentheaxonsand\n",
      "           dendritesconnectingtworandomvariablescouldlearnonlybyobservingtheﬁring\n",
      "            patternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthe\n",
      "          positivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnection\n",
      "            strengthened.ThisisanexampleofaHebbianlearningrule(,)often Hebb1949\n",
      "        summarizedwiththemnemonic“ﬁretogether,wiretogether.” Hebbianlearning\n",
      "          rulesareamongtheoldesthypothesizedexplanationsforlearninginbiological\n",
      "        systemsandremainrelevanttoday( ,). Giudiceetal.2009\n",
      "          Otherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseem\n",
      "            torequireustohypothesizetheexistenceofmoremachinerythanthis.For\n",
      "          example,forthebraintoimplementback-propagationinamultilayerperceptron,\n",
      "           itseemsnecessaryforthebraintomaintainasecondarycommunicationnetwork\n",
      "         fortransmittinggradientinformationbackwardthroughthenetwork.Proposalsfor\n",
      "      biologicallyplausibleimplementations(andapproximations)ofback-propagation\n",
      "            havebeenmade(,;,)butremaintobevalidated,and Hinton2007aBengio2015\n",
      "         Bengio2015()linksback-propagationofgradientstoinferenceinenergy-based\n",
      "          modelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables).\n",
      "          ThenegativephaseofBoltzmannmachinelearningissomewhatharderto\n",
      "             explainfromabiologicalpointofview.Asarguedinsection,dreamsleep 18.2\n",
      "             maybeaformofnegativephasesampling.Thisideaismorespeculativethough.\n",
      "   20.2RestrictedBoltzmannMachines\n",
      "   Inventedunderthename ha r m o ni u m    ( ,),restrictedBoltzmann Smolensky1986\n",
      "           machinesaresomeofthemostcommonbuildingblocksofdeepprobabilistic\n",
      "            models.WebrieﬂydescribeRBMsinsection .Herewereviewtheprevious 16.7.1\n",
      "          informationandgointomoredetail.RBMsareundirectedprobabilisticgraphical\n",
      "          modelscontaininga layerofobservablevariablesandasinglelayerof latent\n",
      "              variables.RBMsmaybestacked(oneontopoftheother)toformdeepermodels.\n",
      "            Seeﬁgureforsomeexamples.Inparticular,ﬁgureashowsthegraph 20.1 20.1\n",
      "             structureoftheRBMitself.Itisabipartitegraph,withnoconnectionspermitted\n",
      "             betweenanyvariablesintheobservedlayerorbetweenanyunitsinthelatent\n",
      "layer.\n",
      "            WebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butas\n",
      "6 5 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "h 1 h 1 h 2 h 2 h 3 h 3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h( 1 )\n",
      "1h( 1 )\n",
      "1h( 1 )\n",
      "2h( 1 )\n",
      "2h( 1 )\n",
      "3 h( 1 )\n",
      "3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n",
      "1 h( 2 )\n",
      "1 h( 2 )\n",
      "2 h( 2 )\n",
      "2 h( 2 )\n",
      "3 h( 2 )\n",
      "3\n",
      "h( 1 )\n",
      "4h( 1 )\n",
      "4\n",
      " (a) (b)\n",
      "h( 1 )\n",
      "1 h( 1 )\n",
      "1 h( 1 )\n",
      "2 h( 1 )\n",
      "2h( 1 )\n",
      "3h( 1 )\n",
      "3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n",
      "1h( 2 )\n",
      "1h( 2 )\n",
      "2h( 2 )\n",
      "2h( 2 )\n",
      "3h( 2 )\n",
      "3\n",
      "h( 1 )\n",
      "4 h( 1 )\n",
      "4\n",
      "(c)\n",
      "            Figure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines.\n",
      "            (a)TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedon\n",
      "                abipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsinthe\n",
      "             otherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamong\n",
      "              thehiddenunits.Typicallyeveryvisibleunitisconnectedtoeveryhiddenunit,butit\n",
      "            ispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A(b)\n",
      "            deepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirected\n",
      "             connections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiple\n",
      "            hiddenlayers,andthusconnectionsbetweenhiddenunitsthatareinseparatelayers.\n",
      "            Allthelocalconditionalprobabilitydistributionsneededbythedeepbeliefnetworkare\n",
      "           copieddirectlyfromthelocalconditionalprobabilitydistributionsofitsconstituentRBMs.\n",
      "            Alternatively,wecouldalsorepresentthedeepbeliefnetworkwithacompletelyundirected\n",
      "           graph,butitwouldneedintralayerconnectionstocapturethedependenciesbetween\n",
      "            parents.AdeepBoltzmannmachineisanundirectedgraphicalmodelwithseveral (c)\n",
      "           layersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayerconnections.\n",
      "              DBMsarelesscloselytiedtoRBMsthanDBNsare. WheninitializingaDBMfroma\n",
      "             stackofRBMs,itisnecessarytomodifytheRBMparametersslightly. Somekindsof\n",
      "          DBMsmaybetrainedwithoutﬁrsttrainingasetofRBMs.\n",
      "6 5 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             weseelater,thereareextensionstoothertypesofvisibleandhiddenunits.\n",
      "          Moreformally,lettheobservedlayerconsistofasetofn v binaryrandom\n",
      "        variables,whichwerefertocollectivelywiththevectorv     .Werefertothelatent,\n",
      "    orhidden,layerofn h    binaryrandomvariablesas.h\n",
      "          LikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisan\n",
      "          energy-basedmodelwiththejointprobabilitydistributionspeciﬁedbyitsenergy\n",
      "function:\n",
      "    P, (= vvh= ) =h1\n",
      "Z    exp(( ))−Evh,. (20.4)\n",
      "        TheenergyfunctionforanRBMisgivenby\n",
      " E,(vhb ) = −  vc−  hv−  Wh, (20.5)\n",
      "          andisthenormalizingconstantknownasthepartitionfunction: Z\n",
      " Z=\n",
      "v\n",
      "h   exp ( ){−Evh,}. (20.6)\n",
      "         ItisapparentfromthedeﬁnitionofthepartitionfunctionZ   thatthenaivemethod\n",
      " ofcomputingZ        (exhaustivelysummingoverallstates)couldbecomputationally\n",
      "          intractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesinthe\n",
      "   probabilitydistributiontocomputeZ      faster.InthecaseofrestrictedBoltzmann\n",
      "          machines, ()formallyprovedthatthepartitionfunction LongandServedio2010 Z\n",
      "     isintractable.TheintractablepartitionfunctionZ    impliesthatthenormalized\n",
      "         jointprobabilitydistribution isalsointractabletoevaluate. P()v\n",
      "  20.2.1ConditionalDistributions\n",
      "ThoughP(v           )isintractable,thebipartitegraphstructureoftheRBMhasthe\n",
      "     specialpropertyofitsconditionaldistributionsP(  hv| )andP(  vh| )being\n",
      "        factorialandrelativelysimpletocomputeandsamplefrom.\n",
      "         Derivingtheconditionaldistributionsfromthejointdistributionisstraightfor-\n",
      "ward:\n",
      "   P( ) =hv|  P,(hv)\n",
      " P()v(20.7)\n",
      "=1\n",
      " P()v1\n",
      "Zexp\n",
      "b  vc+  hv+ Wh\n",
      "(20.8)\n",
      "=1\n",
      "Zexp\n",
      "c  hv+ Wh\n",
      "(20.9)\n",
      "6 5 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "=1\n",
      "Zexp\n",
      "\n",
      "n h \n",
      "j =1c jh j+n h \n",
      "j =1vW : , jh j\n",
      "\n",
      "(20.10)\n",
      "=1\n",
      "Zn h \n",
      "j =1exp\n",
      "c jh j +vW : , jh j\n",
      " . (20.11)\n",
      "       Sinceweareconditioningonthevisibleunitsv      ,wecantreattheseasconstant\n",
      "    withrespecttothedistributionP(  hv|      ).Thefactorialnatureoftheconditional\n",
      "P(  hv|           )followsimmediatelyfromourabilitytowritethejointprobabilityover\n",
      " thevectorh        astheproductof(unnormalized)distributionsovertheindividual\n",
      "elements,h j            .Itisnowasimplematterofnormalizingthedistributionsoverthe\n",
      "  individualbinaryh j.\n",
      " Ph( j = 1 ) =|v˜ Ph( j = 1 )|v\n",
      "˜ Ph( j  = 0 )+|v ˜ Ph( j = 1 )|v(20.12)\n",
      "=exp\n",
      "c j +vW : , j\n",
      "    exp0+exp{}{c j +vW : , j}(20.13)\n",
      "= σ\n",
      "c j +vW : , j\n",
      " . (20.14)\n",
      "             Wecannowexpressthefullconditionaloverthehiddenlayerasthefactorial\n",
      "distribution:\n",
      "   P( ) =hv|n h \n",
      "j =1σ\n",
      "      (2 1) (+h−cWv)\n",
      "j . (20.15)\n",
      "            Asimilarderivationwillshowthattheotherconditionofinteresttous,P(  vh|),\n",
      "    isalsoafactorialdistribution:\n",
      "   P( ) =vh|n v \n",
      "i =1        σ((2 1) (+ )) v−bWhi . (20.16)\n",
      "    20.2.2TrainingRestrictedBoltzmannMachines\n",
      "        BecausetheRBMadmitseﬃcientevaluationanddiﬀerentiationof˜P(v )and\n",
      "             eﬃcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybe\n",
      "            trainedwithanyofthetechniquesdescribedinchapterfortrainingmodels 18\n",
      "         thathaveintractablepartitionfunctions. ThisincludesCD,SML(PCD),ratio\n",
      "            matching,andsoon.Comparedtootherundirectedmodelsusedindeeplearning,\n",
      "          theRBMisrelativelystraightforwardtotrainbecausewecancomputeP(  h|v)\n",
      "6 5 6    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            exactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmann\n",
      "           machine,combineboththediﬃcultyofanintractablepartitionfunctionandthe\n",
      "   diﬃcultyofintractableinference.\n",
      "   20.3DeepBeliefNetworks\n",
      "  Deepbeliefnetworks        (DBNs)wereoneoftheﬁrstnonconvolutionalmodels\n",
      "           tosuccessfullyadmittrainingofdeeparchitectures(Hinton 2006Hinton etal.,;,\n",
      "            2007b).Theintroductionofdeepbeliefnetworksin2006beganthecurrentdeep\n",
      "           learningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodels\n",
      "          wereconsideredtoodiﬃculttooptimize.Kernelmachineswithconvexobjective\n",
      "        functionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstrated\n",
      "         thatdeeparchitecturescanbesuccessfulbyoutperformingkernelizedsupport\n",
      "            vectormachinesontheMNISTdataset( ,).Today,deepbelief Hintonetal.2006\n",
      "             networkshavemostlyfallenoutoffavorandarerarelyused,evencomparedto\n",
      "          otherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedly\n",
      "        recognizedfortheirimportantroleindeeplearninghistory.\n",
      "           Deepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables.\n",
      "            Thelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinary\n",
      "             orreal.Therearenointralayerconnections.Usually,everyunitineachlayeris\n",
      "             connectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstruct\n",
      "           moresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersare\n",
      "           undirected.Theconnectionsbetweenallotherlayersaredirected,withthearrows\n",
      "               pointedtowardthelayerthatisclosesttothedata.Seeﬁgurebforanexample. 20.1\n",
      "  ADBNwithl  hiddenlayerscontainsl weightmatrices: W(1 )     ,..., W( ) l .It\n",
      " alsocontainsl   +1biasvectors b(0 )     ,..., b( ) l ,with b(0 )    providingthebiasesforthe\n",
      "           visiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenby\n",
      " P( h( ) l , h( 1 ) l − ) exp∝\n",
      "b( ) lh( ) l + b( 1 ) l − h( 1 ) l − + h( 1 ) l − W( ) lh( ) l\n",
      " ,(20.17)\n",
      " Ph(( ) k\n",
      "i = 1 | h( +1 ) k) = σ\n",
      "b( ) k\n",
      "i  + W( +1 ) k \n",
      ": , i h( +1 ) k\n",
      "           ∀∀∈− i,k1,...,l2,(20.18)\n",
      " Pv( i = 1 | h(1 )) = σ\n",
      "b(0 )\n",
      "i + W(1 ) \n",
      ": , i h(1 )\n",
      " ∀i. (20.19)\n",
      "       Inthecaseofreal-valuedvisibleunits,substitute\n",
      "  v∼N\n",
      " v b;(0 ) + W(1 ) h(1 ) , β− 1\n",
      "(20.20)\n",
      "6 5 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "with β         diagonalfortractability.Generalizationstootherexponentialfamilyvisible\n",
      "              unitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayeris\n",
      "  justanRBM.\n",
      "              TogenerateasamplefromaDBN,weﬁrstrunseveralstepsofGibbssampling\n",
      "             onthetoptwohiddenlayers.Thisstageisessentiallydrawingasamplefrom\n",
      "                theRBMdeﬁnedbythetoptwohiddenlayers.Wecanthenuseasinglepassof\n",
      "              ancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisible\n",
      "units.\n",
      "           Deepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirected\n",
      "   modelsandundirectedmodels.\n",
      "            Inferenceinadeepbeliefnetworkisintractablebecauseoftheexplainingaway\n",
      "            eﬀectwithineachdirectedlayerandtheinteractionbetweenthetwohiddenlayers\n",
      "         thathaveundirectedconnections.Evaluatingormaximizingthestandardevidence\n",
      "           lowerboundonthelog-likelihoodisalsointractable,becausetheevidencelower\n",
      "             boundtakestheexpectationofcliqueswhosesizeisequaltothenetworkwidth.\n",
      "         Evaluatingormaximizingthelog-likelihoodrequiresconfrontingnotjustthe\n",
      "           problemofintractableinferencetomarginalizeoutthelatentvariables,butalso\n",
      "           theproblemofanintractablepartitionfunctionwithintheundirectedmodelof\n",
      "   thetoptwolayers.\n",
      "             Totrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximize\n",
      "E v∼ p d a t a logp( v       )usingcontrastivedivergenceorstochasticmaximumlikelihood.\n",
      "              TheparametersoftheRBMthendeﬁnetheparametersoftheﬁrstlayerofthe\n",
      "         DBN.Next,asecondRBMistrainedtoapproximatelymaximize\n",
      "E v∼ p d a t a Eh( 1 )∼ p( 1 ) ( h( 1 )| v ) logp(2 )( h(1 ) ), (20.21)\n",
      "wherep(1 )         istheprobabilitydistributionrepresentedbytheﬁrstRBM,andp(2 )\n",
      "           istheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,\n",
      "            thesecondRBMistrainedtomodelthedistributiondeﬁnedbysamplingthe\n",
      "              hiddenunitsoftheﬁrstRBM,whentheﬁrstRBMisdrivenbythedata. This\n",
      "             procedurecanberepeatedindeﬁnitely,toaddasmanylayerstotheDBNas\n",
      "             desired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBM\n",
      "             deﬁnesanotherlayeroftheDBN.Thisprocedurecanbejustiﬁedasincreasinga\n",
      "            variationallowerboundonthelog-likelihoodofthedataundertheDBN(Hinton\n",
      "  etal.,).2006\n",
      "              Inmostapplications,noeﬀortismadetojointlytraintheDBNafterthegreedy\n",
      "          layer-wiseprocedureiscomplete.However,itispossibletoperformgenerative\n",
      "    ﬁne-tuningusingthewake-sleepalgorithm.\n",
      "6 5 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "              ThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostofthe\n",
      "            interestinDBNsarosefromtheirabilitytoimproveclassiﬁcationmodels.Wecan\n",
      "            taketheweightsfromtheDBNandusethemtodeﬁneanMLP:\n",
      "h(1 )= σ\n",
      "b(1 ) + vW(1 )\n",
      " , (20.22)\n",
      "h( ) l= σ\n",
      "b( ) l + h( 1 ) l − W( ) l\n",
      "        ∀∈l2,...,m. (20.23)\n",
      "           AfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerative\n",
      "              trainingoftheDBN,wecantraintheMLPtoperformaclassiﬁcationtask.This\n",
      "          additionaltrainingoftheMLPisanexampleofdiscriminativeﬁne-tuning.\n",
      "            ThisspeciﬁcchoiceofMLPissomewhatarbitrary,comparedtomanyofthe\n",
      "            inferenceequationsinchapterthatarederivedfromﬁrstprinciples.ThisMLP 19\n",
      "              isaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistently\n",
      "          intheliterature.Manyapproximateinferencetechniquesaremotivatedbytheir\n",
      "           abilitytoﬁndamaximallyvariationallowerboundonthelog-likelihood tight\n",
      "             undersomesetofconstraints.Onecanconstructavariationallowerboundonthe\n",
      "           log-likelihoodusingthehiddenunitexpectationsdeﬁnedbytheDBN’sMLP,but\n",
      "              thisistrueofprobabilitydistributionoverthehiddenunits,andthereisno any\n",
      "           reasontobelievethatthisMLPprovidesaparticularlytightbound. Inparticular,\n",
      "           theMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.The\n",
      "           MLPpropagatesinformationupwardfromthevisibleunitstothedeepesthidden\n",
      "            units,butitdoesnotpropagateanyinformationdownwardorsideways.TheDBN\n",
      "          graphicalmodelhasexplainingawayinteractionsbetweenallthehiddenunits\n",
      "           withinthesamelayeraswellasintop-downinteractionsbetweenlayers.\n",
      "            Whilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwith\n",
      "           AIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasa\n",
      " generativemodel.\n",
      "            Theterm“deepbeliefnetwork”iscommonlyusedincorrectlytorefertoany\n",
      "          kindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics.\n",
      "           Thetermshouldreferspeciﬁcallytomodelswithundirectedconnectionsinthe\n",
      "          deepestlayeranddirectedconnectionspointingdownwardbetweenallotherpairs\n",
      "  ofconsecutivelayers.\n",
      "           Thetermmayalsocausesomeconfusionbecause“beliefnetwork”issometimes\n",
      "            usedtorefertopurelydirectedmodels,whiledeepbeliefnetworkscontainan\n",
      "           undirectedlayer.DeepbeliefnetworksalsosharetheacronymDBNwithdynamic\n",
      "          Bayesiannetworks(DeanandKanazawa1989,),whichareBayesiannetworksfor\n",
      "  representingMarkovchains.\n",
      "6 5 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "   20.4DeepBoltzmannMachines\n",
      "A  deepBoltzmannmachine       ,orDBM(SalakhutdinovandHinton2009a,)is\n",
      "           anotherkindofdeepgenerativemodel.Unlikethedeepbeliefnetwork(DBN),\n",
      "             itisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayers\n",
      "            oflatentvariables(RBMshavejustone). ButliketheRBM,withineachlayer,\n",
      "           eachofthevariablesaremutuallyindependent,conditionedonthevariablesin\n",
      "           theneighboringlayers.Seeﬁgureforthegraphstructure.DeepBoltzmann 20.2\n",
      "           machineshavebeenappliedtoavarietyoftasks,includingdocumentmodeling\n",
      "   (Srivastava2013etal.,).\n",
      "         LikeRBMsandDBNs, DBMstypicallycontainonlybinaryunits—aswe\n",
      "           assumeforsimplicityofourpresentationofthemodel—butitisstraightforward\n",
      "    toincludereal-valuedvisibleunits.\n",
      "           ADBMisanenergy-basedmodel,meaningthatthejointprobabilitydistribu-\n",
      "          tionoverthemodelvariablesisparametrizedbyanenergyfunctionE   .Inthecase\n",
      "        ofadeepBoltzmannmachinewithonevisiblelayer, v    ,andthreehiddenlayers,\n",
      "h(1 ) , h(2 )  ,and h(3 )      ,thejointprobabilityisgivenby\n",
      "P\n",
      " v h,(1 ) , h(2 ) , h(3 )\n",
      "=1\n",
      "Z() θexp\n",
      " −E,( v h(1 ) , h(2 ) , h(3 ) ;) θ\n",
      " .(20.24)\n",
      "            Tosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergy\n",
      "     functionisthendeﬁnedasfollows:\n",
      " E,( v h(1 ) , h(2 ) , h(3 ) ;) = θ− vW(1 )h(1 ) − h(1 ) W(2 )h(2 ) − h(2 ) W(3 )h(3 ).\n",
      "(20.25)\n",
      "h( 1 )\n",
      "1h( 1 )\n",
      "1h( 1 )\n",
      "2h( 1 )\n",
      "2h( 1 )\n",
      "3 h( 1 )\n",
      "3\n",
      "v 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n",
      "1 h( 2 )\n",
      "1 h( 2 )\n",
      "2 h( 2 )\n",
      "2 h( 2 )\n",
      "3 h( 2 )\n",
      "3\n",
      "h( 1 )\n",
      "4h( 1 )\n",
      "4\n",
      "             Figure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer\n",
      "            (bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers.\n",
      "    Therearenointralayerconnections.\n",
      "6 6 0    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           IncomparisontotheRBMenergyfunction(equation),theDBMenergy 20.5\n",
      "          functionincludesconnectionsbetweenthehiddenunits(latentvariables)inthe\n",
      "     formoftheweightmatrices( W(2 )and W(3 )      ).Aswewillsee,theseconnections\n",
      "             havesigniﬁcantconsequencesforthemodelbehavioraswellashowwegoabout\n",
      "    performinginferenceinthemodel.\n",
      "          IncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-\n",
      "            nectedtoeveryotherunit),theDBMoﬀerssomeadvantagesthataresimilar\n",
      "             tothoseoﬀeredbytheRBM.Speciﬁcally,asillustratedinﬁgure,theDBM 20.3\n",
      "              layerscanbeorganizedintoabipartitegraph,withoddlayersononesideand\n",
      "             evenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthe\n",
      "            variablesintheevenlayer,thevariablesintheoddlayersbecomeconditionally\n",
      "             independent.Ofcourse,whenweconditiononthevariablesintheoddlayers,the\n",
      "        variablesintheevenlayersalsobecomeconditionallyindependent.\n",
      "             ThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-\n",
      "            tionswehavepreviouslyusedfortheconditionaldistributionsofanRBMto\n",
      "            determinetheconditionaldistributionsinaDBM.Theunitswithinalayerare\n",
      "          conditionallyindependentfromeachothergiventhevaluesoftheneighboring\n",
      "            layers,sothedistributionsoverbinaryvariablescanbefullydescribedbythe\n",
      "           Bernoulliparameters,givingtheprobabilityofeachunitbeingactive.Inour\n",
      "          examplewithtwohiddenlayers,theactivationprobabilitiesaregivenby\n",
      " Pv( i = 1 | h(1 )) = σ\n",
      "W(1 )\n",
      "i , : h(1 )\n",
      " , (20.26)\n",
      "h( 1 )\n",
      "1 h( 1 )\n",
      "1 h( 1 )\n",
      "2 h( 1 )\n",
      "2h( 1 )\n",
      "3h( 1 )\n",
      "3\n",
      "v 1 v 1 v 2 v 2h( 2 )\n",
      "1h( 2 )\n",
      "1h( 2 )\n",
      "2h( 2 )\n",
      "2h( 2 )\n",
      "3h( 2 )\n",
      "3h( 3 )\n",
      "1 h( 3 )\n",
      "1 h( 3 )\n",
      "2 h( 3 )\n",
      "2\n",
      "v1\n",
      "v2h( 2 )\n",
      "1 h( 2 )\n",
      "1\n",
      "h( 2 )\n",
      "2 h( 2 )\n",
      "2\n",
      "h( 2 )\n",
      "3h( 2 )\n",
      "3\n",
      "h( 1 )\n",
      "1 h( 1 )\n",
      "1\n",
      "h( 1 )\n",
      "2 h( 1 )\n",
      "2\n",
      "h( 1 )\n",
      "3 h( 1 )\n",
      "3h( 3 )\n",
      "1 h( 3 )\n",
      "1\n",
      "h( 3 )\n",
      "2 h( 3 )\n",
      "2\n",
      "            Figure20.3:AdeepBoltzmannmachine,rearrangedtorevealitsbipartitegraphstructure.\n",
      "6 6 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      " Ph((1 )\n",
      "i  = 1 |vh,(2 )) = σ\n",
      "vW(1 )\n",
      ": , i +W(2 )\n",
      "i , :h(2 )\n",
      " , (20.27)\n",
      "and\n",
      " Ph((2 )\n",
      "k = 1 |h(1 )) = σ\n",
      "h(1 ) W(2 )\n",
      ": , k\n",
      " . (20.28)\n",
      "          ThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachine\n",
      "           eﬃcient. ThenaiveapproachtoGibbssamplingistoupdateonlyonevariable\n",
      "                atatime.RBMsallowallthevisibleunitstobeupdatedinoneblockandall\n",
      "             thehiddenunitstobeupdatedinasecondblock.Onemightnaivelyassume\n",
      "   thataDBMwithl layersrequiresl      +1updates,witheachiterationupdating\n",
      "              ablockconsistingofonelayerofunits. Instead,itispossibletoupdateallthe\n",
      "             unitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksof\n",
      "            updates,oneincludingallevenlayers(includingthevisiblelayer)andtheother\n",
      "           includingalloddlayers.BecauseofthebipartiteDBMconnectionpattern,given\n",
      "              theevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbe\n",
      "          sampledsimultaneouslyandindependentlyasablock.Likewise,giventheodd\n",
      "           layers,theevenlayerscanbesampledsimultaneouslyandindependentlyasa\n",
      "          block.Eﬃcientsamplingisespeciallyimportantfortrainingwiththestochastic\n",
      "  maximumlikelihoodalgorithm.\n",
      "  20.4.1InterestingProperties\n",
      "      DeepBoltzmannmachineshavemanyinterestingproperties.\n",
      "          DBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-\n",
      "tionP(  hv|       )issimplerforDBMs. Somewhat counterintuitively,thesimplicity\n",
      "          ofthisposteriordistributionallowsricherapproximationsoftheposterior.In\n",
      "           thecaseoftheDBN,weperformclassiﬁcationusingaheuristicallymotivated\n",
      "           approximateinferenceprocedure,inwhichweguessthatareasonablevaluefor\n",
      "              themeanﬁeldexpectationofthehiddenunitscanbeprovidedbyanupwardpass\n",
      "            throughthenetworkinanMLPthatusessigmoidactivationfunctionsandthe\n",
      "       sameweightsastheoriginalDBN.AnydistributionQ(h      )canbeusedtoobtaina\n",
      "         variationallowerboundonthelog-likelihood.Thisheuristicproceduretherefore\n",
      "              enablesustoobtainsuchabound.Yettheboundisnotexplicitlyoptimizedin\n",
      "             anyway,soitmaybefarfromtight. Inparticular,theheuristicestimateofQ\n",
      "            ignoresinteractionsbetweenhiddenunitswithinthesamelayeraswellasthe\n",
      "            top-downfeedbackinﬂuenceofhiddenunitsindeeperlayersonhiddenunitsthat\n",
      "           areclosertotheinput.BecausetheheuristicMLP-basedinferenceprocedurein\n",
      "           theDBNisnotabletoaccountfortheseinteractions,theresultingQ ispresumably\n",
      "6 6 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             farfromoptimal.InDBMs,allthehiddenunitswithinalayerareconditionally\n",
      "          independentgiventheotherlayers. Thislackofintralayerinteractionmakesit\n",
      "           possibletouseﬁxed-pointequationstooptimizethevariationallowerboundand\n",
      "           ﬁndthetrueoptimalmeanﬁeldexpectations(towithinsomenumericaltolerance).\n",
      "           Theuseofpropermeanﬁeldallowstheapproximateinferenceprocedurefor\n",
      "          DBMstocapturetheinﬂuenceoftop-downfeedbackinteractions.Thismakes\n",
      "            DBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrain\n",
      "           isknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,\n",
      "          DBMshavebeenusedascomputationalmodelsofrealneuroscientiﬁcphenomena\n",
      "       ( ,; Seriesetal.2010Reichert2011etal.,).\n",
      "           OneunfortunatepropertyofDBMsisthatsamplingfromthemisrelatively\n",
      "              diﬃcult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.The\n",
      "              otherlayersareusedonlyattheendofthesamplingprocess,inoneeﬃcient\n",
      "             ancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessaryto\n",
      "             useMCMCacrossalllayers,witheverylayerofthemodelparticipatinginevery\n",
      "  Markovchaintransition.\n",
      "    20.4.2DBMMeanFieldInference\n",
      "           TheconditionaldistributionoveroneDBMlayergiventheneighboringlayersis\n",
      "            factorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributions\n",
      "areP(  vh|(1 )),P(h(1 )  |vh,(2 ) ),andP(h(2 ) |h(1 )    ).Thedistributionoverall\n",
      "           hiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers.In\n",
      "     theexamplewithtwohiddenlayers,P(h(1 ) ,h(2 ) |v     )doesnotfactorizebecauseof\n",
      "  theinteractionweightsW(2 )betweenh(1 )andh(2 )    ,whichrenderthesevariables\n",
      " mutuallydependent.\n",
      "               AswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximate\n",
      "         theDBMposteriordistribution. UnliketheDBN,however,theDBMposterior\n",
      "        distributionovertheirhiddenunits—whilecomplicated—iseasytoapproximate\n",
      "         withavariationalapproximation(asdiscussedinsection), speciﬁcallya 19.4\n",
      "           meanﬁeldapproximation. Themeanﬁeldapproximationisasimpleformof\n",
      "         variationalinference,wherewerestricttheapproximatingdistributiontofully\n",
      "           factorialdistributions.InthecontextofDBMs,themeanﬁeldequationscapture\n",
      "           thebidirectionalinteractionsbetweenlayers.Inthissectionwederivetheiterative\n",
      "        approximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton\n",
      "().2009a\n",
      "          Invariationalapproximations toinference,weapproachthetaskofapproxi-\n",
      "          matingaparticulartargetdistribution—inourcase,theposteriordistributionover\n",
      "6 6 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            thehiddenunitsgiventhevisibleunits—bysomereasonablysimplefamilyofdis-\n",
      "           tributions.Inthecaseofthemeanﬁeldapproximation,theapproximatingfamily\n",
      "           isthesetofdistributionswherethehiddenunitsareconditionallyindependent.\n",
      "            Wenowdevelopthemeanﬁeldapproachfortheexamplewithtwohidden\n",
      " layers.LetQ(h(1 ) ,h(2 ) |v    )betheapproximationofP(h(1 ) ,h(2 ) |v  ).Themean\n",
      "   ﬁeldassumptionimpliesthat\n",
      "Q(h(1 ) ,h(2 ) |v) =\n",
      "jQh((1 )\n",
      "j |v)\n",
      "kQh((2 )\n",
      "k  |v). (20.29)\n",
      "            Themeanﬁeldapproximationattemptstoﬁndamemberofthisfamilyof\n",
      "      distributionsthatbestﬁtsthetrueposteriorP(h(1 ) ,h(2 ) |v  ). Importantly,the\n",
      "          inferenceprocessmustberunagaintoﬁndadiﬀerentdistributionQ everytime\n",
      "      weuseanewvalueof.v\n",
      "         OnecanconceiveofmanywaysofmeasuringhowwellQ(  hv| )ﬁtsP(  hv|).\n",
      "      Themeanﬁeldapproachistominimize\n",
      " KL( ) =QP\n",
      "hQ(h(1 ) ,h(2 )  |v)log\n",
      "Q(h(1 ) ,h(2 ) |v)\n",
      " P(h(1 ) ,h(2 ) |v)\n",
      " . (20.30)\n",
      "             Ingeneral,wedonothavetoprovideaparametricformoftheapproximating\n",
      "       distributionbeyondenforcingtheindependenceassumptions.Thevariational\n",
      "           approximationprocedureisgenerallyabletorecoverafunctionalformofthe\n",
      "           approximatedistribution.However,inthecaseofameanﬁeldassumptionon\n",
      "              binaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgenerality\n",
      "         resultingfromﬁxingaparametrizationofthemodelinadvance.\n",
      " WeparametrizeQ         asaproductofBernoullidistributions;thatis,weassociate\n",
      "     theprobabilityofeachelementofh(1 )     withaparameter.Speciﬁcally,foreachj,\n",
      "ˆh(1 )\n",
      "j=Q(h(1 )\n",
      "j= 1 |v ),whereˆh(1 )\n",
      "j∈[0,   1],andforeachk,ˆh(2 )\n",
      "k=Q(h(2 )\n",
      "k= 1 |v),\n",
      "whereˆh(2 )\n",
      "k           ∈[01],.Thuswehavethefollowingapproximationtotheposterior:\n",
      "Q(h(1 ) ,h(2 ) |v) =\n",
      "jQh((1 )\n",
      "j |v)\n",
      "kQh((2 )\n",
      "k  |v) (20.31)\n",
      "=\n",
      "j(ˆh(1 )\n",
      "j)h( 1 )\n",
      "j (1−ˆh(1 )\n",
      "j)(1 − h( 1 )\n",
      "j )×\n",
      "k(ˆh(2 )\n",
      "k)h( 2 )\n",
      "k (1−ˆh(2 )\n",
      "k)(1 − h( 2 )\n",
      "k).\n",
      "(20.32)\n",
      "          Ofcourse,forDBMswithmorelayers,theapproximateposteriorparametrization\n",
      "             canbeextendedintheobviousway,exploitingthebipartitestructureofthegraph\n",
      "6 6 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "              toupdatealltheevenlayerssimultaneouslyandthentoupdatealltheoddlayers\n",
      "       simultaneously,followingthesamescheduleasGibbssampling.\n",
      "         NowthatwehavespeciﬁedourfamilyofapproximatingdistributionsQ ,it\n",
      "             remainstospecifyaprocedureforchoosingthememberofthisfamilythatbest\n",
      "ﬁtsP              .Themoststraightforwardwaytodothisistousethemeanﬁeldequations\n",
      "            speciﬁedbyequation.Theseequationswerederivedbysolvingforwherethe 19.56\n",
      "            derivativesofthevariationallowerboundarezero.Theydescribeinanabstract\n",
      "            mannerhowtooptimizethevariationallowerboundforanymodel,simplyby\n",
      "     takingexpectationswithrespectto.Q\n",
      "          Applyingthesegeneralequations,weobtaintheupdaterules(again,ignoring\n",
      " biasterms):\n",
      "ˆh(1 )\n",
      "j= σ\n",
      "iv iW(1 )\n",
      "i , j+\n",
      "kW(2 )\n",
      "j , kˆh(2 )\n",
      "k\n",
      "  ,j,∀ (20.33)\n",
      "ˆh(2 )\n",
      "k= σ\n",
      "\n",
      "jW(2 )\n",
      "j , kˆh(1 )\n",
      "j\n",
      "  ,k.∀ (20.34)\n",
      "               Ataﬁxedpointofthissystemofequations,wehavealocalmaximumofthe\n",
      "  variationallowerboundL(Q       ).Thustheseﬁxed-pointupdateequationsdeﬁnean\n",
      "      iterativealgorithmwherewealternateupdatesofˆh(1 )\n",
      "j   (usingequation)and 20.33\n",
      " updatesofˆh(2 )\n",
      "k          (usingequation).OnsmallproblemssuchasMNIST,asfew 20.34\n",
      "            asteniterationscanbesuﬃcienttoﬁndanapproximatepositivephasegradient\n",
      "           forlearning,andﬁftyusuallysuﬃcetoobtainahigh-qualityrepresentationof\n",
      "          asinglespeciﬁcexampletobeusedforhigh-accuracyclassiﬁcation.Extending\n",
      "       approximatevariationalinferencetodeeperDBMsisstraightforward.\n",
      "   20.4.3DBMParameterLearning\n",
      "            LearningintheDBMmustconfrontboththechallengeofanintractablepartition\n",
      "            function,usingthetechniquesfromchapter,andthechallengeofanintractable 18\n",
      "       posteriordistribution,usingthetechniquesfromchapter.19\n",
      "          Asdescribedinsection ,variationalinferenceallowstheconstructionof 20.4.2\n",
      " adistributionQ(  hv|    )thatapproximatestheintractableP(  hv|  ).Learningthen\n",
      "  proceedsbymaximizingL(  vθ,Q,       ),thevariationallowerboundontheintractable\n",
      "    log-likelihood, . log(;)Pvθ\n",
      "6 6 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            ForadeepBoltzmannmachinewithtwohiddenlayers,isgivenbyL\n",
      " L( ) =Q, θ\n",
      "i\n",
      "jv iW(1 )\n",
      "i , jˆh(1 )\n",
      "j+\n",
      "j\n",
      "kˆh(1 )\n",
      "jW(2 )\n",
      "j , kˆh(2 )\n",
      "k     −H log()+Z θ ()Q.(20.35)\n",
      "       Thisexpressionstillcontainsthelogpartitionfunction, logZ( θ   ).Becauseadeep\n",
      "        BoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,the\n",
      "           hardnessresultsforcomputingthepartitionfunctionandsamplingthatapplyto\n",
      "          restrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.Thismeans\n",
      "          thatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequires\n",
      "        approximatemethodssuchasannealedimportancesampling.Likewise,training\n",
      "            themodelrequiresapproximationstothegradientofthelogpartitionfunction.See\n",
      "            chapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained 18\n",
      "          usingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedin\n",
      "          chapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe 18\n",
      "          abilitytoevaluatetheunnormalizedprobabilities,ratherthanmerelyobtaina\n",
      "           variationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmann\n",
      "             machinesbecausetheydonotalloweﬃcientsamplingofthehiddenunitsgiventhe\n",
      "         visibleunits—instead,contrastivedivergencewouldrequireburninginaMarkov\n",
      "         chaineverytimeanewnegativephasesampleisneeded.\n",
      "         Thenonvariationalversionofthestochasticmaximumlikelihoodalgorithmis\n",
      "          discussedinsection.Variationalstochasticmaximumlikelihoodasappliedto 18.2\n",
      "             theDBMisgiveninalgorithm.Recallthatwedescribeasimpliﬁedvariant 20.1\n",
      "          oftheDBMthatlacksbiasparameters;includingthemistrivial.\n",
      "  20.4.4Layer-WisePretraining\n",
      "         Unfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribed\n",
      "            above)fromarandominitializationusuallyresultsinfailure.Insomecases,the\n",
      "            modelfailstolearntorepresentthedistributionadequately.Inothercases,the\n",
      "            DBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancould\n",
      "                beobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheﬁrst\n",
      "        layerrepresentsapproximatelythesamedistributionasanRBM.\n",
      "          Varioustechniquesthatpermitjointtraininghavebeendevelopedandare\n",
      "           describedinsection .However,theoriginalandmostpopularmethodfor 20.4.5\n",
      "          overcomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining.\n",
      "               Inthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.The\n",
      "            ﬁrstlayeristrainedtomodeltheinputdata. EachsubsequentRBMistrained\n",
      "           tomodelsamplesfromthepreviousRBM’sposteriordistribution.Afterallthe\n",
      "6 6 6    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      " Algorithm20.1      Thevariationalstochasticmaximumlikelihoodalgorithmfor\n",
      "      trainingaDBMwithtwohiddenlayers\n",
      "         Set,thestepsize,toasmallpositivenumber \n",
      "Setk             ,thenumberofGibbssteps,highenoughtoallowaMarkovchainof\n",
      "p( vh,(1 ) ,h(2 );θ+∆ θ       )toburnin,startingfromsamplesfromp( vh,(1 ) ,h(2 );θ).\n",
      "  Initializethreematrices,˜V,˜H(1 ) ,and˜H(2 ) eachwithm   rowssettorandom\n",
      "         values(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedto\n",
      "  themodel’smarginals).\n",
      "     whilenotconverged(learningloop)do\n",
      "   Sampleaminibatchofm        examplesfromthetrainingdataandarrangethem\n",
      "        astherowsofadesignmatrix.V\n",
      " InitializematricesˆH(1 )andˆH(2 )     ,possiblytothemodel’smarginals.\n",
      "       whilenotconverged(meanﬁeldinferenceloop)do\n",
      "ˆH(1 ) ←σ\n",
      " VW(1 )+ˆH(2 )W(2 ) \n",
      ".\n",
      "ˆH(2 ) ←σ\n",
      "ˆH(1 )W(2 )\n",
      ".\n",
      " endwhile\n",
      "∆W( 1 )←1\n",
      "mVˆH(1 )\n",
      "∆W( 2 )←1\n",
      "mˆH (1 ) ˆH(2 )\n",
      "       for do lk = 1to(Gibbssampling)\n",
      "  Gibbsblock1:\n",
      " ∀i,j,˜V i , j    sampledfromP(˜V i , j= 1) = σ\n",
      "W(1 )\n",
      "j , :\n",
      "˜H(1 )\n",
      "i , :\n",
      ".\n",
      " ∀i,j,˜H(2 )\n",
      "i , j    sampledfromP(˜H(2 )\n",
      "i , j= 1) = σ\n",
      "˜H(1 )\n",
      "i , :W(2 )\n",
      ": , j\n",
      ".\n",
      "  Gibbsblock2:\n",
      " ∀i,j,˜H(1 )\n",
      "i , j    sampledfromP(˜H(1 )\n",
      "i , j= 1) = σ\n",
      "˜V i , :W(1 )\n",
      ": , j+˜H(2 )\n",
      "i , :W(2 ) \n",
      "j , :\n",
      ".\n",
      " endfor\n",
      "∆W( 1 ) ←∆W( 1 )−1\n",
      "mV˜H(1 )\n",
      "∆W( 2 ) ←∆W( 2 )−1\n",
      "m˜H(1 ) ˜H(2 )\n",
      "W(1 ) ←W(1 )+∆W( 1 )         (thisisacartoonillustration,inpracticeuseamore\n",
      "         eﬀectivealgorithm,suchasmomentumwithadecayinglearningrate)\n",
      "W(2 ) ←W(2 ) +∆W( 2 )\n",
      " endwhile\n",
      "               RBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.The\n",
      "             DBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlya\n",
      "             smallchangeinthemodel’sparametersandinitsperformanceasmeasuredbythe\n",
      "6 6 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "              log-likelihooditassignstothedata,oritsabilitytoclassifyinputs.Seeﬁgure20.4\n",
      "      foranillustrationofthetrainingprocedure.\n",
      "           Thisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbears\n",
      "           somepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetof\n",
      "            theparametersateachstep.Thetwomethodsdiﬀerbecausethegreedylayer-wise\n",
      "         trainingprocedureusesadiﬀerentobjectivefunctionateachstep.\n",
      "          Greedylayer-wisepretrainingofaDBMdiﬀersfromgreedylayer-wisepre-\n",
      "             trainingofaDBN.TheparametersofeachindividualRBMmaybecopiedto\n",
      "            thecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparameters\n",
      "               mustbemodiﬁedbeforeinclusionintheDBM.Alayerinthemiddleofthestack\n",
      "             ofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombined\n",
      "            toformtheDBM,thelayerwillhavebothbottom-upandtop-downinput. To\n",
      "          accountforthiseﬀect,SalakhutdinovandHinton2009a()advocatedividingthe\n",
      "               weightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintothe\n",
      "            DBM.Additionally,thebottomRBMmustbetrainedusingtwo“copies”ofeach\n",
      "              visibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeans\n",
      "            thattheweightsareeﬀectivelydoubledduringtheupwardpass.Similarly,thetop\n",
      "          RBMshouldbetrainedwithtwocopiesofthetopmostlayer.\n",
      "            ObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequires\n",
      "              amodiﬁcationofthestandardSMLalgorithm,whichistouseasmallamountof\n",
      "            meanﬁeldduringthenegativephaseofthejointPCDtrainingstep(Salakhutdinov\n",
      "         andHinton2009a,). Speciﬁcally,theexpectationoftheenergygradientshould\n",
      "             becomputedwithrespecttothemeanﬁelddistributioninwhichalltheunits\n",
      "           areindependentfromeachother.Theparametersofthismeanﬁelddistribution\n",
      "            shouldbeobtainedbyrunningthemeanﬁeldﬁxed-pointequationsforjustone\n",
      "             step.See ()foracomparisonoftheperformanceofcentered Goodfellowetal.2013b\n",
      "             DBMswithandwithouttheuseofpartialmeanﬁeldinthenegativephase.\n",
      "     20.4.5JointlyTrainingDeepBoltzmannMachines\n",
      "         ClassicDBMsrequiregreedyunsupervisedpretrainingand,toperformclassiﬁcation\n",
      "            well,requireaseparateMLP-basedclassiﬁerontopofthehiddenfeaturesthey\n",
      "           extract.Thishassomeundesirableproperties.Itishardtotrackperformance\n",
      "           duringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhile\n",
      "             trainingtheﬁrstRBM.Thus,itishardtotellhowwellourhyperparameters\n",
      "          areworkinguntilquitelateinthetrainingprocess.Softwareimplementations\n",
      "            ofDBMsneedtohavemanydiﬀerentcomponentsforCDtrainingofindividual\n",
      "           RBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagation\n",
      "6 6 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             throughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmany\n",
      "           oftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeing\n",
      "         abletoperforminferencewhensomeinputvaluesaremissing.\n",
      "             Therearetwomainwaystoresolvethejointtrainingproblemofthedeep\n",
      "d ) a) b )\n",
      "c )\n",
      "            Figure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNIST\n",
      "            dataset(SalakhutdinovandHinton2009aSrivastava2014 ,; etal.,).TrainanRBMby (a)\n",
      "    usingCDtoapproximatelymaximize  logP( v      ).TrainasecondRBMthatmodels (b) h( 1 )\n",
      "  andtargetclassy  byusingCD-k  toapproximatelymaximize  logP( h( 1 ) ,y ),where h( 1 )\n",
      "          isdrawnfromtheﬁrstRBM’sposteriorconditionedonthedata. Increasek  from1to\n",
      "             20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately (c)\n",
      "maximize  logP( v,y     )usingstochasticmaximumlikelihoodwithk = 5.Delete(d)yfrom\n",
      "       themodel.Deﬁneanewsetoffeatures h( 1 )and h( 2 )     thatareobtainedbyrunningmean\n",
      "     ﬁeldinferenceinthemodellackingy         .UsethesefeaturesasinputtoanMLPwhose\n",
      "               structureisthesameasanadditionalpassofmeanﬁeld,withanadditionaloutputlayer\n",
      "   fortheestimateofy            .InitializetheMLP’sweightstobethesameastheDBM’sweights.\n",
      "     TraintheMLPtoapproximatelymaximize  logP(  y| v    )usingstochasticgradientdescent\n",
      "        anddropout.FigurereprintedfromGoodfellow 2013betal.().\n",
      "6 6 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "    Boltzmannmachine.Theﬁrstis the   centereddeepBoltzmann machine\n",
      "           (MontavonandMuller2012,),whichreparametrizesthemodelinordertomake\n",
      "             theHessianofthecostfunctionbetterconditionedatthebeginningofthelearning\n",
      "            process.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wise\n",
      "         pretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihood\n",
      "         andproduceshigh-qualitysamples.Unfortunately,itremainsunabletocompete\n",
      "           withappropriatelyregularizedMLPsasaclassiﬁer.Thesecondwaytojointly\n",
      "        trainadeepBoltzmannmachineistousea   multi-predictiondeepBoltzmann\n",
      "machine          (Goodfellow 2013betal.,).Thismodelusesanalternativetraining\n",
      "      criterion thatallowsthe useof theback-propagationalgorithm toavoid the\n",
      "          problemswithMCMCestimatesofthegradient.Unfortunately,thenewcriterion\n",
      "             doesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMCapproach,\n",
      "            itdoesleadtosuperiorclassiﬁcationperformanceandabilitytoreasonwellabout\n",
      " missinginputs.\n",
      "            ThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwe\n",
      "               returntothegeneralviewofaBoltzmannmachineasconsistingofasetofunits\n",
      "x   withaweightmatrixU andbiasesb       .Recallfromequationthattheenergy 20.2\n",
      "   functionisgivenby\n",
      "E() = x−x  Uxb− x. (20.36)\n",
      "   Using diﬀerent sparsity patternsin theweightmatrixU , wecan implement\n",
      "           structuresofBoltzmannmachines,suchasRBMsorDBMswithdiﬀerentnumbers\n",
      "      oflayers.Thisisaccomplishedbypartitioningx     intovisibleandhiddenunitsand\n",
      "   zeroingoutelementsofU        forunitsthatdonotinteract.ThecenteredBoltzmann\n",
      "           machineintroducesavectorthatissubtractedfromallthestates: µ\n",
      "E    (; ) = ( ) xUb,−xµ−      Uxµxµ (−)(−−) b. (20.37)\n",
      "Typicallyµ           isahyperparameterﬁxedatthebeginningoftraining.Itisusu-\n",
      "     allychosentomakesurethat    xµ−≈ 0     whenthemodelisinitialized.This\n",
      "          reparametrizationdoesnotchangethesetofprobabilitydistributionsthatthe\n",
      "            modelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescent\n",
      "          appliedtothelikelihood.Speciﬁcally,inmanycases,thisreparametrizationresults\n",
      "            inaHessianmatrixthatisbetterconditioned. ()experimentally Melchioretal.2013\n",
      "           conﬁrmedthattheconditioningoftheHessianmatriximproves,andobservedthat\n",
      "          thecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,\n",
      "the  enhancedgradient        ( ,).Theimprovedconditioningofthe Choetal.2011\n",
      "            Hessianmatrixenableslearningtosucceed,evenindiﬃcultcasesliketraininga\n",
      "     deepBoltzmannmachinewithmultiplelayers.\n",
      "           TheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-\n",
      "          predictiondeepBoltzmannmachine(MP-DBM),whichworksbyviewingthemean\n",
      "6 7 0    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           ﬁeldequationsasdeﬁningafamilyofrecurrentnetworksforapproximatelysolving\n",
      "          everypossibleinferenceproblem( ,).Ratherthantraining Goodfellowetal.2013b\n",
      "             themodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrent\n",
      "          networkobtainanaccurateanswertothecorrespondinginferenceproblem.The\n",
      "           trainingprocessisillustratedinﬁgure. Itconsistsofrandomlysamplinga 20.5\n",
      "           trainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,\n",
      "            andthentrainingtheinferencenetworktopredictthevaluesoftheremaining\n",
      "units.\n",
      "        Thisgeneralprincipleofback-propagatingthroughthecomputationalgraph\n",
      "            forapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011etal.,;\n",
      "               Brakel2013etal.,).InthesemodelsandintheMP-DBM,theﬁnallossisnot\n",
      "             thelowerboundonthelikelihood.Instead,theﬁnallossistypicallybasedon\n",
      "        theapproximateconditionaldistributionthattheapproximateinferencenetwork\n",
      "            imposesoverthemissingvalues.Thismeansthatthetrainingofthesemodels\n",
      "       issomewhatheuristicallymotivated.Ifweinspectthe p( v   )representedbythe\n",
      "           BoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,\n",
      "        inthesensethatGibbssamplingyieldspoorsamples.\n",
      "         Back-propagationthroughtheinferencegraphhastwomainadvantages.First,\n",
      "            ittrainsthemodelasitisreallyused—withapproximateinference.Thismeans\n",
      "            thatapproximateinference,forexample,toﬁllinmissinginputsortoperform\n",
      "            classiﬁcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-\n",
      "             DBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurate\n",
      "            classiﬁeronitsown;thebestclassiﬁcationresultswiththeoriginalDBMwere\n",
      "            basedontrainingaseparateclassiﬁertousefeaturesextractedbytheDBM,\n",
      "             ratherthanbyusinginferenceintheDBMtocomputethedistributionoverthe\n",
      "            classlabels.MeanﬁeldinferenceintheMP-DBMperformswellasaclassiﬁer\n",
      "        withoutspecialmodiﬁcations.Theotheradvantageofback-propagatingthrough\n",
      "         approximateinferenceisthatback-propagationcomputestheexactgradientof\n",
      "            theloss.ThisisbetterforoptimizationthantheapproximategradientsofSML\n",
      "            training,whichsuﬀerfrombothbiasandvariance.ThisprobablyexplainswhyMP-\n",
      "           DBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining.\n",
      "         Thedisadvantageofback-propagatingthroughtheapproximateinferencegraphis\n",
      "              thatitdoesnotprovideawaytooptimizethelog-likelihood,butrathergivesa\n",
      "     heuristicapproximationofthegeneralizedpseudolikelihood.\n",
      "    TheMP-DBMinspiredtheNADE- k       (Raiko 2014etal.,)extensiontothe\n",
      "       NADEframework,whichisdescribedinsection .20.10.10\n",
      "           TheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-\n",
      "         rametersamongmanydiﬀerentcomputationalgraphs,withthediﬀerencebetween\n",
      "6 7 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            Figure20.5:AnillustrationofthemultipredictiontrainingprocessforadeepBoltzmann\n",
      "             machine.Eachrowindicatesadiﬀerentexamplewithinaminibatchforthesametraining\n",
      "           step. Eachcolumnrepresentsatimestepwithinthemeanﬁeldinferenceprocess. For\n",
      "                eachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinference\n",
      "            process.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthe\n",
      "           meanﬁeldinferenceprocess,witharrowsindicatingwhichvariablesinﬂuencewhichother\n",
      "             variablesintheprocess.Inpracticalapplications,weunrollmeanﬁeldforseveralsteps.\n",
      "              Inthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocess\n",
      "                couldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstothe\n",
      "             inferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessfor\n",
      "            eachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationto\n",
      "            traintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.This\n",
      "            trainsthemeanﬁeldprocessfortheMP-DBMtoproduceaccurateestimates.Figure\n",
      "     adaptedfrom (). Goodfellowetal.2013b\n",
      "6 7 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            eachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalso\n",
      "           sharesparametersacrossmanycomputationalgraphs.InthecaseoftheMP-DBM,\n",
      "             thediﬀerencebetweenthegraphsiswhethereachinputunitisobservedornot.\n",
      "              Whenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropout\n",
      "              does.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.Onecould\n",
      "          imagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunits\n",
      "    ratherthanmakingthemlatent.\n",
      "     20.5BoltzmannMachinesforReal-ValuedData\n",
      "          WhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,\n",
      "            manyapplicationssuchasimageandaudiomodelingseemtorequiretheability\n",
      "            torepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossible\n",
      "              totreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofa\n",
      "           binaryvariable.Forexample,Hinton2000()treatsgrayscaleimagesinthetraining\n",
      "            setasdeﬁning[0,1]probabilityvalues.Eachpixeldeﬁnestheprobabilityofa\n",
      "             binaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfromeach\n",
      "            other.Thisisacommonprocedureforevaluatingbinarymodelsongrayscaleimage\n",
      "         datasets.Nonetheless,itisnotaparticularlytheoreticallysatisfyingapproach,\n",
      "            andbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.In\n",
      "           thissection,wepresentBoltzmannmachinesthatdeﬁneaprobabilitydensityover\n",
      " real-valueddata.\n",
      "  20.5.1Gaussian-BernoulliRBMs\n",
      "         RestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamily\n",
      "            conditionaldistributions(Welling2005etal.,).Ofthese,themostcommonisthe\n",
      "           RBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditional\n",
      "            distributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisa\n",
      "    functionofthehiddenunits.\n",
      "         TherearemanywaysofparametrizingGaussian-BernoulliRBMs.Onechoice\n",
      "             iswhethertouseacovariancematrixoraprecisionmatrixfortheGaussian\n",
      "          distribution.Herewepresenttheprecisionformulation.Themodiﬁcationtoobtain\n",
      "         thecovarianceformulationisstraightforward. Wewishtohavetheconditional\n",
      "distribution\n",
      "     p , ( ) = (; vh|NvWhβ− 1 ). (20.38)\n",
      "               Wecanﬁndthetermsweneedtoaddtotheenergyfunctionbyexpandingthe\n",
      "6 7 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "   unnormalizedlogconditionaldistribution:\n",
      "    log (;NvWhβ,− 1) = −1\n",
      "2   ( )vWh−       βvWhβ (− )+(f).(20.39)\n",
      "Heref            encapsulatesallthetermsthatareafunctiononlyoftheparameters\n",
      "          andnottherandomvariablesinthemodel.Wecandiscardf   becauseitsonly\n",
      "            roleistonormalizethedistribution,andthepartitionfunctionofwhateverenergy\n",
      "       functionwechoosewillcarryoutthatrole.\n",
      "          Ifweincludealltheterms(withtheirsignﬂipped)involvingv fromequa-\n",
      "             tion inourenergyfunctionanddonotaddanyothertermsinvolving 20.39 v ,then\n",
      "          ourenergyfunctionwillrepresentthedesiredconditional .p( )vh|\n",
      "        Wehavesomefreedomregardingtheotherconditionaldistribution,p(  hv|).\n",
      "      Notethatequation containsaterm 20.39\n",
      "1\n",
      "2hW  βWh. (20.40)\n",
      "          Thistermcannotbeincludedinitsentiretybecauseitincludesh ih j terms.These\n",
      "            correspondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,we\n",
      "            wouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.When\n",
      "       designingourBoltzmannmachine,wesimplyomittheseh ih j  crossterms.Omitting\n",
      "     themdoesnotchangetheconditionalp(  vh|      ),soequation isstillrespected. 20.39\n",
      "             Westillhaveachoice,however,aboutwhethertoincludethetermsinvolvingonly\n",
      " asingleh i             .Ifweassumeadiagonalprecisionmatrix,weﬁndthatforeachhidden\n",
      " unith i    ,wehaveaterm\n",
      "1\n",
      "2h i\n",
      "jβ jW2\n",
      "j , i . (20.41)\n",
      "       Intheabove,weusedthefactthath2\n",
      "i=h ibecauseh i ∈{0,1}    .Ifweincludethis\n",
      "             term(withitssignﬂipped)intheenergyfunction,thenitwillnaturallybiash i\n",
      "               tobeturnedoﬀwhentheweightsforthatunitarelargeandconnectedtovisible\n",
      "            unitswithhighprecision. Thechoiceofwhethertoincludethisbiastermdoes\n",
      "            notaﬀectthefamilyofdistributionsthatthemodelcanrepresent(assumingthat\n",
      "             weincludebiasparametersforthehiddenunits),butitdoesaﬀectthelearning\n",
      "            dynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivations\n",
      "         remainreasonableevenwhentheweightsrapidlyincreaseinmagnitude.\n",
      "            OnewaytodeﬁnetheenergyfunctiononaGaussian-BernoulliRBMisthus:\n",
      " E,(vh) =1\n",
      "2v      ( )( ) βv−vβ   Whb− h, (20.42)\n",
      "6 7 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "               butwemayalsoaddextratermsorparametrizetheenergyintermsofthevariance\n",
      "     ratherthanprecisionifwechoose.\n",
      "               Inthisderivation,wehavenotincludedabiastermonthevisibleunits,butone\n",
      "             couldeasilybeadded.Oneﬁnalsourceofvariabilityintheparametrizationofa\n",
      "             Gaussian-BernoulliRBMisthechoiceofhowtotreattheprecisionmatrix.Itmay\n",
      "            beeitherﬁxedtoaconstant(perhapsestimatedbasedonthemarginalprecision\n",
      "                ofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,orit\n",
      "              maybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobe\n",
      "          nondiagonalinthiscontext,becausesomeoperationsontheGaussiandistribution\n",
      "            requireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.In\n",
      "            thesectionsahead,wewillseethatotherformsofBoltzmannmachinespermit\n",
      "          modelingthecovariancestructure,usingvarioustechniquestoavoidinvertingthe\n",
      " precisionmatrix.\n",
      "     20.5.2UndirectedModelsofConditionalCovariance\n",
      "           WhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valued\n",
      "             data, ()arguethattheGaussianRBMinductivebiasisnot Ranzatoetal.2010a\n",
      "            wellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,\n",
      "           especiallynaturalimages.Theproblemisthatmuchoftheinformationcontent\n",
      "            presentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthan\n",
      "              intherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsand\n",
      "            nottheirabsolutevalueswheremostoftheusefulinformationinimagesresides.\n",
      "             SincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhe\n",
      "         hiddenunits,itcannotcaptureconditionalcovarianceinformation.Inresponse\n",
      "           tothesecriticisms,alternativemodelshavebeenproposedthatattempttobetter\n",
      "            accountforthecovarianceofreal-valueddata.Thesemodelsincludethemeanand\n",
      "  covarianceRBM(mcRBM1     ),themeanproductofStudent t  -distribution(mPoT)\n",
      "       model,andthespikeandslabRBM(ssRBM).\n",
      "   MeanandCovarianceRBM        ThemcRBMusesitshiddenunitstoindepen-\n",
      "           dentlyencodetheconditionalmeanandcovarianceofallobservedunits.The\n",
      "             mcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovariance\n",
      "            units.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM.\n",
      "              TheotherhalfisacovarianceRBM( ,),alsocalledacRBM, Ranzatoetal.2010a\n",
      "         whosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow.\n",
      "1              Theterm“mcRBM”ispronouncedbysayingthenameofthelettersM-C-R-B-M;the“mc”\n",
      "       isnotpronouncedlikethe“Mc”in“McDonald’s.”\n",
      "675    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "    Speciﬁcally,withbinarymeanunitsh( ) m   andbinarycovarianceunitsh( ) c ,the\n",
      "          mcRBMmodelisdeﬁnedasthecombinationoftwoenergyfunctions:\n",
      "E m c (xh,( ) m ,h( ) c) = E m (xh,( ) m  )+E c (xh,( ) c ), (20.43)\n",
      " whereE m      isthestandardGaussian-BernoulliRBMenergyfunction,2\n",
      "E m (xh,( ) m) =1\n",
      "2x x−\n",
      "jxW : , jh( ) m\n",
      "j−\n",
      "jb( ) m\n",
      "jh( ) m\n",
      "j , (20.44)\n",
      "andE c      isthe cRBMenergyfunctionthat models theconditionalcovariance\n",
      "information:\n",
      "E c (xh,( ) c) =1\n",
      "2\n",
      "jh( ) c\n",
      "j\n",
      "xr( ) j2\n",
      "−\n",
      "jb( ) c\n",
      "jh( ) c\n",
      "j . (20.45)\n",
      " Theparameterr( ) j       correspondstothecovarianceweightvectorassociatedwith\n",
      "h( ) c\n",
      "j ,andb( ) c          isavectorofcovarianceoﬀsets.Thecombinedenergyfunctiondeﬁnes\n",
      "  ajointdistribution,\n",
      "p m c (xh,( ) m ,h( ) c) =1\n",
      "Zexp\n",
      "−E m c (xh,( ) m ,h( ) c)\n",
      " , (20.46)\n",
      "        andacorrespondingconditionaldistributionovertheobservationsgivenh( ) mand\n",
      "h( ) c    asamultivariateGaussiandistribution:\n",
      "p m c  (xh|( ) m ,h( ) c) = N\n",
      " xC;m c\n",
      "x h|\n",
      "\n",
      "jW : , jh( ) m\n",
      "j\n",
      " ,Cm c\n",
      "x h|\n",
      " .(20.47)\n",
      "    NotethatthecovariancematrixCm c\n",
      "x h|=\n",
      "jh( ) c\n",
      "jr( ) jr( ) j +I− 1\n",
      " isnondiagonal\n",
      " andthatW          istheweightmatrixassociatedwiththeGaussianRBMmodelingthe\n",
      "            conditionalmeans.ItisdiﬃculttotrainthemcRBMviacontrastivedivergenceor\n",
      "        persistentcontrastivedivergencebecauseofitsnondiagonalconditionalcovariance\n",
      "          structure.CDandPCDrequiresamplingfromthejointdistributionof xh,( ) m ,h( ) c,\n",
      "            which,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals.\n",
      "     However,inthemcRBM,samplingfromp m c(  xh|( ) m ,h( ) c  )requirescomputing\n",
      "(Cm c)− 1          ateveryiterationoflearning.Thiscanbeanimpracticalcomputational\n",
      "          burdenforlargerobservations.RanzatoandHinton2010()avoiddirectsampling\n",
      "  fromtheconditionalp m c(  xh|( ) m ,h( ) c      )bysamplingdirectlyfromthemarginal\n",
      "p(x           )usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfree Neal1993\n",
      "energy.\n",
      "2            ThisversionoftheGaussian-BernoulliRBMenergyfunctionassumestheimagedatahave\n",
      "                 zeromeanperpixel.Pixeloﬀsetscaneasilybeaddedtothemodeltoaccountfornonzeropixel\n",
      "means.\n",
      "676    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "    MeanProductofStudent-distributions t     ThemeanproductofStudentt-\n",
      "           distribution(mPoT)model( ,)extendsthePoTmodel( Ranzatoetal.2010b Welling\n",
      "              etal.,)inamannersimilartohowthemcRBMextendsthecRBM.This 2003a\n",
      "           isachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussian\n",
      "           RBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionoverthe\n",
      "        observationisamultivariateGaussian(withnondiagonalcovariance)distribution;\n",
      "         however,unlikethemcRBM,thecomplementaryconditionaldistributionoverthe\n",
      "         hiddenvariablesisgivenbyconditionallyindependentGammadistributions.The\n",
      " GammadistributionG( k,θ       ) isaprobabilitydistributionoverpositiverealnumbers,\n",
      " withmeankθ            .Itisnotnecessarytohaveamoredetailedunderstandingofthe\n",
      "          GammadistributiontounderstandthebasicideasunderlyingthemPoTmodel.\n",
      "    ThemPoTenergyfunctionis\n",
      "E m P o T (xh,( ) m ,h( ) c ) (20.48)\n",
      "= E m (xh,( ) m )+\n",
      "j\n",
      "h( ) c\n",
      "j\n",
      " 1+1\n",
      "2\n",
      "r( ) j x2\n",
      "   +(1−γ j  )logh( ) c\n",
      "j\n",
      ",\n",
      "(20.49)\n",
      "wherer( ) j       isthecovarianceweightvectorassociatedwithunith( ) c\n",
      "j ,andE m( xh,( ) m)\n",
      "     isasdeﬁnedinequation.20.44\n",
      "            JustaswiththemcRBM,themPoTmodelenergyfunctionspeciﬁesamul-\n",
      "      tivariateGaussian,withaconditionaldistributionoverx thathasnondiagonal\n",
      "     covariance.Learning inthe mPoT model—again,likethe mcRBM—iscom-\n",
      "          plicatedbytheinabilitytosamplefromthenondiagonalGaussianconditional\n",
      "p m P o T(  xh|( ) m ,h( ) c          ),so ()alsoadvocatedirectsamplingof Ranzatoetal.2010b\n",
      "     p()xviaHamiltonian(hybrid)MonteCarlo.\n",
      "     SpikeandSlabRestrictedBoltzmannMachines    Spikeandslabrestricted\n",
      "          Boltzmannmachines( ,)orssRBMsprovideanothermeans Courvilleetal.2011\n",
      "          ofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,\n",
      "          ssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonian\n",
      "            MonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM’sbinary\n",
      "           hiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseof\n",
      "  auxiliaryreal-valuedvariables.\n",
      "           ThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunits h\n",
      " andreal-valuedslabunits s         .Themeanofthevisibleunitsconditionedonthe\n",
      "     hiddenunitsisgivenby(  hs)W     .Inotherwords,eachcolumnW : , i deﬁnesa\n",
      "       componentthatcanappearintheinputwhenh i   = 1.Thecorrespondingspike\n",
      "6 7 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "variableh i          determineswhetherthatcomponentispresentatall.Thecorresponding\n",
      " slabvariables i           determinestheintensityofthatcomponent,ifitispresent.When\n",
      "            aspikevariableisactive,thecorrespondingslabvariableaddsvariancetothe\n",
      "     inputalongtheaxisdeﬁnedbyW : , i         .Thisallowsustomodelthecovarianceofthe\n",
      "       inputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergence\n",
      "             withGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix.\n",
      "         Formally,thessRBMmodelisdeﬁnedviaitsenergyfunction:\n",
      "E s s   ( ) =xsh,,−\n",
      "ixW : , is ih i+1\n",
      "2x\n",
      " Λ+\n",
      "iΦ ih i\n",
      " x (20.50)\n",
      "+1\n",
      "2\n",
      "iα is2\n",
      "i−\n",
      "iα iµ is ih i−\n",
      "ib ih i+\n",
      "iα iµ2\n",
      "ih i ,(20.51)\n",
      "whereb i     istheoﬀsetofthespikeh i ,andΛ      isadiagonalprecisionmatrixonthe\n",
      "observationsx  .Theparameterα i>       0isascalarprecisionparameterforthe\n",
      "  real-valuedslabvariables i  .TheparameterΦ i    isanonnegativediagonalmatrix\n",
      "  thatdeﬁnesanh    -modulatedquadraticpenaltyonx .Eachµ i   isameanparameter\n",
      "    fortheslabvariables i.\n",
      "           Withthejointdistributiondeﬁnedviatheenergyfunction,derivingthessRBM\n",
      "        conditionaldistributionsisrelativelystraightforward.Forexample,bymarginal-\n",
      "    izingouttheslabvariabless      ,theconditionaldistributionovertheobservations\n",
      "        giventhebinaryspikevariablesisgivenbyh\n",
      "p s s   ( )=xh|1\n",
      " P()h1\n",
      "Z\n",
      "     exp ( ){−Exsh,,}ds (20.52)\n",
      " =N\n",
      " xC;s s\n",
      "x h|\n",
      "iW : , iµ ih i ,Cs s\n",
      "x h|\n",
      "(20.53)\n",
      "whereCs s\n",
      "x h|=\n",
      " Λ+\n",
      "iΦ ih i−\n",
      "iα− 1\n",
      "ih iW : , iW\n",
      ": , i− 1      .Thelastequalityholdsonlyif\n",
      "   thecovariancematrixCs s\n",
      "x h|  ispositivedeﬁnite.\n",
      "           Gatingbythespikevariablesmeansthatthetruemarginaldistributionover\n",
      " hs            issparse.Thisisdiﬀerentfromsparsecoding,wheresamplesfromthemodel\n",
      "             “almostnever”(inthemeasuretheoreticsense)containzerosinthecode,andMAP\n",
      "     inferenceisrequiredtoimposesparsity.\n",
      "           ComparingthessRBMtothemcRBMandthemPoTmodels,thessRBM\n",
      "          parametrizestheconditionalcovarianceoftheobservationinasigniﬁcantlydiﬀerent\n",
      "            way.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservation\n",
      "as\n",
      "jh( ) c\n",
      "jr( ) jr( ) j +I− 1\n",
      "       ,usingtheactivationofthehiddenunitsh j> 0to\n",
      "6 7 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "        enforceconstraintsontheconditionalcovarianceinthedirection r( ) j  .Incontrast,\n",
      "           thessRBMspeciﬁestheconditionalcovarianceoftheobservationsusingthehidden\n",
      " spikeactivations h i          = 1topinchtheprecisionmatrixalongthedirectionspeciﬁed\n",
      "           bythecorrespondingweightvector.ThessRBMconditionalcovarianceissimilarto\n",
      "           thatgivenbyadiﬀerentmodel:theproductofprobabilisticprincipalcomponents\n",
      "         analysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercompletesetting,\n",
      "        sparseactivationswiththessRBMparametrizationpermitsigniﬁcantvariance\n",
      "     (abovethenominalvariancegivenby Λ− 1      )onlyintheselecteddirectionsof\n",
      "  thesparselyactivated h i       .InthemcRBMormPoTmodels, anovercomplete\n",
      "            representationwouldmeanthattocapturevariationinaparticulardirectioninthe\n",
      "         observationspacewouldrequireremovingpotentiallyallconstraintswithpositive\n",
      "           projectioninthatdirection. Thiswouldsuggestthatthesemodelsarelesswell\n",
      "    suitedtotheovercompletesetting.\n",
      "          TheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachine\n",
      "            isthatsomesettingsoftheparameterscancorrespondtoacovariancematrix\n",
      "           thatisnotpositivedeﬁnite.Suchacovariancematrixplacesmoreunnormalized\n",
      "            probabilityonvaluesthatarefartherfromthemean,causingtheintegralover\n",
      "            allpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimple\n",
      "          heuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Using\n",
      "          constrainedoptimizationtoexplicitlyavoidtheregionswheretheprobabilityis\n",
      "           undeﬁnedisdiﬃculttodowithoutbeingoverlyconservativeandalsopreventing\n",
      "        themodelfromaccessinghigh-performingregionsofparameterspace.\n",
      "        Qualitatively,convolutionalvariantsofthessRBMproduceexcellentsamples\n",
      "         ofnaturalimages.Someexamplesareshowninﬁgure.16.1\n",
      "        ThessRBMallowsforseveralextensions.Includinghigher-orderinteractions\n",
      "            andaverage-poolingoftheslabvariables( ,)enablesthemodel Courvilleetal.2014\n",
      "            tolearnexcellentfeaturesforaclassiﬁerwhenlabeleddataisscarce. Addinga\n",
      "           termtotheenergyfunctionthatpreventsthepartitionfunctionfrombecoming\n",
      "            undeﬁnedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellow\n",
      "      etal.,),alsoknownasS3C. 2013d\n",
      "   20.6ConvolutionalBoltzmannMachines\n",
      "            Aswediscussinchapter,extremelyhigh-dimensionalinputssuchasimagesplace 9\n",
      "          greatstrainonthecomputation,memory andstatisticalrequirementsofmachine\n",
      "         learningmodels.Replacingmatrixmultiplicationbydiscreteconvolutionwitha\n",
      "             smallkernelisthestandardwayofsolvingtheseproblemsforinputsthathave\n",
      "         translationinvariantspatialortemporalstructure. () DesjardinsandBengio2008\n",
      "6 7 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "         showedthatthisapproachworkswellwhenappliedtoRBMs.\n",
      "          Deepconvolutionalnetworksusuallyrequireapoolingoperationsothatthe\n",
      "         spatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworks\n",
      "              oftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled.\n",
      "             Itisunclearhowtogeneralizethistothesettingofenergy-basedmodels.We\n",
      "     couldintroduceabinarypoolingunitpovern  binarydetectorunits d andenforce\n",
      "p=max id i      bysettingtheenergyfunctiontobe∞    wheneverthatconstraintis\n",
      "           violated.Thisdoesnotscalewellthough,asitrequiresevaluating2ndiﬀerent\n",
      "          energyconﬁgurationstocomputethenormalizationconstant.Forasmall3×3\n",
      "    poolingregionthisrequires29      = 512energyfunctionevaluationsperpoolingunit!\n",
      "          Lee 2009etal.()developedasolutiontothisproblemcalledprobabilistic\n",
      " maxpooling          (nottobeconfusedwith“stochasticpooling,”whichisatechnique\n",
      "        forimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).The\n",
      "           strategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitsso\n",
      "             atmostonemaybeactiveatatime.Thismeansthereareonlyn  + 1total\n",
      "      states(onestateforeachofthen        detectorunitsbeingon,andanadditionalstate\n",
      "              correspondingtoallthedetectorunitsbeingoﬀ).Thepoolingunitisonifand\n",
      "                onlyifoneofthedetectorunitsison.Thestatewithallunitsoﬀisassigned\n",
      "               energyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethat\n",
      "hasn         +1states,orequivalentlyasamodelthathasn    +1variablesthatassigns\n",
      "           energytoallbutjointassignmentsofvariables. ∞ n+1\n",
      "           Whileeﬃcient,probabilisticmaxpoolingdoesforcethedetectorunitstobe\n",
      "           mutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontexts\n",
      "              oraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupport\n",
      "        overlappingpoolingregions.Overlappingpoolingregionsareusuallyrequired\n",
      "          toobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothis\n",
      "        constraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann\n",
      "machines.\n",
      "           Lee2009etal.()demonstratedthatprobabilisticmaxpoolingcouldbeused\n",
      "     tobuildconvolutionaldeepBoltzmannmachines.3     Thismodelisabletoperform\n",
      "           operationssuchasﬁllinginmissingportionsofitsinput.Whileintellectually\n",
      "            appealing,thismodelischallengingtomakeworkinpractice,andusuallydoes\n",
      "           notperformaswellasaclassiﬁerastraditionalconvolutionalnetworkstrained\n",
      "  withsupervisedlearning.\n",
      "          Manyconvolutionalmodelsworkequallywellwithinputsofmanydiﬀerent\n",
      "3              Thepublicationdescribesthemodelasa“deepbeliefnetwork,”butbecauseitcanbe\n",
      "            describedasapurelyundirectedmodelwithtractablelayer-wisemeanﬁeldﬁxed-pointupdates,\n",
      "         itbestﬁtsthedeﬁnitionofadeepBoltzmannmachine.\n",
      "680    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             spatialsizes.ForBoltzmannmachines,itisdiﬃculttochangetheinputsizefor\n",
      "            variousreasons.Thepartitionfunctionchangesasthesizeoftheinputchanges.\n",
      "          Moreover,manyconvolutionalnetworksachievesizeinvariancebyscalingupthe\n",
      "             sizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscaling\n",
      "        Boltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneural\n",
      "            networkscanuseaﬁxednumberofpoolingunitsanddynamicallyincreasethe\n",
      "             sizeoftheirpoolingregionstoobtainaﬁxed-sizerepresentationofavariablysized\n",
      "           input.ForBoltzmannmachines,largepoolingregionsbecometooexpensiveforthe\n",
      "              naiveapproach.Theapproachof ()ofmakingeachofthedetector Leeetal.2009\n",
      "          unitsinthesamepoolingregionmutuallyexclusivesolvesthecomputational\n",
      "           problemsbutstilldoesnotallowvariablysizedpoolingregions.Forexample,\n",
      "      supposewelearnamodelwith2 ×      2probabilisticmaxpoolingoverdetectorunits\n",
      "            thatlearnedgedetectors.Thisenforcestheconstraintthatonlyoneofthese\n",
      "     edgesmayappearineach2 ×          2region.Ifwethenincreasethesizeoftheinput\n",
      "              imageby50percentineachdirection,wewouldexpectthenumberofedgesto\n",
      "           increasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregions\n",
      "       by50percentineachdirectionto3 ×     3,thenthemutualexclusivityconstraint\n",
      "             nowspeciﬁesthateachoftheseedgesmayappearonlyonceina3 × 3region.\n",
      "              Aswegrowamodel’sinputimageinthisway,themodelgeneratesedgeswith\n",
      "             lessdensity.Ofcourse,theseissuesonlyarisewhenthemodelmustusevariable\n",
      "             amountsofpoolinginordertoemitaﬁxed-sizeoutputvector.Modelsthatuse\n",
      "            probabilisticmaxpoolingmaystillacceptvariablysizedinputimagesaslongas\n",
      "                theoutputofthemodelisafeaturemapthatcanscaleinsizeproportionaltothe\n",
      " inputimage.\n",
      "             Pixelsattheboundaryoftheimagealsoposesomediﬃculty,whichisexacer-\n",
      "             batedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.Ifwe\n",
      "              donotimplicitlyzeropadtheinput,therewillbefewerhiddenunitsthanvisible\n",
      "               units,andthevisibleunitsattheboundaryoftheimagewillnotbemodeledwell\n",
      "              becausetheylieinthereceptiveﬁeldoffewerhiddenunits.However,ifwedo\n",
      "              implicitlyzeropadtheinput,thenthehiddenunitsattheboundarywillbedriven\n",
      "          byfewerinputpixelsandmayfailtoactivatewhenneeded.\n",
      "      20.7BoltzmannMachinesforStructuredorSequential\n",
      "Outputs\n",
      "              Inthestructuredoutputscenario,wewishtotrainamodelthatcanmapfrom\n",
      " someinput x  tosomeoutput y     ,andthediﬀerententriesof y   arerelatedtoeach\n",
      "            otherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,\n",
      "6 8 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             yisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance.\n",
      "          Anaturalwaytorepresenttherelationshipsbetweentheentriesiny isto\n",
      "   useaprobabilitydistributionp(  y|x     ).Boltzmannmachines,extendedtomodel\n",
      "      conditionaldistributions,cansupplythisprobabilisticmodel.\n",
      "            ThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeused\n",
      "             notjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelatter\n",
      "     case,ratherthanmappinganinputx  toanoutputy     ,themodelmustestimatea\n",
      "      probabilitydistributionoverasequenceofvariables,p(x(1 )     ,...,x( ) τ ).Conditional\n",
      "       Boltzmannmachinescanrepresentfactorsoftheformp(x( ) t |x(1 )     ,...,x( 1 ) t − )in\n",
      "    ordertoaccomplishthistask.\n",
      "           Animportantsequencemodelingtaskforthevideogameandﬁlmindustry\n",
      "            ismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters.\n",
      "           Thesesequencesareoftencollectedusingmotioncapturesystemstorecordthe\n",
      "          movementsofactors.Aprobabilisticmodelofacharacter’smovementallows\n",
      "     thegeneration ofnew, previouslyunseen, butrealistic animations. To solve\n",
      "           thissequencemodelingtask,Taylor 2007etal.()introducedaconditionalRBM\n",
      "modelingp(x( ) t |x( 1 ) t −     ,...,x( ) t m −  )forsmallm      .ThemodelisanRBMover\n",
      "p(x( ) t          )whosebiasparametersarealinearfunctionoftheprecedingm  valuesofx.\n",
      "      Whenweconditionondiﬀerentvaluesofx( 1 ) t −      andearliervariables,wegetanew\n",
      " RBMoverx      .TheweightsintheRBMoverx     neverchange,butbyconditioningon\n",
      "             diﬀerentpastvalues,wecanchangetheprobabilityofdiﬀerenthiddenunitsinthe\n",
      "           RBMbeingactive.Byactivatinganddeactivatingdiﬀerentsubsetsofhiddenunits,\n",
      "          wecanmakelargechangestotheprobabilitydistributioninducedonx. Other\n",
      "            variantsofconditionalRBM( ,)andothervariantsofsequence Mnihetal.2011\n",
      "          modelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever ,;\n",
      "      etal.,;2009Boulanger-Lewandowski2012etal.,).\n",
      "          Anothersequencemodelingtaskistomodelthedistributionoversequences\n",
      "          ofmusicalnotesusedtocomposesongs.Boulanger-Lewan dowski 2012etal.()\n",
      " introducedtheRNN-RBM         sequencemodelandappliedittothistask.The\n",
      "         RNN-RBMisagenerativemodelofasequenceofframesx( ) t   consistingofanRNN\n",
      "            thatemitstheRBMparametersforeachtimestep.Unlikepreviousapproachesin\n",
      "               whichonlythebiasparametersoftheRBMvariedfromonetimesteptothenext,\n",
      "             theRNN-RBMusestheRNNtoemitalltheparametersoftheRBM,including\n",
      "              theweights.Totrainthemodel,weneedtobeabletoback-propagatethegradient\n",
      "              ofthelossfunctionthroughtheRNN.Thelossfunctionisnotapplieddirectlyto\n",
      "              theRNNoutputs.Instead,itisappliedtotheRBM.Thismeansthatwemust\n",
      "          approximatelydiﬀerentiatethelosswithrespecttotheRBMparametersusing\n",
      "          contrastivedivergenceorarelatedalgorithm.Thisapproximategradientmaythen\n",
      "6 8 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "         beback-propagatedthroughtheRNNusingtheusualback-propagationthrough\n",
      " timealgorithm.\n",
      "   20.8OtherBoltzmannMachines\n",
      "       ManyothervariantsofBoltzmannmachinesarepossible.\n",
      "          Boltzmannmachinesmaybeextendedwithdiﬀerenttrainingcriteria.Wehave\n",
      "         focusedonBoltzmannmachinestrainedtoapproximatelymaximizethegenerative\n",
      "criterion  logp( v           ).ItisalsopossibletotraindiscriminativeRBMsthataimto\n",
      "maximize logp(  y| v        )instead( ,).Thisapproachoften LarochelleandBengio2008\n",
      "            performsthebestwhenusingalinearcombinationofboththegenerativeand\n",
      "           thediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerful\n",
      "        supervisedlearnersasMLPs,atleastusingexistingmethodology.\n",
      "         MostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractions\n",
      "             intheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmany\n",
      "           terms,andeachindividualtermincludesonlytheproductbetweentworandom\n",
      "       variables.Anexampleofsuchatermisv iW i , jh j      .Itisalsopossibletotrain\n",
      "        higher-orderBoltzmannmachines( ,)whoseenergyfunctionterms Sejnowski1987\n",
      "         involvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweena\n",
      "           hiddenunitandtwodiﬀerentimagescanmodelspatialtransformationsfromone\n",
      "            frameofvideotothenext( ,,).Multiplicationby MemisevicandHinton20072010\n",
      "           aone-hotclassvariablecanchangetherelationshipbetweenvisibleandhidden\n",
      "            unitsdependingonwhichclassispresent(NairandHinton2009,).Onerecent\n",
      "            exampleoftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwo\n",
      "            groupsofhiddenunits,onegroupthatinteractswithboththevisibleunits vand\n",
      "  theclasslabely        ,andanothergroupthatinteractsonlywiththe v inputvalues\n",
      "             ( ,).Thiscanbeinterpretedasencouragingsomehiddenunitsto Luoetal.2011\n",
      "               learntomodeltheinputusingfeaturesthatarerelevanttotheclass,butalsoto\n",
      "           learnextrahiddenunitsthatexplainnuisancedetailsnecessaryforthesamples\n",
      "of v            toberealisticwithoutdeterminingtheclassoftheexample.Anotheruseof\n",
      "           higher-orderinteractionsistogatesomefeatures. ()introduced Sohnetal.2013\n",
      "         aBoltzmannmachinewiththird-orderinteractionsandbinarymaskvariables\n",
      "            associatedwitheachvisibleunit.Whenthesemaskingvariablesaresettozero,\n",
      "              theyremovetheinﬂuenceofavisibleunitonthehiddenunits.Thisallowsvisible\n",
      "             unitsthatarenotrelevanttotheclassiﬁcationproblemtoberemovedfromthe\n",
      "     inferencepathwaythatestimatestheclass.\n",
      "           Moregenerally,theBoltzmannmachineframeworkisarichspaceofmodels\n",
      "           permittingmanymoremodelstructuresthanhavebeenexploredsofar.Developing\n",
      "6 8 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            anewformofBoltzmannmachinerequiressomemorecareandcreativitythan\n",
      "              developinganewneuralnetworklayer,becauseitisoftendiﬃculttoﬁndanenergy\n",
      "         functionthatmaintainstractabilityofallthediﬀerentconditionaldistributions\n",
      "           neededtousetheBoltzmannmachine.Despitethisrequiredeﬀort,theﬁeld\n",
      "   remainsopentoinnovation.\n",
      "    20.9Back-PropagationthroughRandomOperations\n",
      "        Traditionalneuralnetworksimplementadeterministictransformationofsome\n",
      " inputvariables x         .Whendevelopinggenerativemodels,weoftenwishtoextend\n",
      "      neuralnetworkstoimplementstochastictransformationsof x  .Onestraightforward\n",
      "            waytodothisistoaugmenttheneuralnetworkwithextrainputs z thatare\n",
      "           sampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussian\n",
      "         distribution.Theneuralnetworkcanthencontinuetoperformdeterministic\n",
      "   computationinternally, butthefunctionf( x z,     )willappearstochastictoan\n",
      "     observerwhodoesnothaveaccess to z  .Providedthatf  iscontinuousand\n",
      "          diﬀerentiable,wecanthencomputethegradientsnecessaryfortrainingusing\n",
      "  back-propagationasusual.\n",
      "           Asanexample,letusconsidertheoperationconsistingofdrawingsamplesy\n",
      "         fromaGaussiandistributionwithmeanandvariance µ σ2:\n",
      "   y∼N(µ,σ2 ). (20.54)\n",
      "    Becauseanindividualsampleofy        isproducednotbyafunction,butratherby\n",
      "             asamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseem\n",
      "     counterintuitivetotakethederivativesofy     withrespecttotheparametersof\n",
      " itsdistribution,µandσ2       .However, wecanrewritethesamplingprocessas\n",
      "    transforminganunderlyingrandomvalue  z∼N(z ;0,     1)toobtainasamplefrom\n",
      "  thedesireddistribution:\n",
      "    yµσz. = + (20.55)\n",
      "           Wearenowabletoback-propagatethroughthesamplingoperation,byregard-\n",
      "         ingitasadeterministicoperationwithanextrainputz    .Crucially,theextrainput\n",
      "              isarandomvariablewhosedistributionisnotafunctionofanyofthevariables\n",
      "           whosederivativeswewanttocalculate. Theresulttellsushowaninﬁnitesimal\n",
      " changeinµorσ          wouldchangetheoutputifwecouldrepeatthesamplingoperation\n",
      "      againwiththesamevalueofz.\n",
      "          Beingabletoback-propagatethroughthissamplingoperationallowsusto\n",
      "                incorporateitintoalargergraph.Wecanbuildelementsofthegraphontopofthe\n",
      "6 8 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           outputofthesamplingdistribution.Forexample,wecancomputethederivatives\n",
      "   ofsomelossfunctionJ(y          ).Wecanalsobuildelementsofthegraphwhoseoutputs\n",
      "             aretheinputsortheparametersofthesamplingoperation.Forexample,wecould\n",
      "    buildalargergraphwithµ=f(x;θ )andσ=g(x;θ    ).Inthisaugmentedgraph,\n",
      "         wecanuseback-propagationthroughthesefunctionstoderive∇ θJy().\n",
      "           TheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-\n",
      "         cable.Wecanexpressanyprobabilitydistributionoftheformp(y;θ )orp(  y|x;θ)\n",
      "asp(  y|ω ),whereω     isavariablecontainingbothparametersθ   ,andifapplicable,\n",
      " theinputsx   .Givenavaluey   sampledfromdistributionp(  y|ω ),whereω mayin\n",
      "         turnbeafunctionofothervariables,wecanrewrite\n",
      "    y y ∼p(|ω) (20.56)\n",
      "as\n",
      "   yzω = (f;), (20.57)\n",
      "wherez           isasourceofrandomness.Wemaythencomputethederivativesofywith\n",
      " respecttoω        usingtraditionaltoolssuchastheback-propagationalgorithmapplied\n",
      "tof   ,aslongasf      iscontinuousanddiﬀerentiablealmosteverywhere.Crucially,ω\n",
      "     mustnotbeafunctionofz ,andz     mustnotbeafunctionofω  .Thistechnique\n",
      "   isoftencalledthe  reparametrizationtrick,  stochasticback-propagation ,or\n",
      " perturbationanalysis.\n",
      "  Therequirementthatf      becontinuousanddiﬀerentiableofcourserequiresy\n",
      "            tobecontinuous.Ifwewishtoback-propagatethroughasamplingprocessthat\n",
      "            producesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradienton\n",
      "ω          ,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCE\n",
      "      algorithm( ,),discussedinsection . Williams1992 20.9.1\n",
      "      Inneuralnetworkapplications,wetypicallychoosez    tobedrawnfromsome\n",
      "           simpledistribution,suchasaunituniformorunitGaussiandistribution,and\n",
      "          achievemorecomplexdistributionsbyallowingthedeterministicportionofthe\n",
      "    networktoreshapeitsinput.\n",
      "         Theideaofpropagatinggradientsoroptimizingthroughstochasticoperations\n",
      "           datesbacktothemid-twentiethcentury(,; ,)andwas Price1958Bonnet1964\n",
      "           ﬁrstusedformachinelearninginthecontextofreinforcementlearning( , Williams\n",
      "         1992). Morerecently,ithasbeenappliedtovariationalapproximations(Opper\n",
      "         andArchambeau2009,)andstochasticandgenerativeneuralnetworks(Bengio\n",
      "            etal.,; ,; 2013bKingma2013KingmaandWelling2014baRezende2014 ,,; etal.,;\n",
      "          Goodfellow 2014cetal.,).Manynetworks,suchasdenoisingautoencodersor\n",
      "       networksregularized withdropout,arealso naturallydesigned totakenoise\n",
      "6 8 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           asaninputwithoutrequiringanyspecialreparametrizationtomakethenoise\n",
      "   independentfromthemodel.\n",
      "     20.9.1Back-Propaga tingthroughDiscreteStochasticOperations\n",
      "      Whenamodelemitsadiscretevariabley     ,thereparametrizationtrickisnot\n",
      "    applicable.Supposethat themodel takesinputsx andparametersθ, both\n",
      "   encapsulatedinthevectorω      ,andcombinesthemwithrandomnoisez toproduce\n",
      "y:\n",
      "   yzω = (f;). (20.58)\n",
      "Becausey isdiscrete,f          mustbeastepfunction.Thederivativesofastepfunction\n",
      "             arenotusefulatanypoint.Rightateachstepboundary,thederivativesare\n",
      "             undeﬁned,butthatisasmallproblem.Thelargeproblemisthatthederivatives\n",
      "           arezeroalmosteverywhereontheregionsbetweenstepboundaries.Thederivatives\n",
      "   ofanycostfunctionJ(y          )thereforedonotgiveanyinformationforhowtoupdate\n",
      "   themodelparameters.θ\n",
      "       TheREINFORCEalgorithm(REwardIncrement=nonnegativeFactor ×\n",
      " OﬀsetReinforcement ×       CharacteristicEligibility)providesaframeworkdeﬁninga\n",
      "           familyofsimplebutpowerfulsolutions( ,). Thecoreideaisthat Williams1992\n",
      " eventhoughJ(f(z;ω         ))isastepfunctionwithuselessderivatives,theexpected\n",
      "cost E z z ∼p ( )  Jf((;))zω        isoftenasmoothfunctionamenabletogradientdescent.\n",
      "       Althoughthatexpectationistypicallynottractablewheny ishigh-dimensional\n",
      "              (oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbe\n",
      "           estimatedwithoutbiasusingaMonteCarloaverage.Thestochasticestimateof\n",
      "           thegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimization\n",
      "techniques.\n",
      "          ThesimplestversionofREINFORCEcanbederivedbysimplydiﬀerentiating\n",
      "  theexpectedcost:\n",
      "Ez[()] =Jy\n",
      "y Jp, ()y()y (20.59)\n",
      "∂J E[()]y\n",
      "∂ω=\n",
      "yJ()y∂p()y\n",
      "∂ω(20.60)\n",
      "=\n",
      "yJp()y()y  ∂plog()y\n",
      "∂ω(20.61)\n",
      "≈1\n",
      "mm \n",
      "y( ) i ∼p,i ( ) y =1J(y( )i)  ∂plog(y( )i)\n",
      "∂ω . (20.62)\n",
      "6 8 6    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "      Equation reliesontheassumptionthat 20.60 J  doesnotreferenceω   directly.Itis\n",
      "           trivialtoextendtheapproachtorelaxthisassumption.Equation exploits 20.61\n",
      "     thederivativeruleforthelogarithm,  ∂p l o g ( ) y\n",
      "∂ ω=1\n",
      "p ( ) y∂p ( ) y\n",
      "∂ ω   .Equation gives 20.62\n",
      "       anunbiasedMonteCarloestimatorofthegradient.\n",
      "  Anywherewewritep(y       )inthissection,onecouldequallywritep(  yx| ).This\n",
      " isbecausep(y   )isparametrizedbyω ,andω  containsbothθandx ,ifx ispresent.\n",
      "             OneissuewiththesimpleREINFORCEestimatoristhatithasaveryhigh\n",
      "     variance,sothatmanysamplesofy         needtobedrawntoobtainagoodestimatorof\n",
      "             thegradient,orequivalently,ifonlyonesampleisdrawn,SGDwillconvergevery\n",
      "             slowlyandwillrequireasmallerlearningrate.Itispossibletoconsiderablyreduce\n",
      "      thevarianceofthatestimatorbyusing  variancereduction  methods(,Wilson\n",
      "              1984L’Ecuyer1994 ; ,).Theideaistomodifytheestimatorsothatitsexpectedvalue\n",
      "           remainsunchangedbutitsvariancegetsreduced.InthecontextofREINFORCE,\n",
      "         theproposedvariancereductionmethodsinvolvethecomputationofabaseline\n",
      "    thatisusedtooﬀsetJ(y    ).Notethatanyoﬀsetb(ω     )thatdoesnotdependony\n",
      "         wouldnotchangetheexpectationoftheestimatedgradientbecause\n",
      "Ep ( ) y  ∂plog()y\n",
      "∂ω\n",
      "=\n",
      "yp()y  ∂plog()y\n",
      "∂ω(20.63)\n",
      "=\n",
      "y∂p()y\n",
      "∂ω(20.64)\n",
      "=∂\n",
      "∂ω\n",
      "yp() =y∂\n",
      "∂ω 1 = 0, (20.65)\n",
      "  whichmeansthat\n",
      "Ep ( ) y\n",
      "  (()()) Jy−bω  ∂plog()y\n",
      "∂ω\n",
      "= Ep ( ) y\n",
      "J()y  ∂plog()y\n",
      "∂ω\n",
      " −bE()ωp ( ) y  ∂plog()y\n",
      "∂ω\n",
      "(20.66)\n",
      "= Ep ( ) y\n",
      "J()y  ∂plog()y\n",
      "∂ω\n",
      " . (20.67)\n",
      "     Furthermore,wecanobtaintheoptimalb(ω     ) bycomputingthevarianceof(J(y)−\n",
      "b(ω))  ∂p l o g ( ) y\n",
      "∂ ωunderp(y     )andminimizingwithrespecttob(ω    ).Whatweﬁndis\n",
      "    thatthisoptimalbaselineb∗()ωi     isdiﬀerentforeachelementωi   ofthevector:ω\n",
      "b∗()ωi=Ep ( ) y\n",
      "J()y  ∂p l o g ( ) y\n",
      "∂ω i2\n",
      "Ep ( ) y  ∂p l o g ( ) y\n",
      "∂ω i2 . (20.68)\n",
      "6 8 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "      Thegradientestimatorwithrespecttoωi thenbecomes\n",
      "  (()()J y−b ωi)  ∂plog() y\n",
      "∂ωi , (20.69)\n",
      "whereb( ω)i   estimatestheaboveb∗( ω)i  .Theestimateb   isusuallyobtainedby\n",
      "             addingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimate\n",
      "Ep ( ) y[J( y)  ∂p l o g ( ) y\n",
      "∂ω i2 ]andEp ( ) y\n",
      "  ∂p l o g ( ) y\n",
      "∂ω i2\n",
      "   foreachelementof ω  .Theseextra\n",
      "           outputscanbetrainedwiththemeansquarederrorobjective,usingrespectively\n",
      "J( y)  ∂p l o g ( ) y\n",
      "∂ω i2and  ∂p l o g ( ) y\n",
      "∂ω i2  astargetswhen y  issampledfromp( y   ),foragiven\n",
      "ω  .Theestimateb        maythenberecoveredbysubstitutingtheseestimatesinto\n",
      "            equation. ()preferredtouseasinglesharedoutput 20.68MnihandGregor2014\n",
      "  (acrossallelementsiof ω    )trainedwiththetargetJ( y   ),usingasbaselineb( ω)≈\n",
      "Ep ( ) y[()]J y.\n",
      "         Variancereductionmethodshavebeenintroducedinthereinforcementlearning\n",
      "           context( ,; Suttonetal.2000WeaverandTao2001,),generalizingpreviouswork\n",
      "             onthecaseofbinaryrewardbyDayan1990 Bengio 2013bMnih (). See etal.(),\n",
      "                andGregor2014Ba 2014Mnih 2014 Xu 2015 (),etal.(),etal.(),oretal.()for\n",
      "           examplesofmodernusesoftheREINFORCEalgorithmwithreducedvariancein\n",
      "             thecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaseline\n",
      "b( ω          ) ( , ()foundthatthescaleof MnihandGregor2014 J( y) −b( ω  ))couldbe\n",
      "            adjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbya\n",
      "            movingaverageduringtraining,asakindofadaptivelearningrate,tocounter\n",
      "             theeﬀectofimportantvariationsthatoccurduringthecourseoftraininginthe\n",
      "          magnitudeofthisquantity. ()calledthisheuristic MnihandGregor2014 v a r i a nc e\n",
      "nor m a l i z a t i o n.\n",
      "        REINFORCE-basedestimatorscanbeunderstoodasestimatingthegradient\n",
      "   bycorrelatingchoicesof y   withcorrespondingvaluesofJ( y     ).Ifagoodvalueof y\n",
      "              isunlikelyunderthecurrentparametrization,itmighttakealongtimetoobtainit\n",
      "            bychanceandgettherequiredsignalthatthisconﬁgurationshouldbereinforced.\n",
      "   20.10DirectedGenerativeNets\n",
      "            Asdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass 16\n",
      "          ofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopular\n",
      "          withinthegreatermachinelearningcommunity,withinthesmallerdeeplearning\n",
      "          communitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodels\n",
      "   suchastheRBM.\n",
      "6 8 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            Inthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthat\n",
      "        havetraditionallybeenassociatedwiththedeeplearningcommunity.\n",
      "           Wehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirected\n",
      "            model.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethought\n",
      "            ofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearners\n",
      "             inthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsample\n",
      "            generationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirected\n",
      "models.\n",
      "   20.10.1SigmoidBeliefNetworks\n",
      "            Sigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodel Neal1990\n",
      "           withaspeciﬁckindofconditionalprobabilitydistribution.Ingeneral,wecan\n",
      "            thinkofasigmoidbeliefnetworkashavingavectorofbinarystates s  ,witheach\n",
      "       elementofthestateinﬂuencedbyitsancestors:\n",
      "ps( i) = σ\n",
      "\n",
      "j <iW j , is j +b i\n",
      " . (20.70)\n",
      "            Themostcommonstructureofsigmoidbeliefnetworkisonethatisdivided\n",
      "           intomanylayers,withancestralsamplingproceedingthroughaseriesofmany\n",
      "           hiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureis\n",
      "             verysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginning\n",
      "           ofthesamplingprocessareindependentfromeachother,ratherthansampled\n",
      "            fromarestrictedBoltzmannmachine.Suchastructureisinterestingforavariety\n",
      "           ofreasons. Oneisthatthestructureisauniversalapproximatorofprobability\n",
      "           distributionsoverthevisibleunits, inthesensethatitcanapproximateany\n",
      "         probabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,\n",
      "              evenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthe\n",
      "     visiblelayer(SutskeverandHinton2008,).\n",
      "             Whilegeneratingasampleofthevisibleunitsisveryeﬃcientinasigmoid\n",
      "            beliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiven\n",
      "            thevisibleunitsisintractable.Meanﬁeldinferenceisalsointractablebecausethe\n",
      "         variationallowerboundinvolvestakingexpectationsofcliquesthatencompass\n",
      "           entirelayers.Thisproblemhasremaineddiﬃcultenoughtorestrictthepopularity\n",
      "   ofdirecteddiscretenetworks.\n",
      "            Oneapproachforperforminginferenceinasigmoidbeliefnetworkistocon-\n",
      "            structadiﬀerentlowerboundthatisspecializedforsigmoidbeliefnetworks(Saul\n",
      "6 8 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             etal.,).Thisapproachhasonlybeenappliedtoverysmallnetworks.Another 1996\n",
      "            approachistouselearnedinferencemechanismsasdescribedinsection.The 19.5\n",
      "            Helmholtzmachine(Dayan 1995DayanandHinton1996 etal.,; ,)isasigmoid\n",
      "          beliefnetworkcombinedwithaninferencenetworkthatpredictstheparame-\n",
      "           tersofthemeanﬁelddistributionoverthehiddenunits.Modernapproaches\n",
      "             ( ,; ,)tosigmoidbeliefnetworksstilluse Gregoretal.2014MnihandGregor2014\n",
      "         thisinferencenetworkapproach.Thesetechniquesremaindiﬃcultbecauseof\n",
      "          thediscretenatureofthelatentvariables.Onecannotsimplyback-propagate\n",
      "            throughtheoutputoftheinferencenetwork,butinsteadmustusetherelatively\n",
      "       unreliablemachineryforback-propagatingthroughdiscretesamplingprocesses,\n",
      "          asdescribedinsection .Recentapproachesbasedonimportancesampling, 20.9.1\n",
      "        reweightedwake-sleep( ,)andbidirectionalHelmholtz BornscheinandBengio2015\n",
      "            machines(Bornschein2015etal.,)makeitpossibletoquicklytrainsigmoidbelief\n",
      "       networksandreachstate-of-the-artperformanceonbenchmarktasks.\n",
      "              Aspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatent\n",
      "             variables.Learninginthiscaseiseﬃcient,becausethereisnoneedtomarginalize\n",
      "          latentvariablesoutofthelikelihood. Afamilyofmodelscalledauto-regressive\n",
      "           networksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariables\n",
      "         besidesbinaryvariablesandotherstructuresofconditionaldistributionsbesides\n",
      "        log-linearrelationships.Auto-regressivenetworksaredescribedinsection .20.10.7\n",
      "   20.10.2DiﬀerentiableGeneratorNetworks\n",
      "           Manygenerativemodelsarebasedontheideaofusingadiﬀerentiable g e ner a t o r\n",
      "net w o r k       .Themodeltransformssamplesoflatentvariables z tosamples xor\n",
      "   todistributionsoversamples x   usingadiﬀerentiablefunction g ( z ; θ( ) g  ),whichis\n",
      "          typicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariational\n",
      "         autoencoders, whichpairthegeneratornetwithaninferencenet;generative\n",
      "  adversarial networks, which pairthe generator network witha discriminator\n",
      "        network;andtechniquesthattraingeneratornetworksinisolation.\n",
      "       Generatornetworksareessentiallyjustparametrizedcomputationalprocedures\n",
      "          forgeneratingsamples,wherethearchitectureprovidesthefamilyofpossible\n",
      "           distributionstosamplefromandtheparametersselectadistributionfromwithin\n",
      " thatfamily.\n",
      "           Asanexample,thestandardprocedurefordrawingsamplesfromanormal\n",
      "  distributionwithmean µ andcovariance Σ   istofeedsamples z  fromanormal\n",
      "           distributionwithzeromeanandidentitycovarianceintoaverysimplegenerator\n",
      "6 9 0    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "        network.Thisgeneratornetworkcontainsjustoneaﬃnelayer:\n",
      "    xzLz = (g) = +µ, (20.71)\n",
      "         whereisgivenbytheCholeskydecompositionof. L Σ\n",
      "        Pseudorandomnumbergeneratorscanalsousenonlineartransformationsof\n",
      "   simpledistributions.Forexample,  inversetransformsampling  (Devroye2013,)\n",
      "  drawsascalarzfromU(0,        1)andappliesanonlineartransformationtoascalar\n",
      "x   .Inthiscaseg(z          )isgivenbytheinverseofthecumulativedistributionfunction\n",
      "F(x) =x\n",
      "−∞p(v)dv      .Ifweareabletospecifyp(x  ),integrateoverx   ,andinvertthe\n",
      "          resultingfunction,wecansamplefrom withoutusingmachinelearning. px()\n",
      "         Togeneratesamplesfrommorecomplicateddistributionsthatarediﬃcult\n",
      "           tospecifydirectly,diﬃculttointegrateover,orwhoseresultingintegralsare\n",
      "            diﬃculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamily\n",
      "  ofnonlinearfunctionsg         ,andusetrainingdatatoinfertheparametersselecting\n",
      "  thedesiredfunction.\n",
      "   Wecanthinkofg        asprovidinganonlinearchangeofvariablesthattransforms\n",
      "        thedistributionoverintothedesireddistributionover. z x\n",
      "         Recallfromequationthat,forinvertible,diﬀerentiable,continuous, 3.47 g\n",
      "p z() = zp x(())gzdet(∂g\n",
      "∂z) . (20.72)\n",
      "       Thisimplicitlyimposesaprobabilitydistributionover:x\n",
      "p x() =xp z(g− 1())xdet(∂ g\n",
      "∂ z) . (20.73)\n",
      "             Ofcourse,thisformulamaybediﬃculttoevaluate,dependingonthechoiceof\n",
      "g        ,soweoftenuseindirectmeansoflearningg     ,ratherthantryingtomaximize\n",
      "  log()pxdirectly.\n",
      "     Insomecases,ratherthanusingg    toprovideasampleofx   directly,weuseg\n",
      "     todeﬁneaconditionaldistributionoverx       .Forexample,wecoulduseagenerator\n",
      "            netwhoseﬁnallayerconsistsofsigmoidoutputstoprovidethemeanparameters\n",
      "  ofBernoullidistributions:\n",
      "p(x i = 1 ) = ()|zgz i . (20.74)\n",
      "     Inthiscase,whenweuseg todeﬁnep(  xz|     ),weimposeadistributionoverxby\n",
      " marginalizing:z\n",
      "p() = x E z   p. ( )xz| (20.75)\n",
      "6 9 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "    Bothapproachesdeﬁneadistribution p g(x      )andallowustotrainvarious\n",
      "  criteriaof p g      usingthereparametrizationtrickofsection.20.9\n",
      "        Thetwodiﬀerentapproachestoformulatinggeneratornets—emittingthe\n",
      "        parametersofaconditionaldistributionversusdirectlyemittingsamples—have\n",
      "       complementarystrengthsandweaknesses.Whenthe generatornetdeﬁnes a\n",
      "  conditionaldistributionoverx          ,itiscapableofgeneratingdiscretedataaswellas\n",
      "            continuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableof\n",
      "          generatingonlycontinuousdata(wecouldintroducediscretizationintheforward\n",
      "             propagation, butdoingsowouldmeanthemodelcouldnolongerbetrainedusing\n",
      "           back-propagation).Theadvantagetodirectsamplingisthatwearenolonger\n",
      "            forcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownand\n",
      "     algebraicallymanipulatedbyahumandesigner.\n",
      "         Approachesbasedondiﬀerentiablegeneratornetworksaremotivatedbythe\n",
      "       successofgradientdescentapplied todiﬀerentiablefeedforwardnetworks for\n",
      "        classiﬁcation. Inthecontextofsupervisedlearning,deepfeedforwardnetworks\n",
      "         trainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgiven\n",
      "            enoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccess\n",
      "   transfertogenerativemodeling?\n",
      "          Generativemodelingseemstobemorediﬃcultthanclassiﬁcationorregression\n",
      "          becausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext\n",
      "           ofdiﬀerentiablegeneratornets,thecriteriaareintractablebecausethedatadoes\n",
      "    notspecifyboththeinputsz  andtheoutputsx      ofthegeneratornet.Inthecase\n",
      "     ofsupervisedlearning,boththeinputsx  andtheoutputsy   weregiven,andthe\n",
      "           optimizationprocedureneedsonlytolearnhowtoproducethespeciﬁedmapping.\n",
      "            Inthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehow\n",
      "                toarrangespaceinausefulwayandadditionallyhowtomapfromto. z zx\n",
      "          Dosovitskiy2015etal.()studiedasimpliﬁedproblem,wherethecorrespondence\n",
      "betweenzandx       isgiven.Speciﬁcally,thetrainingdataiscomputer-rendered\n",
      "     imageryofchairs.Thelatentvariablesz     areparametersgiventotherendering\n",
      "              enginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,\n",
      "            andotherconﬁgurationdetailsthataﬀecttherenderingoftheimage.Usingthis\n",
      "           syntheticallygenerateddata,aconvolutionalnetworkisabletolearntomapz\n",
      "       descriptionsofthecontentofanimagetox    approximationsofrenderedimages.\n",
      "        Thissuggeststhatcontemporarydiﬀerentiablegeneratornetworkshavesuﬃcient\n",
      "          modelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimization\n",
      "             algorithmshavetheabilitytoﬁtthem.Thediﬃcultyliesindetermininghowto\n",
      "      traingeneratornetworkswhenthevalueofz foreachx    isnotﬁxedandknown\n",
      "   aheadofeachtime.\n",
      "6 9 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "        Thefollowingsectionsdescribeseveralapproachestotrainingdiﬀerentiable\n",
      "       generatornetsgivenonlytrainingsamplesof.x\n",
      "  20.10.3VariationalAutoencoders\n",
      "The  variationalautoencoder        ,orVAE( ,; ,), Kingma2013Rezendeetal.2014\n",
      "            isadirectedmodelthatuseslearnedapproximateinferenceandcanbetrained\n",
      "   purelywithgradient-basedmethods.\n",
      "            Togenerateasamplefromthemodel,theVAEﬁrstdrawsasamplezfrom\n",
      "  thecodedistributionp m o d e l(z        ).Thesampleisthenrunthroughadiﬀerentiable\n",
      " generatornetworkg(z ).Finally,x    issampledfromadistributionp m o d e l(x;g(z)) =\n",
      "p m o d e l(  xz|        ). Duringtraining,however,theapproximateinferencenetwork(or\n",
      "encoder)q(  zx|    )isusedtoobtainz ,andp m o d e l(  xz|      )isthenviewedasadecoder\n",
      "network.\n",
      "            Thekeyinsightbehindvariationalautoencodersisthattheycanbetrainedby\n",
      "          maximizingthevariationallowerboundassociatedwithdatapoint: L()q x\n",
      "L() = q E z z x ∼ q (| ) logp m o d e l      ()+(( )) zx,Hqz|x (20.76)\n",
      "= E z z x ∼ q (| ) logp m o d e l    ( )xz|−D K L  (( )qz|x||p m o d e l ())z (20.77)\n",
      "  ≤logp m o d e l ()x. (20.78)\n",
      "              Inequation,werecognizetheﬁrsttermasthejointlog-likelihoodofthevisible 20.76\n",
      "           andhiddenvariablesundertheapproximateposterioroverthelatentvariables(just\n",
      "             aswithEM,exceptthatweuseanapproximateratherthantheexactposterior).\n",
      "            Werecognizealsoasecondterm,theentropyoftheapproximateposterior.When\n",
      "q             ischosentobeaGaussiandistribution,withnoiseaddedtoapredictedmean\n",
      "         value,maximizingthisentropytermencouragesincreasingthestandarddeviation\n",
      "           ofthisnoise.Moregenerally,thisentropytermencouragesthevariationalposterior\n",
      "      toplacehighprobabilitymassonmanyz    valuesthatcouldhavegeneratedx,\n",
      "             ratherthancollapsingtoasinglepointestimateofthemostlikelyvalue.In\n",
      "          equation,werecognizetheﬁrsttermasthereconstructionlog-likelihood 20.77\n",
      "           foundinotherautoencoders.Thesecondtermtriestomaketheapproximate\n",
      " posteriordistributionq(  z|x    )andthemodelpriorp m o d e l(z   )approacheachother.\n",
      "       Traditionalapproachestovariationalinferenceandlearninginferq  viaanopti-\n",
      "        mizationalgorithm,typicallyiteratedﬁxed-pointequations(section).These 19.4\n",
      "         approachesareslowandoftenrequiretheabilitytocompute E z∼ q logp m o d e l( zx,)\n",
      "             inclosedform.Themainideabehindthevariationalautoencoderistotraina\n",
      "         parametricencoder(alsosometimescalledaninferencenetworkorrecognition\n",
      "     model)thatproducestheparametersofq   .Aslongasz    isacontinuousvariable,we\n",
      "6 9 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "     canthenback-propagatethroughsamplesofz drawnfrom q(  zx|) = q(z; f(x;θ))\n",
      "      toobtainagradientwithrespecttoθ      .Learningthenconsistssolelyofmaximizing\n",
      "L            withrespecttotheparametersoftheencoderanddecoder.Alltheexpectations\n",
      "        inmaybeapproximatedbyMonteCarlosampling. L\n",
      "        Thevariationalautoencoderapproachiselegant,theoreticallypleasing,and\n",
      "            simpletoimplement.Italsoobtainsexcellentresultsandisamongthestate-of-the-\n",
      "           artapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfrom\n",
      "           variationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecauses\n",
      "             ofthisphenomenonarenotyetknown.Onepossibilityisthattheblurrinessis\n",
      "       anintrinsiceﬀectofmaximumlikelihood,whichminimizes D K L( p d a ta p m o d e l ).As\n",
      "             illustratedinﬁgure,thismeansthatthemodelwillassignhighprobabilityto 3.6\n",
      "              pointsthatoccurinthetrainingsetbutmayalsoassignhighprobabilitytoother\n",
      "             points.Theseotherpointsmayincludeblurryimages.Partofthereasonthatthe\n",
      "            modelwouldchoosetoputprobabilitymassonblurryimagesratherthansome\n",
      "             otherpartofthespaceisthatthevariationalautoencodersusedinpracticeusually\n",
      "    haveaGaussiandistributionfor p m o d e l(x; g(z     )). Maximizingalowerboundon\n",
      "            thelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoder\n",
      "              withmeansquarederror,inthesensethatithasatendencytoignorefeatures\n",
      "               oftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthe\n",
      "              brightnessofthepixelsthattheyoccupy.ThisissueisnotspeciﬁctoVAEsand\n",
      "          issharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,\n",
      "D K L( p d a ta p m o d e l            ),asarguedby ()andby ().Another Theisetal.2015 Huszar2015\n",
      "              troublingissuewithcontemporaryVAEmodelsisthattheytendtouseonlyasmall\n",
      "    subsetofthedimensionsofz          ,asiftheencoderwerenotabletotransformenough\n",
      "             ofthelocaldirectionsininputspacetoaspacewherethemarginaldistribution\n",
      "   matchesthefactorizedprior.\n",
      "            TheVAEframeworkisstraightforwardtoextendtoawiderangeofmodel\n",
      "          architectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequire\n",
      "          extremelycarefulmodeldesigntomaintaintractability.VAEsworkverywell\n",
      "        withadiversefamilyofdiﬀerentiableoperators. Oneparticularlysophisticated\n",
      "  VAEisthe   deeprecurrentattentionwriter     (DRAW)model( , Gregoretal.\n",
      "          2015).DRAWusesarecurrentencoderandrecurrentdecodercombinedwith\n",
      "          anattentionmechanism.ThegenerationprocessfortheDRAWmodelconsists\n",
      "           ofsequentiallyvisitingdiﬀerentsmallimagepatchesanddrawingthevaluesof\n",
      "             thepixelsatthosepoints.VAEscanalsobeextendedtogeneratesequencesby\n",
      "            deﬁningvariationalRNNs(Chung2015betal.,)byusingarecurrentencoderand\n",
      "           decoderwithintheVAEframework.GeneratingasamplefromatraditionalRNN\n",
      "         involvesonlynondeterministicoperationsattheoutputspace.VariationalRNNs\n",
      "           alsohaverandomvariabilityatthepotentiallymoreabstractlevelcapturedby\n",
      "6 9 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "   theVAElatentvariables.\n",
      "            TheVAEframeworkhasbeenextendedtomaximizenotjustthetraditionalvari-\n",
      "     ationallowerbound,butalsothe  importance-weightedautoencoder(Burda\n",
      "   etal.,)objective: 2015\n",
      "L k () = x,q Ez( 1 ) , . . . , z( ) k∼ | q ( z x )\n",
      "log1\n",
      "kk \n",
      "i =1p m o d e l (xz,( ) i)\n",
      "q(z( ) i |x)\n",
      " . (20.79)\n",
      "         ThisnewobjectiveisequivalenttothetraditionallowerboundLwhenk = 1.How-\n",
      "            ever,itmayalsobeinterpretedasforminganestimateofthetrue logp m o d e l(x )using\n",
      "  importancesamplingofz  fromproposaldistributionq(  zx|  ). Theimportance-\n",
      "        weightedautoencoderobjectiveisalsoalowerboundon  logp m o d e l(x ) andbecomes\n",
      "   tighterasincreases.k\n",
      "        VariationalautoencodershavesomeinterestingconnectionstotheMP-DBM\n",
      "        andotherapproachesthatinvolveback-propagationthroughtheapproximate\n",
      "             inferencegraph(Goodfellow2013bStoyanov2011Brakel 2013 etal.,; etal.,;etal.,).\n",
      "           Thesepreviousapproachesrequiredaninferenceproceduresuchasmeanﬁeldﬁxed-\n",
      "         pointequationstoprovidethecomputationalgraph.Thevariationalautoencoder\n",
      "            isdeﬁnedforarbitrarycomputationalgraphs,whichmakesitapplicabletoawider\n",
      "             rangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice\n",
      "           ofmodelstothosewithtractablemeanﬁeldﬁxed-pointequations.Thevariational\n",
      "           autoencoderalsohastheadvantageofincreasingaboundonthelog-likelihood\n",
      "             ofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremore\n",
      "          heuristicandhavelittleprobabilisticinterpretationbeyondmakingtheresultsof\n",
      "        approximateinferenceaccurate.Onedisadvantageofthevariationalautoencoder\n",
      "           isthatitlearnsaninferencenetworkforonlyoneproblem,inferringzgivenx.\n",
      "            Theoldermethodsareabletoperformapproximateinferenceoveranysubsetof\n",
      "           variablesgivenanyothersubsetofvariables,becausethemeanﬁeldﬁxed-point\n",
      "          equationsspecifyhowtoshareparametersbetweenthecomputationalgraphsfor\n",
      "   allthesediﬀerentproblems.\n",
      "          Oneverynicepropertyofthevariationalautoencoderisthatsimultaneously\n",
      "           trainingaparametricencoderincombinationwiththegeneratornetworkforcesthe\n",
      "            modeltolearnapredictablecoordinatesystemthattheencodercancapture.This\n",
      "            makesitanexcellentmanifoldlearningalgorithm.Seeﬁgureforexamplesof 20.6\n",
      "          low-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthe\n",
      "          casesdemonstratedintheﬁgure,thealgorithmdiscoveredtwoindependentfactors\n",
      "            ofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression.\n",
      "6 9 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "          Figure20.6:Examplesof2-Dcoordinatesystemsforhigh-dimensionalmanifolds,learned\n",
      "           byavariationalautoencoder(KingmaandWelling2014a,).Twodimensionsmaybe\n",
      "               plotteddirectlyonthepageforvisualization,sowecangainanunderstandingofhowthe\n",
      "                modelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievetheintrinsic\n",
      "             dimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenotexamples\n",
      "     fromthetrainingsetbutimagesx     actuallygeneratedbythemodelp(  xz|  ),simplyby\n",
      "   changingthe2-D“code”z        (eachimagecorrespondstoadiﬀerentchoiceof“code”z ona\n",
      "              2-Duniformgrid).(Left)The2-DmapoftheFreyfacesmanifold.Onedimensionthat\n",
      "             hasbeendiscovered(horizontal)mostlycorrespondstoarotationoftheface,whilethe\n",
      "            other(vertical)correspondstotheemotionalexpression. The2-Dmapofthe (Right)\n",
      " MNISTmanifold.\n",
      "   20.10.4GenerativeAdversarialNetworks\n",
      "          Generativeadversarialnetworks,orGANs( ,),areanother Goodfellowetal.2014c\n",
      "       generativemodelingapproachbasedondiﬀerentiablegeneratornetworks.\n",
      "          Generativeadversarialnetworksarebasedonagametheoreticscenarioin\n",
      "          whichthegeneratornetworkmustcompeteagainstanadversary.Thegenerator\n",
      "   networkdirectlyproducessamplesx=g(z;θ( ) g   ).Itsadversary,the di s c r i m i n a t o r\n",
      "net w o r k          ,attemptstodistinguishbetweensamplesdrawnfromthetrainingdata\n",
      "           andsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvalue\n",
      " givenbyd(x;θ( ) d    ),indicatingtheprobabilitythatx    isarealtrainingexample\n",
      "        ratherthanafakesampledrawnfromthemodel.\n",
      "          Thesimplestwaytoformulatelearningingenerativeadversarialnetworksis\n",
      "       asazero-sumgame,inwhichafunctionv(θ( ) g ,θ( ) d     )determinesthepayoﬀofthe\n",
      "6 9 6    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "   discriminator.Thegeneratorreceives−v( θ( ) g , θ( ) d     )asitsownpayoﬀ.During\n",
      "            learning,eachplayerattemptstomaximizeitsownpayoﬀ,sothatatconvergence\n",
      "g∗ = argmin\n",
      "gmax\n",
      "d  vg,d. () (20.80)\n",
      "     Thedefaultchoiceforisv\n",
      "v( θ( ) g , θ( ) d) = E x ∼ p d a t a   log()+d x E x ∼ p m o d e l     log(1 ())−d x. (20.81)\n",
      "             Thisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasreal\n",
      "           orfake.Simultaneously,thegeneratorattemptstofooltheclassiﬁerintobelieving\n",
      "          itssamplesarereal.Atconvergence,thegenerator’ssamplesareindistinguishable\n",
      "      fromrealdata,andthediscriminatoroutputs1\n",
      "2  everywhere.Thediscriminator\n",
      "   maythenbediscarded.\n",
      "            ThemainmotivationforthedesignofGANsisthatthelearningprocess\n",
      "         requiresneitherapproximateinferencenorapproximationofapartitionfunction\n",
      " gradient.Whenmax dv( g,d   )isconvexin θ( ) g     (suchasthecasewhereoptimization\n",
      "           isperformeddirectlyinthespaceofprobabilitydensityfunctions),theprocedure\n",
      "       isguaranteedtoconvergeandisasymptoticallyconsistent.\n",
      "         Unfortunately,learninginGANscanbediﬃcultinpracticewhengandd\n",
      "     arerepresentedbyneuralnetworksandmax dv( g,d    )isnotconvex.Goodfellow\n",
      "           ()identiﬁednonconvergenceasanissuethatmaycauseGANstounderﬁt. 2014\n",
      "           Ingeneral,simultaneousgradientdescentontwoplayers’costsisnotguaranteed\n",
      "        toreachanequilibrium. Consider,forexample,thevaluefunctionv( a,b )=ab,\n",
      "   whereoneplayercontrolsa  andincurscostab     ,whiletheotherplayercontrolsb\n",
      "   andreceivesacost−ab         .Ifwemodeleachplayerasmakinginﬁnitesimallysmall\n",
      "             gradientsteps,eachplayerreducingtheirowncostattheexpenseoftheother\n",
      " player,thenaandb          gointoastable,circularorbit,ratherthanarrivingatthe\n",
      "             equilibriumpointattheorigin.Notethattheequilibriaforaminimaxgameare\n",
      "   notlocalminimaofv        .Instead,theyarepointsthataresimultaneouslyminima\n",
      "           forbothplayers’costs.Thismeansthattheyaresaddlepointsofv  thatarelocal\n",
      "            minimawithrespecttotheﬁrstplayer’sparametersandlocalmaximawithrespect\n",
      "              tothesecondplayer’sparameters.Itispossibleforthetwoplayerstotaketurns\n",
      "  increasingthendecreasingv        forever,ratherthanlandingexactlyonthesaddle\n",
      "               point,whereneitherplayeriscapableofreducingitscost.Itisnotknowntowhat\n",
      "     extentthisnonconvergenceproblemaﬀectsGANs.\n",
      "          Goodfellow2014()identiﬁedanalternativeformulationofthepayoﬀs,inwhich\n",
      "             thegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximum\n",
      "        likelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximum\n",
      "6 9 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "          likelihoodtrainingconverges,thisreformulationoftheGANgameshouldalso\n",
      "        converge,givenenoughsamples.Unfortunately,thisalternativeformulationdoes\n",
      "           notseemtoimproveconvergenceinpractice,possiblybecauseofsuboptimalityof\n",
      "        thediscriminatororhighvariancearoundtheexpectedgradient.\n",
      "         Inrealisticexperiments,thebest-performingformulationoftheGANgame\n",
      "           isadiﬀerentformulationthatisneitherzero-sumnorequivalenttomaximum\n",
      "           likelihood,introducedby ()withaheuristicmotivation.In Goodfellowetal.2014c\n",
      "         thisbest-performingformulation,thegeneratoraimstoincreasethelog-probability\n",
      "            thatthediscriminatormakesamistake,ratherthanaimingtodecreasethelog-\n",
      "         probabilitythatthediscriminatormakesthecorrectprediction.Thisreformulation\n",
      "             ismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator’s\n",
      "             costfunctionwithrespecttothediscriminator’slogitstoremainlargeeveninthe\n",
      "        situationwhenthediscriminatorconﬁdentlyrejectsallgeneratorsamples.\n",
      "        StabilizationofGANlearningremainsanopenproblem. Fortunately,GAN\n",
      "          learningperformswellwhenthemodelarchitectureandhyperparametersarecare-\n",
      "           fullyselected. ()craftedadeepconvolutionalGAN(DCGAN) Radfordetal.2015\n",
      "             thatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre-\n",
      "           sentationspacecapturesimportantfactorsofvariation,asshowninﬁgure.15.9\n",
      "           SeeﬁgureforexamplesofimagesgeneratedbyaDCGANgenerator. 20.7\n",
      "           TheGANlearningproblemcanalsobesimpliﬁedbybreakingthegeneration\n",
      "             processintomanylevelsofdetail.ItispossibletotrainconditionalGANs(Mirza\n",
      "         andOsindero2014,)thatlearntosamplefromadistribution p(  xy| )rather\n",
      "      thansimplysamplingfromamarginaldistribution p(x    ).Denton 2015etal.()\n",
      "              showedthataseriesofconditionalGANscanbetrainedtoﬁrstgenerateavery\n",
      "           low-resolutionversionofanimage,thenincrementallyadddetailstotheimage.\n",
      "              ThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramid\n",
      "          togeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgenerators\n",
      "            areabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,with\n",
      "             experimentalsubjectsidentifyingupto40percentoftheoutputsofthenetworkas\n",
      "             beingrealdata.SeeﬁgureforexamplesofimagesgeneratedbyaLAPGAN 20.7\n",
      "generator.\n",
      "             OneunusualcapabilityoftheGANtrainingprocedureisthatitcanﬁtproba-\n",
      "           bilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthan\n",
      "           maximizingthelog-probabilityofspeciﬁcpoints,thegeneratornetlearnstotrace\n",
      "            outamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-\n",
      "            doxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinﬁnity\n",
      "             tothetestset,whilestillrepresentingamanifoldthatahumanobserverjudges\n",
      "              tocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageor\n",
      "6 9 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           Figure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset.(Left)Images\n",
      "           ofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadford\n",
      "            etal.(). ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith 2015(Right)\n",
      "     permissionfrom (). Dentonetal.2015\n",
      "           adisadvantage,andonemayalsoguaranteethatthegeneratornetworkassigns\n",
      "             nonzeroprobabilitytoallpointssimplybymakingthelastlayerofthegenerator\n",
      "           networkaddGaussiannoisetoallthegeneratedvalues.Generatornetworksthat\n",
      "            addGaussiannoiseinthismannersamplefromthesamedistributionthatone\n",
      "            obtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional\n",
      " Gaussiandistribution.\n",
      "         Dropoutseemstobeimportantinthediscriminatornetwork. Inparticular,\n",
      "   units shouldbe stochasticallydropped whilecomputing the gradi ent for the\n",
      "           generatornetworktofollow.Followingthegradientofthedeterministicversionof\n",
      "              thediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseﬀective.\n",
      "        Likewise,neverusingdropoutseemstoyieldpoorresults.\n",
      "         WhiletheGANframeworkisdesignedfordiﬀerentiablegeneratornetworks,\n",
      "            similarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-\n",
      " supervisedboosting           canbeusedtotrainanRBMgeneratortofoolalogistic\n",
      "     regressiondiscriminator(Welling2002etal.,).\n",
      "    20.10.5GenerativeMomentMatchingNetworks\n",
      "   Generativemomentmatchingnetworks      ( ,; , Lietal.2015Dziugaiteetal.\n",
      "          2015)areanotherformofgenerativemodelbasedondiﬀerentiablegenerator\n",
      "             networks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetwork\n",
      "            withanyothernetwork—neitheraninferencenetwork,asusedwithVAEs,nora\n",
      "6 9 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "     discriminatornetwork,asusedwithGANs.\n",
      "         Generativemomentmatchingnetworksaretrainedwithatechniquecalled\n",
      " momentmatching          .Thebasicideabehindmomentmatchingistotrainthe\n",
      "              generatorinsuchawaythatmanyofthestatisticsofsamplesgeneratedbythe\n",
      "               modelareassimilaraspossibletothoseofthestatisticsoftheexamplesinthe\n",
      "     trainingset.Inthiscontext,amoment       isanexpectationofdiﬀerentpowersofa\n",
      "            randomvariable.Forexample,theﬁrstmomentisthemean,thesecondmoment\n",
      "              isthemeanofthesquaredvalues,andsoon.Inmultipledimensions,eachelement\n",
      "               oftherandomvectormayberaisedtodiﬀerentpowers,sothatamomentmaybe\n",
      "    anyquantityoftheform\n",
      "E xΠ ixn i\n",
      "i , (20.82)\n",
      "  wheren= [n 1 ,n 2     ,...,n d]     isavectorofnonnegativeintegers.\n",
      "         Uponﬁrstexamination,thisapproachseemstobecomputationallyinfeasible.\n",
      "            Forexample,ifwewanttomatchallthemomentsoftheformx ix j   ,thenweneed\n",
      "             tominimizethediﬀerencebetweenanumberofvaluesthatisquadraticinthe\n",
      " dimensionofx          .Moreover,evenmatchingalltheﬁrstandsecondmomentswould\n",
      "           onlybesuﬃcienttoﬁtamultivariateGaussiandistribution,whichcapturesonly\n",
      "          linearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksareto\n",
      "         capturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments.\n",
      "           GANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga\n",
      "        dynamicallyupdateddiscriminatorwhichautomaticallyfocusesitsattentionon\n",
      "         whicheverstatisticthegeneratornetworkismatchingtheleasteﬀectively.\n",
      "         Instead,generativemomentmatchingnetworkscanbetrainedbyminimizing\n",
      "   acostfunctioncalled   maximummeandiscrepancy    ,orMMD(Schölkopfand\n",
      "             Smola2002Gretton 2012 ,; etal.,).Thiscostfunctionmeasurestheerrorinthe\n",
      "           ﬁrstmomentsinaninﬁnite-dimensionalspace,usinganimplicitmappingtofeature\n",
      "          spacedeﬁnedbyakernelfunctiontomakecomputationsoninﬁnite-dimensional\n",
      "              vectorstractable.TheMMDcostiszeroifandonlyifthetwodistributionsbeing\n",
      "  comparedareequal.\n",
      "         Visually,thesamplesfromgenerativemomentmatchingnetworksaresomewhat\n",
      "         disappointing.Fortunately,theycanbeimprovedbycombiningthegenerator\n",
      "           networkwithanautoencoder.First,anautoencoderistrainedtoreconstructthe\n",
      "             trainingset.Next,theencoderoftheautoencoderisusedtotransformtheentire\n",
      "            trainingsetintocodespace.Thegeneratornetworkisthentrainedtogenerate\n",
      "            codesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder.\n",
      "             UnlikeGANs,thecostfunctionisdeﬁnedonlywithrespecttoabatchof\n",
      "             examplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossible\n",
      "              tomakeatrainingupdateasafunctionofonlyonetrainingexampleoronly\n",
      "7 0 0    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "            onesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbe\n",
      "             computedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoo\n",
      "           small,MMDcanunderestimatethetrueamountofvariationinthedistributions\n",
      "            beingsampled.Noﬁnitebatchsizeissuﬃcientlylargetoeliminatethisproblem\n",
      "           entirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatch\n",
      "           sizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemany\n",
      "            examplesmustbeprocessedinordertocomputeasinglesmallgradientstep.\n",
      "               AswithGANs,itispossibletotrainageneratornetusingMMDevenifthat\n",
      "        generatornetassignszeroprobabilitytothetrainingpoints.\n",
      "   20.10.6ConvolutionalGenerativeNetworks\n",
      "              Whengeneratingimages,itisoftenusefultouseageneratornetworkthatincludesa\n",
      "          convolutionalstructure(see,forexample,Goodfellow 2014c Dosovitskiy etal.[]or\n",
      "            etal.[]).Todoso, weusethe“transpose”oftheconvolutionoperator, 2015\n",
      "            describedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes 9.5\n",
      "          sousingfewerparametersthanusingfullyconnectedlayerswithoutparameter\n",
      "sharing.\n",
      "         Convolutionalnetworksforrecognitiontaskshaveinformationﬂowfromthe\n",
      "              imagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel.\n",
      "            Asthisimageﬂowsupwardthroughthenetwork,informationisdiscardedasthe\n",
      "         representationoftheimagebecomesmoreinvarianttonuisancetransformations.\n",
      "            Inageneratornetwork, theoppositeistrue.Richdetailsmustbeaddedas\n",
      "           therepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,\n",
      "             culminatingintheﬁnalrepresentationoftheimage,whichisofcoursetheimage\n",
      "             itself,inallitsdetailedglory,withobjectpositionsandposesandtexturesand\n",
      "        lighting. Theprimarymechanismfordiscardinginformationinaconvolutional\n",
      "            recognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedto\n",
      "             addinformation.Wecannotputtheinverseofapoolinglayerintothegenerator\n",
      "           networkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationis\n",
      "            tomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseems\n",
      "             toperformacceptablyistousean“un-pooling”asintroducedbyDosovitskiyetal.\n",
      "           ().Thislayercorrespondstotheinverseofthemax-poolingoperationunder 2015\n",
      "         certainsimplifyingconditions. First,thestrideofthemax-poolingoperationis\n",
      "             constrainedtobeequaltothewidthofthepoolingregion.Second,themaximum\n",
      "             inputwithineachpoolingregionisassumedtobetheinputintheupper-left\n",
      "           corner.Finally,allnonmaximalinputswithineachpoolingregionareassumedto\n",
      "             bezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthe\n",
      "         max-poolingoperatortobeinverted.Theinverseun-poolingoperationallocates\n",
      "7 0 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "          atensorofzeros,thencopieseachvaluefromspatialcoordinatei  oftheinput\n",
      "  tospatialcoordinate  ik×     oftheoutput.Theintegervaluek   deﬁnesthesize\n",
      "           ofthepoolingregion.Eventhoughtheassumptionsmotivatingthedeﬁnitionof\n",
      "            theun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearnto\n",
      "             compensateforitsunusualoutput,sothesamplesgeneratedbythemodelasa\n",
      "   wholearevisuallypleasing.\n",
      "  20.10.7Auto-RegressiveNetworks\n",
      "         Auto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandom\n",
      "         variables.Theconditionalprobabilitydistributionsinthesemodelsarerepresented\n",
      "          byneuralnetworks(sometimesextremelysimpleneuralnetworks,suchaslogistic\n",
      "           regression).Thegraphstructureofthesemodelsisthecompletegraph.They\n",
      "            decomposeajointprobabilityovertheobservedvariablesusingthechainruleof\n",
      "         probabilitytoobtainaproductofconditionalsoftheformP(x d |x d − 1     ,...,x 1).\n",
      "    Suchmodelshavebeencalled   fully-visibleBayesnetworks   (FVBNs)andused\n",
      "     successfully inmanyforms, ﬁrst withlogistic regressionfor eachconditional\n",
      "           distribution(Frey1998,),andthenwithneuralnetworkswithhiddenunits(Bengio\n",
      "         andBengio2000bLarochelleandMurray2011 , ; ,).Insome forms ofauto-\n",
      "          regressivenetworks,suchasNADE( ,),describedin LarochelleandMurray2011\n",
      "             section ,wecanintroduceaformofparametersharingthatbringsbotha 20.10.10\n",
      "        statisticaladvantage(feweruniqueparameters)andacomputationaladvantage\n",
      "            (lesscomputation).Thisisonemoreinstanceoftherecurringdeeplearningmotif\n",
      "   ofreuseoffeatures.\n",
      "   20.10.8LinearAuto-RegressiveNetworks\n",
      "            Thesimplestformofauto-regressivenetworkhasnohiddenunitsandnosharing\n",
      "    ofparametersorfeatures.EachP(x i |x i − 1     ,...,x 1     )isparametrizedasalinear\n",
      "          model(linearregressionforreal-valueddata,logisticregressionforbinarydata,\n",
      "           softmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()\n",
      " andhasO(d2    )parameterswhenthereared       variablestomodel.Itisillustratedin\n",
      " ﬁgure.20.8\n",
      "           Ifthevariablesarecontinuous,alinearauto-regressivemodelismerelyanother\n",
      "         waytoformulateamultivariateGaussiandistribution,capturinglinearpairwise\n",
      "    interactionsbetweentheobservedvariables.\n",
      "        Linearauto-regressivenetworksareessentiallythegeneralizationoflinear\n",
      "         classiﬁcationmethodstogenerativemodeling.Theythereforehavethesame\n",
      "7 0 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 P x ( 4  | x 1 , x 2 , x 3  ) P x ( 4  | x 1 , x 2 , x 3 ) P x ( 3  | x 1 , x 2  ) P x ( 3  | x 1 , x 2 )\n",
      " P x ( 2  | x 1  ) P x ( 2  | x 1 ) P x ( 1  ) P x ( 1 )x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4\n",
      "     Figure20.8:A fullyvisible belief networkpredictsthei  -thvariable fromthe i − 1\n",
      "         previousones. ( T o p ) ( Bo t t o m ) ThedirectedgraphicalmodelforanFVBN. Corresponding\n",
      "             computationalgraphforthelogisticFVBN,whereeachpredictionismadebyalinear\n",
      "predictor.\n",
      "          advantagesanddisadvantagesaslinearclassiﬁers.Likelinearclassiﬁers,theymay\n",
      "           betrainedwithconvexlossfunctionsandsometimesadmitclosedformsolutions\n",
      "             (asintheGaussiancase).Likelinearclassiﬁers,themodelitselfdoesnotoﬀer\n",
      "             awayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslike\n",
      "        basisexpansionsoftheinputorthekerneltrick.\n",
      "   20.10.9NeuralAuto-RegressiveNetworks\n",
      "         Neuralauto-regressivenetworks( ,,)havethesame BengioandBengio2000ab\n",
      "         left-to-rightgraphicalmodelaslogisticauto-regressivenetworks(ﬁgure)but20.8\n",
      "         employadiﬀerentparametrizationoftheconditionaldistributionswithinthat\n",
      "           graphicalmodelstructure.Thenewparametrizationismorepowerfulinthesense\n",
      "            thatitscapacitycanbeincreasedasmuchasneeded,allowingapproximationof\n",
      "         anyjointdistribution.Thenewparametrizationcanalsoimprovegeneralization\n",
      "           byintroducingaparametersharingandfeaturesharingprinciplecommontodeep\n",
      "            learningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthe\n",
      "          curseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharing\n",
      "7 0 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           thesamestructureasﬁgure.Intabulardiscreteprobabilisticmodels,each 20.8\n",
      "           conditionaldistributionisrepresentedbyatableofprobabilities,withoneentry\n",
      "           andoneparameterforeachpossibleconﬁgurationofthevariablesinvolved.By\n",
      "        usinganeuralnetworkinstead,twoadvantagesareobtained:\n",
      "1.   TheparametrizationofeachP(x i |x i − 1     ,...,x 1     )byaneuralnetworkwith\n",
      "( i−1) ×k inputsandk        outputs(ifthevariablesarediscreteandtakek\n",
      "         values,encodedone-hot)enablesonetoestimatetheconditionalprobability\n",
      "         withoutrequiringanexponentialnumberofparameters(andexamples),yet\n",
      "          stillisabletocapturehigh-orderdependenciesbetweentherandomvariables.\n",
      "2.            Insteadofhavingadiﬀerentneuralnetworkforthepredictionofeachx i,\n",
      "          a connectivity,illustratedinﬁgure,allowsonetomerge left-to-right 20.9\n",
      "           alltheneuralnetworksintoone.Equivalently,itmeansthatthehidden\n",
      "    layerfeaturescomputedforpredictingx i    canbereusedforpredictingx i k +\n",
      "( k>        0).Thehiddenunitsare thus organizedingroupsthat havethe\n",
      "      particularitythatalltheunitsinthei      -thgrouponlydependontheinput\n",
      "valuesx 1     ,...,x i         .Theparametersusedtocomputethesehiddenunitsare\n",
      "            jointlyoptimizedtoimprovethepredictionofallthevariablesinthesequence.\n",
      "            Thisisaninstanceofthereuseprinciplethatrecursthroughoutdeeplearning\n",
      "        inscenariosrangingfromrecurrentandconvolutionalnetworkarchitectures\n",
      "    tomultitaskandtransferlearning.\n",
      "x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3 P x ( 4  | x 1 , x 2 , x 3  ) P x ( 4  | x 1 , x 2 , x 3 ) P x ( 3  | x 1 , x 2  ) P x ( 3  | x 1 , x 2 )\n",
      " P x ( 2  | x 1  ) P x ( 2  | x 1 ) P x ( 1  ) P x ( 1 )\n",
      "       Figure20.9:Aneuralauto-regressivenetworkpredictsthei -thvariablexi fromthe i−1\n",
      "            previousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenotedh i)\n",
      "   thatarefunctionsofx 1     ,...,x i        canbereusedinpredictingallthesubsequentvariables\n",
      "x i + 1 ,x i + 2     ,...,x d.\n",
      "7 0 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "EachP(x i |x i − 1     ,...,x 1       )canrepresentaconditionaldistributionbyhaving\n",
      "          outputsoftheneuralnetworkpredictparametersoftheconditionaldistribution\n",
      "ofx i          ,asdiscussedinsection .Althoughtheoriginalneuralauto-regressive 6.2.1.1\n",
      "          networkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariate\n",
      "             data(withasigmoidoutputforaBernoullivariableorsoftmaxoutputfora\n",
      "           multinoullivariable),itisnaturaltoextendsuchmodelstocontinuousvariables\n",
      "        orjointdistributionsinvolvingbothdiscreteandcontinuousvariables.\n",
      " 20.10.10NADE\n",
      "The   neuralauto-regressivedensityestimator     (NADE)isaverysuccessful\n",
      "          recentformofneuralauto-regressivenetwork(LarochelleandMurray2011,).The\n",
      "            connectivityisthesameasfortheoriginalneuralauto-regressivenetworkofBengio\n",
      "          andBengio2000b(),butNADEintroducesanadditionalparametersharingscheme,\n",
      "             asillustratedinﬁgure.Theparametersofthehiddenunitsofdiﬀerentgroups 20.10\n",
      "  jareshared.\n",
      " TheweightsW\n",
      "j , k , i fromthei -thinputx i tothek   -thelementofthej -thgroup\n",
      "x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3 P x ( 4  | x 1 , x 2 , x 3  ) P x ( 4  | x 1 , x 2 , x 3 ) P x ( 3  | x 1 , x 2  ) P x ( 3  | x 1 , x 2 )\n",
      " P x ( 2  | x 1  ) P x ( 2  | x 1 ) P x ( 1  ) P x ( 1 )\n",
      "W : 1 , W : 1 , W : 1 ,\n",
      "W : 2 , W : 2 , W : 3 ,\n",
      "           Figure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).The\n",
      "     hiddenunitsareorganizedingroups h( ) j    sothatonlytheinputsx1     ,...,xiparticipate\n",
      " incomputing h( ) i andpredictingP(x j |x j − 1     ,...,x 1 ),for  j>i   .NADEisdiﬀerentiated\n",
      "            fromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharing\n",
      "pattern:W\n",
      "j , k , i=W k , i             isshared(indicatedintheﬁgurebytheuseofthesamelinepattern\n",
      "             foreveryinstanceofareplicatedweight)foralltheweightsgoingoutfromx i tothek-th\n",
      "           unitofanygroup.Recallthatthevector ji≥ (W 1 , i ,W 2 , i     ,...,W n , i   )isdenoted W : , i.\n",
      "7 0 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "   ofhiddenunith( ) j\n",
      "k       ()aresharedamongthegroups: ji≥\n",
      "W\n",
      "j , k , i= W k , i . (20.83)\n",
      "        Theremainingweights,where,arezero. j<i\n",
      "          LarochelleandMurray2011()chosethissharingschemesothatforward\n",
      "          propagationinaNADEmodelwouldlooselyresemblethecomputationsperformed\n",
      "               inmeanﬁeldinferencetoﬁllinmissinginputsinanRBM.Thismeanﬁeldinference\n",
      "            correspondstorunningarecurrentnetworkwithsharedweights,andtheﬁrststep\n",
      "               ofthatinferenceisthesameasinNADE.TheonlydiﬀerenceisthatwithNADE,\n",
      "           theoutputweightsconnectingthehiddenunitstotheoutputareparametrized\n",
      "            independentlyfromtheweightsconnectingtheinputunitstothehiddenunits.In\n",
      "          theRBM,thehidden-to-outputweightsarethetransposeoftheinput-to-hidden\n",
      "             weights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestep\n",
      "      ofthemeanﬁeldrecurrentinferencebutk     steps.ThisapproachiscalledNADE-k\n",
      "   (Raiko2014etal.,).\n",
      "         Asmentionedpreviously,auto-regressivenetworksmaybeextendedtoprocess\n",
      "         continuous-valueddata.Aparticularlypowerfulandgenericwayofparametrizing\n",
      "            acontinuousdensityisasaGaussianmixture(introducedinsection)with 3.9.6\n",
      " mixtureweightsα i      (thecoeﬃcientorpriorprobabilityforcomponenti ),per-\n",
      "  componentconditionalmeanµ i   andper-componentconditionalvarianceσ2\n",
      "i. A\n",
      "            modelcalledRNADE( ,)usesthisparametrizationtoextendNADE Uriaetal.2013\n",
      "            torealvalues.Aswithothermixturedensitynetworks,theparametersofthis\n",
      "          distributionareoutputsofthenetwork,withthemixtureweightprobabilities\n",
      "            producedbyasoftmaxunit,andthevariancesparametrizedsothattheyare\n",
      "         positive. Stochasticgradientdescentcanbenumericallyill-behavedduetothe\n",
      "    interactionsbetweentheconditionalmeansµ i   andtheconditionalvariancesσ2\n",
      "i.\n",
      "             Toreducethisdiﬃculty, ()useapseudogradientthatreplacesthe Uriaetal.2013\n",
      "       gradientonthemean,intheback-propagationphase.\n",
      "        Anotherveryinterestingextensionoftheneuralauto-regressivearchitectures\n",
      "              getsridoftheneedtochooseanarbitraryorderfortheobservedvariables(Uria\n",
      "               etal.,).Inauto-regressivenetworks,theideaistotrainthenetworktobeable 2014\n",
      "            tocopewithanyorderbyrandomlysamplingordersandprovidingtheinformation\n",
      "               tohiddenunitsspecifyingwhichoftheinputsareobserved(ontherightsideofthe\n",
      "            conditioningbar)andwhicharetobepredictedandarethusconsideredmissing\n",
      "                (ontheleftsideoftheconditioningbar).Thisisnicebecauseitallowsonetouse\n",
      "          atrainedauto-regressivenetworktoperformanyinferenceproblem(i.e.,predict\n",
      "            orsamplefromtheprobabilitydistributionoveranysubsetofvariablesgivenany\n",
      "          subset)extremelyeﬃciently.Finally,sincemanyordersofvariablesarepossible\n",
      "7 0 6    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "(n !forn    variables)andeachordero    ofvariablesyieldsadiﬀerentp(  x|o  ),wecan\n",
      "         formanensembleofmodelsformanyvaluesof:o\n",
      "p e n s e m b l e() =x1\n",
      "kk \n",
      "i =1  po(x|( ) i ). (20.84)\n",
      "          Thisensemblemodelusuallygeneralizesbetterandassignshigherprobabilityto\n",
      "            thetestsetthandoesanindividualmodeldeﬁnedbyasingleordering.\n",
      "            Inthesamepaper,theauthorsproposedeepversionsofthearchitecture,but\n",
      "          unfortunatelythatimmediatelymakescomputationasexpensiveasintheoriginal\n",
      "          neuralauto-regressivenetwork( ,).Theﬁrstlayerand BengioandBengio2000b\n",
      "       theoutputlayercanstillbecomputedinO(nh     )multiply-addoperations,asinthe\n",
      "  regularNADE,whereh          isthenumberofhiddenunits(thesizeofthegroupsh i,\n",
      "       inﬁguresand),whereasitis 20.1020.9 O(n2h      )in ().For BengioandBengio2000b\n",
      "       theotherhiddenlayers,however,thecomputationisO(n2h2   )ifevery“previous”\n",
      "  groupatlayerl        participatesinpredictingthe“next”groupatlayerl  +1,assuming\n",
      "n  groupsofh       hiddenunitsateachlayer.Makingthei   -thgroupatlayerl  +1only\n",
      "  dependonthei         -thgroup,asin (),atlayer Uriaetal.2014 l   reducesittoO(nh2),\n",
      "         whichisstilltimesworsethantheregularNADE. h\n",
      "    20.11DrawingSamplesfromAutoencoders\n",
      "             Inchapter,wesawthatmanykindsofautoencoderslearnthedatadistribution. 14\n",
      "         Therearecloseconnectionsbetweenscorematching,denoisingautoencoders,and\n",
      "        contractiveautoencoders.Theseconnectionsdemonstratethatsomekindsof\n",
      "             autoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhow\n",
      "     todrawsamplesfromsuchmodels.\n",
      "         Somekindsofautoencoders,suchasthevariationalautoencoder,explicitly\n",
      "        representaprobabilitydistributionandadmitstraightforwardancestralsampling.\n",
      "       MostotherkindsofautoencodersrequireMCMCsampling.\n",
      "          Contractiveautoencodersaredesignedtorecoveranestimateofthetangent\n",
      "            planeofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwith\n",
      "             injectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifai\n",
      "              etal. etal. ,;2012Mesnil,).Thismanifolddiﬀusiontechniqueisakindof 2012\n",
      " Markovchain.\n",
      "             ThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoising\n",
      "autoencoder.\n",
      "7 0 7    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "       20.11.1MarkovChainAssociatedwithAnyDenoisingAutoen-\n",
      "coder\n",
      "              Theabovediscussionleftopenthequestionofwhatnoisetoinjectandwhereto\n",
      "            obtainaMarkovchainthatwouldgeneratefromthedistributionestimatedbythe\n",
      "            autoencoder. ()showedhowtoconstructsuchaMarkovchain Bengioetal.2013c\n",
      "for   generalizeddenoising autoencoders   .Generalizeddenoisingautoencoders\n",
      "             arespeciﬁedbyadenoisingdistributionforsamplinganestimateofthecleaninput\n",
      "   giventhecorruptedinput.\n",
      "           EachstepoftheMarkovchainthatgeneratesfromtheestimateddistribution\n",
      "        consistsofthefollowingsubsteps,illustratedinﬁgure:20.11\n",
      "1.     Startingfromthepreviousstatex    ,injectcorruptionnoise,sampling˜xfrom\n",
      "C(˜  xx|).\n",
      "xx˜ x ˜ xh h\n",
      "ωω\n",
      "ˆ x ˆ xC ( ˜   x x | )   p ( ) x | ωf g\n",
      "             Figure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-\n",
      "            coder,whichgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythe\n",
      "           denoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruption\n",
      "process C instatex ,yielding˜x     ,(b)encodingitwithfunction f ,yieldingh= f(˜x),\n",
      "     (c)decodingtheresultwithfunction g  ,yieldingparametersω  forthereconstruction\n",
      "   distribution,and(d)givenω        ,samplinganewstatefromthereconstructiondistribution\n",
      "p( x |ω= g( f(˜x       ))).Inthetypicalsquaredreconstructionerrorcase, g(h )=ˆx ,which\n",
      "estimates E[ x|˜x         ],corruptionconsistsofaddingGaussiannoise,andsamplingfrom\n",
      "p( x |ω           )consistsofaddingGaussiannoiseasecondtimetothereconstructionˆx. The\n",
      "            latternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereas\n",
      "              theinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellasthe\n",
      "            extenttowhichtheestimatorsmoothstheempiricaldistribution( ,).Inthe Vincent2011\n",
      "    exampleillustratedhere,onlythe Cand p     conditionalsarestochasticsteps( fand gare\n",
      "          deterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,\n",
      "        asingenerativestochasticnetworks( ,). Bengioetal.2014\n",
      "7 0 8    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "2. Encode˜   xintoh= ( f˜x).\n",
      "            3. Decodetoobtaintheparameters of h ωh= ( g) p g p ( = x |ω ()) = h (x|˜x).\n",
      "          4. Samplethenextstatefromx p g p ( = x|ω ()) = h (x|˜x).\n",
      "        Bengio 2014etal.()showedthatiftheautoencoder p(x |˜x   )formsaconsistent\n",
      "         estimatorofthecorrespondingtrueconditionaldistribution,thenthestationary\n",
      "           distributionoftheaboveMarkovchainformsaconsistentestimator(albeitan\n",
      "       implicitone)ofthedata-generatingdistributionof.x\n",
      "    20.11.2ClampingandConditionalSampling\n",
      "        SimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations\n",
      "             (suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-\n",
      "bution p(x f |x o      ),simplybyclampingtheobservedunitsx o  andonlyresampling\n",
      "  thefreeunitsx fgivenx o        andthesampledlatentvariables(ifany).Forexample,\n",
      "            MP-DBMscanbeinterpretedasaformofdenoisingautoencoderandareable\n",
      "            tosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentin\n",
      "             MP-DBMstoperformthesameoperation( ,). () Bengioetal.2014Alainetal.2015\n",
      "             identiﬁedamissingconditionfromProposition1of (),whichis Bengioetal.2014\n",
      "           thatthetransitionoperator(deﬁnedbythestochasticmappinggoingfromone\n",
      "           stateofthechaintothenext)shouldsatisfyapropertycalled  detailedbalance,\n",
      "           whichspeciﬁesthataMarkovchainatequilibriumwillremaininequilibrium\n",
      "         whetherthetransitionoperatorisruninforwardorreverse.\n",
      "              Anexperimentinclampinghalfofthepixels(therightpartoftheimage)and\n",
      "            runningtheMarkovchainontheotherhalfisshowninﬁgure.20.12\n",
      "   20.11.3Walk-BackTrainingProcedure\n",
      "             Thewalk-backtrainingprocedurewasproposedby ()asaway Bengioetal.2013c\n",
      "         toacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders.\n",
      "        Insteadofperformingaone-stepencode-decodereconstruction,thisprocedure\n",
      "          consistsofalternativemultiplestochasticencode-decodesteps(asinthegenerative\n",
      "           Markovchain),initializedatatrainingexample(justaswiththecontrastive\n",
      "          divergencealgorithm,describedinsection),andpenalizingthelastprobabilistic 18.2\n",
      "       reconstructions(orallthereconstructionsalongtheway).\n",
      " Trainingwith k          stepsisequivalent(inthesenseofachievingthesamestationary\n",
      "           distribution)astrainingwithonestepbutpracticallyhastheadvantagethat\n",
      "          spuriousmodesfurtherfromthedatacanberemovedmoreeﬃciently.\n",
      "7 0 9    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "              Figure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkov\n",
      "               chainbyresamplingonlythelefthalfateachstep.ThesesamplescomefromaGSN\n",
      "            trainedtoreconstructMNISTdigitsateachtimestepusingthewalk-backprocedure.\n",
      "   20.12GenerativeStochasticNetworks\n",
      "  Generativestochasticnetworks        ,orGSNs( ,)aregeneraliza- Bengioetal.2014\n",
      "       tionsofdenoisingautoencodersthatincludelatentvariables h  inthegenerative\n",
      "          Markovchain,inadditiontothevisiblevariables(usuallydenoted). x\n",
      "          AGSNisparametrizedbytwoconditionalprobabilitydistributionsthatspecify\n",
      "     onestepoftheMarkovchain:\n",
      "1. p( x( ) k | h( ) k           )tellshowtogeneratethenextvisiblevariablegiventhecurrent\n",
      "          latentstate.Sucha“reconstructiondistribution”isalsofoundindenoising\n",
      "    autoencoders,RBMs,DBNsandDBMs.\n",
      "2. p( h( ) k | h( 1 ) k − , x( 1 ) k −         )tellshowtoupdatethelatentstatevariable,given\n",
      "      thepreviouslatentstateandvisiblevariable.\n",
      "        DenoisingautoencodersandGSNsdiﬀerfromclassicalprobabilisticmodels\n",
      "           (directedorundirected)inthattheyparametrizethegenerativeprocessitselfrather\n",
      "           thanthemathematicalspeciﬁcationofthejointdistributionofvisibleandlatent\n",
      "            variables.Instead,thelatterisdeﬁned , ,asthestationary implicitlyifitexists\n",
      "7 1 0    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           distributionofthegenerativeMarkovchain.Theconditionsforexistenceofthe\n",
      "           stationarydistributionaremildandarethesameconditionsrequiredbystandard\n",
      "          MCMCmethods(seesection).Theseconditionsarenecessarytoguarantee 17.3\n",
      "              thatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransition\n",
      "      distributions(forexample,iftheyaredeterministic).\n",
      "           OnecouldimaginediﬀerenttrainingcriteriaforGSNs.Theoneproposedand\n",
      "           evaluatedby ()issimplyreconstructionlog-probabilityonthe Bengioetal.2014\n",
      "           visibleunits,justasfordenoisingautoencoders.Thisisachievedbyclamping\n",
      "x(0 )=x         totheobservedexampleandmaximizingtheprobabilityofgeneratingx\n",
      "       atsomesubsequenttimesteps,thatis,maximizing  log p(x( ) k=  x|h( ) k ),where\n",
      "h( ) k     issampledfromthechain,givenx(0 )=x      .Inordertoestimatethegradient\n",
      "of  log p(x( ) k=  x|h( ) k            )withrespecttotheotherpiecesofthemodel,Bengioetal.\n",
      "        ()usethereparametrizationtrick,introducedinsection. 2014 20.9\n",
      "        Thewalk-backtrainingprocedure(describedinsection ) wasused 20.11.3\n",
      "         ( ,)toimprovetrainingconvergenceofGSNs. Bengioetal.2014\n",
      "  20.12.1Discrimin antGSNs\n",
      "            TheoriginalformulationofGSNs( ,)wasmeantforunsupervised Bengioetal.2014\n",
      "   learningandimplicitlymodeling p(x   )forobserveddatax     ,butitispossibleto\n",
      "       modifytheframeworktooptimize . p( )y|x\n",
      "           Forexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,by\n",
      "        onlyback-propagatingthereconstructionlog-probabilityovertheoutputvariables,\n",
      "           keepingtheinputvariablesﬁxed.Theyappliedthissuccessfullytomodelsequences\n",
      "       (proteinsecondarystructure)andintroduceda(one-dimensional)convolutional\n",
      "           structureinthetransitionoperatorofthe Markovchain.Itisimportantto\n",
      "             rememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequence\n",
      "              foreachlayer,andthatsequenceistheinputforcomputingotherlayervalues(say\n",
      "           theonebelowandtheoneabove)atthenexttimestep.\n",
      "           Hence,theMarkovchainisreallyovertheoutputvariable(andassociated\n",
      "           higher-levelhiddenlayers),andtheinputsequenceonlyservestoconditionthat\n",
      "           chain,withback-propagationenablingittolearnhowtheinputsequencecan\n",
      "           conditiontheoutputdistributionimplicitlyrepresentedbytheMarkovchain.Itis\n",
      "            thereforeacaseofusingtheGSNinthecontextofstructuredoutputs.\n",
      "           ZöhrerandPernkopf2014()introducedahybridmodelthatcombinesasu-\n",
      "            pervisedobjective(asintheabovework)andanunsupervisedobjective(asin\n",
      "            theoriginalGSNwork)bysimplyadding(withadiﬀerentweight)thesupervised\n",
      "        andunsupervisedcosts,thatis,thereconstructionlog-probabilitiesofyandx\n",
      "7 1 1    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "         respectively. SuchahybridcriterionhadpreviouslybeenintroducedforRBMs\n",
      "         byLarochelleandBengio2008().Theyshowimprovedclassiﬁcationperformance\n",
      "  usingthisscheme.\n",
      "   20.13OtherGenerationSchemes\n",
      "          ThemethodswehavedescribedsofaruseeitherMCMCsampling, ancestral\n",
      "            sampling,orsomemixtureofthetwotogeneratesamples. Whilethesearethe\n",
      "            mostpopularapproachestogenerativemodeling,theyarebynomeanstheonly\n",
      "approaches.\n",
      "     Sohl-Dickstein 2015etal.()developeda  diﬀusioninversion  trainingscheme\n",
      "         forlearningagenerativemodel,basedonnonequilibriumthermodynamics.The\n",
      "             approachisbasedontheideathattheprobabilitydistributionswewishtosample\n",
      "           fromhavestructure.Thisstructurecangraduallybedestroyedbyadiﬀusion\n",
      "       processthat incrementallychangestheprobabilitydistribution tohavemore\n",
      "              entropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytraining\n",
      "          amodelthatgraduallyrestoresthestructuretoanunstructureddistribution.\n",
      "            Byiterativelyapplyingaprocessthatbringsadistributionclosertothetarget\n",
      "          one,wecangraduallyapproachthattargetdistribution.Thisapproachresembles\n",
      "             MCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample.\n",
      "           However,themodelisdeﬁnedtobetheprobabilitydistributionproducedby\n",
      "             theﬁnalstepofthechain. Inthissense,thereisnoapproximationinducedby\n",
      "          theiterativeprocedure.Theapproachintroducedby () Sohl-Dicksteinetal.2015\n",
      "           isalsoveryclosetothegenerativeinterpretationofthedenoisingautoencoder\n",
      "          (section ).Aswiththedenoisingautoencoder,diﬀusioninversiontrainsa 20.11.1\n",
      "          transitionoperatorthatattemptstoprobabilisticallyundotheeﬀectofadding\n",
      "            somenoise.Thediﬀerenceisthatdiﬀusioninversionrequiresundoingonlyonestep\n",
      "               ofthediﬀusionprocess,ratherthantravelingallthewaybacktoacleandatapoint.\n",
      "         Thisaddressesthefollowingdilemmapresentwiththeordinaryreconstruction\n",
      "          log-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethe\n",
      "            learneronlyseesconﬁgurationsnearthedatapoints,whilewithlargelevelsof\n",
      "             noiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistribution\n",
      "          ishighlycomplexandmultimodal).Withthediﬀusioninversionobjective,the\n",
      "             learnercanlearntheshapeofthedensityaroundthedatapointsmoreprecisely\n",
      "              aswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints.\n",
      "      Anotherapproachtosamplegenerationisthe   approximateBayesiancom-\n",
      "putation           (ABC)framework( ,).Inthisapproach,samplesare Rubinetal.1984\n",
      "            rejectedormodiﬁedtomakethemomentsofselectedfunctionsofthesamples\n",
      "7 1 2    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "             matchthoseofthedesireddistribution.Whilethisideausesthemomentsofthe\n",
      "            samplesasinmomentmatching,itisdiﬀerentfrommomentmatchingbecauseit\n",
      "          modiﬁesthesamplesthemselves,ratherthantrainingthemodeltoautomatically\n",
      "           emitsampleswiththecorrectmoments. ()showedhow BachmanandPrecup2015\n",
      "                touseideasfromABCinthecontextofdeeplearning,byusingABCtoshapethe\n",
      "   MCMCtrajectoriesofGSNs.\n",
      "          Weexpectthatmanyotherpossibleapproachestogenerativemodelingawait\n",
      "discovery.\n",
      "   20.14EvaluatingGenerativeModels\n",
      "         Researchersstudyinggenerativemodelsoftenneedtocompareonegenerative\n",
      "            modeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerative\n",
      "          modelisbetteratcapturingsomedistributionthanthepre-existingmodels.\n",
      "             Thiscanbeadiﬃcultandsubtletask.Often,wecannotactuallyevaluatethe\n",
      "            log-probabilityofthedataunderthemodel,butcanevaluateonlyanapproximation.\n",
      "             Inthesecases,itisimportanttothinkandcommunicateclearlyaboutwhatexactly\n",
      "            isbeingmeasured.Forexample,supposewecanevaluateastochasticestimateof\n",
      "            thelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihood\n",
      "                 formodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwe\n",
      "           careaboutdeterminingwhichmodelhasabetterinternalrepresentationofthe\n",
      "            distribution,weactuallycannottell,unlesswehavesomewayofdetermininghow\n",
      "                loosetheboundformodelBis.However,ifwecareabouthowwellwecanuse\n",
      "              themodelinpractice,forexampletoperformanomalydetection,thenitisfairto\n",
      "               saythatamodelispreferablebasedonacriterionspeciﬁctothepracticaltaskof\n",
      "            interest,forexample,basedonrankingtestexamplesandrankingcriteriasuchas\n",
      "  precisionandrecall.\n",
      "          Anothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetrics\n",
      "             areoftenhardresearchproblemsinandofthemselves.Itcanbeverydiﬃcult\n",
      "            toestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuse\n",
      "  AIStoestimate  log Z   inordertocomputelog ˜ p( x)  −log Z    foranewmodelwe\n",
      "          havejustinvented.AcomputationallyeconomicalimplementationofAISmayfail\n",
      "         toﬁndseveralmodesofthemodeldistributionandunderestimate Z  ,whichwill\n",
      "   resultinusoverestimating  log p( x          ).Itcanthusbediﬃculttotellwhetherahigh\n",
      "             likelihoodestimateistheresultofagoodmodelorabadAISimplementation.\n",
      "            Otherﬁeldsofmachinelearningusuallyallowforsomevariationinthepre-\n",
      "           processingofthedata.Forexample,whencomparingtheaccuracyofobject\n",
      "7 1 3    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "          recognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimages\n",
      "           slightlydiﬀerentlyforeachalgorithmbasedonwhatkindofinputrequirements\n",
      "          ithas.Generativemodelingisdiﬀerentbecausechangesinpreprocessing,even\n",
      "            verysmallandsubtleones,arecompletelyunacceptable.Anychangetotheinput\n",
      "           datachangesthedistributiontobecapturedandfundamentallyaltersthetask.\n",
      "            Forexample,multiplyingtheinputby0.1willartiﬁciallyincreaselikelihoodbya\n",
      "  factorof10.\n",
      "        Issueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodels\n",
      "           ontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks.\n",
      "           MNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints\n",
      "              inarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthe\n",
      "           grayscalevaluesasprobabilitiesforbinarysamples.Itisessentialtocompare\n",
      "          real-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonly\n",
      "          tootherbinary-valuedmodels. Otherwisethelikelihoodsmeasuredarenotonthe\n",
      "            samespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,while\n",
      "              forreal-valuedmodels,itcanbearbitrarilyhigh,sinceitisthemeasurementofa\n",
      "           density.Amongbinarymodels,itisimportanttocomparemodelsusingexactly\n",
      "                thesamekindofbinarization.Forexample,wemightbinarizeagraypixelto0or1\n",
      "             bythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing\n",
      "               1isgivenbythegraypixelintensity.Ifweusetherandombinarization,wemight\n",
      "             binarizethewholedatasetonce,orwemightdrawadiﬀerentrandomexamplefor\n",
      "             eachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthese\n",
      "         threeschemesyieldswildlydiﬀerentlikelihoodnumbers,andwhencomparing\n",
      "            diﬀerentmodelsitisimportantthatbothmodelsusethesamebinarizationscheme\n",
      "            fortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom\n",
      "            binarizationstepshareaﬁlecontainingtheresultsoftherandombinarization,so\n",
      "             thatthereisnodiﬀerenceinresultsbasedondiﬀerentoutcomesofthebinarization\n",
      "step.\n",
      "          Becausebeingabletogeneraterealisticsamplesfromthedatadistribution\n",
      "            isoneofthegoalsofagenerativemodel,practitionersoftenevaluategenerative\n",
      "               modelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbythe\n",
      "           researchersthemselves,butbyexperimentalsubjectswhodonotknowthesource\n",
      "              ofthesamples(Denton 2015etal.,).Unfortunately,itispossibleforaverypoor\n",
      "            probabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyif\n",
      "             themodelonlycopiessomeofthetrainingexamplesisillustratedinﬁgure.16.1\n",
      "              Theideaistoshowforsomeofthegeneratedsamplestheirnearestneighborin\n",
      "          thetrainingset,accordingtoEuclideandistanceinthespaceof x  . Thistestis\n",
      "             intendedtodetectthecasewherethemodeloverﬁtsthetrainingsetandjust\n",
      "          reproducestraininginstances.Itisevenpossibletosimultaneouslyunderﬁtand\n",
      "7 1 4    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      "           overﬁtyetstillproducesamplesthatindividuallylookgood.Imagineagenerative\n",
      "             modeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethe\n",
      "             trainingimagesofdogs.Suchamodelhasclearlyoverﬁt,becauseitdoesnot\n",
      "              producesimagesthatwerenotinthetrainingset,butithasalsounderﬁt,because\n",
      "             itassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserver\n",
      "              wouldjudgeeachindividualimageofadogtobehighquality.Inthissimple\n",
      "              example,itwouldbeeasyforahumanobserverwhocaninspectmanysamplesto\n",
      "            determinethatthecatsareabsent.Inmorerealisticsettings,agenerativemodel\n",
      "              trainedondatawithtensofthousandsofmodesmayignoreasmallnumberof\n",
      "             modes,andahumanobserverwouldnoteasilybeabletoinspectorremember\n",
      "      enoughimagestodetectthemissingvariation.\n",
      "    Since thevisual quality ofsamples is not areliable guide, weoften also\n",
      "             evaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisis\n",
      "         computationallyfeasible.Unfortunately,insomecasesthelikelihoodseemsnot\n",
      "             tomeasureanyattributeofthemodelthatwereallycareabout.Forexample,\n",
      "          real-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigning\n",
      "          arbitrarilylowvariancetobackground pixelsthatneverchange.Modelsand\n",
      "          algorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,even\n",
      "               thoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacost\n",
      "    approaching negativeinﬁnity ispresent forany kindof maximum likelihood\n",
      "            problemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsof\n",
      "             MNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstrongly\n",
      "          suggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels.\n",
      "            Theis2015etal.()reviewmanyoftheissuesinvolvedinevaluatinggenerative\n",
      "           models,includingmanyoftheideasdescribedabove.Theyhighlightthefact\n",
      "             thattherearemanydiﬀerentusesofgenerativemodelsandthatthechoiceof\n",
      "            metricmustmatchtheintendeduseofthemodel.Forexample,somegenerative\n",
      "            modelsarebetteratassigninghighprobabilitytomostrealisticpoints,whileother\n",
      "          generativemodelsarebetteratrarelyassigninghighprobabilitytounrealistic\n",
      "           points.Thesediﬀerencescanresultfromwhetheragenerativemodelisdesigned\n",
      " tominimize D K L( p d a ta p m o d e l )or D K L( p m o d e l p d a ta     ),asillustratedinﬁgure.3.6\n",
      "               Unfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismost\n",
      "            suitedfor,allthemetricscurrentlyinusecontinuetohaveseriousweaknesses.\n",
      "            Oneofthemostimportantresearchtopicsingenerativemodelingisthereforenot\n",
      "            justhowtoimprovegenerativemodels,butinfact,designingnewtechniquesto\n",
      "  measureourprogress.\n",
      "7 1 5    C HAP T E R 2 0 . D E E P G E NE R A T I VE M O D E L S\n",
      " 20.15Conclusion\n",
      "            Traininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodels\n",
      "            understandtheworldrepresentedinthegiventrainingdata.Bylearningamodel\n",
      "p m o d e l(x   )andarepresentation p m o d e l(  hx|     ),agenerativemodelcanprovide\n",
      "          answerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariables\n",
      "inx       andcanoﬀermanydiﬀerentwaysofrepresentingx  bytakingexpectations\n",
      "ofh          atdiﬀerentlayersofthehierarchy. Generativemodelsholdthepromiseto\n",
      "            provideAIsystemswithaframeworkforallthemanydiﬀerentintuitiveconcepts\n",
      "            theyneedtounderstand,givingthemtheabilitytoreasonabouttheseconcepts\n",
      "               inthefaceofuncertainty.Wehopethatourreaderswillﬁndnewwaystomake\n",
      "          theseapproachesmorepowerfulandcontinuethejourneytounderstandingthe\n",
      "     principlesthatunderlielearningandintelligence.\n",
      "7 1 6B i bl i ograp h y\n",
      "               Abadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,\n",
      "              A.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,\n",
      "             Jia,Y.,Jozefowicz ,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Mané,D.,Monga,R.,\n",
      "             Moore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,\n",
      "            Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Viégas,F.,Vinyals,O.,Warden,\n",
      "            P.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scale\n",
      "         machinelearningonheterogeneoussystems.Softwareavailablefromtensorﬂow.org.,25\n",
      " 210441,\n",
      "              Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).Alearningalgorithmfor\n",
      "       Boltzmannmachines.CognitiveScience,,147–169. , 9 567651\n",
      "           Alain,G.andBengio,Y.(2013). Whatregularizedauto-encoderslearnfromthedata\n",
      "         generatingdistribution.In .,,, ICLR’2013,arXiv:1211.4246504509512518\n",
      "             Alain,G.,Bengio,Y.,Yao,L.,ÉricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015).\n",
      "      GSNs:Generativestochasticnetworks.arXiv:1503. 05571. ,507709\n",
      "            Allen,R.B.(1987).Severalstudiesonnaturallanguageandback-propagation.InIEEE\n",
      "          FirstInternationalConferenceonNeuralNetworks,volume2,pages335–341, San\n",
      " Diego.468\n",
      "             Anderson,E.(1935).TheIrisesoftheGaspéPeninsula.BulletinoftheAmericanIris\n",
      "   Society,,2–5. 5 919\n",
      "            Ba,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisual\n",
      "   attention. . arXiv:1412.7755688\n",
      "          Bachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswith\n",
      "          collaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachine\n",
      "           Learning,ICML2015,Lille,France,6-11July2015,pages1964–1972. 713\n",
      "            Bacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationin\n",
      "         neuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConference\n",
      "        onReinforcementLearningandDecisionMaking(RLDM2015).445\n",
      "717BIBLIOGRAPHY\n",
      "             Bagnell,J.A.andBradley,D.M.(2009).Diﬀerentiablesparsecoding.InD.Koller,\n",
      "           D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformation\n",
      "      ProcessingSystems21(NIPS’08),pages113–120. 494\n",
      "            Bahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointly\n",
      "             learningtoalignandtranslate.In .,,,,, ICLR’2015,arXiv:1409.04732599392412415\n",
      "  459470471,,\n",
      "              Bahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987). Speechrecognition\n",
      "        withcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage, 2,\n",
      " 219–234. 453\n",
      "           Baldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:\n",
      "          Learningfromexampleswithoutlocalminima.NeuralNetworks,,53–58. 2 283\n",
      "             Baldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthe\n",
      "         pastandthefutureinproteinsecondarystructureprediction. , Bioinformatics 1 5(11),\n",
      " 937–946. 388\n",
      "       Baldi, P., Sadowski, P., andWhiteson, D.(2014).Searchingforexoticparticlesin\n",
      "        high-energyphysicswithdeeplearning.Naturecommunications,. 526\n",
      "             Ballard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation.\n",
      " Nature.447\n",
      "          Barlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295–311. 1 144\n",
      "           Barron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidal\n",
      "        function.IEEETrans.onInformationTheory,,930–945. 3 9 195\n",
      "           Bartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversity\n",
      " Press.486\n",
      "          Basilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:Theoryand\n",
      "  Applications.Wiley.486\n",
      "          Bastien,F.,Lamblin,P., Pascanu,R.,Bergstra,J., Goodfellow,I.J.,Bergeron,A.,\n",
      "           Bouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements.\n",
      "           DeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,, 2580210\n",
      " 218441,\n",
      "         Basu,S.andChristensen,J.(2013). Teachingclassiﬁcationboundariestohumans. In\n",
      "  AAAI’2013.325\n",
      "           Baxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternational\n",
      "         ConferenceonComputationalLearningTheory(COLT’95),pages311–320, SantaCruz,\n",
      "   California.ACMPress.241\n",
      "7 1 8BIBLIOGRAPHY\n",
      "          Bayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXiv\n",
      " e-prints.262\n",
      "            Becker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfaces\n",
      "      inrandom-dotstereograms.Nature,,161–163. 3 5 5 539\n",
      "          Behnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstraction\n",
      "         pyramid.Int.J.ComputationalIntelligenceandApplications,(4),427–438. 1 511\n",
      "             Beiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthreshold\n",
      "       logic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson, 1 4 (5),1217–\n",
      " 1243.446\n",
      "        Belkin, M.and Niyogi, P.(2002).Laplacianeigenmapsandspectraltechniquesfor\n",
      "          embeddingandclustering. InT.Dietterich,S.Becker,andZ.Ghahramani,editors,\n",
      "         AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.\n",
      "  MITPress.240\n",
      "           Belkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionand\n",
      "       datarepresentation.NeuralComputation,(6),1373–1396. , 1 5 160516\n",
      "            Bengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationin\n",
      "      neuralnetworksforfastermodels.arXiv:1511. 06297.445\n",
      "        Bengio, S. andBengio, Y. (2000a). Taking onthecurseofdimensionalityinjoint\n",
      "         distributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,special\n",
      "         issueonDataMiningandKnowledgeDiscovery,(3),550–557. 1 1 703\n",
      "            Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingfor\n",
      "        sequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506. 03099.\n",
      "378\n",
      "           Bengio,Y.(1991).ArtiﬁcialNeuralNetworksandtheirApplicationtoSequenceRecognition.\n",
      "        Ph.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.402\n",
      "        Bengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,\n",
      "  1 2(8),1889–1900. 430\n",
      "          Bengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,\n",
      "     Dept.IRO,UniversitédeMontréal.462\n",
      "            Bengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,197621\n",
      "          Bengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatistical\n",
      "           LanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,\n",
      "        pages1–37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.443\n",
      "         Bengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation.\n",
      "      TechnicalReportarXiv:1510. 02777,UniversitedeMontreal.653\n",
      "7 1 9BIBLIOGRAPHY\n",
      "           Bengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-\n",
      "              layerneuralnetworks.In ,pages400–406. MITPress.,,, NIPS12 702703705707\n",
      "          Bengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence.\n",
      "     NeuralComputation,(6),1601–1621. , 2 1 509609\n",
      "             Bengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-fold\n",
      "            cross-validation.InS.Thrun,L.Saul,andB.Schölkopf,editors,AdvancesinNeural\n",
      "         InformationProcessingSystems16(NIPS’03),Cambridge,MA.MITPress,Cambridge.\n",
      "120\n",
      "             Bengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScale\n",
      "  KernelMachines.18\n",
      "            Bengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,\n",
      "           Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems\n",
      "       17(NIPS’04),pages129–136. MITPress.,157518\n",
      "            Bengio,Y.andSénécal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsby\n",
      "        importancesampling.InProceedingsofAISTATS2003.465\n",
      "           Bengio,Y.andSénécal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetraining\n",
      "         ofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks, 1 9 (4),713–722.\n",
      "465\n",
      "            Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivated\n",
      "         acousticparametersforcontinuousspeechrecognitionusingartiﬁcialneuralnetworks.\n",
      "      InProceedingsofEuroSpeech’91.,23454\n",
      "            Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussian\n",
      "             mixturehybridforspeechrecognitionordensityestimation.In ,pages175–182. NIPS4\n",
      "  MorganKaufmann.454\n",
      "            Bengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-term\n",
      "         dependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeural\n",
      "         Networks,pages1183–1195, SanFrancisco.IEEEPress.(invitedpaper).398\n",
      "           Bengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswith\n",
      "            gradientdescentisdiﬃcult.IEEETr.NeuralNets.,,,, 17396398399407\n",
      "           Bengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-\n",
      "    parameters.LearningConference,Snowbird.430\n",
      "            Bengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel.\n",
      "               InT.K.Leen,T.G.Dietterich,andV.Tresp,editors, ,pages932–938. MIT NIPS’2000\n",
      "       Press.,,,,,, 17442458461467472477\n",
      "            Bengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilistic\n",
      "      languagemodel. ,,1137–1155. , JMLR 3 461467\n",
      "7 2 0BIBLIOGRAPHY\n",
      "             Bengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convex\n",
      "       neuralnetworks.In ,pages123–130. NIPS’2005 255\n",
      "              Bengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctions\n",
      "       forlocalkernelmachines.In . NIPS’2005155\n",
      "           Bengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows.\n",
      "      In .MITPress., NIPS’2005 157517\n",
      "           Bengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wise\n",
      "             trainingofdeepnetworks.In .,,,,,, NIPS’20061318197319320526528\n",
      "            Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.In\n",
      "  ICML’09.324\n",
      "             Bengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeep\n",
      "    representations.In . ICML’2013601\n",
      "           Bengio,Y.,Léonard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradients\n",
      "       throughstochasticneuronsforconditionalcomputation. arXiv:1308.3432. ,,443445\n",
      " 685688,\n",
      "            Bengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-\n",
      "         encodersasgenerativemodels.In .,, NIPS’2013504708709\n",
      "            Bengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewand\n",
      "         newperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),\n",
      "  3 5(8),1798–1828. 552\n",
      "          Bengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014). Deepgenerative\n",
      "           stochasticnetworkstrainablebybackprop.In .,,, ICML’2014708709710711\n",
      "            Bennett,C.(1976).EﬃcientestimationoffreeenergydiﬀerencesfromMonteCarlodata.\n",
      "      JournalofComputationalPhysics,(2),245–268. 2 2 627\n",
      "         Bennett,J.andLanning,S.(2007).TheNetﬂixprize.475\n",
      "               Berger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropy\n",
      "         approachtonaturallanguageprocessing. ,,39–71. ComputationalLinguistics 2 2 468\n",
      "           Berglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastive\n",
      "       divergenceandpersistentcontrastivedivergence., . CoRR a b s/ 1 3 1 2 . 6 0 0 2612\n",
      "        Bergstra, J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor Pattern\n",
      "      Classiﬁcation.Ph.D.thesis,UniversitédeMontréal.252\n",
      "           Bergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplex\n",
      "     cell-likenetworks.In . NIPS’2009490\n",
      "7 2 1BIBLIOGRAPHY\n",
      "           Bergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J.\n",
      "      MachineLearningRes.,,281–305. , 1 3 428429\n",
      "            Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,\n",
      "             J.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpression\n",
      "        compiler.InProc.SciPy.,,,, 2580210218441\n",
      "            Bergstra,J.,Bardenet,R.,Bengio,Y.,andKégl,B.(2011).Algorithmsforhyper-parameter\n",
      "    optimization.In . NIPS’2011430\n",
      "              Berkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplex\n",
      "       cellproperties. ,(6),579–602. JournalofVision 5 491\n",
      "          Bertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScientiﬁc.\n",
      "104\n",
      "         Besag,J.(1975).Statisticalanalysisofnon-latticedata. , TheStatistician 2 4 (3),179–195.\n",
      "613\n",
      "       Bishop,C.M.(1994).Mixturedensitynetworks.185\n",
      "          Bishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks.\n",
      "         InProceedingsInternationalConferenceonArtiﬁcialNeuralNetworksICANN’95,\n",
      "     volume1,page141–148. ,238247\n",
      "           Bishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization.\n",
      "    NeuralComputation,(1),108–116. 7 238\n",
      "           Bishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,96142\n",
      "              Blum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete.\n",
      "289\n",
      "            Blumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).Learnabilityand\n",
      "          theVapnik–Chervonenkisdimension. ,(4),929––865. JournaloftheACM 3 6 112\n",
      "           Bonnet,G.(1964).Transformationsdessignauxaléatoiresàtraverslessystèmesnon\n",
      "        linéairessansmémoire.AnnalesdesTélécommunications,(9–10),203–220. 1 9 685\n",
      "    Bordes, A., Weston, J., Collobert, R., andBengio, Y.(2011).Learningstructured\n",
      "        embeddingsofknowledgebases.In . AAAI2011479\n",
      "              Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsand\n",
      "          meaningrepresentationsforopen-textsemanticparsing.AISTATS’2012.,,396479480\n",
      "             Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergy\n",
      "          functionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueon\n",
      "  LearningSemantics.479\n",
      "7 2 2BIBLIOGRAPHY\n",
      "          Bordes,A.,Usunier, N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b).\n",
      "          Translatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,\n",
      "           M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformation\n",
      "         ProcessingSystems26,pages2787–2795. CurranAssociates,Inc.479\n",
      "         Bornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR’2015,\n",
      "  arXiv:1406.2751.690\n",
      "           Bornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).Trainingbidirectional\n",
      "     Helmholtz machines.Technicalreport,arXiv:1506. 03877.690\n",
      "               Boser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-\n",
      "            malmarginclassiﬁers.InCOLT’92:Proceedingsoftheﬁfthannualworkshopon\n",
      "           Computationallearningtheory,pages144–152, NewYork,NY,USA.ACM. ,17139\n",
      "           Bottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,\n",
      "          OnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.292\n",
      "        Bottou, L.(2011).Frommachinelearning tomachinereasoning.Technicalreport,\n",
      "  arXiv.1102. 1808. ,394396\n",
      "          Bottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.434\n",
      "              Bottou,L.andBousquet,O.(2008).Thetradeoﬀsoflargescalelearning.In . NIPS’2008\n",
      " 279292,\n",
      "         Boulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporal\n",
      "        dependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgeneration\n",
      "     andtranscription.In . ICML’12682\n",
      "              Boureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolingin\n",
      "         visionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML’10).\n",
      "339\n",
      "              Boureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011). Askthelocals:\n",
      "          multi-waylocalpoolingforimagerecognition.InProc.InternationalConferenceon\n",
      "    ComputerVision(ICCV’11).IEEE.339\n",
      "          Bourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsand\n",
      "       singularvaluedecomposition.BiologicalCybernetics,,291–294. 5 9 499\n",
      "          Bourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layered\n",
      "       perceptrons.ComputerSpeechandLanguage,,1–19. 3 454\n",
      "         Boyd,S.andVandenberghe,L.(2004). .CambridgeUniversity ConvexOptimization\n",
      "     Press,NewYork,NY,USA.91\n",
      "7 2 3BIBLIOGRAPHY\n",
      "            Brady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparate\n",
      "        whereperceptronssucceed.IEEETransactionsonCircuitsandSystems, 3 6 ,665–674.\n",
      "282\n",
      "           Brakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfor\n",
      "      time-seriesimputation.JournalofMachineLearningResearch, 1 4  ,2771–2797. ,671\n",
      "695\n",
      "             Brand,M.(2003).Chartingamanifold.In ,pages961–968. MITPress., NIPS’2002 160\n",
      "516\n",
      "         Breiman,L.(1994).Baggingpredictors.MachineLearning,(2),123–140. 2 4 253\n",
      "              Breiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).Classiﬁcationand\n",
      "       RegressionTrees.WadsworthInternationalGroup,Belmont,CA.142\n",
      "            Bridle,J.S.(1990).Alphanets:arecurrent‘neural’networkarchitecturewithahidden\n",
      "       Markovmodelinterpretation.SpeechCommunication,(1),83–92. 9 182\n",
      "             Briggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009).\n",
      "           Maximinaﬃnitylearningofimagesegmentation.In ,pages1865–1873. NIPS’2009 353\n",
      "                 Brown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,Laﬀerty,J.D.,\n",
      "             Mercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation.\n",
      "    Computationallinguistics,(2),79–85. 1 6 19\n",
      "                  Brown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-\n",
      "       based-grammodelsofnaturallanguage. , n ComputationalLinguistics 1 8 ,467–479.\n",
      "458\n",
      "         Bryson,A.andHo,Y.(1969). Appliedoptimalcontrol: optimization,estimation,and\n",
      "    control.BlaisdellPub.Co.221\n",
      "             Bryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolving\n",
      "       optimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,\n",
      "    MissleandSpaceDivision.221\n",
      "    Buciluˇa, C.,Caruana,R., and Niculescu-Mizil, A. (2006).Model compression.In\n",
      "          Proceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscovery\n",
      "      anddatamining,pages535–541. ACM.443\n",
      "          Burda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders.\n",
      "    arXivpreprintarXiv:1509.00519.695\n",
      "              Cai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition.\n",
      "         InAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshop\n",
      "    on,pages291–296. IEEE.190\n",
      "7 2 4BIBLIOGRAPHY\n",
      "           Carreira-Perpiñan,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning.\n",
      "            InR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternational\n",
      "         WorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS’05),pages33–40.Society\n",
      "     forArtiﬁcialIntelligenceandStatistics.609\n",
      "          Caruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModels\n",
      "    SummerSchool,pages372–379. 241\n",
      "           Cauchy,A.(1847).Méthodegénéralepourlarésolutiondesystèmesd’équationssimul-\n",
      "            tanées.InCompterendudesséancesdel’académiedessciences,pages536–538. ,81\n",
      "221\n",
      "         Cayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,\n",
      " UCSD.160\n",
      "            Chandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACM\n",
      "     computingsurveys(CSUR),(3),15. 4 1 100\n",
      "           Chapelle,O.,Weston,J.,andSchölkopf,B.(2003).Clusterkernelsforsemi-supervised\n",
      "            learning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeural\n",
      "         InformationProcessingSystems15(NIPS’02),pages585–592, Cambridge,MA.MIT\n",
      " Press.240\n",
      "           Chapelle,O.,Schölkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MIT\n",
      "    Press,Cambridge,MA.,240539\n",
      "           Chellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeural\n",
      "    Networks forDocumentProcessing.In GuyLorette, editor, Tenth International\n",
      "          WorkshoponFrontiersinHandwritingRecognition,LaBaule(France).Universitéde\n",
      "      Rennes1,Suvisoft.http://www.suvisoft.com.,,2223440\n",
      "               Chen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariant\n",
      "        spatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervised\n",
      "   FeatureLearningWorkshop.354\n",
      "              Chen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesfor\n",
      "         languagemodeling.Computer,SpeechandLanguage,(4),359–393. , 1 3 457468\n",
      "                Chen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:\n",
      "        Asmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-\n",
      "          ceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramming\n",
      "       languagesandoperatingsystems,pages269–284. ACM.446\n",
      "                 Chen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,\n",
      "           andZhang,Z.(2015).MXNet: Aﬂexibleandeﬃcientmachinelearninglibraryfor\n",
      "       heterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25\n",
      "7 2 5BIBLIOGRAPHY\n",
      "                   Chen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,\n",
      "        etal. Microarchitecture (2014b).DaDianNao:Amachine-learningsupercomputer.In\n",
      "         (MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609–622.\n",
      " IEEE.446\n",
      "           Chilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:\n",
      "           Buildinganeﬃcientandscalabledeeplearningtrainingsystem.In11thUSENIX\n",
      "        SymposiumonOperatingSystemsDesignandImplementation(OSDI’14).442\n",
      "              Cho,K.,Raiko,T.,andIlin,A.(2010).Paralleltemperingiseﬃcientforlearningrestricted\n",
      "      Boltzmannmachines.In ., IJCNN’2010601612\n",
      "              Cho,K.,Raiko,T.,andIlin,A.(2011).Enhancedgradientandadaptivelearningratefor\n",
      "         trainingrestrictedBoltzmannmachines.In ,pages105–112. ICML’2011 670\n",
      "             Cho,K.,vanMerriënboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y.\n",
      "        (2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatistical\n",
      "         machinetranslation. InProceedingsoftheEmpiricialMethodsinNaturalLanguage\n",
      "     Processing(EMNLP2014).,,390469470\n",
      "             Cho,K.,VanMerriënboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-\n",
      "        ertiesofneuralmachinetranslation:Encoder-decoderapproaches. , ArXive-prints\n",
      " a b s/ 1 4 0 9 . 1 2 5 9.407\n",
      "            Choromanska,A.,Henaﬀ,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014). The\n",
      "      losssurfaceofmultilayernetworks.,282283\n",
      "          Chorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014). End-to-endcontinuous\n",
      "        speechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412. 1602.\n",
      "455\n",
      "         Chrisman,L.(1991).Learningrecursivedistributedrepresentationsforholisticcomputa-\n",
      "     tion.ConnectionScience,(4),345–366. 3 468\n",
      "          Christianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalof\n",
      "    NumericalAnalysis,(2),135–150. 1 2 220\n",
      "           Chrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures.\n",
      "  arXiv1506.03694. 407\n",
      "             Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgated\n",
      "         recurrentneuralnetworksonsequencemodeling.NIPS’2014DeepLearningworkshop,\n",
      "   arXiv1412.3555. ,407455\n",
      "            Chung,J.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrent\n",
      "     neuralnetworks.In . ICML’15407\n",
      "              Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).A\n",
      "          recurrentlatentvariablemodelforsequentialdata.In . NIPS’2015694\n",
      "7 2 6BIBLIOGRAPHY\n",
      "            Ciresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneural\n",
      "          networkfortraﬃcsignclassiﬁcation.NeuralNetworks,,333–338. , 3 2 24197\n",
      "            Ciresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010). Deepbig\n",
      "        simpleneuralnetsforhandwrittendigitrecognition.NeuralComputation, 2 2 ,1–14.\n",
      "  2223441,,\n",
      "              Coates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparse\n",
      "         codingandvectorquantization.In .,, ICML’201123252494\n",
      "           Coates, A.,Lee, H., andNg,A.Y. (2011).Ananalysisofsingle-layernetworksin\n",
      "         unsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConference\n",
      "        onArtiﬁcialIntelligenceandStatistics(AISTATS2011).,357450\n",
      "        Coates,A.,Huval,B.,Wang, T.,Wu, D.,Catanzaro, B.,and Andrew,N. (2013).\n",
      "            DeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,\n",
      "         Proceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),\n",
      "           volume28(3),pages1337–1345. JMLRWorkshopandConferenceProceedings.,,2223\n",
      " 358442,\n",
      "              Cohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:\n",
      "    Atensoranalysis.arXiv:1509. 05009.552\n",
      "            Collobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,UniversitédeParisVI,\n",
      " LIP6.193\n",
      "           Collobert,R.(2011).Deeplearningforeﬃcientdiscriminativeparsing.InAISTATS’2011.\n",
      " 99473,\n",
      "            Collobert,R.andWeston,J.(2008a).Auniﬁedarchitecturefornaturallanguageprocessing:\n",
      "          Deepneuralnetworkswithmultitasklearning.In ., ICML’2008466473\n",
      "   Collobert, R. and Weston,J. (2008b).A uniﬁed architecture fornatural language\n",
      "          processing:Deepneuralnetworkswithmultitasklearning.In . ICML’2008533\n",
      "             Collobert,R.,Bengio,S.,andBengio,Y.(2001). AparallelmixtureofSVMsforvery\n",
      "       largescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.445\n",
      "              Collobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylarge\n",
      "      scaleproblems.NeuralComputation,(5),1105–1114. 1 4 445\n",
      "             Collobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a).\n",
      "          Naturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearning\n",
      "      Research,,2493–2537. ,,, 1 2 324473533534\n",
      "           Collobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-\n",
      "          mentformachinelearning.InBigLearn,NIPSWorkshop.,,25209441\n",
      "7 2 7BIBLIOGRAPHY\n",
      "           Comon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,\n",
      "  3 6,287–314. 487\n",
      "          Cortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning, 2 0,\n",
      "  273–297. ,17139\n",
      "            Couprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentation\n",
      "        usingdepthinformation.InInternationalConferenceonLearningRepresentations\n",
      "  (ICLR2013).,24197\n",
      "            Courbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeep\n",
      "     learning.InArxiv:1412.7024,ICLR’2015Workshop.447\n",
      "            Courville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesby\n",
      "      spike-and-slabRBMs.In ., ICML’11558677\n",
      "           Courville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slab\n",
      "           RBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisand\n",
      "       MachineIntelligence,IEEETransactionson,(9),1874–1887. 3 6 679\n",
      "             Cover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition.\n",
      " Wiley-Interscience.71\n",
      "            Cox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearch\n",
      "          approachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognition\n",
      "           andWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8–15.IEEE.\n",
      "357\n",
      "          Cramér,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,133\n",
      "292\n",
      "             Crick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature, 3 0 4,\n",
      " 111–114. 607\n",
      "          Cybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.Mathematics\n",
      "       ofControl,Signals,andSystems,,303–314. 2 194\n",
      "             Dahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognition\n",
      "         withthemean-covariancerestrictedBoltzmannmachine.In . NIPS’201024\n",
      "             Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeep\n",
      "          neuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,\n",
      "      Speech,andLanguageProcessing,(1),33–42. 2 0 454\n",
      "              Dahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).Improvingdeepneuralnetworks\n",
      "           forLVCSRusingrectiﬁedlinearunitsanddropout.In . ICASSP’2013454\n",
      "            Dahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksfor\n",
      "   QSARpredictions.arXiv:1406. 1231.26\n",
      "7 2 8BIBLIOGRAPHY\n",
      "         Dauphin, Y.andBengio, Y.(2013).Stochasticratiomatchingof RBMsforsparse\n",
      "       high-dimensionalinputs.In .NIPSFoundation. NIPS26 617\n",
      "            Dauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswith\n",
      "     reconstructionsampling.In . ICML’2011466\n",
      "             Dauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014).\n",
      "         Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convex\n",
      "      optimization.In .,, NIPS’2014282283284\n",
      "             Davis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T.\n",
      "           (2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactions\n",
      "      onGraphics(Proc.SIGGRAPH),(4),79:1–79:10. 3 3 447\n",
      "         Dayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsof\n",
      "         the1990ConnectionistSummerSchool,SanMateo,CA.688\n",
      "            Dayan,P.andHinton,G.E.(1996).VarietiesofHelmholtz machine.NeuralNetworks,\n",
      "  9(8),1385–1403. 690\n",
      "               Dayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtz machine.\n",
      "    Neuralcomputation,(5),889–904. 7 690\n",
      "               Dean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,\n",
      "              Senior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeep\n",
      "     networks.In ., NIPS’201225442\n",
      "             Dean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation.\n",
      "    ComputationalIntelligence,(3),142–150. 5 659\n",
      "              Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990).\n",
      "           Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformation\n",
      "    Science,(6),391–407. , 4 1 472477\n",
      "            Delalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS\n",
      " 18551,\n",
      "              Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet: A\n",
      "       Large-ScaleHierarchicalImageDatabase.In . CVPR0919\n",
      "               Deng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan\n",
      "            10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceon\n",
      "          ComputerVision:PartV,ECCV’10, pages71–84,Berlin,Heidelberg.Springer-Verlag.\n",
      "19\n",
      "             Deng,L.andYu,D.(2014).Deeplearning–methodsandapplications.Foundationsand\n",
      "    TrendsinSignalProcessing.455\n",
      "7 2 9BIBLIOGRAPHY\n",
      "              Deng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binary\n",
      "            codingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,\n",
      "  Chiba,Japan.24\n",
      "              Denil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattend\n",
      "       withdeeparchitecturesforimagetracking.NeuralComputation, 2 4  (8),2151–2184. 361\n",
      "             Denton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).Deepgenerativeimagemodels\n",
      "           usingaLaplacianpyramidofadversarialnetworks..,, NIPS698699714\n",
      "           Desjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsfor\n",
      "        vision. TechnicalReport1327,Départementd’InformatiqueetdeRechercheOpéra-\n",
      "    tionnelle,UniversitédeMontréal.679\n",
      "   Desjardins, G., Courville, A.C., Bengio, Y., Vinc ent, P., andDelalleau, O.(2010).\n",
      "           TemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.In\n",
      "         InternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages145–152. ,601\n",
      "612\n",
      "            Desjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction.\n",
      "   In . NIPS’2011628\n",
      "          Desjardins,G.,Simonyan,K.,Pascanu,R.,(2015). Naturalneuralnetworks. In etal.\n",
      "        AdvancesinNeuralInformationProcessingSystems,pages2062–2070. 316\n",
      "              Devlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fast\n",
      "          androbustneuralnetworkjointmodelsforstatisticalmachinetranslation. InProc.\n",
      "  ACL’2014.468\n",
      "         Devroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:Bücher.\n",
      "   SpringerNewYork.691\n",
      "          DiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs.\n",
      "      neuronsvs.machines.NIPSTutorial.,25360\n",
      "           Dinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponents\n",
      "  estimation.arXiv:1410. 8516.489\n",
      "           Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,\n",
      "          K.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisual\n",
      "    recognitionanddescription.arXiv:1411. 4389.100\n",
      "            Donoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembedding\n",
      "      techniquesforhigh-dimensional data.TechnicalReport2003-08, Dept.Statistics,\n",
      "   StanfordUniversity.,160516\n",
      "             Dosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswith\n",
      "          convolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputer\n",
      "       VisionandPatternRecognition,pages1538–1546. ,692701\n",
      "7 3 0BIBLIOGRAPHY\n",
      "           Doya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning.\n",
      "        IEEETransactionsonNeuralNetworks,,75–80. , 1 396399\n",
      "      Dreyfus, S. E.(1962).The numerical solutionofvariational problems.Journalof\n",
      "      MathematicalAnalysisandApplications,,30–45. 5 ( 1 ) 221\n",
      "            Dreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtime\n",
      "        lag.IEEETransactionsonAutomaticControl,,383–385. 1 8 ( 4 ) 221\n",
      "          Drucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdouble\n",
      "        back-propagation.IEEETransactionsonNeuralNetworks,(6),991–997. 3 269\n",
      "           Duchi,J.,Hazan,E.,andSinger,Y.(2011). Adaptivesubgradientmethodsforonline\n",
      "         learningandstochasticoptimization.JournalofMachineLearningResearch.303\n",
      "             Dudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning.\n",
      "           InProceedingsofthe28thInternationalConferenceonMachinelearning,ICML’11.\n",
      "477\n",
      "          Dugas,C.,Bengio,Y.,Bélisle,F.,andNadeau,C.(2001). Incorporatingsecond-order\n",
      "             functionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,\n",
      "     editors, AdvancesinNeural InformationProcessingSystems 13(NIPS’00), pages\n",
      "    472–478. MITPress.,66193\n",
      "             Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-\n",
      "         worksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906.\n",
      "699\n",
      "            ElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-term\n",
      "     dependencies.In ., NIPS’1995394403\n",
      "              Elkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachfor\n",
      "          crossdomainusermodelinginrecommendationsystems. InProceedingsofthe24th\n",
      "        InternationalConferenceonWorldWideWeb,pages278–288. 475\n",
      "            Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceof\n",
      "     startingsmall.Cognition,,781–799. 4 8 324\n",
      "             Erhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thediﬃculty\n",
      "           oftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InProceedings\n",
      "   ofAISTATS’2009.197\n",
      "             Erhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010).\n",
      "          Whydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes.\n",
      "  527531532,,\n",
      "             Fahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitectures\n",
      "     for AI:NETL,thistle, andBoltzmann machines.In Proceedings ofthe National\n",
      "       ConferenceonArtiﬁcialIntelligenceAAAI-83.,567651\n",
      "7 3 1BIBLIOGRAPHY\n",
      "               Fang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Dollár,P.,Gao,J.,He,X.,\n",
      "               Mitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisual\n",
      "    conceptsandback.arXiv:1411. 4952.100\n",
      "            Farabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,and\n",
      "         Talay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,\n",
      "          M.Bilenko,andJ.Langford, editors,ScalingupMachineLearning:Paralleland\n",
      "     DistributedApproaches.CambridgeUniversityPress.521\n",
      "            Farabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeatures\n",
      "          forscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,\n",
      "    3 5(8),1915–1929. ,,24197353\n",
      "            Fei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories.\n",
      "       IEEETransactionsonPatternAnalysisandMachineIntelligence, 2 8  (4),594–611. 536\n",
      "               Finn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learning\n",
      "          visualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXiv\n",
      "   preprintarXiv:1509.06113.25\n",
      "            Fisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.Annals\n",
      "     ofEugenics,,179–188. , 7 19103\n",
      "           Földiák,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternational\n",
      "          JointConferenceonNeuralNetworks(IJCNN),volume1,pages401–405, Washington\n",
      "    1989.IEEE,NewYork.490\n",
      "          Forcada,M.andÑeco,R.(1997).Learningrecursivedistributedrepresentationsfor\n",
      "         holisticcomputation.InBiologicalandArtiﬁcialComputation:FromNeuroscienceto\n",
      "   Technology,pages453–462. 468\n",
      "             Franzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,\n",
      "    head-direction,andspatial-viewcells.491\n",
      "            Franzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslow\n",
      "          featureanalysis.InArtiﬁcialNeuralNetworks-ICANN2008,pages961–970. Springer.\n",
      "492\n",
      "             Frasconi,P.,Gori,M.,andSperduti,A.(1997).Ontheeﬃcientclassiﬁcationofdata\n",
      "            structuresbyneuralnetworks.InProc.Int.JointConf.onArtiﬁcialIntelligence.,394\n",
      "396\n",
      "       Frasconi, P., Gori, M., andSperduti, A.(1998).Ageneralframeworkforadaptive\n",
      "        processingofdatastructures.IEEETransactionsonNeuralNetworks, 9 (5),768–786.\n",
      " 394396,\n",
      "             Freund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.In\n",
      "        MachineLearning:ProceedingsofThirteenthInternationalConference,pages148–156,\n",
      "  USA.ACM.255\n",
      "7 3 2BIBLIOGRAPHY\n",
      "             Freund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.In\n",
      "          ProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages\n",
      " 325–332. 255\n",
      "           Frey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication.\n",
      "  MITPress.702\n",
      "               Frey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngood\n",
      "           densityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advances\n",
      "          inNeuralInformationProcessingSystems8(NIPS’95),pages661–670. MITPress,\n",
      "  Cambridge,MA.649\n",
      "            Frobenius,G.(1908).Übermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss.\n",
      "  Berlin,Germany.594\n",
      "         Fukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.Biological\n",
      "     Cybernetics,,121–136. ,, 2 0 15222526\n",
      "         Fukushima, K.(1980).Neocognitr on:Aself-organizingneuralnetworkmodelfora\n",
      "          mechanismofpatternrecognitionunaﬀectedbyshiftinposition.BiologicalCybernetics,\n",
      "      3 6,193–202. ,,,, 152223222361\n",
      "           Gal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulli\n",
      "       approximatevariationalinference.arXivpreprintarXiv:1506.02158.261\n",
      "           Gallinari,P.,LeCun,Y.,Thiria,S.,andFogelman- Soulie,F.(1987).Memoiresassociatives\n",
      "          distribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.511\n",
      "           Garcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwo\n",
      "           andthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprint\n",
      "  arXiv:1506.00999.479\n",
      "                Garofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993).\n",
      "          Darpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1.\n",
      "        NASASTI/ReconTechnicalReportN,,27403. 9 3 454\n",
      "             Garson,J.(1900).Themetricsystemofidentiﬁcationofcriminals,asusedinGreat\n",
      "            BritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainand\n",
      "   Ireland,(2),177–227. 19\n",
      "           Gers,F.A.,Schmidhuber,J.,andCummins,F.(2000). Learningtoforget:Continual\n",
      "        predictionwithLSTM.Neuralcomputation,(10),2451–2471. , 1 2 404408\n",
      "             Ghahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactor\n",
      "           analyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.485\n",
      "           Gillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguage\n",
      "       processingfrombytes.arXivpreprintarXiv:1512.00103.472\n",
      "7 3 3BIBLIOGRAPHY\n",
      "           Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutional\n",
      "       networksforaccurateobjectdetectionandsegmentation.421\n",
      "             Giudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogeny\n",
      "       ofmirrorneurons. ,(2),350––363. Dev.Sci. 1 2 653\n",
      "            Glorot,X.andBengio,Y.(2010).Understandingthediﬃcultyoftrainingdeepfeedforward\n",
      "     neuralnetworks.InAISTATS’2010.299\n",
      "             Glorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserectiﬁerneuralnetworks.In\n",
      "     AISTATS’2011.,,, 15171193222\n",
      "        Glorot, X.,Bordes, A.,andBengio, Y.(2011b).Domainadaptationforlarge-scale\n",
      "          sentimentclassiﬁcation:Adeeplearningapproach.In ., ICML’2011504535\n",
      "           Goldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhood\n",
      "             componentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeural\n",
      "       InformationProcessingSystems17(NIPS’04).MITPress.113\n",
      "             Gong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFace\n",
      "     Recognition.ImperialCollegePress.,161516\n",
      "            Goodfellow,I.,Le,Q.,Saxe,A., and Ng,A.(2009).Measuringinvariancesindeep\n",
      "      networks.In ,pages646–654. NIPS’2009 252\n",
      "             Goodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010).\n",
      "             Helpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction\n",
      "       (HRI),Osaka,Japan.ACMPress,ACMPress.98\n",
      "        Goodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolution\n",
      "       forautoencoders.Technicalreport,UniversitédeMontréal.350\n",
      "          Goodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels.\n",
      "         InInternationalConferenceonLearningRepresentations,WorkshopsTrack.,620697\n",
      "           Goodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecoding\n",
      "          forunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearning\n",
      "   HierarchicalModels.,530536\n",
      "            Goodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a).\n",
      "            Maxoutnetworks.InS.DasguptaandD.McAllester,editors, ,pages1319– ICML’13\n",
      "     1327. ,,,, 190261338359450\n",
      "            Goodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeep\n",
      "             Boltzmannmachines.In .NIPSFoundation.,,,,,,, NIPS26 98615668669670671672\n",
      "695\n",
      "7 3 4BIBLIOGRAPHY\n",
      "            Goodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,\n",
      "            Bergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearch\n",
      "      library.arXivpreprintarXiv:1308.4214.,25441\n",
      "            Goodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodels\n",
      "          forunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachine\n",
      "       Intelligence,(8),1902–1914. ,,,, 3 5 493494495647679\n",
      "              Goodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempirical\n",
      "          investigationofcatastrophicforgetingingradient-basedneuralnetworks.In . ICLR’2014\n",
      "191\n",
      "            Goodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-\n",
      "        sarialexamples., .,,,, CoRR a b s/ 1 4 1 2 . 6 5 7 2265266269553554\n",
      "            Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,\n",
      "           Courville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In . NIPS’2014\n",
      "    542685696698701 ,,,,\n",
      "             Goodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digit\n",
      "          numberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks.\n",
      "           InInternationalConferenceonLearningRepresentations.,,,,,, 2599197198199385\n",
      " 417444,\n",
      "            Goodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneural\n",
      "        networkoptimizationproblems.InInternationalConferenceonLearningRepresenta-\n",
      "    tions.,,, 282283284287\n",
      "        Goodman,J.(2001).Classes forfast maximumentropytraining.InInternational\n",
      "         ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.462\n",
      "              Gori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEE\n",
      "      TransactionsonPatternAnalysisandMachineIntelligence, P A M I - 1 4  (1),76–86.282\n",
      "          Gosset,W.S.(1908).Theprobableerrorofamean. , Biometrika 6  (1),1–25.Originally\n",
      "     publishedunderthepseudonym“Student”.19\n",
      "           Gouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributed\n",
      "        representationswithoutwordalignments.Technicalreport,arXiv:1410. 2455. ,472537\n",
      "             Graf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.Circuits\n",
      "       andDevicesMagazine,IEEE,(4),44–49. 5 446\n",
      "            Graves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In . NIPS’2011238\n",
      "          Graves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.Studies\n",
      "       inComputationalIntelligence.Springer.,,, 368388407455\n",
      "7 3 5BIBLIOGRAPHY\n",
      "          Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,\n",
      "    arXiv:1308. 0850. ,,, 186404411415\n",
      "           Graves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrent\n",
      "     neuralnetworks.In . ICML’2014404\n",
      "          Graves,A.andSchmidhuber,J.(2005).Framewisephonemeclassiﬁcationwithbidirec-\n",
      "        tionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks, 1 8 (5),602–610.\n",
      "388\n",
      "          Graves,A.andSchmidhuber,J.(2009).Oﬄinehandwritingrecognitionwithmultidi-\n",
      "           mensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,and\n",
      "       L.Bottou,editors, ,pages545–552. NIPS’2008 388\n",
      "           Graves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporal\n",
      "         classiﬁcation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In\n",
      "      ICML’2006,pages369–376, Pittsburgh,USA.455\n",
      "            Graves,A.,Liwicki, M.,Bunke,H.,Schmidhuber,J.,andFernández,S.(2008).Uncon-\n",
      "          strainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,\n",
      "            D.Koller,Y.Singer,andS.Roweis,editors, ,pages577–584. NIPS’2007 388\n",
      "            Graves,A.,Liwicki, M.,Fernández,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J.\n",
      "         (2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.Pattern\n",
      "         AnalysisandMachineIntelligence,IEEETransactionson,(5),855–868. 3 1 404\n",
      "            Graves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrent\n",
      "             neuralnetworks.In ,pages6645–6649. ,,,,,, ICASSP’2013 388393394404406407455\n",
      "           Graves,A.,Wayne,G.,andDanihelka,I.(2014).NeuralTuringmachines.arXiv:1410. 5401.\n",
      " 25412,\n",
      "            Grefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningto\n",
      "       transducewithunboundedmemory.In . NIPS’2015412\n",
      "             Greﬀ,K.,Srivastava,R.K.,Koutník,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015).\n",
      "         LSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.408\n",
      "             Gregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproduct\n",
      "        networkwithlocalreceptiveﬁelds.Technicalreport,arXiv:1006. 0448.346\n",
      "            Gregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.In\n",
      "          L.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternational\n",
      "      ConferenceonMachineLearning(ICML-10).ACM.650\n",
      "      Gregor, K.,Danihelka, I.,Mnih,A., Blundell,C.,and Wierstra, D. (2014).Deep\n",
      "        autoregressivenetworks.InInternationalConferenceonMachineLearning(ICML’2014).\n",
      "690\n",
      "7 3 6BIBLIOGRAPHY\n",
      "             Gregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneural\n",
      "        networkforimagegeneration.arXivpreprintarXiv:1502.04623.694\n",
      "              Gretton,A.,Borgwardt,K.M.,Rasch,M.J.,Schölkopf,B.,andSmola,A.(2012).A\n",
      "        kerneltwo-sampletest.TheJournalofMachineLearningResearch, 1 3 (1),723–773.\n",
      "700\n",
      "           Gülçehre,Ç.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformation\n",
      "        foroptimization.InInternationalConferenceonLearningRepresentations(ICLR’2013).\n",
      "25\n",
      "            Guo,H.andGelfand,S.B.(1992).Classiﬁcationtreeswithneuralnetworkfeature\n",
      "        extraction.NeuralNetworks,IEEETransactionson,(6),923–933. 3 445\n",
      "           Gupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearning\n",
      "      withlimitednumericalprecision., . CoRR a b s/ 1 5 0 2 . 0 2 5 5 1447\n",
      "         Gutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation: Anewestima-\n",
      "         tionprincipleforunnormalizedstatisticalmodels. InProceedingsofTheThirteenth\n",
      "        InternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10).618\n",
      "    Hadsell, R., Sermanet, P., Ben, J.,Erkan,A., Han, J., Muller, U., and LeCun, Y.\n",
      "           (2007).Onlinelearningforoﬀroadrobots:Spatiallabelpropagationtolearnlong-range\n",
      "           traversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.448\n",
      "             Hajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuits\n",
      "         ofboundeddepth. ,,129–154. J.Comput.System.Sci. 4 6 196\n",
      "            Håstad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedings\n",
      "            ofthe18thannualACMSymposiumonTheoryofComputing,pages6–20,Berkeley,\n",
      "   California.ACMPress.196\n",
      "            Håstad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits.\n",
      "    ComputationalComplexity,,113–129. 1 196\n",
      "            Hastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:\n",
      "         datamining,inferenceandprediction. SpringerSeriesinStatistics.SpringerVerlag.\n",
      "142\n",
      "              He,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorectiﬁers:Surpassing\n",
      "        human-levelperformanceonImageNetclassiﬁcation.arXivpreprintarXiv:1502.01852.\n",
      " 24190,\n",
      "             Hebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1316653\n",
      "           Henaﬀ,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearning\n",
      "          ofsparsefeaturesforscalableaudioclassiﬁcation.In . ISMIR’11521\n",
      "7 3 7BIBLIOGRAPHY\n",
      "         Henderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatistical\n",
      "     parsing.InHLT-NAACL,pages103–110. 473\n",
      "           Henderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.In\n",
      "          Proceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,\n",
      "  page95.473\n",
      "             Henniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andLücke,J.(2010).Binarysparse\n",
      "          coding.InLatentVariableAnalysisandSignalSeparation,pages450–457. Springer.\n",
      "638\n",
      "            Herault,J.andAns,B.(1984).Circuitsneuronauxàsynapsesmodiﬁables:Décodagede\n",
      "         messagescompositesparapprentissagenonsupervisé.ComptesRendusdel’Académie\n",
      "    desSciences, ,525––528. 2 9 9 ( I I I - 1 3 ) 487\n",
      "           Hinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.303\n",
      "              Hinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,\n",
      "            Nguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacoustic\n",
      "       modelinginspeechrecognition.IEEESignalProcessingMagazine, 2 9  (6),82–97. ,24\n",
      "454\n",
      "              Hinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork.\n",
      "    arXivpreprintarXiv:1503.02531.443\n",
      "        Hinton,G.E.(1989).Connectionistlearningprocedures.ArtiﬁcialIntelligence, 4 0,\n",
      " 185–234. 490\n",
      "          Hinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.Artiﬁcial\n",
      "   Intelligence,(1),47–75. 4 6 412\n",
      "          Hinton,G.E.(1999).Productsofexperts.In . ICANN’1999567\n",
      "           Hinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence.\n",
      "          TechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,608\n",
      "673\n",
      "             Hinton,G.E.(2006).Torecognizeshapes,ﬁrstlearntogenerateimages.TechnicalReport\n",
      "       UTMLTR2006-003,UniversityofToronto.,526592\n",
      "              Hinton,G.E.(2007a).Howtodobackpropagationinabrain.Invitedtalkatthe\n",
      "    NIPS’2007DeepLearningWorkshop.653\n",
      "         Hinton,G.E.(2007b). Learningmultiplelayersofrepresentation. Trendsincognitive\n",
      "   sciences,(10),428–434. 1 1 657\n",
      "          Hinton, G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines.\n",
      "          TechnicalReportUTMLTR2010-003,DepartmentofComputerScience,Universityof\n",
      " Toronto.608\n",
      "7 3 8BIBLIOGRAPHY\n",
      "           Hinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparse\n",
      "         distributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon.\n",
      "144\n",
      "            Hinton,G.E.andMcClelland,J.L.(1988).Learningrepresentationsbyrecirculation.In\n",
      "    NIPS’1987,pages358–366. 499\n",
      "             Hinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In . NIPS’2002516\n",
      "            Hinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawith\n",
      "         neuralnetworks.Science,(5786),504–507. ,,,, 3 1 3 506522526527531\n",
      "             Hinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines.\n",
      "           InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,\n",
      "          volume1,chapter7,pages282–317. MITPress,Cambridge.,567651\n",
      "            Hinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneural\n",
      "   computation.MITpress.539\n",
      "            Hinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsof\n",
      "      acquireddyslexia.Psychologicalreview,(1),74. 9 8 13\n",
      "            Hinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,and\n",
      "      Helmholtz freeenergy.In . NIPS’1993499\n",
      "             Hinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraint\n",
      "       satisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-Mellon\n",
      "      University,Dept.ofComputerScience.,567651\n",
      "         Hinton,G.E.,McClelland,J.,andRumelhart,D.(1986). Distributedrepresentations.\n",
      "           InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing:\n",
      "           ExplorationsintheMicrostructureofCognition,volume1,pages77–109.MITPress,\n",
      "   Cambridge.,,16221524\n",
      "            Hinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusing\n",
      "             mixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,Advances\n",
      "          inNeuralInformationProcessingSystems7(NIPS’94),pages1015–1022. MITPress,\n",
      "  Cambridge,MA.485\n",
      "               Hinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithm\n",
      "        forunsupervisedneuralnetworks.Science,,1558–1161. , 2 6 8 501649\n",
      "              Hinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesof\n",
      "         handwrittendigits.IEEETransactionsonNeuralNetworks,,65–74. 8 496\n",
      "                 Hinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.In\n",
      "         Proceedingsof3rdInternationalConferenceonIndependentComponentAnalysisand\n",
      "         BlindSignalSeparation(ICA’01),pages746–751, SanDiego,CA.487\n",
      "7 3 9BIBLIOGRAPHY\n",
      "               Hinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbelief\n",
      "            nets.NeuralComputation,,1527–1554. ,,,,,,, 1 8 131823141526527657658\n",
      "           Hinton,G.E., Deng,L.,Yu, D.,Dahl,G.E.,Mohamed, A.,Jaitly, N.,Senior,A.,\n",
      "            Vanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneural\n",
      "            networksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearch\n",
      "       groups.IEEESignalProcess.Mag.,(6),82–97. 2 9 99\n",
      "            Hinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c).\n",
      "         Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technical\n",
      "    report,arXiv:1207. 0580. ,,235260264\n",
      "             Hinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge. Invitedtalkatthe\n",
      "      BayLearnBayAreaMachineLearningSymposium.443\n",
      "        Hochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diploma\n",
      "     thesis,T.U.München.,,17396398\n",
      "          Hochreiter,S.andSchmidhuber,J.(1995). Simplifyingneuralnetsbydiscoveringﬂat\n",
      "            minima.InAdvancesinNeuralInformationProcessingSystems7,pages529–536. MIT\n",
      " Press.239\n",
      "          Hochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,\n",
      "    9(8),1735–1780. ,,17404407\n",
      "             Hochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradientﬂowinrecurrentnets:the\n",
      "            diﬃcultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,Field\n",
      "       GuidetoDynamicalRecurrentNetworks.IEEEPress.407\n",
      "             Holi,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetwork\n",
      "        hardwareimplementations.Computers,IEEETransactionson,(3),281–290. 4 2 446\n",
      "            Holt,J.L.andBaker,T.E.(1991). Backpropagationsimulationsusinglimitedpreci-\n",
      "        sioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJoint\n",
      "       Conferenceon,volume2,pages121–126. IEEE.446\n",
      "           Hornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksare\n",
      "      universalapproximators.NeuralNetworks,,359–366. 2 194\n",
      "           Hornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofan\n",
      "         unknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neural\n",
      "   networks,(5),551–560. 3 194\n",
      "            Hsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorld\n",
      "        ChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2\n",
      "          Huang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkov\n",
      "          randomﬁeldsonlattice.AnnalsoftheInstituteofStatisticalMathematics, 5 4 (1),1–18.\n",
      "614\n",
      "7 4 0BIBLIOGRAPHY\n",
      "               Huang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeep\n",
      "           structuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsof\n",
      "          the22ndACMinternationalconferenceonConferenceoninformation&knowledge\n",
      "    management,pages2333–2338. ACM.475\n",
      "            Hubel,D.andWiesel,T.(1968).Receptiveﬁeldsandfunctionalarchitectureofmonkey\n",
      "        striatecortex.JournalofPhysiology(London),,215–243. 1 9 5 358\n",
      "               Hubel,D.H.andWiesel,T.N.(1959).Receptiveﬁeldsofsingleneuronsinthecat’s\n",
      "       striatecortex.JournalofPhysiology,,574–591. 1 4 8 358\n",
      "          Hubel,D.H.andWiesel, T.N.(1962).Receptiveﬁelds, binocularinteraction,and\n",
      "          functionalarchitectureinthecat’svisualcortex.JournalofPhysiology(London), 1 6 0,\n",
      " 106–154. 358\n",
      "            Huszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,\n",
      "  adversary? . arXiv:1511.05101694\n",
      "         Hutter,F.,Hoos,H.,andLeyton-Brown,K.(2011). Sequentialmodel-basedoptimization\n",
      "            forgeneralalgorithmconﬁguration.In .ExtendedversionasUBCTechreport LION-5\n",
      " TR-2010-10.430\n",
      "            Hyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP’96,pages\n",
      " 13–24.372\n",
      "        Hyvärinen,A.(1999). Surveyonindependentcomponentanalysis.NeuralComputing\n",
      "   Surveys,,94–128. 2 487\n",
      "          Hyvärinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching.\n",
      "        JournalofMachineLearningResearch,,695–709. , 6 509615\n",
      "        Hyvärinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,\n",
      "        andpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeural\n",
      "   Networks,,1529–1531. 1 8 616\n",
      "          Hyvärinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsand\n",
      "    DataAnalysis,,2499–2512. 5 1 616\n",
      "            Hyvärinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcell\n",
      "             propertiesfromnaturalimagesusingextensionsofica.In,pages827–833. NIPS 489\n",
      "       Hyvärinen, A.andPajunen, P.(1999).Nonlinearindependentcomponentanalysis:\n",
      "        Existenceanduniquenessresults.NeuralNetworks,(3),429–439. 1 2 489\n",
      "          Hyvärinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis.\n",
      " Wiley-Interscience.487\n",
      "            Hyvärinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponent\n",
      "     analysis.NeuralComputation,(7),1527–1558. 1 3 489\n",
      "7 4 1BIBLIOGRAPHY\n",
      "             Hyvärinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilistic\n",
      "      approachtoearlycomputationalvision.Springer-Verlag.364\n",
      "           Iba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,\n",
      "  C 1 2,623–656. 601\n",
      "   Inayoshi, H. and Kurita, T. (2005).Improved generalizationbyadding both auto-\n",
      "       associationandhidden-layernoisetoneural-network-based-classiﬁers.IEEEWorkshop\n",
      "        onMachineLearningforSignalProcessing,pages141—-146.511\n",
      "           Ioﬀe,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktraining\n",
      "       byreducinginternalcovariateshift.,,98313316\n",
      "          Jacobs,R.A.(1988). Increasedratesofconvergencethroughlearningrateadaptation.\n",
      "    Neuralnetworks,(4),295–307. 1 303\n",
      "               Jacobs,R.A.,Jordan,M.I.,Nowlan, S.J.,andHinton,G.E.(1991).Adaptivemixtures\n",
      "        oflocalexperts.NeuralComputation,,79–87. , 3 185445\n",
      "           Jaeger,H.(2003).Adaptivenonlinearsystemidentiﬁcationwithechostatenetworks.In\n",
      "        AdvancesinNeuralInformationProcessingSystems15.399\n",
      "          Jaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostate\n",
      "     networks.Technicalreport,JacobsUniversity.394\n",
      "         Jaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330. 2 399\n",
      "             Jaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulation\n",
      "        study.Technicalreport,Technicalreport,JacobsUniversityBremen.400\n",
      "           Jaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsand\n",
      "         savingenergyinwirelesscommunication.Science,(5667),78–80. , 3 0 4 23399\n",
      "           Jaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationand\n",
      "          applicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,\n",
      "  2 0(3),335–352. 403\n",
      "              Jain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,\n",
      "             M.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestoration\n",
      "         withconvolutionalnetworks.InComputer Vision,2007.ICCV2007.IEEE11th\n",
      "      InternationalConferenceon,pages1–8.IEEE.352\n",
      "            Jaitly,N.andHinton,G.(2011).Learningabetterrepresentationofspeechsoundwaves\n",
      "        usingrestrictedBoltzmannmachines.InAcoustics, SpeechandSignalProcessing\n",
      "         (ICASSP),2011IEEEInternationalConferenceon,pages5884–5887. IEEE.453\n",
      "            Jaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improves\n",
      "     speechrecognition.In . ICML’2013237\n",
      "7 4 2BIBLIOGRAPHY\n",
      "             Jarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebest\n",
      "             multi-stagearchitectureforobjectrecognition?In .,,,,,, ICCV’09152223171190222\n",
      "  357358521,,\n",
      "           Jarzynski,C.(1997).Nonequilibriumequalityforfreeenergydiﬀerences.Phys.Rev.Lett.,\n",
      "   7 8,2690–2693. ,623626\n",
      "           Jaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversity\n",
      " Press.51\n",
      "              Jean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetarget\n",
      "       vocabularyforneuralmachinetranslation.arXiv:1412. 2007. ,469470\n",
      "            Jelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparameters\n",
      "              fromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitionin\n",
      "    Practice.North-Holland,Amsterdam.,457468\n",
      "            Jia,Y.(2013).Caﬀe:Anopensourceconvolutionalarchitectureforfastfeatureembedding.\n",
      "  http://caffe.berkeleyvision.org/.,25209\n",
      "            Jia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptiveﬁeld\n",
      "          learningforpooledimagefeatures.InComputerVisionandPatternRecognition\n",
      "        (CVPR),2012IEEEConferenceon,pages3370–3377. IEEE.339\n",
      "                Jim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneural\n",
      "       networks: convergenceandgeneralization.IEEETransactionsonNeuralNetworks,\n",
      "  7(6),1424–1438. 238\n",
      "           Jordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.17\n",
      "          Joulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmented\n",
      "      recurrentnets.arXivpreprintarXiv:1503.01007.412\n",
      "            Jozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrent\n",
      "       networkarchitectures.In .,, ICML’2015302407408\n",
      "             Judd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress.\n",
      "289\n",
      "          Jutten, C.andHerault, J.(1991).Blindseparationofsources, partI:anadaptive\n",
      "         algorithmbasedonneuromimeticarchitecture.SignalProcessing,,1–10. 2 4 487\n",
      "             Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gülçehre,c.,Memisevic,R.,Vincent,\n",
      "               P.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,\n",
      "        Y.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer, J.,Lamblin,P.,Raymond,\n",
      "           J.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,\n",
      "             E.,Côté,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeciﬁcdeep\n",
      "             neuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMon\n",
      "     InternationalConferenceonMultimodalInteraction.197\n",
      "7 4 3BIBLIOGRAPHY\n",
      "          Kalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.In\n",
      "   EMNLP’2013.,469470\n",
      "          Kalchbrenner,N.,Danihelka,I.,andGraves,A.(2015). Gridlongshort-termmemory.\n",
      "    arXivpreprintarXiv:1507.01526.390\n",
      "           Kamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder.\n",
      "        IEEETransactionsonPatternAnalysisandMachineIntelligence.511\n",
      "           Karpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimage\n",
      "     descriptions.In .arXiv:1412. 2306. CVPR’2015 100\n",
      "             Karpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014).\n",
      "         Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.In .CVPR19\n",
      "            Karush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSide\n",
      "         Constraints.Master’sthesis,Dept.ofMathematics,Univ.ofChicago.93\n",
      "             Katz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodel\n",
      "           componentofaspeechrecognizer.IEEETransactionsonAcoustics,Speech,andSignal\n",
      "    Processing, (3),400–401. , A S S P - 3 5 457468\n",
      "           Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008). Fastinferenceinsparsecoding\n",
      "         algorithmswithapplicationstoobjectrecognition.Technicalreport,Computationaland\n",
      "        Biological LearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01.\n",
      "521\n",
      "           Kavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariant\n",
      "        featuresthroughtopographicﬁltermaps.In . CVPR’2009521\n",
      "            Kavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y.\n",
      "          (2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In . NIPS’2010\n",
      " 358521,\n",
      "           Kelley,H.J.(1960).Gradienttheoryofoptimalﬂightpaths. , ARSJournal 3 0(10),\n",
      " 947–954. 221\n",
      "              Khan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearning\n",
      "          andteachingdimension.InAdvancesinNeuralInformationProcessingSystems24\n",
      "   (NIPS’11),pages1449–1457. 324\n",
      "               Kim,S.K.,McAfee,L.C.,McMahon,P.L.,andOlukotun,K.(2009).Ahighlyscalable\n",
      "        restrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogic\n",
      "          andApplications,2009.FPL2009.InternationalConferenceon,pages367–372. IEEE.\n",
      "446\n",
      "         Kindermann,R.(1980).MarkovRandomFieldsandTheirApplications(Contemporary\n",
      "       Mathematics;V.1).AmericanMathematicalSociety.563\n",
      "7 4 4BIBLIOGRAPHY\n",
      "            Kingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXiv\n",
      "   preprintarXiv:1412.6980.305\n",
      "            Kingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscore\n",
      "     matching.In ., NIPS’2010509618\n",
      "           Kingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearning\n",
      "       withdeepgenerativemodels.In . NIPS’2014421\n",
      "          Kingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariable\n",
      "         modelsinauxiliaryform.Technicalreport,arxiv:1306.0733. ,,650685693\n",
      "           Kingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedings\n",
      "         oftheInternationalConferenceonLearningRepresentations(ICLR).,685696\n",
      "        Kingma, D.P.andWelling, M.(2014b).Eﬃcientgradient-basedinferencethrough\n",
      "         transformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480.\n",
      "685\n",
      "              Kirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulated\n",
      "    annealing.Science,,671–680. 2 2 0 323\n",
      "           Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels.\n",
      "   In . ICML’2014100\n",
      "          Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddings\n",
      "        withmultimodalneurallanguagemodels. ., arXiv: 1 4 1 1 . 2 5 3 9 [ c s . L G ]100404\n",
      "          Klementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributed\n",
      "          representationsofwords.InProceedingsofCOLING2012.,472537\n",
      "              Knowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,and\n",
      "           Pﬁster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26\n",
      "         Koller,D.andFriedman, N.(2009).ProbabilisticGraphicalModels:Principlesand\n",
      "     Techniques.MITPress.,,580592643\n",
      "           Konig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationand\n",
      "         maximizationofaposterioriprobabilities–applicationtotransition-basedconnectionist\n",
      "            speechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advancesin\n",
      "          NeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.454\n",
      "            Koren,Y.(2009).TheBellKorsolutiontotheNetﬂixgrandprize.,255475\n",
      "              Kotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividual\n",
      "       labelsusingdeepfeatures.In . ACMSIGKDD104\n",
      "             Koutnik,J.,Greﬀ,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.In\n",
      "  ICML’2014.403\n",
      "7 4 5BIBLIOGRAPHY\n",
      "            Kočiský,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-\n",
      "        sentationsbyMarginalizingAlignments.InProceedingsofACL.470\n",
      "           Krause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).Approximationproperties\n",
      "             ofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In . ICML’2013551\n",
      "          Krizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,\n",
      "      UniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-\n",
      " cifar10-aug2010.pdf.441\n",
      "            Krizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtiny\n",
      "       images.Technicalreport,UniversityofToronto.,19558\n",
      "            Krizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-based\n",
      "     imageretrieval.In .ESANN523\n",
      "           Krizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassiﬁcationwithdeep\n",
      "             convolutionalneuralnetworks.In .,,,,,,, NIPS’201222232498197365449453\n",
      "              Krueger,K.A.andDayan,P.(2009).Flexible shaping:howlearninginsmallstepshelps.\n",
      "   Cognition,,380–394. 1 1 0 324\n",
      "             Kuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsofthe\n",
      "         SecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481–492,\n",
      "      Berkeley,Calif.UniversityofCaliforniaPress.93\n",
      "              Kumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,\n",
      "            M.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworks\n",
      "       fornaturallanguageprocessing. ., arXiv:1506.07285412480\n",
      "             Kumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariable\n",
      "    models.In . NIPS’2010324\n",
      "              Lang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetwork\n",
      "       architectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-Mellon\n",
      "   University.,,361368402\n",
      "              Lang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetwork\n",
      "         architectureforisolatedwordrecognition.Neuralnetworks,(1),23–43. 3 368\n",
      "           Langford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armed\n",
      "      bandits.In ,pages1096––1103. NIPS’2008 476\n",
      "          Lappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinear\n",
      "        independentcomponentanalysisusingensemblelearning:Experimentsanddiscussion.\n",
      "    InProc.ICA.Citeseer.489\n",
      "   Larochelle, H. and Bengio, Y. (2008).Classiﬁcation usingdiscriminative restricted\n",
      "         Boltzmannmachines.In .,,,, ICML’2008240252528683712\n",
      "7 4 6BIBLIOGRAPHY\n",
      "             Larochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswitha\n",
      "         third-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems\n",
      "    23,pages1243–1251. 361\n",
      "          Larochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator.\n",
      "     InAISTATS’2011.,,702705706\n",
      "            Larochelle,H.,Erhan,D.,andBengio,Y.(2008). Zero-datalearningofnewtasks.In\n",
      "     AAAIConferenceonArtiﬁcialIntelligence.537\n",
      "            Larochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfor\n",
      "           trainingdeepneuralnetworks.JournalofMachineLearningResearch,,1–40. 1 0 533\n",
      "               Lasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeand\n",
      "          discriminativemodels.InProceedingsoftheComputerVisionandPatternRecognition\n",
      "         Conference(CVPR’06),pages87–94,Washington,DC,USA.IEEEComputerSociety.\n",
      " 240250,\n",
      "                 Le,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiled\n",
      "           convolutionalneuralnetworks.InJ.Laﬀerty,C.K.I.Williams, J.Shawe-Taylor,\n",
      "           R.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems\n",
      "    23(NIPS’10),pages1279–1287. 346\n",
      "               Le,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimization\n",
      "         methodsfordeeplearning.InProc.ICML’2011.ACM.312\n",
      "               Le,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,\n",
      "          A.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.In\n",
      "   ICML’2012.,2223\n",
      "           LeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmann\n",
      "          machinesanddeepbeliefnetworks.NeuralComputation,(6),1631–1649. , 2 0 551652\n",
      "             LeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-\n",
      "     mators.NeuralComputation,(8),2192–2207. 2 2 551\n",
      "           LeCun,Y.(1985).Uneprocédured’apprentissagepourRéseauàseuilassymétrique.In\n",
      "            Cognitiva85:AlaFrontièredel’IntelligenceArtiﬁcielle,desSciencesdelaConnaissance\n",
      "         etdesNeurosciences,pages599–604, Paris1985.CESTA,Paris.221\n",
      "            LeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-\n",
      "          Soulié,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiological\n",
      "       Organization,pages233–240. Springer-Verlag,LesHouches,France.345\n",
      "          LeCun,Y.(1987).Modèlesconnexionistesdel’apprentissage.Ph.D.thesis,Universitéde\n",
      "    ParisVI.,,17499511\n",
      "        LeCun, Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReport\n",
      "     CRG-TR-89-4,UniversityofToronto.,326345\n",
      "7 4 7BIBLIOGRAPHY\n",
      "                LeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,\n",
      "          Howard,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applications\n",
      "         ofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,\n",
      "  2 7(11),41–46.362\n",
      "             LeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K.-R.(1998a).Eﬃcientbackprop.In\n",
      "            NeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524.\n",
      "   SpringerVerlag.,307424\n",
      "            LeCun,Y.,Bottou,L.,Bengio,Y.,andHaﬀner,P.(1998b).Gradientbasedlearning\n",
      "            appliedtodocumentrecognition.Proc.IEEE.,,,,,, 15171923365453455\n",
      "     LeCun, Y., Kavukcuoglu, K., andFarabet, C.(2010).Convolutionalnetworksand\n",
      "          applicationsinvision. InCircuitsandSystems(ISCAS),Proceedingsof2010IEEE\n",
      "      InternationalSymposiumon,pages253–256. IEEE.365\n",
      "          L’Ecuyer,P.(1994).Eﬃciencyimprovementandvariancereduction.InProceedingsof\n",
      "       the1994WinterSimulationConference,pages122––132. 687\n",
      "             Lee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets.\n",
      "    arXivpreprintarXiv:1409.5185.322\n",
      "             Lee,H.,Battle,A.,Raina,R.,andNg,A.(2007).Eﬃcientsparsecodingalgorithms.\n",
      "            InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformation\n",
      "        ProcessingSystems19(NIPS’06),pages801–808. MITPress.635\n",
      "               Lee,H.,Ekanadham,C.,andNg,A.(2008).Sparsedeepbeliefnetmodelforvisualarea\n",
      "    V2.In . NIPS’07252\n",
      "             Lee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbelief\n",
      "          networksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.Bottou\n",
      "          andM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceon\n",
      "        MachineLearning(ICML’09).ACM,Montreal,Canada.,,357680681\n",
      "             Lee,Y.J.andGrauman,K.(2011).Learningtheeasythingsﬁrst:self-pacedvisual\n",
      "     categorydiscovery.In . CVPR’2011324\n",
      "              Leibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,\n",
      " 2010).220\n",
      "            Lenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-\n",
      "           tionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc.\n",
      "2\n",
      "            Leshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforward\n",
      "         networkswithanonpolynomialactivationfunctioncanapproximateanyfunction.\n",
      "     NeuralNetworks,,861––867. , 6 195196\n",
      "7 4 8BIBLIOGRAPHY\n",
      "             Levenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleast\n",
      "        squares.QuarterlyJournalofAppliedMathematics,(2),164–168. I I 308\n",
      "            L’Hôpital,G.F.A.(1696).Analysedesinﬁnimentpetits,pourl’intelligencedeslignes\n",
      "    courbes.Paris:L’ImprimerieRoyale.220\n",
      "            Li,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks.\n",
      "  CoRR, . a b s/ 1 5 0 2 . 0 2 7 6 1699\n",
      "              Lin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependencies\n",
      "            isnotasdiﬃcultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeural\n",
      "   Networks,(6),1329–1338. 7 402\n",
      "               Lin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelation\n",
      "         embeddingsforknowledgegraphcompletion.InProc.AAAI’15.479\n",
      "            Linde,N.(1992).Themachinethatchangedtheworld,episode4.Documentaryminiseries.\n",
      "2\n",
      "            Lindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser’s\n",
      "           perspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHigh\n",
      "       EnergyPhysics,pages195––202, Isolad’Elba,Italy.446\n",
      "      Linnainmaa, S. (1976).Taylorexpansionofthe accumulated roundingerror.BIT\n",
      "    NumericalMathematics,(2),146–160. 1 6 221\n",
      "             Long,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardto\n",
      "          approximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConference\n",
      "    onMachineLearning(ICML’10).655\n",
      "            Lotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructure\n",
      "         usingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,542543\n",
      "            Lovelace,A.(1842).NotesuponL.F.Menabrea’s“SketchoftheAnalyticalEngine\n",
      "    inventedbyCharlesBabbage”.1\n",
      "                Lu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetwork\n",
      "         encoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.455\n",
      "            Lu,T.,Pál,D.,andPál,M.(2010).Contextualmulti-armedbandits.InInternational\n",
      "        ConferenceonArtiﬁcialIntelligenceandStatistics,pages485–492. 476\n",
      "          Luenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.312\n",
      "          Lukoševičius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrent\n",
      "        neuralnetworktraining.ComputerScienceReview,(3),127–149. 3 399\n",
      "7 4 9BIBLIOGRAPHY\n",
      "             Luo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesand\n",
      "          class-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceon\n",
      "      ArtiﬁcialIntelligenceandStatistics,pages470–478. 683\n",
      "             Luo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwith\n",
      "         convolutionalspike-and-slabRBMsanddeepextensions.InAISTATS’2013.100\n",
      "            Lyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsofthe\n",
      "        Twenty-ﬁfthConferenceinUncertaintyinArtiﬁcialIntelligence(UAI’09).616\n",
      "                Ma,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnets\n",
      "           asamethodforquantitativestructure–activityrelationships.J.Chemicalinformation\n",
      "  andmodeling.528\n",
      "              Maas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Rectiﬁernonlinearitiesimproveneural\n",
      "            networkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,and\n",
      "  LanguageProcessing.190\n",
      "            Maass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalog\n",
      "             neuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,\n",
      "  pages335–344. 196\n",
      "             Maass,W.,Schnitger,G.,andSontag,E.D.(1994).Acomparisonofthecomputational\n",
      "          powerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeural\n",
      "     ComputationandLearning,pages127–151. 196\n",
      "          Maass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithout\n",
      "           stablestates:Anewframeworkforneuralcomputationbasedonperturbations.Neural\n",
      "   Computation,(11),2531–2560. 1 4 399\n",
      "        MacKay,D.(2003). InformationTheory,InferenceandLearningAlgorithms.Cambridge\n",
      "  UniversityPress.71\n",
      "          Maclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameter\n",
      "        optimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.430\n",
      "                Mao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioning\n",
      "         withmultimodalrecurrentneuralnetworks.In .arXiv:1410. 1090. ICLR’2015 100\n",
      "           Marcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem.\n",
      "       ZeitschriftfürOperationsResearch(Theory),,517–545. 3 6 273\n",
      "            Marlin,B.anddeFreitas,N.(2011).Asymptoticeﬃciencyofdeterministicestimatorsfor\n",
      "          discreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In ., UAI’2011615\n",
      "617\n",
      "7 5 0BIBLIOGRAPHY\n",
      "             Marlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).Inductiveprinciplesfor\n",
      "         restrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternational\n",
      "         ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10),volume9,pages\n",
      "   509–516. ,,611616617\n",
      "           Marquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-\n",
      "         eters.JournaloftheSocietyofIndustrialandAppliedMathematics, 1 1 (2),431–441.\n",
      "308\n",
      "           Marr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,\n",
      " 1 9 4.361\n",
      "          Martens, J.(2010).DeeplearningviaHessian-freeoptimization.InL.Bottouand\n",
      "         M.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceon\n",
      "      MachineLearning(ICML-10),pages735–742. ACM.300\n",
      "            Martens,J.andMedabalimi,V.(2014).Ontheexpressiveeﬃciencyofsumproduct\n",
      "   networks. . arXiv:1411.7717551\n",
      "           Martens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-free\n",
      "      optimization.InProc.ICML’2011.ACM.408\n",
      "          Mase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuous\n",
      "        statespaceGibbsianprocesses.TheAnnalsofAppliedProbability, 5  (3),pp.603–612.\n",
      "614\n",
      "            McClelland,J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributed\n",
      "         processing.InComputation&intelligence,pages305–341. AmericanAssociationfor\n",
      "  ArtiﬁcialIntelligence.16\n",
      "              McCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervous\n",
      "        activity.BulletinofMathematicalBiophysics,,115–133. , 5 1314\n",
      "             Mead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80.\n",
      "     SpringerScience&BusinessMedia.446\n",
      "             Melchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmann\n",
      "     machines.arXivpreprintarXiv:1311.1354.670\n",
      "           Memisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations.\n",
      "          InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).\n",
      "683\n",
      "           Memisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformations\n",
      "      withfactoredhigher-orderBoltzmannmachines.NeuralComputation, 2 2 (6),1473–1492.\n",
      "683\n",
      "7 5 1BIBLIOGRAPHY\n",
      "             Mesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,\n",
      "           Muller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,\n",
      "           J.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.In\n",
      "           JMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,197530536\n",
      "              Mesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Surﬁngonthe\n",
      "    manifold.LearningWorkshop,Snowbird.707\n",
      "           Miikkulain en,R.andDyer,M.G.(1991).Naturallanguageprocessingwithmodular\n",
      "         PDPnetworksanddistributedlexicon.CognitiveScience,,343–399. 1 5 472\n",
      "           Mikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,\n",
      "    BrnoUniversityofTechnology.410\n",
      "            Mikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empirical\n",
      "           evaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-\n",
      "        nualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH\n",
      " 2011).467\n",
      "             Mikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfor\n",
      "           traininglargescaleneuralnetworklanguagemodels.InProc.ASRU’2011.,324467\n",
      "              Mikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).Eﬃcientestimationofwordrep-\n",
      "         resentationsinvectorspace.InInternationalConferenceonLearningRepresentations:\n",
      "  WorkshopsTrack.534\n",
      "            Mikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguages\n",
      "      formachinetranslation.Technicalreport,arXiv:1309. 4168.537\n",
      "          Minka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridge\n",
      "      UKTechRepMSRTR2005173,(TR-2005-173). 7 2 623\n",
      "            Minsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.14\n",
      "           Mirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprint\n",
      "  arXiv:1411.1784.698\n",
      "             Mishkin,D.and Matas, J.(2015).Allyouneedisagoodinit.arXivpreprint\n",
      "  arXiv:1511.06422.301\n",
      "             Misra,J.andSaha,I.(2010). Artiﬁcialneuralnetworksinhardware:Asurveyoftwo\n",
      "      decadesofprogress.Neurocomputing,(1),239–255. 7 4 446\n",
      "         Mitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.97\n",
      "            Miyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributional\n",
      "         smoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507. 00677. ICLR 266\n",
      "7 5 2BIBLIOGRAPHY\n",
      "           Mnih,A.andGregor, K.(2014).Neuralvariationalinferenceandlearninginbelief\n",
      "     networks.In ., ICML’2014688690\n",
      "             Mnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguage\n",
      "         modelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternational\n",
      "        ConferenceonMachineLearning(ICML’07),pages641–648. ACM.460\n",
      "            Mnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.\n",
      "             InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeural\n",
      "       InformationProcessingSystems21(NIPS’08),pages1081–1088. 462\n",
      "           Mnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingseﬃcientlywithnoise-\n",
      "           contrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,and\n",
      "           K.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages\n",
      "     2265–2273. CurranAssociates,Inc.,467620\n",
      "          Mnih, A.andTeh, Y. W.(2012).Afastandsimple algorithmfortrainingneural\n",
      "        probabilisticlanguagemodels.In ,pages1751–1758. ICML’2012 467\n",
      "             Mnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages.\n",
      "           InProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).100\n",
      "         Mnih,V.,Larochelle,H., and Hinton,G.(2011).ConditionalrestrictedBoltzmann\n",
      "           machinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArtiﬁcial\n",
      "  Intelligence(UAI).682\n",
      "             Mnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013).\n",
      "         PlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312. 5602.104\n",
      "             Mnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisual\n",
      "            attention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,\n",
      "     editors, ,pages2204–2212. NIPS’2014 688\n",
      "              Mnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,\n",
      "             A.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,\n",
      "             Antonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015).\n",
      "         Human-levelcontrolthroughdeepreinforcementlearning.Nature,,529–533. 5 1 8 25\n",
      "            Mobahi,H.andFisher, III,J.W.(2015).Atheoreticalanalysisofoptimizationby\n",
      "     Gaussiancontinuation.In . AAAI’2015323\n",
      "            Mobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherence\n",
      "             invideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternational\n",
      "        ConferenceonMachineLearning,pages737–744, Montreal.Omnipress.490\n",
      "             Mohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition.\n",
      "454\n",
      "7 5 3BIBLIOGRAPHY\n",
      "             Mohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,\n",
      "            M.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.In\n",
      "         Acoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConference\n",
      "    on,pages5060–5063. IEEE.454\n",
      "           Mohamed,A.,Dahl,G.,andHinton,G.(2012a). Acousticmodelingusingdeepbelief\n",
      "        networks.IEEETrans.onAudio,SpeechandLanguageProcessing, 2 0  (1),14–22.454\n",
      "            Mohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworks\n",
      "         performacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),\n",
      "        2012IEEEInternationalConferenceon,pages4273–4276. IEEE.454\n",
      "            Moller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning.\n",
      "    NeuralNetworks,,525–533. 6 312\n",
      "          Montavon,G.andMuller,K.-R.(2012). DeepBoltzmannmachinesandthecentering\n",
      "             trick.InG.Montavon,G.Orr,andK.-R.Müller,editors,NeuralNetworks:Tricksof\n",
      "            theTrade,volume7700ofLectureNotesinComputerScience,pages621–637. Preprint:\n",
      " http://arxiv.org/abs/1203.3783.670\n",
      "           Montúfar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworks\n",
      "      withdiscreteunits.NeuralComputation,. 2 6551\n",
      "           Montúfar,G.andAy,N.(2011).Reﬁnementsofuniversalapproximationresultsfor\n",
      "        deepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation, 2 3(5),\n",
      " 1306–1319. 551\n",
      "               Montufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinear\n",
      "          regionsofdeepneuralnetworks.In .,, NIPS’201418196197\n",
      "             Mor-Yosef,S.,Samueloﬀ,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Ranking\n",
      "            theriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.Obstet\n",
      "   Gynecol,(6),944–7. 7 5 3\n",
      "          Morin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguage\n",
      "     model.InAISTATS’2005.,462464\n",
      "              Mozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.Hanson\n",
      "       andR.Lippmann, editors, AdvancesinNeural InformationProcessingSystems4\n",
      "        (NIPS’91),pages275–282, SanMateo,CA.MorganKaufmann.403\n",
      "      Murphy,K. P.(2012).MachineLearning:a Probabilistic Perspective.MIT Press,\n",
      "     Cambridge,MA,USA.,,6096142\n",
      "           Nair,V.andHinton,G.(2010).RectiﬁedlinearunitsimproverestrictedBoltzmann\n",
      "      machines.In .,, ICML’201015171193\n",
      "7 5 4BIBLIOGRAPHY\n",
      "                Nair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,\n",
      "              D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams, andA.Culotta,editors,Advancesin\n",
      "          NeuralInformationProcessingSystems22,pages1339–1347. CurranAssociates,Inc.\n",
      "683\n",
      "            Narayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis.\n",
      "   In . NIPS’2010160\n",
      "        Naumann,U.(2008).OptimalJacobianaccumulationisNP-complete.Mathematical\n",
      "   Programming,(2),427–441. 1 1 2 218\n",
      "         Navigli,R.andVelardi,P.(2005). Structuralsemanticinterconnections:aknowledge-\n",
      "          basedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisand\n",
      "    MachineIntelligence,(7),1075––1086. 2 7 480\n",
      "              Neal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjustiﬁesincremental,\n",
      "             sparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MIT\n",
      "   Press,Cambridge,MA.632\n",
      "          Neal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.689\n",
      "          Neal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods.\n",
      "          TechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.676\n",
      "          Neal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions.\n",
      "         TechnicalReport9421,Dept.ofStatistics,UniversityofToronto.601\n",
      "            Neal,R.M.(1996).BayesianLearningforNeuralNetworks.LectureNotesinStatistics.\n",
      " Springer.262\n",
      "         Neal,R.M.(2001).Annealedimportancesampling. , StatisticsandComputing 1 1(2),\n",
      "    125–139. ,,, 623625626627\n",
      "           Neal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportance\n",
      " sampling.627\n",
      "            Nesterov,Y.(1983).Amethodofsolvingaconvexprogrammingproblemwithconvergence\n",
      " rate O /k ( 12      ). ,,372–376. SovietMathematicsDoklady 2 7 296\n",
      "            Nesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Applied\n",
      "       optimization.KluwerAcademicPubl.,Boston,Dordrecht,London.296\n",
      "               Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Reading\n",
      "   digits in naturalimages withunsupervised feature learning.Deep Learning and\n",
      "     UnsupervisedFeatureLearningWorkshop,NIPS.19\n",
      "           Ney,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatistical\n",
      "         languagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology\n",
      "    (Eurospeech),pages973–976, Berlin.458\n",
      "7 5 5BIBLIOGRAPHY\n",
      "       Ng, A.(2015). Adviceforapplyingmachinelearning.\n",
      " https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.416\n",
      "              Niesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-\n",
      "         speechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition.\n",
      "         InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),\n",
      "  pages177–180. 458\n",
      "              Ning,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005).\n",
      "         Towardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,\n",
      "     IEEETransactionson,(9),1360–1371. 1 4 353\n",
      "          Nocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9093\n",
      "              Norouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.In\n",
      "  ICML’2011.523\n",
      "          Nowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociative\n",
      "        mixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.445\n",
      "             Nowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing.\n",
      "    NeuralComputation,(4),473–493. 4 137\n",
      "              Olshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?Neural\n",
      "   Computation,,1665–1699. 1 7 15\n",
      "             Olshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptiveﬁeldproperties\n",
      "       bylearningasparsecodefornaturalimages. Nature, 3 8 1     ,607–609. ,,, 144252364492\n",
      "             Olshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiological\n",
      "           modelofvisualattentionandinvariantpatternrecognitionbasedondynamicrouting\n",
      "      ofinformation.J.Neurosci.,(11),4700–4719. 1 3 445\n",
      "          Opper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited.\n",
      "    Neuralcomputation,(3),786–792. 2 1 685\n",
      "             Oquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-level\n",
      "         imagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionand\n",
      "          PatternRecognition(CVPR),2014IEEEConferenceon,pages1717–1724. IEEE.534\n",
      "             Osindero,S.andHinton,G.E.(2008).Modelingimagepatcheswithadirectedhierarchy\n",
      "              ofMarkovrandomﬁelds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\n",
      "         AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1121–1128,\n",
      "    Cambridge,MA.MITPress.630\n",
      "        OvidandMartin,C.(2004). .W.W.Norton. Metamorphoses 1\n",
      "7 5 6BIBLIOGRAPHY\n",
      "           Paccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconcepts\n",
      "          andrelationsfrompositiveandnegativepropositions.InInternationalJointConference\n",
      "         onNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.479\n",
      "               Paine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervised\n",
      "          pre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.530\n",
      "            Palatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shot\n",
      "            learningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,\n",
      "            C.K.I.Williams, andA.Culotta,editors,AdvancesinNeuralInformationProcessing\n",
      "        Systems22,pages1410–1418. CurranAssociates,Inc.537\n",
      "           Parker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.Research\n",
      "      inEconomicsandManagementSci.,MIT.221\n",
      "             Pascanu,R.,Mikolov,T.,andBengio,Y.(2013).Onthediﬃcultyoftrainingrecurrent\n",
      "           neuralnetworks.In .,,,,,, ICML’2013285396399403409410411\n",
      "             Pascanu,R.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeep\n",
      "           recurrentneuralnetworks.In .,,,,, ICLR’201418262393394406455\n",
      "             Pascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregions\n",
      "            ofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In . ICLR’2014548\n",
      "          Pati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:\n",
      "         Recursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-\n",
      "            ceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,\n",
      "  pages40–44.252\n",
      "           Pearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidential\n",
      "        reasoning.In Proceedingsofthe7thConferenceofthe CognitiveScience Society,\n",
      "      UniversityofCalifornia,Irvine,pages329–334. 560\n",
      "        Pearl,J.(1988). ProbabilisticReasoninginIntelligentSystems: NetworksofPlausible\n",
      "   Inference.MorganKaufmann.52\n",
      "        Perron,O.(1907).Zurtheoriedermatrices. , MathematischeAnnalen 6 4  (2),248–263. 594\n",
      "             Petersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003. 29\n",
      "              Peterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner’sdiscoveryofshaping.\n",
      "          JournaloftheExperimentalAnalysisofBehavior,(3),317–328. 8 2 324\n",
      "             Pham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependent\n",
      "          sourcesthroughamaximumlikelihoodapproach.In ,pages771–774. EUSIPCO 487\n",
      "7 5 7BIBLIOGRAPHY\n",
      "             Pham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012).\n",
      "         NeuFlow:dataﬂowvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-\n",
      "          CAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044–1047. IEEE.\n",
      "446\n",
      "            Pinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksfor\n",
      "      scenelabeling.In ., ICML’2014352353\n",
      "             Pinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwith\n",
      "         convolutionalnetworks.InConferenceonComputerVisionandPatternRecognition\n",
      "  (CVPR).,352353\n",
      "               Pinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognition\n",
      "     hard?PLoSComputBiol,. 4451\n",
      "            Pinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologicall y-inspired\n",
      "           computervision:Acasestudyinunconstrainedfacerecognitiononfacebook.In\n",
      "         ComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputer\n",
      "      SocietyConferenceon,pages35–42.IEEE.357\n",
      "        Pollack,J.B.(1990).Recursivedistributedrepresentations.ArtiﬁcialIntelligence, 4 6(1),\n",
      " 77–105.394\n",
      "           Polyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging.\n",
      "       SIAMJ.ControlandOptimization,,838–855. 3 0 ( 4 ) 318\n",
      "             Polyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods.\n",
      "        USSRComputationalMathematicsandMathematicalPhysics,(5),1–17. 4 292\n",
      "          Poole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014). Analyzingnoiseinautoencoders\n",
      "     anddeepnetworks., . CoRR a b s/ 1 4 0 6 . 1 8 3 1237\n",
      "            Poon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.In\n",
      "         ProceedingsoftheTwenty-seventhConferenceinUncertaintyinArtiﬁcialIntelligence\n",
      "   (UAI),Barcelona,Spain.551\n",
      "              Presley,R.K.andHaggard,R.L.(1994).Aﬁxedpointimplementationofthebackpropa-\n",
      "        gationlearningalgorithm.InSoutheastcon’94.CreativeTechnologyTransfer-AGlobal\n",
      "          Aﬀair.,Proceedingsofthe1994IEEE,pages136–138. IEEE.446\n",
      "            Price,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEE\n",
      "      TransactionsonInformationTheory,(2),69–72. 4 685\n",
      "              Quiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisual\n",
      "        representationbysingleneuronsinthehumanbrain.Nature, 4 3 5 (7045),1102–1107.\n",
      "360\n",
      "7 5 8BIBLIOGRAPHY\n",
      "           Radford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwith\n",
      "        deepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434.\n",
      "   549550698699 ,,,\n",
      "           Raiko,T.,Yao,L.,Cho,K.,andBengio, Y.(2014).Iterativeneuralautoregressive\n",
      "       distributionestimator(NADE-k).Technicalreport,arXiv:1406. 1485. ,671706\n",
      "            Raina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearning\n",
      "           usinggraphicsprocessors. InL.BottouandM.Littman,editors,Proceedingsofthe\n",
      "        Twenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages873–880,\n",
      "      NewYork,NY,USA.ACM. ,23441\n",
      "             Ramsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundations\n",
      "           ofMathematicsandotherLogicalEssays,chapter7,pages156–198. McMasterUniversity\n",
      "       ArchivefortheHistoryofEconomicThought.54\n",
      "            Ranzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusing\n",
      "         factorizedthird-orderBoltzmannmachines.In ,pages2551–2558. CVPR’2010 676\n",
      "             Ranzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).Eﬃcientlearningofsparse\n",
      "            representationswithanenergy-basedmodel.In .,,,, NIPS’20061318504526528\n",
      "            Ranzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningof\n",
      "          invariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsof\n",
      "          theComputerVisionandPatternRecognitionConference(CVPR’07).IEEEPress.358\n",
      "             Ranzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbelief\n",
      "    networks.In . NIPS’2007504\n",
      "           Ranzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestricted\n",
      "           Boltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010.\n",
      "675\n",
      "            Ranzato,M.,Mnih,V.,andHinton,G.(2010b).Generatingmorerealisticimagesusing\n",
      "     gatedMRFs.In . NIPS’2010677\n",
      "            Rao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatistical\n",
      "          parameters.BulletinoftheCalcuttaMathematicalSociety,,81–89. , 3 7 133292\n",
      "            Rasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervised\n",
      "         learningwithladdernetwork.arXivpreprintarXiv:1507.02672.,421528\n",
      "              Recht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachto\n",
      "       parallelizingstochasticgradientdescent.In . NIPS’2011442\n",
      "             Reichert,D.P.,Seriès,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-\n",
      "          basedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformation\n",
      "    ProcessingSystems,pages2357–2365. 663\n",
      "7 5 9BIBLIOGRAPHY\n",
      "          Rezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagation\n",
      "      and approximateinferencein deepgenerative models.In .Preprint: ICML’2014\n",
      "   arXiv:1401. 4082. ,,650685693\n",
      "             Rifai,S.,Vincent,P.,Muller,X.,Glorot,X.,andBengio,Y.(2011a).Contractiveauto-\n",
      "           encoders:Explicitinvarianceduringfeatureextraction.In .,,, ICML’2011518519520\n",
      "521\n",
      "              Rifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X.\n",
      "         (2011b).Higherordercontractiveauto-encoder.In ., ECMLPKDD518519\n",
      "            Rifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c). Themanifold\n",
      "       tangentclassiﬁer.In .,, NIPS’2011268269520\n",
      "             Rifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessfor\n",
      "      samplingcontractiveauto-encoders.In . ICML’2012707\n",
      "          Ringach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.Cognitive\n",
      "   Science,(2),147–166. 2 8 362\n",
      "          Roberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesand\n",
      "    practice.CambridgeUniversityPress.489\n",
      "            Robinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeech\n",
      "         recognitionsystem.ComputerSpeechandLanguage,(3),259–274. , 5 23454\n",
      "          Rockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.91\n",
      "             Romero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y.\n",
      "           (2015).Fitnets:Hintsforthindeepnets.In . ICLR’2015,arXiv:1412.6550321\n",
      "            Rosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti.\n",
      "          linearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics, 8(1),\n",
      "  pp.181–217. 91\n",
      "           Rosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageand\n",
      "           organizationinthebrain.PsychologicalReview,,386–408. ,, 6 5 131423\n",
      "          Rosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1423\n",
      "            Roweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinear\n",
      "    embedding.Science,(5500)., 2 9 0 160516\n",
      "              Roweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.In\n",
      "           T.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformation\n",
      "        ProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.485\n",
      "            Rubin,D.B.(1984).Bayesianlyjustiﬁableandrelevantfrequencycalculationsfor etal.\n",
      "         theappliedstatistician. ,(4),1151–1172. TheAnnalsofStatistics 1 2 712\n",
      "7 6 0BIBLIOGRAPHY\n",
      "     Rumelhart, D., Hinton, G., andWilliams, R. (1986a).Learningrepresentationsby\n",
      "            back-propagatingerrors.Nature,,533–536. ,,,,,,, 3 2 3 131722200221367472477\n",
      "             Rumelhart,D.E.,Hinton,G.E.,andWilliams, R.J.(1986b).Learninginternalrepresen-\n",
      "             tationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland,editors,Parallel\n",
      "           DistributedProcessing,volume1,chapter8,pages318–362. MITPress,Cambridge.,19\n",
      " 23221,\n",
      "            Rumelhart,D.E.,McClelland,J.L.,andthePDPResearchGroup(1986c).Parallel\n",
      "         DistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,\n",
      " Cambridge.16\n",
      "              Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\n",
      "             A.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLarge\n",
      "    ScaleVisualRecognitionChallenge.19\n",
      "              Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\n",
      "            A.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognition etal.\n",
      "     challenge.arXivpreprintarXiv:1409.0575.24\n",
      "            Russel,S.J.andNorvig,P.(2003).ArtiﬁcialIntelligence:aModernApproach.Prentice\n",
      " Hall.84\n",
      "           Rust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).Spatiotemporal\n",
      "         elementsofmacaqueV1receptiveﬁelds.Neuron,(6),945–956. 4 6 361\n",
      "           Sainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-\n",
      "         tionalneuralnetworksforLVCSR.In . ICASSP2013455\n",
      "           Salakhutdinov,R.(2010).LearninginMarkovrandomﬁeldsusingtemperedtransitions.In\n",
      "            Y.Bengio,D.Schuurmans,C.Williams, J.Laﬀerty,andA.Culotta,editors,Advances\n",
      "       inNeuralInformationProcessingSystems22(NIPS’09).601\n",
      "           Salakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsof\n",
      "          theInternationalConferenceonArtiﬁcialIntelligenceandStatistics,volume5,pages\n",
      "       448–455. ,,,,,, 2223527660663668669\n",
      "           Salakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalof\n",
      "  ApproximateReasoning.522\n",
      "    Salakhutdinov, R. andHinton, G. E.(2007a).Learning a nonl inearembedding by\n",
      "         preservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternational\n",
      "         ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’07),SanJuan,Porto\n",
      "  Rico.Omnipress.525\n",
      "            Salakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In . SIGIR’2007522\n",
      "7 6 1BIBLIOGRAPHY\n",
      "             Salakhutdinov,R.andHinton,G.E.(2008).Usingdeepbeliefnetstolearncovariance\n",
      "              kernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\n",
      "         AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1249–1256,\n",
      "    Cambridge,MA.MITPress.240\n",
      "           Salakhutdinov,R.andLarochelle,H.(2010).EﬃcientlearningofdeepBoltzmannmachines.\n",
      "          InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceand\n",
      "         Statistics(AISTATS2010),JMLRW&CP,volume9,pages693–700. 650\n",
      "           Salakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In . NIPS’2008\n",
      "475\n",
      "            Salakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbelief\n",
      "             networks.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,Proceedingsof\n",
      "         theTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),volume25,\n",
      "    pages872–879. ACM. ,626659\n",
      "           Salakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesfor\n",
      "    collaborativeﬁltering.In.ICML475\n",
      "         Sanger, T.D. (1994).Neuralnetworklearningcontrolofrobotmanipulatorsusing\n",
      "         graduallyincreasingtaskdiﬃculty.IEEETransactionsonRoboticsandAutomation,\n",
      " 1 0(3).324\n",
      "            Saul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractable\n",
      "            networks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeural\n",
      "         InformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.636\n",
      "               Saul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Meanﬁeldtheoryforsigmoidbelief\n",
      "         networks.JournalofArtiﬁcialIntelligenceResearch,,61–76. , 4 23689\n",
      "             Savich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentation\n",
      "           onimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,\n",
      "  1 8(1),240–252. 446\n",
      "                 Saxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandom\n",
      "          weightsandunsupervisedfeaturelearning.InProc.ICML’2011.ACM.357\n",
      "              Saxe,A.M.,McClelland,J.L.,andGanguli,S.(2013).Exactsolutionstothenonlinear\n",
      "            dynamicsoflearningindeeplinearneuralnetworks.In.,, ICLR282283299\n",
      "            Schaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization.\n",
      "      InInternationalConferenceonLearningRepresentations.306\n",
      "          Schmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleof\n",
      "      historycompression.NeuralComputation,(2),234–242. 4 394\n",
      "         Schmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118.\n",
      "384\n",
      "7 6 2BIBLIOGRAPHY\n",
      "           Schmidhuber,J.andHeil,S.(1996).Sequentialneuraltextcompression.IEEETransac-\n",
      "      tionsonNeuralNetworks,(1),142–146. 7 472\n",
      "            Schölkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,\n",
      "       regularization,optimization,andbeyond.MITpress.700\n",
      "            Schölkopf,B.,Smola,A.,andMüller,K.-R.(1998).Nonlinearcomponentanalysisasa\n",
      "        kerneleigenvalueproblem.NeuralComputation,,1299–1319. , 1 0 160516\n",
      "               Schölkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods—\n",
      "        SupportVectorLearning.MITPress,Cambridge,MA.,17140\n",
      "              Schölkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).On\n",
      "         causalandanticausallearning.In ,pages1255–1262. ICML’2012 543\n",
      "           Schuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsfor\n",
      "  speechrecognition.186\n",
      "          Schuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEE\n",
      "      TransactionsonSignalProcessing,(11),2673–2681. 4 5 388\n",
      "          Schwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,\n",
      "  2 1,492–518. 461\n",
      "          Schwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation.\n",
      "        ThePragueBulletinofMathematicalLinguistics,,137–146. 9 3 468\n",
      "         Schwenk,H.(2014).Cleaned subsetofWMT’14dataset.19\n",
      "             Schwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-\n",
      "             works.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformation\n",
      "        ProcessingSystems10(NIPS’97),pages647–653. MITPress.255\n",
      "       Schwenk, H.andGauvain, J.-L.(2002).Connectionistlanguagemodeling forlarge\n",
      "        vocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,\n",
      "         SpeechandSignalProcessing(ICASSP),pages765–768, Orlando,Florida.461\n",
      "            Schwenk,H.,Costa-jussà,M.R.,andFonollosa,J.A.R.(2006).Continuousspace\n",
      "           languagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpoken\n",
      "    LanguageTranslation,pages166–173. 468\n",
      "            Seide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-\n",
      "          dependentdeepneuralnetworks.InInterspeech2011,pages437–440. 24\n",
      "         Sejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings\n",
      "           151onNeuralNetworksforComputing,pages398–403. AmericanInstituteofPhysics\n",
      " Inc.683\n",
      "7 6 3BIBLIOGRAPHY\n",
      "             Series,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnet\n",
      "           syndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesin\n",
      "      NeuralInformationProcessingSystems,pages2020–2028. 663\n",
      "           Sermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksapplied\n",
      "       tohousenumbersdigitclassiﬁcation., . CoRR a b s/ 1 2 0 4 . 3 9 6 8452\n",
      "           Sermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetection\n",
      "         withunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceon\n",
      "        ComputerVisionandPatternRecognition(CVPR’13).IEEE.,24197\n",
      "           Shilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications.\n",
      "29\n",
      "        Siegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science, 2 6 8(5210),\n",
      " 545–548. 372\n",
      "           Siegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.Applied\n",
      "    MathematicsLetters,(6),77–80. 4 372\n",
      "              Siegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets.\n",
      "         JournalofComputerandSystemsSciences,(1),132–150. , 5 0 372399\n",
      "            Sietsma,J.andDow,R.(1991).Creatingartiﬁcialneuralnetworksthatgeneralize.Neural\n",
      "   Networks,(1),67–79. 4 237\n",
      "             Simard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutional\n",
      "    neuralnetworks.In . ICDAR’2003365\n",
      "           Simard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvances\n",
      "       inNeuralInformationProcessingSystems,pages232–239. 446\n",
      "              Simard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalism\n",
      "             forspecifyingselectedinvariancesinanadaptivenetwork.In .,,, NIPS’1991267268269\n",
      "350\n",
      "             Simard,P.Y.,LeCun,Y.,andDenker,J.(1993).Eﬃcientpatternrecognitionusinga\n",
      "      newtransformationdistance.In . NIPS’92267\n",
      "             Simard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformation\n",
      "          invarianceinpatternrecognition—tangentdistanceandtangentpropagation.Lecture\n",
      "     NotesinComputerScience,. 1 5 2 4267\n",
      "               Simons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringa\n",
      "        real-worldinteraction.PsychonomicBulletin&Review,(4),644–649. 5 541\n",
      "           Simonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scale\n",
      "    imagerecognition.In.ICLR319\n",
      "7 6 4BIBLIOGRAPHY\n",
      "            Sjöberg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,\n",
      "        withapplicationtoneuralnetworks.InternationalJournalofControl, 6 2 (6),1391–1407.\n",
      "247\n",
      "          Skinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94–99. 1 3 324\n",
      "         Smolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsof\n",
      "            harmonytheory.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributed\n",
      "            Processing,volume1,chapter6,pages194–281. MITPress,Cambridge.,,568584653\n",
      "            Snoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationof\n",
      "      machinelearningalgorithms.In . NIPS’2012430\n",
      "               Socher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamic\n",
      "          poolingandunfoldingrecursiveautoencodersforparaphrasedetection.In . NIPS’2011\n",
      " 394396,\n",
      "              Socher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-\n",
      "          guagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternational\n",
      "     ConferenceonMachineLearning(ICML’2011).394\n",
      "      Socher, R., Pennington, J., Huang, E.H., Ng, A. Y.,andManning, C.D.(2011c).\n",
      "      Semi-supervisedrecursiveautoencoders forpredictingsentimentdistributions.In\n",
      "  EMNLP’2011.394\n",
      "                Socher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,\n",
      "          C.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentiment\n",
      "     treebank.In ., EMNLP’2013394396\n",
      "              Socher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthrough\n",
      "         cross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessing\n",
      "   Systems(NIPS2013).537\n",
      "           Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deep\n",
      "     unsupervisedlearningusingnonequilibriumthermodynamics.712\n",
      "             Sohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywith\n",
      "       point-wisegatedBoltzmannmachines.In . ICML’2013683\n",
      "            Solomonoﬀ,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-\n",
      " bility.324\n",
      "             Sontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputer\n",
      "      andSystemsSciences,,69–96. , 1 6 8 545549\n",
      "              Sontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocal\n",
      "           minimaevenfornetworkswithouthiddenlayers. ,,91–106. ComplexSystems 3 281\n",
      "7 6 5BIBLIOGRAPHY\n",
      "             Sparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1\n",
      "              Spitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how\n",
      "          “lessismore”inunsuperviseddependencyparsing.InHLT’10.324\n",
      "            Squire,W.andTrapp,G.(1998). Usingcomplexvariablestoestimatederivativesofreal\n",
      "     functions.SIAMRev.,(1),110––112. 4 0 434\n",
      "            Srebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsof\n",
      "          the18thAnnualConferenceonLearningTheory,pages545–560. Springer-Verlag.235\n",
      "          Srivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master’sthesis,U.\n",
      " Toronto.533\n",
      "          Srivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmann\n",
      "    machines.In . NIPS’2012539\n",
      "            Srivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).Modelingdocumentswith\n",
      "       deepBoltzmannmachines.arXivpreprintarXiv:1309.6865.660\n",
      "           Srivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014).\n",
      "            Dropout:Asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachine\n",
      "        LearningResearch,,1929–1958. ,,,, 1 5 255261262264669\n",
      "          Srivastava,R.K.,Greﬀ,K.,andSchmidhuber,J.(2015).Highwaynetworks.\n",
      "  arXiv:1505.00387.322\n",
      "             Steinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearning\n",
      "         algorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,\n",
      "  0,1115–1119. 440\n",
      "            Stoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphical\n",
      "         modelparametersgivenapproximateinference,decoding,andmodelstructure.In\n",
      "          Proceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics\n",
      "          (AISTATS) JMLRWorkshopandConferenceProceedings ,volume15of ,pages725–733,\n",
      "         FortLauderdale.Supplementarymaterial(4pages)alsoavailable.,671695\n",
      "            Sukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemory\n",
      "     networks.arXivpreprintarXiv:1503.08895.412\n",
      "           Supancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.In\n",
      "  CVPR’2013.324\n",
      "          Sussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworks\n",
      "        withsmartinitialization ., .,,, CoRR a b s/ 1 4 1 2 . 6 5 5 8287300302398\n",
      "          Sutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentof\n",
      "      computerscience,UniversityofToronto.,401408\n",
      "7 6 6BIBLIOGRAPHY\n",
      "             Sutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversal\n",
      "     approximators.NeuralComputation,(11),2629–2636. 2 0 689\n",
      "           Sutskever,I.andTieleman, T.(2010).OntheConvergencePropertiesofContrastive\n",
      "            Divergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternational\n",
      "          ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),volume9,pages789–795.\n",
      "610\n",
      "           Sutskever,I.,Hinton,G.,andTaylor,G.(2009).Therecurrenttemporalrestricted\n",
      "     Boltzmannmachine.In . NIPS’2008682\n",
      "            Sutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrent\n",
      "       neuralnetworks.In ,pages1017–1024. ICML’2011 472\n",
      "           Sutskever, I.,Martens,J.,Dahl, G.,andHinton,G.(2013).Ontheimportanceof\n",
      "          initialization andmomentumindeeplearning.In.,, ICML296401408\n",
      "             Sutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwith\n",
      "            neuralnetworks.In .,,,,,, NIPS’2014,arXiv:1409.32152599390404407469470\n",
      "           Sutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress.\n",
      "104\n",
      "             Sutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethods\n",
      "          forreinforcementlearningwithfunctionapproximation.In ,pages1057– NIPS’1999\n",
      "   –1063.MITPress.688\n",
      "             Swersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).On\n",
      "            autoencodersandscorematchingforenergybasedmodels.In .ACM. ICML’2011 509\n",
      "           Swersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization.\n",
      "    arXivpreprintarXiv:1406.3896.431\n",
      "              Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,\n",
      "          V.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,\n",
      "       arXiv:1409. 4842.,,,,,, 2223197255265322341\n",
      "             Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,and\n",
      "        Fergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLR a b s/ 1 3 1 2 . 6 1 9 9.\n",
      "  265266269,,\n",
      "             Szegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,andWojna,Z.(2015).Rethinkingthe\n",
      "        InceptionArchitectureforComputerVision. ., ArXive-prints240318\n",
      "              Taigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegapto\n",
      "        human-levelperformanceinfaceveriﬁcation.In . CVPR’201498\n",
      "             Tandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocial\n",
      "     Sciences.UniversityofCaliforniaPress.1\n",
      "7 6 7BIBLIOGRAPHY\n",
      "            Tang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.In\n",
      "          Proceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,\n",
      "   2010,Haifa,Israel.237\n",
      "            Tang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers.\n",
      "    arXivpreprintarXiv:1206.4635.485\n",
      "          Taylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachines\n",
      "           formodelingmotionstyle.InL.BottouandM.Littman, editors,Proceedingsof\n",
      "        theTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages\n",
      "     1025–1032, Montreal,Quebec,Canada.ACM.682\n",
      "             Taylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinary\n",
      "             latentvariables.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeural\n",
      "         InformationProcessingSystems19(NIPS’06),pages1345–1352. MITPress,Cambridge,\n",
      " MA.682\n",
      "            Teh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodels\n",
      "        forsparseovercompleterepresentations.JournalofMachineLearningResearch, 4,\n",
      " 1235–1260. 487\n",
      "             Tenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframework\n",
      "         fornonlineardimensionalityreduction.Science,(5500),2319–2323. ,, 2 9 0 160516532\n",
      "                Theis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerative\n",
      "   models.arXiv:1511. 01844. ,694715\n",
      "              Thompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutional\n",
      "            networkandagraphicalmodelforhumanposeestimation.In . NIPS’2014353\n",
      "             Thrun,S.(1995).Learningtoplaythegameofchess.In . NIPS’1994269\n",
      "             Tibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.Journalofthe\n",
      "      RoyalStatisticalSocietyB,,267–288. 5 8 233\n",
      "         Tieleman, T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationsto\n",
      "              thelikelihoodgradient.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,Pro-\n",
      "         ceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),\n",
      "   pages1064–1071. ACM.610\n",
      "            Tieleman, T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastive\n",
      "           divergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixth\n",
      "        InternationalConferenceonMachineLearning(ICML’09),pages1033–1040. ACM.\n",
      "612\n",
      "           Tipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis.\n",
      "         JournaloftheRoyalStatisticalSocietyB,(3),611–622. 6 1 487\n",
      "7 6 8BIBLIOGRAPHY\n",
      "             Torralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesfor\n",
      "          recognition.InProceedingsoftheComputerVisionandPatternRecognitionConference\n",
      "    (CVPR’08),pages1–8.,522523\n",
      "             Touretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsof\n",
      "         aconnectionistinferencearchitecture. InProceedingsofthe9thInternationalJoint\n",
      "            ConferenceonArtiﬁcialIntelligence-Volume1,IJCAI’85,pages238–243, SanFrancisco,\n",
      "      CA,USA.MorganKaufmannPublishersInc.16\n",
      "             Tu,K.andHonavar,V.(2011). Ontheutilityofcurriculainunsupervisedlearningof\n",
      "     probabilisticgrammars.In . IJCAI’2011324\n",
      "              Turaga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,\n",
      "            W.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogenerateaﬃnity\n",
      "        graphsforimagesegmentation.NeuralComputation,(2),511–538. 2 2 353\n",
      "            Turian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleand\n",
      "          generalmethodforsemi-supervisedlearning.InProc.ACL’2010,pages384–394. 533\n",
      "             Töscher,A.,Jahrer,M.,andBell,R.M.(2009). TheBigChaossolutiontotheNetﬂix\n",
      "  grandprize.475\n",
      "            Uria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-\n",
      "     sivedensity-estimator.In . NIPS’2013706\n",
      "             Uria,B.,Murray,I.,andLarochelle,H.(2014).Adeepandtractabledensityestimator.\n",
      "     In .,, ICML’2014186706707\n",
      "            vandenOörd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusic\n",
      "    recommendation.In . NIPS’2013475\n",
      "              vanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.Machine\n",
      "    LearningRes.,., 9473516\n",
      "              Vanhoucke,V.,Senior,A.,andMao,M.Z.(2011).Improvingthespeedofneuralnetworks\n",
      "           onCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop.\n",
      " 439447,\n",
      "           Vapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-\n",
      "  Verlag,Berlin.112\n",
      "            Vapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork.\n",
      "112\n",
      "             Vapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelative\n",
      "           frequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,\n",
      "  1 6,264–280. 112\n",
      "7 6 9BIBLIOGRAPHY\n",
      "         Vincent,P.(2011). Aconnectionbetweenscorematchinganddenoisingautoencoders.\n",
      "     NeuralComputation,(7).,, 2 3509511708\n",
      "             Vincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In .MITPress. NIPS’2002\n",
      "517\n",
      "           Vincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingand\n",
      "           composingrobustfeatureswithdenoisingautoencoders.In ., ICML2008237511\n",
      "            Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stacked\n",
      "           denoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocal\n",
      "       denoisingcriterion.J.MachineLearningRes.,. 1 1511\n",
      "            Vincent,P.,deBrébisson,A.,andBouthillier,X.(2015).Eﬃcientexactgradientupdate\n",
      "              fortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,\n",
      "            D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformation\n",
      "         ProcessingSystems28,pages1108–1116. CurranAssociates,Inc.460\n",
      "  Vinyals, O., Kai ser, L., Koo, T., Petrov, S., Sutskever, I., andHinton, G.(2014a).\n",
      "        Grammarasaforeignlanguage.Technicalreport,arXiv:1412. 7449.404\n",
      "               Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimage\n",
      "    captiongenerator.arXiv1411.4555. 404\n",
      "           Vinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprint\n",
      "  arXiv:1506.03134.412\n",
      "               Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimage\n",
      "      captiongenerator.In .arXiv:1411. 4555. CVPR’2015 100\n",
      "           Viola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternational\n",
      "    JournalofComputerVision.444\n",
      "             Visin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015).\n",
      "          ReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXiv\n",
      "   preprintarXiv:1505.00393.390\n",
      "              VonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinal\n",
      "         projectionsdirectedtotheauditorypathway.Nature,(6780),871–876. 4 0 4 15\n",
      "            Wager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization.\n",
      "           InAdvancesinNeuralInformationProcessingSystems26,pages351–359. 262\n",
      "             Waibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phoneme\n",
      "         recognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,\n",
      "       andSignalProcessing,,328–339. ,, 3 7 368448454\n",
      "              Wan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneural\n",
      "      networksusingdropconnect.In . ICML’2013263\n",
      "7 7 0BIBLIOGRAPHY\n",
      "            Wang,S.andManning,C.(2013).Fastdropouttraining.In . ICML’2013263\n",
      "              Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointly\n",
      "     embedding.InProc.EMNLP’2014.479\n",
      "            Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b). Knowledgegraphembeddingby\n",
      "       translatingonhyperplanes.InProc.AAAI’2014.479\n",
      "            Warde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempirical\n",
      "            analysisofdropoutinpiecewiselinearnetworks.In .,, ICLR’2014259263264\n",
      "            Wawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N.\n",
      "         (1996).Spert-II:Avectormicroprocessorsystem. ,(3),79–86. Computer 2 9 446\n",
      "            Weaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-\n",
      "        mentlearning.InProc.UAI’2001,pages538–545. 688\n",
      "             Weinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsby\n",
      "        semideﬁniteprogramming.In ,pages988–995. , CVPR’2004 160516\n",
      "      Weiss, Y., Torralba, A., andFergus, R.(2008).Spectral hashing.In,pagesNIPS\n",
      " 1753–1760. 523\n",
      "              Welling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvances\n",
      "       inNeuralInformationProcessingSystems,pages665–672. 699\n",
      "        Welling, M.,Hinton,G.E., andOsindero, S.(2003a).Learningsparsetopographic\n",
      "         representationswithproductsofStudent-tdistributions.In . NIPS’2002677\n",
      "             Welling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,\n",
      "          S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessing\n",
      "       Systems15(NIPS’02),pages665–672. MITPress.621\n",
      "           Welling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniums\n",
      "             withanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,\n",
      "          editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),volume17,\n",
      "    Cambridge,MA.MITPress.673\n",
      "          Werbos, P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.In\n",
      "             Proceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762–770. 221\n",
      "             Weston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningto\n",
      "         rankwithjointword-imageembeddings.MachineLearning,(1),21–35. 8 1 396\n",
      "      Weston, J., Chopra, S., andBordes, A.(2014).Memorynetworks.arXivpreprint\n",
      "   arXiv:1410.3916.,412480\n",
      "             Widrow,B.andHoﬀ,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCON\n",
      "            ConventionRecord,volume4,pages96–104.IRE,NewYork.,,, 14192223\n",
      "7 7 1BIBLIOGRAPHY\n",
      "             Wikipedia(2015).Listofanimalsbynumberofneurons—Wikipedia,thefreeencyclopedia.\n",
      "    [Online;accessed 4-March-2015]. ,2223\n",
      "            Williams, C.K.I.andAgakov,F.V.(2002). ProductsofGaussiansandProbabilistic\n",
      "       MinorComponentAnalysis.NeuralComputation,,1169–1182. 1 4 ( 5 ) 679\n",
      "           Williams, C.K.I.andRasmussen,C.E.(1996). Gaussianprocessesforregression. In\n",
      "           D.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformation\n",
      "          ProcessingSystems8(NIPS’95),pages514–520. MITPress,Cambridge,MA.140\n",
      "        Williams, R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionist\n",
      "       reinforcementlearning.MachineLearning,,229–256. , 8 685686\n",
      "             Williams, R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfully\n",
      "       recurrentneuralnetworks.NeuralComputation,,270–280. 1 219\n",
      "              Wilson,D.R.andMartinez,T.R.(2003).Thegeneralineﬃciencyofbatchtrainingfor\n",
      "       gradientdescentlearning.NeuralNetworks,(10),1429–1451. 1 6 276\n",
      "          Wilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.American\n",
      "        JournalofMathematicalandManagementSciences,(3),277––312. 4 687\n",
      "            Wiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningof\n",
      "      invariances.NeuralComputation,(4),715–770. , 1 4 489490\n",
      "            Wolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEE\n",
      "      TransactionsonEvolutionaryComputation,,67–82. 1 289\n",
      "             Wolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.Neural\n",
      "   Computation,(7),1341–1390. 8 114\n",
      "                Wu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimage\n",
      "  recognition.arXiv:1501. 02876.442\n",
      "           Wu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalof\n",
      "   Optimization,,814–836. 7 323\n",
      "             Xiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulated\n",
      "       splicingusingRNAsequenceandcellularcontext. , Bioinformatics 2 7 (18),2554–2562.\n",
      "262\n",
      "                Xu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,and\n",
      "            Bengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisual\n",
      "       attention.In .,, ICML’2015,arXiv:1502.03044100404688\n",
      "              Yildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty.\n",
      "    Neuralnetworks,,1–9. 3 5400\n",
      "7 7 2BIBLIOGRAPHY\n",
      "             Yosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeatures\n",
      "        indeepneuralnetworks?In ., NIPS’2014321534\n",
      "           Younes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidly\n",
      "          decreasingergodicityrates.InStochasticsandStochasticsModels,pages177–228. 610\n",
      "     Yu,D.,Wang,S.,and Deng, L. (2010).Sequential labeling using deep-structured\n",
      "           conditionalrandomﬁelds.IEEEJournalofSelectedTopicsinSignalProcessing.319\n",
      "           Zaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615. 325\n",
      "          Zaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines.\n",
      "  arXiv:1505.00521.415\n",
      "          Zaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitions\n",
      "            ofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematical\n",
      "    Society.AmericanMathematicalSociety.548\n",
      "           Zeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks.\n",
      "   In . ECCV’146\n",
      "               Zeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,\n",
      "             A.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013). Onrectiﬁedlinearunitsfor\n",
      "      speechprocessing.In . ICASSP2013454\n",
      "            Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015). Objectdetectors\n",
      "       emergeindeepsceneCNNs.ICLR’2015,arXiv:1412. 6856.549\n",
      "           Zhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerative\n",
      "          stochasticnetworkforproteinsecondarystructureprediction.In . ICML’2014711\n",
      "             Zhou,Y.andChellappa,R.(1988).Computationofopticalﬂowusinganeuralnetwork.\n",
      "           InNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71–78.IEEE.335\n",
      "           Zöhrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassiﬁcation.In\n",
      "  NIPS’2014.711\n",
      "7 7 3I ndex\n",
      "   0-1loss,, 1 0 0271\n",
      "   Absolutevaluerectiﬁcation,187\n",
      " Accuracy,415\n",
      "  Activationfunction,165\n",
      "  Activeconstraint,92\n",
      " AdaGrad,301\n",
      "    ADALINE,seeadaptivelinearelement\n",
      "  Adam,,303417\n",
      "     Adaptivelinearelement,,,142123\n",
      "  Adversarialexample,263\n",
      "    Adversarialtraining,,,264267523\n",
      " Aﬃne,107\n",
      "    AIS,seeannealedimportancesampling\n",
      "  Almosteverywhere,68\n",
      "   Almostsureconvergence,126\n",
      "   Ancestralsampling,,573588\n",
      "    ANN,seeArtiﬁcialneuralnetwork\n",
      "   Annealedimportancesampling, ,, 618659\n",
      "706\n",
      "   ApproximateBayesiancomputation,706\n",
      "  Approximateinference,576\n",
      "  Artiﬁcialintelligence,1\n",
      "    Artiﬁcialneuralnetwork, seeNeuralnet-\n",
      "work\n",
      "    ASR,seeautomaticspeechrecognition\n",
      "  Asymptoticallyunbiased,121\n",
      "   Audio,,,99351450\n",
      "   Autoencoder,,,4348 4 9 4\n",
      "   Automaticspeechrecognition,450\n",
      " Back-propagation,198\n",
      "   Back-propagationthroughtime, 3 7 6\n",
      "  Backprop,seeback-propagation   Bagofwords,462\n",
      " Bagging,250\n",
      "   Batchnormalization,,262417\n",
      "  Bayeserror, 1 1 4\n",
      "  Bayes’rule,67\n",
      "   Bayesianhyperparameteroptimization,427\n",
      " Bayesian network, seedirected graphical\n",
      "model\n",
      "  Bayesianprobability,52\n",
      "  Bayesianstatistics, 1 3 2\n",
      "     Beliefnetwork,seedirectedgraphicalmodel\n",
      "  Bernoullidistribution,59\n",
      " BFGS,310\n",
      "  Bias,,121223\n",
      "  Biasparameter,107\n",
      "   Biasedimportancesampling,586\n",
      " Bigram,453\n",
      "  Binaryrelation,474\n",
      "   BlockGibbssampling,592\n",
      "  Boltzmanndistribution,563\n",
      "   Boltzmannmachine,,563645\n",
      "    BPTT,seeback-propagationthroughtime\n",
      " Broadcasting,31\n",
      " Burn-in,590\n",
      "   CAE,seecontractiveautoencoder\n",
      "   Calculusofvariations,173\n",
      "    Categoricaldistribution,seemultinoullidis-\n",
      "tribution\n",
      "   CD,seecontrastivedivergence\n",
      "   Centeringtrick(DBM),664\n",
      "   Centrallimittheorem,61\n",
      "   Chainrule(calculus),199\n",
      "    Chainruleofprobability,56\n",
      "774INDEX\n",
      " Chess,2\n",
      " Chord,570\n",
      "  Chordalgraph,570\n",
      "   Class-basedlanguagemodels,455\n",
      "   Classicaldynamicalsystem,367\n",
      " Classiﬁcation,97\n",
      "     Cliquepotential,seefactor(graphicalmodel)\n",
      "    CNN,seeconvolutionalneuralnetwork\n",
      "  CollaborativeFiltering,470\n",
      "   Collider,seeexplainingaway\n",
      "  Colorimages,351\n",
      "  Complexcell,357\n",
      "  Computationalgraph,199\n",
      "  Computervision,444\n",
      "  Conceptdrift,531\n",
      "  Conditionnumber,274\n",
      "    Conditionalcomputation,seedynamicstruc-\n",
      "ture\n",
      "   Conditionalindependence,,xiv57\n",
      "  Conditionalprobability,56\n",
      "  ConditionalRBM,676\n",
      "  Connectionism,,16435\n",
      "   Connectionisttemporalclassiﬁcation,452\n",
      "  Consistency,,126504\n",
      "   Constrainedoptimization,,90231\n",
      "  Content-basedaddressing,410\n",
      "   Content-basedrecommendersystems,471\n",
      "  Context-speciﬁcindependence,566\n",
      "  Contextualbandits,471\n",
      "  Continuationmethods,321\n",
      "  Contractiveautoencoder,513\n",
      " Contrast,446\n",
      "    Contrastivedivergence,,,285603662\n",
      "  Convexoptimization,138\n",
      "  Convolution,,324673\n",
      "  Convolutionalnetwork,15\n",
      "   Convolutionalneuralnetwork,,248 3 2 4 ,,417\n",
      "451\n",
      "   Coordinatedescent,,315660\n",
      " Correlation,58\n",
      "    Costfunction,seeobjectivefunction\n",
      "  Covariance,,xiv58\n",
      "  Covariancematrix,59\n",
      " Coverage,416  Criticaltemperature,596\n",
      " Cross-correlation,326\n",
      "  Cross-entropy,, 7 2129\n",
      " Cross-validation,119\n",
      "    CTC,seeconnectionisttemporalclassiﬁca-\n",
      "tion\n",
      "  Curriculumlearning,322\n",
      "   Curseofdimensionality,151\n",
      " Cyc,2\n",
      " D-separation,565\n",
      "   DAE,seedenoisingautoencoder\n",
      "    Datageneratingdistribution,, 1 0 8128\n",
      "   Datageneratingprocess,108\n",
      "  Dataparallelism,439\n",
      " Dataset,101\n",
      "   Datasetaugmentation,,267449\n",
      "    DBM,seedeepBoltzmannmachine\n",
      "   DCGAN,,,544545691\n",
      "   Decisiontree,, 1 4 0541\n",
      " Decoder,4\n",
      "       Deepbeliefnetwork,,,,,, 23522623648651\n",
      " 674682,\n",
      "  DeepBlue,2\n",
      "      DeepBoltzmannmachine,,,,, 2123522623\n",
      "    644648653662674 ,,,,\n",
      "    Deepfeedforwardnetwork,,162417\n",
      "   Deeplearning,,25\n",
      "   Denoisingautoencoder,,502679\n",
      "   Denoisingscorematching,612\n",
      "  Densityestimation,100\n",
      "  Derivative,,xiv80\n",
      "  Designmatrix, 1 0 3\n",
      "  Detectorlayer,333\n",
      " Determinant,xiii\n",
      "  Diagonalmatrix,38\n",
      "   Diﬀerentialentropy,,71638\n",
      "   Diracdeltafunction,62\n",
      "      Directedgraphicalmodel,,,, 74499556682\n",
      "  Directionalderivative,82\n",
      "   Discriminativeﬁne-tuning,seesupervised\n",
      "ﬁne-tuning\n",
      "  DiscriminativeRBM,677\n",
      "    Distributedrepresentation,,,16147539\n",
      "  Domainadaptation,529\n",
      "7 7 5INDEX\n",
      "   Dotproduct,,31137\n",
      "  Doublebackprop,267\n",
      "    Doublyblockcirculantmatrix,327\n",
      "   Dreamsleep,,602644\n",
      " DropConnect,261\n",
      "      Dropout,,,,,, 2 5 3417422423662679\n",
      "  Dynamicstructure,440\n",
      " E-step,626\n",
      "    Earlystopping,,–, 239242244417\n",
      "   EBM,seeenergy-basedmodel\n",
      "     Echostatenetwork,,,2123396\n",
      "  Eﬀectivecapacity,111\n",
      " Eigendecomposition,39\n",
      " Eigenvalue,39\n",
      " Eigenvector,39\n",
      "    ELBO,seeevidencelowerbound\n",
      "    Element-wiseproduct,seeHadamardprod-\n",
      "   uct,seeHadamardproduct\n",
      "   EM,seeexpectationmaximization\n",
      " Embedding,510\n",
      "  Empiricaldistribution,63\n",
      "  Empiricalrisk,271\n",
      "   Empiricalriskminimization,271\n",
      " Encoder,4\n",
      "  Energyfunction,563\n",
      "     Energy-basedmodel,,,, 562588645654\n",
      "  Ensemblemethods,250\n",
      " Epoch,241\n",
      "  Equalityconstraint,91\n",
      " Equivariance,332\n",
      "    Errorfunction,seeobjectivefunction\n",
      "    ESN,seeechostatenetwork\n",
      "  Euclideannorm,36\n",
      "  Euler-Lagrangeequation,638\n",
      "    Evidencelowerbound,,625652\n",
      " Example,96\n",
      " Expectation,57\n",
      "  Expectationmaximization,626\n",
      "   Expectedvalue,seeexpectation\n",
      "    Explainingaway,,,567623636\n",
      " Exploitation,472\n",
      " Exploration,472\n",
      "  Exponentialdistribution, 6 2 F-score,415\n",
      "   Factor(graphicalmodel),560\n",
      "  Factoranalysis,481\n",
      "  Factorgraph,570\n",
      "   Factorsofvariation,4\n",
      " Feature,96\n",
      "  Featureselection,230\n",
      "   Feedforwardneuralnetwork,162\n",
      " Fine-tuning,317\n",
      "  Finitediﬀerences,431\n",
      "  Forgetgate,300\n",
      "  Forwardpropagation,198\n",
      "   Fouriertransform,,351354\n",
      " Fovea,358\n",
      " FPCD,607\n",
      "   Freeenergy,, 5 6 4670\n",
      " Freebase,474\n",
      "  Frequentistprobability,52\n",
      "  Frequentiststatistics, 1 3 2\n",
      "  Frobeniusnorm,43\n",
      "   Fully-visibleBayesnetwork,695\n",
      "  Functionalderivatives,637\n",
      "    FVBN,seefully-visibleBayesnetwork\n",
      "  Gaborfunction,360\n",
      "    GANs,seegenerativeadversarialnetworks\n",
      "   Gatedrecurrentunit,417\n",
      "    Gaussiandistribution,seenormaldistribu-\n",
      "tion\n",
      "  Gaussiankernel,138\n",
      "   Gaussianmixture,,64183\n",
      "    GCN,seeglobalcontrastnormalization\n",
      " GeneOntology,474\n",
      " Generalization,107\n",
      "    GeneralizedLagrangefunction,seegeneral-\n",
      " izedLagrangian\n",
      "  GeneralizedLagrangian,91\n",
      "    Generativeadversarialnetworks,,679689\n",
      "    Generativemomentmatchingnetworks,693\n",
      "  Generatornetwork,684\n",
      "  Gibbsdistribution,561\n",
      "   Gibbssampling,,574592\n",
      "   Globalcontrastnormalization,446\n",
      "    GPU,seegraphicsprocessingunit\n",
      " Gradient,81\n",
      "7 7 6INDEX\n",
      "   Gradientclipping,,283407\n",
      "   Gradientdescent,,8082\n",
      " Graph,xiii\n",
      "    Graphicalmodel,seestructuredprobabilis-\n",
      " ticmodel\n",
      "   Graphicsprocessingunit,436\n",
      "  Greedyalgorithm,317\n",
      "   Greedylayer-wiseunsupervisedpretraining,\n",
      "521\n",
      "   Greedysupervisedpretraining,317\n",
      "  Gridsearch,424\n",
      "   Hadamardproduct,,xiii31\n",
      "  Hard,tanh191\n",
      "    Harmonium,seerestrictedBoltzmannma-\n",
      "chine\n",
      "  Harmonytheory,564\n",
      "     Helmholtz freeenergy,seeevidencelower\n",
      "bound\n",
      " Hessian,217\n",
      "   Hessianmatrix,,xiv84\n",
      " Heteroscedastic,182\n",
      "   Hiddenlayer,,6162\n",
      "  Hillclimbing,83\n",
      "  Hyperparameteroptimization,424\n",
      "  Hyperparameters,,117422\n",
      "   Hypothesisspace,,109115\n",
      "    i.i.d.assumptions,,,108119263\n",
      "  Identitymatrix,33\n",
      "     ILSVRC,seeImageNetLargeScaleVisual\n",
      " RecognitionChallenge\n",
      "    ImageNetLargeScaleVisualRecognition\n",
      " Challenge,22\n",
      " Immorality,570\n",
      "    Importancesampling,,,585616688\n",
      "   Importanceweightedautoencoder,688\n",
      "  Independence,,xiv57\n",
      "    Independentandidenticallydistributed,see\n",
      " i.i.d.assumptions\n",
      "   Independentcomponentanalysis,482\n",
      "   Independentsubspaceanalysis,484\n",
      "  Inequalityconstraint,91\n",
      "       Inference,,,,,,,, 555576623625627630640\n",
      "642  Informationretrieval,517\n",
      " Initialization,295\n",
      " Integral,xiv\n",
      " Invariance,333\n",
      " Isotropic,62\n",
      "    Jacobianmatrix,,,xiv6983\n",
      "  Jointprobability,54\n",
      "  k-means,,355541\n",
      "   k-nearestneighbors,, 1 3 9541\n",
      "   Karush-Kuhn-Tuckerconditions,,92231\n",
      " Karush–Kuhn–Tucker,91\n",
      "   Kernel(convolution),,325326\n",
      "  Kernelmachine,541\n",
      "  Kerneltrick,137\n",
      "  KKT,seeKarush–Kuhn–Tucker\n",
      "   KKTconditions,seeKarush-Kuhn-Tucker\n",
      "conditions\n",
      "    KLdivergence,seeKullback-Leiblerdiver-\n",
      "gence\n",
      "   Knowledgebase,,2474\n",
      "  Krylovmethods,218\n",
      "   Kullback-Leiblerdivergence,,xiv 7 1\n",
      "  Labelsmoothing,237\n",
      "   Lagrangemultipliers,,91638\n",
      "   Lagrangian,seegeneralizedLagrangian\n",
      " LAPGAN,692\n",
      "   Laplacedistribution,, 6 2487\n",
      "  Latentvariable,64\n",
      "   Layer(neuralnetwork),162\n",
      "    LCN,seelocalcontrastnormalization\n",
      "  LeakyReLU,187\n",
      "  Leakyunits,399\n",
      "  Learningrate,82\n",
      "    Linesearch,,,828390\n",
      "  Linearcombination,34\n",
      "  Lineardependence,35\n",
      "   Linearfactormodels,480\n",
      "    Linearregression,,, 1 0 4107136\n",
      "  Linkprediction,475\n",
      "  Lipschitzconstant,89\n",
      "  Lipschitzcontinuous,89\n",
      "   Liquidstatemachine,396\n",
      "7 7 7INDEX\n",
      "   Localconditionalprobabilitydistribution,\n",
      "557\n",
      "   Localcontrastnormalization,448\n",
      "    Logisticregression,,,3 1 3 7137\n",
      "   Logisticsigmoid,,764\n",
      "     Longshort-termmemory,,,, 1724300 4 0 1,\n",
      "417\n",
      " Loop,570\n",
      "   Loopybeliefpropagation,578\n",
      "    Lossfunction,seeobjectivefunction\n",
      "Lp norm,36\n",
      "    LSTM,seelongshort-termmemory\n",
      " M-step,626\n",
      "  Machinelearning,2\n",
      "  Machinetranslation,98\n",
      "  Maindiagonal,30\n",
      " Manifold,156\n",
      "  Manifoldhypothesis,157\n",
      "  Manifoldlearning,156\n",
      "   Manifoldtangentclassiﬁer,267\n",
      "   MAPapproximation,,135497\n",
      "  Marginalprobability,55\n",
      "  Markovchain,588\n",
      "    MarkovchainMonteCarlo,588\n",
      "    Markovnetwork,seeundirectedmodel\n",
      "     Markovrandomﬁeld,seeundirectedmodel\n",
      "   Matrix,,,xiixiii29\n",
      "  Matrixinverse,33\n",
      "  Matrixproduct,31\n",
      "  Maxnorm,37\n",
      "  Maxpooling,333\n",
      "  Maximumlikelihood, 1 2 8\n",
      "  Maxout,,188417\n",
      "     MCMC,seeMarkovchainMonteCarlo\n",
      "    Meanﬁeld,,,630631662\n",
      "   Meansquarederror,105\n",
      "  Measuretheory,68\n",
      "  Measurezero,68\n",
      "  Memorynetwork,409\n",
      "   Methodof steepestdescent, seegradient\n",
      "descent\n",
      " Minibatch,274\n",
      "  Missinginputs,97\n",
      "   Mixing(Markovchain),594   Mixturedensitynetworks,183\n",
      "  Mixturedistribution,63\n",
      "   Mixturemodel,,183502\n",
      "    Mixtureofexperts,,441541\n",
      "   MLP,seemultilayerperception\n",
      "   MNIST,,,1920662\n",
      "  Modelaveraging,250\n",
      "  Modelcompression,439\n",
      "  Modelidentiﬁability,279\n",
      "  Modelparallelism,439\n",
      "  Momentmatching,693\n",
      "   Moore-Penrosepseudoinverse,,42234\n",
      "  Moralizedgraph,570\n",
      "   MP-DBM,seemulti-predictionDBM\n",
      "    MRF(Markov RandomField),seeundi-\n",
      " rectedmodel\n",
      "    MSE,seemeansquarederror\n",
      "  Multi-modallearning,533\n",
      "  Multi-predictionDBM,664\n",
      "   Multi-tasklearning,,238531\n",
      "  Multilayerperception,5\n",
      "  Multilayerperceptron,23\n",
      "  Multinomialdistribution,59\n",
      "  Multinoullidistribution,59\n",
      " n-gram, 4 5 3\n",
      " NADE,698\n",
      "  NaiveBayes,3\n",
      " Nat,70\n",
      "  Naturalimage,552\n",
      "   Naturallanguageprocessing,452\n",
      "   Nearestneighborregression, 1 1 2\n",
      "  Negativedeﬁnite,86\n",
      "    Negativephase,,,461599601\n",
      "    Neocognitron,,,, 152123359\n",
      "  Nesterovmomentum,294\n",
      "    NetﬂixGrandPrize,,253471\n",
      "    Neurallanguagemodel,,455468\n",
      "  Neuralnetwork,13\n",
      "   NeuralTuringmachine,409\n",
      " Neuroscience,14\n",
      "   Newton’s method,,86305\n",
      "    NLM,seeneurallanguagemodel\n",
      "    NLP,seenaturallanguageprocessing\n",
      "    Nofreelunchtheorem,113\n",
      "7 7 8INDEX\n",
      "  Noise-contrastiveestimation,613\n",
      "  Nonparametricmodel, 1 1 1\n",
      "  Norm,,xv36\n",
      "    Normaldistribution,,,6061122\n",
      "     Normalequations,,,, 1 0 6106109228\n",
      "  Normalizedinitialization ,297\n",
      "    Numericaldiﬀerentiation,seeﬁnitediﬀer-\n",
      "ences\n",
      "  Objectdetection,444\n",
      "  Objectrecognition,444\n",
      "  Objectivefunction,79\n",
      "    OMP-, kseeorthogonalmatchingpursuit\n",
      "  One-shotlearning,531\n",
      " Operation,199\n",
      "  Optimization,,7779\n",
      "    Orthodoxstatistics,seefrequentiststatistics\n",
      "    Orthogonalmatchingpursuit,,23 2 5 0\n",
      "  Orthogonalmatrix,39\n",
      " Orthogonality,38\n",
      "  Outputlayer,162\n",
      "   Paralleldistributedprocessing,16\n",
      "   Parameterinitialization ,,295398\n",
      "      Parametersharing,,,,, 247329365367380\n",
      "    Parametertying,seeParametersharing\n",
      "  Parametricmodel, 1 1 1\n",
      "  ParametricReLU,187\n",
      "  Partialderivative,81\n",
      "    Partitionfunction,,,561598660\n",
      "    PCA,seeprincipalcomponentsanalysis\n",
      "    PCD,seestochasticmaximumlikelihood\n",
      "  Perceptron,,1423\n",
      "    Persistentcontrastivedivergence,seestochas-\n",
      "  ticmaximumlikelihood\n",
      "   Perturbationanalysis,seereparametrization\n",
      "trick\n",
      "  Pointestimator,119\n",
      " Policy,471\n",
      "  Pooling,,324673\n",
      "  Positivedeﬁnite,86\n",
      "      Positivephase,,,,, 461599601647659\n",
      " Precision,415\n",
      "      Precision(ofanormaldistribution),,6062\n",
      "   Predictivesparsedecomposition,516 Preprocessing,445\n",
      "  Pretraining,,317521\n",
      "   Primaryvisualcortex,356\n",
      "     Principalcomponentsanalysis,,,, 44143144\n",
      " 481623,\n",
      "   Priorprobabilitydistribution, 1 3 2\n",
      "   Probabilisticmaxpooling,674\n",
      "    ProbabilisticPCA,,,481482624\n",
      "   Probabilitydensityfunction,55\n",
      "  Probabilitydistribution,53\n",
      "   Probabilitymassfunction,53\n",
      "    Probabilitymassfunctionestimation,100\n",
      "   Productofexperts,563\n",
      "      Productruleofprobability,seechainrule\n",
      " ofprobability\n",
      "    PSD,seepredictivesparsedecomposition\n",
      " Pseudolikelihood,608\n",
      "  Quadraturepair,361\n",
      "  Quasi-Newtonmethods,310\n",
      "   Radialbasisfunction,191\n",
      "  Randomsearch,426\n",
      "  Randomvariable,53\n",
      "  Ratiomatching,611\n",
      " RBF,191\n",
      "    RBM,seerestrictedBoltzmannmachine\n",
      " Recall,415\n",
      "  Receptiveﬁeld,330\n",
      "  RecommenderSystems,469\n",
      "      Rectiﬁedlinearunit,,,, 166187417499\n",
      "  Recurrentnetwork,23\n",
      "   Recurrentneuralnetwork,370\n",
      " Regression,97\n",
      "     Regularization,,,,, 1 1 7117172222422\n",
      " Regularizer,116\n",
      " REINFORCE,680\n",
      "     Reinforcementlearning,,,, 25103471679\n",
      "  Relationaldatabase,474\n",
      " Relations,474\n",
      "  Reparametrizationtrick,679\n",
      "  Representationlearning,3\n",
      "  Representationalcapacity,111\n",
      "  RestrictedBoltzmannmachine, , , 348451\n",
      "      471579623647648662667 ,,,,,,,\n",
      "7 7 9INDEX\n",
      "  669671673,,\n",
      "    Ridgeregression,seeweightdecay\n",
      " Risk,270\n",
      " RNN-RBM,676\n",
      "  Saddlepoints,280\n",
      "  Samplemean,122\n",
      "   Scalar,,,xiixiii28\n",
      "   Scorematching,,504610\n",
      "  Secondderivative,83\n",
      "   Secondderivativetest,86\n",
      " Self-information,70\n",
      "  Semantichashing,517\n",
      "  Semi-supervisedlearning,238\n",
      "  Separableconvolution,354\n",
      "   Separation(probabilisticmodeling),565\n",
      " Set,xiii\n",
      "    SGD,seestochasticgradientdescent\n",
      "   Shannonentropy,,xiv70\n",
      " Shortlist,457\n",
      "    Sigmoid,,xvseelogisticsigmoid\n",
      "   Sigmoidbeliefnetwork,23\n",
      "  Simplecell,357\n",
      "     Singularvalue,seesingularvaluedecompo-\n",
      "sition\n",
      "     Singularvaluedecomposition,,,41144470\n",
      "     Singularvector,seesingularvaluedecom-\n",
      "position\n",
      "   Slowfeatureanalysis,484\n",
      "    SML,seestochasticmaximumlikelihood\n",
      "   Softmax,,,178409441\n",
      "   Softplus,,,xv65191\n",
      "  Spamdetection,3\n",
      "      Sparsecoding,,,,, 315348487623682\n",
      "   Sparseinitialization ,,298398\n",
      "     Sparserepresentation,,,,, 142220248497\n",
      "549\n",
      " Spearmint,427\n",
      "  Spectralradius,396\n",
      "   Speechrecognition,see automaticspeech\n",
      "recognition\n",
      "  Sphering,seewhitening\n",
      "   Spikeand slabrestricted Boltzmannma-\n",
      " chine,671\n",
      "   SPN,seesum-productnetwork  Squarematrix,35\n",
      "      ssRBM,seespikeandslabrestrictedBoltz-\n",
      " mannmachine\n",
      "  Standarddeviation,58\n",
      "  Standarderror,124\n",
      "      Standarderrorofthemean,,124273\n",
      " Statistic,119\n",
      "   Statisticallearningtheory,107\n",
      "    Steepestdescent,seegradientdescent\n",
      "   Stochasticback-propagation,seereparametriza-\n",
      " tiontrick\n",
      "     Stochasticgradientdescent,,,, 14147274\n",
      " 2 8 8,662\n",
      "    Stochasticmaximumlikelihood,,605662\n",
      "  Stochasticpooling,261\n",
      "  Structurelearning,575\n",
      "   Structuredoutput,,98675\n",
      "    Structuredprobabilisticmodel,,74551\n",
      "    Sumruleofprobability,55\n",
      "  Sum-productnetwork,546\n",
      "   Supervisedﬁne-tuning,,522653\n",
      "  Supervisedlearning, 1 0 2\n",
      "   Supportvectormachine,137\n",
      "   Surrogatelossfunction,271\n",
      "    SVD,seesingularvaluedecomposition\n",
      "   Symmetricmatrix,,3840\n",
      "  Tangentdistance,265\n",
      "  Tangentplane,508\n",
      "  Tangentprop,265\n",
      "    TDNN,seetime-delayneuralnetwork\n",
      "  Teacherforcing,375\n",
      " Tempering,596\n",
      "  Templatematching,138\n",
      "   Tensor,,,xiixiii30\n",
      "  Testset,107\n",
      "    Tikhonovregularization,seeweightdecay\n",
      "  Tiledconvolution,344\n",
      "    Time-delayneuralnetwork,,359366\n",
      "  Toeplitzmatrix,327\n",
      "  TopographicICA,484\n",
      "  Traceoperator,43\n",
      "  Trainingerror,107\n",
      " Transcription,98\n",
      "  Transferlearning,529\n",
      "7 8 0INDEX\n",
      "  Transpose,,xiii30\n",
      "  Triangleinequality,36\n",
      "    Triangulatedgraph,seechordalgraph\n",
      " Trigram,453\n",
      " Unbiased,121\n",
      "    Undirectedgraphicalmodel,,74499\n",
      "  Undirectedmodel,558\n",
      "  Uniformdistribution,54\n",
      " Unigram,453\n",
      "  Unitnorm,38\n",
      "  Unitvector,38\n",
      "   Universalapproximationtheorem,192\n",
      "  Universalapproximator,546\n",
      "   Unnormalizedprobabilitydistribution,560\n",
      "   Unsupervisedlearning,, 1 0 2142\n",
      "   Unsupervisedpretraining,,451521\n",
      "   V-structure,seeexplainingaway\n",
      " V1,356\n",
      "   VAE,seevariationalautoencoder\n",
      "  Vapnik-Chervonenkisdimension,111\n",
      "   Variance,,,xiv58223\n",
      "   Variationalautoencoder,,679 6 8 6\n",
      "    Variationalderivatives,seefunctionalderiva-\n",
      "tives\n",
      "     Variationalfreeenergy,seeevidencelower\n",
      "bound\n",
      "    VCdimension,seeVapnik-Chervonenkisdi-\n",
      "mension\n",
      "   Vector,,,xiixiii29\n",
      "   Virtualadversarialexamples,264\n",
      "  Visiblelayer,6\n",
      "  Volumetricdata,351\n",
      "  Wake-sleep,,643652\n",
      "     Weightdecay,,,, 1 1 5172225423\n",
      "   Weightspacesymmetry,279\n",
      "  Weights,,14104\n",
      " Whitening,448\n",
      " Wikibase, 474\n",
      " Wikibase,474\n",
      "  Wordembedding,455\n",
      "  Word-sensedisambiguation,476\n",
      " WordNet,474    Zero-datalearning,seezero-shotlearning\n",
      "  Zero-shotlearning,531\n",
      "7 8 1\n"
     ]
    }
   ],
   "source": [
    "# leitura do arquivo\n",
    "\n",
    "file_path = r\"C:\\Users\\aryck\\Desktop\\Learning notebooks\\nlp-project\\ian_goodfellow.pdf\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    content = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        content += reader.pages[page_num].extract_text()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d992c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
